{% raw %}
<!DOCTYPE html>
<html lang="en" data-theme="light">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>24. Loss Functions - Study Materials</title>
    <link rel="stylesheet" href="/study/static/css/style.css">
    <link rel="stylesheet" href="/study/static/css/highlight.css">
    <!-- KaTeX for LaTeX math rendering -->
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.11/dist/katex.min.css">
    <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.11/dist/katex.min.js"></script>
    <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.11/dist/contrib/auto-render.min.js"
        onload="renderMathInElement(document.body, {
            delimiters: [
                {left: '$$', right: '$$', display: true},
                {left: '$', right: '$', display: false}
            ],
            throwOnError: false
        });"></script>
    
</head>
<body>
    <div class="app-container">
        <!-- Sidebar -->
        <aside class="sidebar" id="sidebar">
            <div class="sidebar-header">
                <a href="/study/en/" class="logo">Study Materials</a>
                <button class="sidebar-toggle" id="sidebar-toggle" aria-label="Toggle sidebar">
                    <span></span>
                </button>
            </div>

            <nav class="sidebar-nav">
                <a href="/study/en/" class="nav-item ">
                    <span class="nav-icon">üè†</span>
                    <span class="nav-text">Home</span>
                </a>
                <a href="/study/examples/" class="nav-item ">
                    <span class="nav-icon">üíª</span>
                    <span class="nav-text">Examples</span>
                </a>
            </nav>

            <div class="sidebar-search">
                <form action="/study/en/search.html" method="get" id="search-form">
                    <input type="search" name="q" placeholder="Search..." id="search-sidebar-input">
                </form>
            </div>

            <!-- Language Selector -->
            <div class="sidebar-lang">
                <select id="lang-select" class="lang-selector" onchange="switchLanguage(this.value)">
                    
                    <option value="en" selected>
                        English
                    </option>
                    
                    <option value="ko" >
                        ÌïúÍµ≠Ïñ¥
                    </option>
                    
                </select>
            </div>

            <div class="sidebar-footer">
                <button class="theme-toggle" id="theme-toggle" aria-label="Toggle theme">
                    <span class="theme-icon light">‚òÄÔ∏è</span>
                    <span class="theme-icon dark">üåô</span>
                </button>
            </div>
        </aside>

        <!-- Main Content -->
        <main class="main-content">
            <header class="main-header">
                <button class="menu-toggle" id="menu-toggle" aria-label="Open menu">
                    <span></span>
                </button>
                
<nav class="breadcrumb">
    <a href="/study/en/">Topics</a>
    <span class="separator">/</span>
    <a href="/study/en/Deep_Learning/">Deep Learning</a>
    <span class="separator">/</span>
    <span class="current">24. Loss Functions</span>
</nav>

            </header>

            <div class="content">
                
<article class="lesson-article">
    <header class="lesson-header">
        <h1>24. Loss Functions</h1>
    </header>

    
<div class="lesson-toolbar lesson-toolbar--top">
    <div class="toolbar-nav toolbar-nav--prev">
        
        <a href="/study/en/Deep_Learning/23_Training_Optimization.html" class="nav-prev">
            <span class="nav-label">Previous</span>
            <span class="nav-title">23. Training Optimization</span>
        </a>
        
    </div>
    <div class="toolbar-actions">
        <button class="btn btn-copy-link" data-action="copy-link" title="Copy link">
            <span class="icon">üîó</span>
            <span class="text">Copy link</span>
        </button>
        <a href="/study/en/Deep_Learning/" class="btn btn-topic-link" title="Back to topic list">
            <span class="icon">üìã</span>
            <span class="text">Topic list</span>
        </a>
    </div>
    <div class="toolbar-nav toolbar-nav--next">
        
        <a href="/study/en/Deep_Learning/25_Optimizers.html" class="nav-next">
            <span class="nav-label">Next</span>
            <span class="nav-title">25. Optimizers</span>
        </a>
        
    </div>
</div>


    
    <nav class="toc" id="toc">
        <h2>Table of Contents</h2>
        <div class="toc">
<ul>
<li><a href="#learning-objectives">Learning Objectives</a></li>
<li><a href="#1-introduction-to-loss-functions">1. Introduction to Loss Functions</a><ul>
<li><a href="#11-role-in-neural-network-training">1.1 Role in Neural Network Training</a></li>
<li><a href="#12-loss-landscape-visualization">1.2 Loss Landscape Visualization</a></li>
<li><a href="#13-relationship-with-optimization">1.3 Relationship with Optimization</a></li>
</ul>
</li>
<li><a href="#2-regression-losses">2. Regression Losses</a><ul>
<li><a href="#21-mean-squared-error-l2-loss">2.1 Mean Squared Error (L2 Loss)</a></li>
<li><a href="#22-mean-absolute-error-l1-loss">2.2 Mean Absolute Error (L1 Loss)</a></li>
<li><a href="#23-huber-loss-smooth-l1">2.3 Huber Loss (Smooth L1)</a></li>
<li><a href="#24-log-cosh-loss">2.4 Log-Cosh Loss</a></li>
<li><a href="#25-regression-loss-comparison">2.5 Regression Loss Comparison</a></li>
</ul>
</li>
<li><a href="#3-classification-losses">3. Classification Losses</a><ul>
<li><a href="#31-binary-cross-entropy-bce">3.1 Binary Cross-Entropy (BCE)</a></li>
<li><a href="#32-cross-entropy-loss-multi-class">3.2 Cross-Entropy Loss (Multi-Class)</a></li>
<li><a href="#33-label-smoothing">3.3 Label Smoothing</a></li>
<li><a href="#34-focal-loss">3.4 Focal Loss</a></li>
<li><a href="#35-bce-vs-cross-entropy">3.5 BCE vs Cross-Entropy</a></li>
</ul>
</li>
<li><a href="#4-ranking-and-metric-learning-losses">4. Ranking and Metric Learning Losses</a><ul>
<li><a href="#41-contrastive-loss">4.1 Contrastive Loss</a></li>
<li><a href="#42-triplet-loss">4.2 Triplet Loss</a></li>
<li><a href="#43-infonce-nt-xent-loss">4.3 InfoNCE / NT-Xent Loss</a></li>
</ul>
</li>
<li><a href="#5-segmentation-and-detection-losses">5. Segmentation and Detection Losses</a><ul>
<li><a href="#51-dice-loss">5.1 Dice Loss</a></li>
<li><a href="#52-iou-loss-giou-loss">5.2 IoU Loss / GIoU Loss</a></li>
<li><a href="#53-combined-losses">5.3 Combined Losses</a></li>
</ul>
</li>
<li><a href="#6-generative-model-losses">6. Generative Model Losses</a><ul>
<li><a href="#61-adversarial-loss-gan">6.1 Adversarial Loss (GAN)</a></li>
<li><a href="#62-vae-loss">6.2 VAE Loss</a></li>
<li><a href="#63-perceptual-loss">6.3 Perceptual Loss</a></li>
</ul>
</li>
<li><a href="#7-advanced-topics">7. Advanced Topics</a><ul>
<li><a href="#71-multi-task-loss-weighting">7.1 Multi-Task Loss Weighting</a></li>
<li><a href="#72-curriculum-loss">7.2 Curriculum Loss</a></li>
<li><a href="#73-custom-loss-functions">7.3 Custom Loss Functions</a></li>
<li><a href="#74-numerical-stability-tips">7.4 Numerical Stability Tips</a></li>
</ul>
</li>
<li><a href="#8-practical-guide">8. Practical Guide</a><ul>
<li><a href="#81-decision-tree-for-loss-selection">8.1 Decision Tree for Loss Selection</a></li>
<li><a href="#82-comparison-table">8.2 Comparison Table</a></li>
<li><a href="#83-common-pitfalls">8.3 Common Pitfalls</a></li>
<li><a href="#84-debugging-tips">8.4 Debugging Tips</a></li>
<li><a href="#85-loss-function-impact-on-convergence">8.5 Loss Function Impact on Convergence</a></li>
</ul>
</li>
<li><a href="#exercises">Exercises</a><ul>
<li><a href="#exercise-1-implement-tversky-loss">Exercise 1: Implement Tversky Loss</a></li>
<li><a href="#exercise-2-multi-scale-perceptual-loss">Exercise 2: Multi-Scale Perceptual Loss</a></li>
<li><a href="#exercise-3-adaptive-loss-balancing">Exercise 3: Adaptive Loss Balancing</a></li>
</ul>
</li>
<li><a href="#references">References</a></li>
</ul>
</div>

    </nav>
    

    <div class="lesson-content markdown-body">
        <p><a href="./23_Training_Optimization.md">Previous: Training Optimization</a> | <a href="./25_Optimizers.md">Next: Optimizers</a></p>
<hr />
<h1 id="24-loss-functions">24. Loss Functions<a class="header-link" href="#24-loss-functions" title="Permanent link">&para;</a></h1>
<h2 id="learning-objectives">Learning Objectives<a class="header-link" href="#learning-objectives" title="Permanent link">&para;</a></h2>
<ul>
<li>Understand the role of loss functions in training neural networks and their relationship with optimization</li>
<li>Master regression losses (MSE, MAE, Huber) and their applications in different scenarios</li>
<li>Learn classification losses (BCE, Cross-Entropy, Focal Loss) and their use cases for balanced/imbalanced datasets</li>
<li>Explore metric learning losses (Contrastive, Triplet, InfoNCE) for representation learning</li>
<li>Implement custom loss functions in PyTorch for segmentation, detection, and generative models</li>
</ul>
<p><strong>Difficulty</strong>: ‚≠ê‚≠ê‚≠ê</p>
<hr />
<h2 id="1-introduction-to-loss-functions">1. Introduction to Loss Functions<a class="header-link" href="#1-introduction-to-loss-functions" title="Permanent link">&para;</a></h2>
<h3 id="11-role-in-neural-network-training">1.1 Role in Neural Network Training<a class="header-link" href="#11-role-in-neural-network-training" title="Permanent link">&para;</a></h3>
<p>A loss function (also called objective function or cost function) measures how well a neural network's predictions match the ground truth. During training, we minimize this loss using optimization algorithms like SGD or Adam.</p>
<div class="highlight"><pre><span></span><code><span class="nv">Training</span><span class="w"> </span><span class="k">Loop</span>:
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ<span class="w">                                                         </span>‚îÇ
‚îÇ<span class="w">  </span><span class="nv">Input</span><span class="w"> </span><span class="ss">(</span><span class="nv">x</span><span class="ss">)</span><span class="w"> </span>‚îÄ‚îÄ‚ñ∂<span class="w"> </span><span class="nv">Model</span><span class="ss">(</span>Œ∏<span class="ss">)</span><span class="w"> </span>‚îÄ‚îÄ‚ñ∂<span class="w"> </span><span class="nv">Prediction</span><span class="w"> </span><span class="ss">(</span>≈∑<span class="ss">)</span><span class="w">            </span>‚îÇ
‚îÇ<span class="w">                                  </span>‚îÇ<span class="w">                      </span>‚îÇ
‚îÇ<span class="w">                                  </span>‚ñº<span class="w">                      </span>‚îÇ
‚îÇ<span class="w">                         </span><span class="nv">Loss</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="nv">L</span><span class="ss">(</span>≈∑,<span class="w"> </span><span class="nv">y</span><span class="ss">)</span><span class="w">                 </span>‚îÇ
‚îÇ<span class="w">                                  </span>‚îÇ<span class="w">                      </span>‚îÇ
‚îÇ<span class="w">                                  </span>‚ñº<span class="w">                      </span>‚îÇ
‚îÇ<span class="w">                         </span>‚àÇ<span class="nv">L</span><span class="o">/</span>‚àÇŒ∏<span class="w"> </span><span class="ss">(</span><span class="nv">Backprop</span><span class="ss">)</span><span class="w">               </span>‚îÇ
‚îÇ<span class="w">                                  </span>‚îÇ<span class="w">                      </span>‚îÇ
‚îÇ<span class="w">                                  </span>‚ñº<span class="w">                      </span>‚îÇ
‚îÇ<span class="w">                    </span>Œ∏<span class="w"> </span>‚Üê<span class="w"> </span>Œ∏<span class="w"> </span><span class="o">-</span><span class="w"> </span>Œ∑¬∑‚àÇ<span class="nv">L</span><span class="o">/</span>‚àÇŒ∏<span class="w"> </span><span class="ss">(</span><span class="nv">Update</span><span class="ss">)</span><span class="w">            </span>‚îÇ
‚îÇ<span class="w">                                  </span>‚îÇ<span class="w">                      </span>‚îÇ
‚îÇ<span class="w">                                  </span>‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
‚îÇ<span class="w">                                                         </span>‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
</code></pre></div>

<p><strong>Key Properties of Good Loss Functions:</strong>
- <strong>Differentiable</strong>: Must have gradients for backpropagation
- <strong>Convex (ideally)</strong>: Single global minimum makes optimization easier
- <strong>Task-aligned</strong>: Reflects the actual evaluation metric when possible
- <strong>Numerically stable</strong>: Avoids overflow/underflow</p>
<h3 id="12-loss-landscape-visualization">1.2 Loss Landscape Visualization<a class="header-link" href="#12-loss-landscape-visualization" title="Permanent link">&para;</a></h3>
<p>The loss landscape shows how loss varies with model parameters:</p>
<div class="highlight"><pre><span></span><code><span class="kn">import</span><span class="w"> </span><span class="nn">torch</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">numpy</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">np</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">matplotlib.pyplot</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">plt</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">mpl_toolkits.mplot3d</span><span class="w"> </span><span class="kn">import</span> <span class="n">Axes3D</span>

<span class="k">def</span><span class="w"> </span><span class="nf">visualize_loss_landscape</span><span class="p">():</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Visualize a simple 2D loss landscape&quot;&quot;&quot;</span>
    <span class="c1"># Create a grid of parameter values</span>
    <span class="n">w1</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="o">-</span><span class="mi">5</span><span class="p">,</span> <span class="mi">5</span><span class="p">,</span> <span class="mi">100</span><span class="p">)</span>
    <span class="n">w2</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="o">-</span><span class="mi">5</span><span class="p">,</span> <span class="mi">5</span><span class="p">,</span> <span class="mi">100</span><span class="p">)</span>
    <span class="n">W1</span><span class="p">,</span> <span class="n">W2</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">meshgrid</span><span class="p">(</span><span class="n">w1</span><span class="p">,</span> <span class="n">w2</span><span class="p">)</span>

    <span class="c1"># Example loss: Rosenbrock function (non-convex)</span>
    <span class="n">a</span><span class="p">,</span> <span class="n">b</span> <span class="o">=</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">100</span>
    <span class="n">Z</span> <span class="o">=</span> <span class="p">(</span><span class="n">a</span> <span class="o">-</span> <span class="n">W1</span><span class="p">)</span><span class="o">**</span><span class="mi">2</span> <span class="o">+</span> <span class="n">b</span> <span class="o">*</span> <span class="p">(</span><span class="n">W2</span> <span class="o">-</span> <span class="n">W1</span><span class="o">**</span><span class="mi">2</span><span class="p">)</span><span class="o">**</span><span class="mi">2</span>

    <span class="c1"># Plot 3D surface</span>
    <span class="n">fig</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">12</span><span class="p">,</span> <span class="mi">5</span><span class="p">))</span>

    <span class="n">ax1</span> <span class="o">=</span> <span class="n">fig</span><span class="o">.</span><span class="n">add_subplot</span><span class="p">(</span><span class="mi">121</span><span class="p">,</span> <span class="n">projection</span><span class="o">=</span><span class="s1">&#39;3d&#39;</span><span class="p">)</span>
    <span class="n">ax1</span><span class="o">.</span><span class="n">plot_surface</span><span class="p">(</span><span class="n">W1</span><span class="p">,</span> <span class="n">W2</span><span class="p">,</span> <span class="n">Z</span><span class="p">,</span> <span class="n">cmap</span><span class="o">=</span><span class="s1">&#39;viridis&#39;</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.8</span><span class="p">)</span>
    <span class="n">ax1</span><span class="o">.</span><span class="n">set_xlabel</span><span class="p">(</span><span class="s1">&#39;w1&#39;</span><span class="p">)</span>
    <span class="n">ax1</span><span class="o">.</span><span class="n">set_ylabel</span><span class="p">(</span><span class="s1">&#39;w2&#39;</span><span class="p">)</span>
    <span class="n">ax1</span><span class="o">.</span><span class="n">set_zlabel</span><span class="p">(</span><span class="s1">&#39;Loss&#39;</span><span class="p">)</span>
    <span class="n">ax1</span><span class="o">.</span><span class="n">set_title</span><span class="p">(</span><span class="s1">&#39;3D Loss Landscape&#39;</span><span class="p">)</span>

    <span class="c1"># Plot contour</span>
    <span class="n">ax2</span> <span class="o">=</span> <span class="n">fig</span><span class="o">.</span><span class="n">add_subplot</span><span class="p">(</span><span class="mi">122</span><span class="p">)</span>
    <span class="n">contour</span> <span class="o">=</span> <span class="n">ax2</span><span class="o">.</span><span class="n">contour</span><span class="p">(</span><span class="n">W1</span><span class="p">,</span> <span class="n">W2</span><span class="p">,</span> <span class="n">Z</span><span class="p">,</span> <span class="n">levels</span><span class="o">=</span><span class="mi">30</span><span class="p">,</span> <span class="n">cmap</span><span class="o">=</span><span class="s1">&#39;viridis&#39;</span><span class="p">)</span>
    <span class="n">ax2</span><span class="o">.</span><span class="n">set_xlabel</span><span class="p">(</span><span class="s1">&#39;w1&#39;</span><span class="p">)</span>
    <span class="n">ax2</span><span class="o">.</span><span class="n">set_ylabel</span><span class="p">(</span><span class="s1">&#39;w2&#39;</span><span class="p">)</span>
    <span class="n">ax2</span><span class="o">.</span><span class="n">set_title</span><span class="p">(</span><span class="s1">&#39;Contour Plot&#39;</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">colorbar</span><span class="p">(</span><span class="n">contour</span><span class="p">,</span> <span class="n">ax</span><span class="o">=</span><span class="n">ax2</span><span class="p">)</span>

    <span class="n">plt</span><span class="o">.</span><span class="n">tight_layout</span><span class="p">()</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">savefig</span><span class="p">(</span><span class="s1">&#39;loss_landscape.png&#39;</span><span class="p">,</span> <span class="n">dpi</span><span class="o">=</span><span class="mi">150</span><span class="p">,</span> <span class="n">bbox_inches</span><span class="o">=</span><span class="s1">&#39;tight&#39;</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>

<span class="n">visualize_loss_landscape</span><span class="p">()</span>
</code></pre></div>

<h3 id="13-relationship-with-optimization">1.3 Relationship with Optimization<a class="header-link" href="#13-relationship-with-optimization" title="Permanent link">&para;</a></h3>
<p>Different loss functions create different optimization challenges:</p>
<table>
<thead>
<tr>
<th>Loss Type</th>
<th>Landscape</th>
<th>Optimization Challenge</th>
</tr>
</thead>
<tbody>
<tr>
<td>MSE</td>
<td>Smooth, convex</td>
<td>Easy, stable gradients</td>
</tr>
<tr>
<td>Cross-Entropy</td>
<td>Smooth, convex</td>
<td>Can have vanishing gradients</td>
</tr>
<tr>
<td>Triplet Loss</td>
<td>Non-convex, many local minima</td>
<td>Requires careful mining</td>
</tr>
<tr>
<td>GAN Loss</td>
<td>Non-convex, saddle points</td>
<td>Unstable, mode collapse</td>
</tr>
</tbody>
</table>
<hr />
<h2 id="2-regression-losses">2. Regression Losses<a class="header-link" href="#2-regression-losses" title="Permanent link">&para;</a></h2>
<p>Regression losses are used when predicting continuous values (e.g., house prices, temperatures, coordinates).</p>
<h3 id="21-mean-squared-error-l2-loss">2.1 Mean Squared Error (L2 Loss)<a class="header-link" href="#21-mean-squared-error-l2-loss" title="Permanent link">&para;</a></h3>
<p><strong>Formula:</strong></p>
<div class="highlight"><pre><span></span><code>MSE = (1/n) Œ£(≈∑·µ¢ - y·µ¢)¬≤
</code></pre></div>

<p><strong>Properties:</strong>
- Penalizes large errors heavily (quadratic)
- Sensitive to outliers
- Smooth gradients everywhere</p>
<p><strong>PyTorch Implementation:</strong></p>
<div class="highlight"><pre><span></span><code><span class="kn">import</span><span class="w"> </span><span class="nn">torch</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">torch.nn</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">nn</span>

<span class="c1"># Built-in version</span>
<span class="n">mse_loss</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">MSELoss</span><span class="p">()</span>

<span class="c1"># Example usage</span>
<span class="n">predictions</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([</span><span class="mf">2.5</span><span class="p">,</span> <span class="mf">0.0</span><span class="p">,</span> <span class="mf">2.1</span><span class="p">,</span> <span class="mf">7.8</span><span class="p">])</span>
<span class="n">targets</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([</span><span class="mf">3.0</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.5</span><span class="p">,</span> <span class="mf">2.0</span><span class="p">,</span> <span class="mf">8.0</span><span class="p">])</span>

<span class="n">loss</span> <span class="o">=</span> <span class="n">mse_loss</span><span class="p">(</span><span class="n">predictions</span><span class="p">,</span> <span class="n">targets</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;MSE Loss: </span><span class="si">{</span><span class="n">loss</span><span class="o">.</span><span class="n">item</span><span class="p">()</span><span class="si">:</span><span class="s2">.4f</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>  <span class="c1"># 0.0825</span>

<span class="c1"># Manual implementation</span>
<span class="k">def</span><span class="w"> </span><span class="nf">mse_manual</span><span class="p">(</span><span class="n">pred</span><span class="p">,</span> <span class="n">target</span><span class="p">):</span>
    <span class="k">return</span> <span class="n">torch</span><span class="o">.</span><span class="n">mean</span><span class="p">((</span><span class="n">pred</span> <span class="o">-</span> <span class="n">target</span><span class="p">)</span> <span class="o">**</span> <span class="mi">2</span><span class="p">)</span>

<span class="n">loss_manual</span> <span class="o">=</span> <span class="n">mse_manual</span><span class="p">(</span><span class="n">predictions</span><span class="p">,</span> <span class="n">targets</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Manual MSE: </span><span class="si">{</span><span class="n">loss_manual</span><span class="o">.</span><span class="n">item</span><span class="p">()</span><span class="si">:</span><span class="s2">.4f</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>  <span class="c1"># 0.0825</span>
</code></pre></div>

<p><strong>When to Use:</strong>
- Regression tasks where large errors should be heavily penalized
- Data without significant outliers
- When you want smooth gradients for optimization</p>
<h3 id="22-mean-absolute-error-l1-loss">2.2 Mean Absolute Error (L1 Loss)<a class="header-link" href="#22-mean-absolute-error-l1-loss" title="Permanent link">&para;</a></h3>
<p><strong>Formula:</strong></p>
<div class="highlight"><pre><span></span><code>MAE = (1/n) Œ£|≈∑·µ¢ - y·µ¢|
</code></pre></div>

<p><strong>Properties:</strong>
- Linear penalty (robust to outliers)
- Gradient has constant magnitude
- Can be unstable at zero (non-differentiable)</p>
<p><strong>PyTorch Implementation:</strong></p>
<div class="highlight"><pre><span></span><code><span class="c1"># Built-in version</span>
<span class="n">mae_loss</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">L1Loss</span><span class="p">()</span>

<span class="n">predictions</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([</span><span class="mf">2.5</span><span class="p">,</span> <span class="mf">0.0</span><span class="p">,</span> <span class="mf">2.1</span><span class="p">,</span> <span class="mf">7.8</span><span class="p">])</span>
<span class="n">targets</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([</span><span class="mf">3.0</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.5</span><span class="p">,</span> <span class="mf">2.0</span><span class="p">,</span> <span class="mf">8.0</span><span class="p">])</span>

<span class="n">loss</span> <span class="o">=</span> <span class="n">mae_loss</span><span class="p">(</span><span class="n">predictions</span><span class="p">,</span> <span class="n">targets</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;MAE Loss: </span><span class="si">{</span><span class="n">loss</span><span class="o">.</span><span class="n">item</span><span class="p">()</span><span class="si">:</span><span class="s2">.4f</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>  <span class="c1"># 0.2250</span>

<span class="c1"># Manual implementation</span>
<span class="k">def</span><span class="w"> </span><span class="nf">mae_manual</span><span class="p">(</span><span class="n">pred</span><span class="p">,</span> <span class="n">target</span><span class="p">):</span>
    <span class="k">return</span> <span class="n">torch</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">abs</span><span class="p">(</span><span class="n">pred</span> <span class="o">-</span> <span class="n">target</span><span class="p">))</span>

<span class="n">loss_manual</span> <span class="o">=</span> <span class="n">mae_manual</span><span class="p">(</span><span class="n">predictions</span><span class="p">,</span> <span class="n">targets</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Manual MAE: </span><span class="si">{</span><span class="n">loss_manual</span><span class="o">.</span><span class="n">item</span><span class="p">()</span><span class="si">:</span><span class="s2">.4f</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>  <span class="c1"># 0.2250</span>
</code></pre></div>

<p><strong>When to Use:</strong>
- Data with outliers
- When all errors should be weighted equally
- Robust regression tasks</p>
<h3 id="23-huber-loss-smooth-l1">2.3 Huber Loss (Smooth L1)<a class="header-link" href="#23-huber-loss-smooth-l1" title="Permanent link">&para;</a></h3>
<p><strong>Formula:</strong></p>
<div class="highlight"><pre><span></span><code>         ‚éß  0.5(≈∑ - y)¬≤          if |≈∑ - y| ‚â§ Œ¥
L_Œ¥(≈∑,y) = ‚é®
         ‚é©  Œ¥|≈∑ - y| - 0.5Œ¥¬≤    otherwise
</code></pre></div>

<p><strong>Properties:</strong>
- Combines benefits of L1 and L2
- Quadratic for small errors, linear for large errors
- Controlled by Œ¥ (transition point)</p>
<p><strong>PyTorch Implementation:</strong></p>
<div class="highlight"><pre><span></span><code><span class="c1"># Built-in version (SmoothL1Loss uses Œ¥=1.0)</span>
<span class="n">huber_loss</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">SmoothL1Loss</span><span class="p">(</span><span class="n">beta</span><span class="o">=</span><span class="mf">1.0</span><span class="p">)</span>  <span class="c1"># beta is Œ¥</span>

<span class="n">predictions</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([</span><span class="mf">2.5</span><span class="p">,</span> <span class="mf">0.0</span><span class="p">,</span> <span class="mf">2.1</span><span class="p">,</span> <span class="mf">10.0</span><span class="p">])</span>  <span class="c1"># Last value is outlier</span>
<span class="n">targets</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([</span><span class="mf">3.0</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.5</span><span class="p">,</span> <span class="mf">2.0</span><span class="p">,</span> <span class="mf">8.0</span><span class="p">])</span>

<span class="n">loss</span> <span class="o">=</span> <span class="n">huber_loss</span><span class="p">(</span><span class="n">predictions</span><span class="p">,</span> <span class="n">targets</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Huber Loss: </span><span class="si">{</span><span class="n">loss</span><span class="o">.</span><span class="n">item</span><span class="p">()</span><span class="si">:</span><span class="s2">.4f</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>  <span class="c1"># 0.4125</span>

<span class="c1"># Manual implementation</span>
<span class="k">def</span><span class="w"> </span><span class="nf">huber_manual</span><span class="p">(</span><span class="n">pred</span><span class="p">,</span> <span class="n">target</span><span class="p">,</span> <span class="n">delta</span><span class="o">=</span><span class="mf">1.0</span><span class="p">):</span>
    <span class="n">error</span> <span class="o">=</span> <span class="n">pred</span> <span class="o">-</span> <span class="n">target</span>
    <span class="n">abs_error</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">abs</span><span class="p">(</span><span class="n">error</span><span class="p">)</span>
    <span class="n">quadratic</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">clamp</span><span class="p">(</span><span class="n">abs_error</span><span class="p">,</span> <span class="nb">max</span><span class="o">=</span><span class="n">delta</span><span class="p">)</span>
    <span class="n">linear</span> <span class="o">=</span> <span class="n">abs_error</span> <span class="o">-</span> <span class="n">quadratic</span>
    <span class="k">return</span> <span class="n">torch</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="mf">0.5</span> <span class="o">*</span> <span class="n">quadratic</span><span class="o">**</span><span class="mi">2</span> <span class="o">+</span> <span class="n">delta</span> <span class="o">*</span> <span class="n">linear</span><span class="p">)</span>

<span class="n">loss_manual</span> <span class="o">=</span> <span class="n">huber_manual</span><span class="p">(</span><span class="n">predictions</span><span class="p">,</span> <span class="n">targets</span><span class="p">,</span> <span class="n">delta</span><span class="o">=</span><span class="mf">1.0</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Manual Huber: </span><span class="si">{</span><span class="n">loss_manual</span><span class="o">.</span><span class="n">item</span><span class="p">()</span><span class="si">:</span><span class="s2">.4f</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>  <span class="c1"># 0.4125</span>
</code></pre></div>

<p><strong>When to Use:</strong>
- Data with some outliers, but you still want to penalize large errors
- Object detection (bounding box regression)
- Robotics (sensor fusion)</p>
<h3 id="24-log-cosh-loss">2.4 Log-Cosh Loss<a class="header-link" href="#24-log-cosh-loss" title="Permanent link">&para;</a></h3>
<p><strong>Formula:</strong></p>
<div class="highlight"><pre><span></span><code>L(≈∑, y) = Œ£ log(cosh(≈∑·µ¢ - y·µ¢))
</code></pre></div>

<p><strong>Properties:</strong>
- Twice differentiable (smoother than Huber)
- Approximately equal to (x¬≤/2) for small x, |x| for large x
- Less sensitive to outliers than MSE</p>
<p><strong>PyTorch Implementation:</strong></p>
<div class="highlight"><pre><span></span><code><span class="k">def</span><span class="w"> </span><span class="nf">log_cosh_loss</span><span class="p">(</span><span class="n">pred</span><span class="p">,</span> <span class="n">target</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Log-Cosh Loss&quot;&quot;&quot;</span>
    <span class="n">error</span> <span class="o">=</span> <span class="n">pred</span> <span class="o">-</span> <span class="n">target</span>
    <span class="k">return</span> <span class="n">torch</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">cosh</span><span class="p">(</span><span class="n">error</span><span class="p">)))</span>

<span class="n">predictions</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([</span><span class="mf">2.5</span><span class="p">,</span> <span class="mf">0.0</span><span class="p">,</span> <span class="mf">2.1</span><span class="p">,</span> <span class="mf">10.0</span><span class="p">])</span>
<span class="n">targets</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([</span><span class="mf">3.0</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.5</span><span class="p">,</span> <span class="mf">2.0</span><span class="p">,</span> <span class="mf">8.0</span><span class="p">])</span>

<span class="n">loss</span> <span class="o">=</span> <span class="n">log_cosh_loss</span><span class="p">(</span><span class="n">predictions</span><span class="p">,</span> <span class="n">targets</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Log-Cosh Loss: </span><span class="si">{</span><span class="n">loss</span><span class="o">.</span><span class="n">item</span><span class="p">()</span><span class="si">:</span><span class="s2">.4f</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
</code></pre></div>

<p><strong>When to Use:</strong>
- When you need twice-differentiable loss (e.g., for Hessian-based optimizers)
- XGBoost and other gradient boosting methods</p>
<h3 id="25-regression-loss-comparison">2.5 Regression Loss Comparison<a class="header-link" href="#25-regression-loss-comparison" title="Permanent link">&para;</a></h3>
<div class="highlight"><pre><span></span><code><span class="kn">import</span><span class="w"> </span><span class="nn">torch</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">matplotlib.pyplot</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">plt</span>

<span class="k">def</span><span class="w"> </span><span class="nf">compare_regression_losses</span><span class="p">():</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Compare different regression losses&quot;&quot;&quot;</span>
    <span class="n">errors</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="o">-</span><span class="mi">5</span><span class="p">,</span> <span class="mi">5</span><span class="p">,</span> <span class="mi">200</span><span class="p">)</span>

    <span class="c1"># Calculate losses</span>
    <span class="n">mse</span> <span class="o">=</span> <span class="n">errors</span> <span class="o">**</span> <span class="mi">2</span>
    <span class="n">mae</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">abs</span><span class="p">(</span><span class="n">errors</span><span class="p">)</span>
    <span class="n">huber</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">where</span><span class="p">(</span>
        <span class="n">torch</span><span class="o">.</span><span class="n">abs</span><span class="p">(</span><span class="n">errors</span><span class="p">)</span> <span class="o">&lt;=</span> <span class="mf">1.0</span><span class="p">,</span>
        <span class="mf">0.5</span> <span class="o">*</span> <span class="n">errors</span> <span class="o">**</span> <span class="mi">2</span><span class="p">,</span>
        <span class="n">torch</span><span class="o">.</span><span class="n">abs</span><span class="p">(</span><span class="n">errors</span><span class="p">)</span> <span class="o">-</span> <span class="mf">0.5</span>
    <span class="p">)</span>
    <span class="n">log_cosh</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">cosh</span><span class="p">(</span><span class="n">errors</span><span class="p">))</span>

    <span class="c1"># Plot</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">12</span><span class="p">,</span> <span class="mi">5</span><span class="p">))</span>

    <span class="n">plt</span><span class="o">.</span><span class="n">subplot</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">errors</span><span class="o">.</span><span class="n">numpy</span><span class="p">(),</span> <span class="n">mse</span><span class="o">.</span><span class="n">numpy</span><span class="p">(),</span> <span class="n">label</span><span class="o">=</span><span class="s1">&#39;MSE (L2)&#39;</span><span class="p">,</span> <span class="n">linewidth</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">errors</span><span class="o">.</span><span class="n">numpy</span><span class="p">(),</span> <span class="n">mae</span><span class="o">.</span><span class="n">numpy</span><span class="p">(),</span> <span class="n">label</span><span class="o">=</span><span class="s1">&#39;MAE (L1)&#39;</span><span class="p">,</span> <span class="n">linewidth</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">errors</span><span class="o">.</span><span class="n">numpy</span><span class="p">(),</span> <span class="n">huber</span><span class="o">.</span><span class="n">numpy</span><span class="p">(),</span> <span class="n">label</span><span class="o">=</span><span class="s1">&#39;Huber (Œ¥=1)&#39;</span><span class="p">,</span> <span class="n">linewidth</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">errors</span><span class="o">.</span><span class="n">numpy</span><span class="p">(),</span> <span class="n">log_cosh</span><span class="o">.</span><span class="n">numpy</span><span class="p">(),</span> <span class="n">label</span><span class="o">=</span><span class="s1">&#39;Log-Cosh&#39;</span><span class="p">,</span> <span class="n">linewidth</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s1">&#39;Prediction Error (≈∑ - y)&#39;</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s1">&#39;Loss&#39;</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s1">&#39;Regression Losses&#39;</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">legend</span><span class="p">()</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">grid</span><span class="p">(</span><span class="kc">True</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.3</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">ylim</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">10</span><span class="p">)</span>

    <span class="c1"># Gradient plot</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">subplot</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>
    <span class="n">grad_mse</span> <span class="o">=</span> <span class="mi">2</span> <span class="o">*</span> <span class="n">errors</span>
    <span class="n">grad_mae</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">sign</span><span class="p">(</span><span class="n">errors</span><span class="p">)</span>
    <span class="n">grad_huber</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">where</span><span class="p">(</span>
        <span class="n">torch</span><span class="o">.</span><span class="n">abs</span><span class="p">(</span><span class="n">errors</span><span class="p">)</span> <span class="o">&lt;=</span> <span class="mf">1.0</span><span class="p">,</span>
        <span class="n">errors</span><span class="p">,</span>
        <span class="n">torch</span><span class="o">.</span><span class="n">sign</span><span class="p">(</span><span class="n">errors</span><span class="p">)</span>
    <span class="p">)</span>
    <span class="n">grad_log_cosh</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tanh</span><span class="p">(</span><span class="n">errors</span><span class="p">)</span>

    <span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">errors</span><span class="o">.</span><span class="n">numpy</span><span class="p">(),</span> <span class="n">grad_mse</span><span class="o">.</span><span class="n">numpy</span><span class="p">(),</span> <span class="n">label</span><span class="o">=</span><span class="s1">&#39;MSE grad&#39;</span><span class="p">,</span> <span class="n">linewidth</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">errors</span><span class="o">.</span><span class="n">numpy</span><span class="p">(),</span> <span class="n">grad_mae</span><span class="o">.</span><span class="n">numpy</span><span class="p">(),</span> <span class="n">label</span><span class="o">=</span><span class="s1">&#39;MAE grad&#39;</span><span class="p">,</span> <span class="n">linewidth</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">errors</span><span class="o">.</span><span class="n">numpy</span><span class="p">(),</span> <span class="n">grad_huber</span><span class="o">.</span><span class="n">numpy</span><span class="p">(),</span> <span class="n">label</span><span class="o">=</span><span class="s1">&#39;Huber grad&#39;</span><span class="p">,</span> <span class="n">linewidth</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">errors</span><span class="o">.</span><span class="n">numpy</span><span class="p">(),</span> <span class="n">grad_log_cosh</span><span class="o">.</span><span class="n">numpy</span><span class="p">(),</span> <span class="n">label</span><span class="o">=</span><span class="s1">&#39;Log-Cosh grad&#39;</span><span class="p">,</span> <span class="n">linewidth</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s1">&#39;Prediction Error (≈∑ - y)&#39;</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s1">&#39;Gradient&#39;</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s1">&#39;Loss Gradients&#39;</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">legend</span><span class="p">()</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">grid</span><span class="p">(</span><span class="kc">True</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.3</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">ylim</span><span class="p">(</span><span class="o">-</span><span class="mi">5</span><span class="p">,</span> <span class="mi">5</span><span class="p">)</span>

    <span class="n">plt</span><span class="o">.</span><span class="n">tight_layout</span><span class="p">()</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">savefig</span><span class="p">(</span><span class="s1">&#39;regression_losses.png&#39;</span><span class="p">,</span> <span class="n">dpi</span><span class="o">=</span><span class="mi">150</span><span class="p">,</span> <span class="n">bbox_inches</span><span class="o">=</span><span class="s1">&#39;tight&#39;</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>

<span class="n">compare_regression_losses</span><span class="p">()</span>
</code></pre></div>

<p><strong>Comparison Table:</strong></p>
<table>
<thead>
<tr>
<th>Loss</th>
<th>Outlier Robustness</th>
<th>Gradient Smoothness</th>
<th>Use Case</th>
</tr>
</thead>
<tbody>
<tr>
<td><strong>MSE</strong></td>
<td>Low</td>
<td>High</td>
<td>Clean data, stable training</td>
</tr>
<tr>
<td><strong>MAE</strong></td>
<td>High</td>
<td>Low (discontinuous at 0)</td>
<td>Outlier-prone data</td>
</tr>
<tr>
<td><strong>Huber</strong></td>
<td>Medium</td>
<td>Medium</td>
<td>Balance between MSE/MAE</td>
</tr>
<tr>
<td><strong>Log-Cosh</strong></td>
<td>Medium-High</td>
<td>High (twice differentiable)</td>
<td>Advanced optimizers</td>
</tr>
</tbody>
</table>
<hr />
<h2 id="3-classification-losses">3. Classification Losses<a class="header-link" href="#3-classification-losses" title="Permanent link">&para;</a></h2>
<p>Classification losses are used for discrete label prediction tasks.</p>
<h3 id="31-binary-cross-entropy-bce">3.1 Binary Cross-Entropy (BCE)<a class="header-link" href="#31-binary-cross-entropy-bce" title="Permanent link">&para;</a></h3>
<p><strong>Formula:</strong></p>
<div class="highlight"><pre><span></span><code>BCE = -(1/n) Œ£ [y·µ¢ log(≈∑·µ¢) + (1-y·µ¢) log(1-≈∑·µ¢)]

where:
- y·µ¢ ‚àà {0, 1} (true label)
- ≈∑·µ¢ ‚àà (0, 1) (predicted probability)
</code></pre></div>

<p><strong>Properties:</strong>
- For binary classification (2 classes)
- Requires sigmoid activation to output probabilities
- Convex loss function</p>
<p><strong>PyTorch Implementation:</strong></p>
<div class="highlight"><pre><span></span><code><span class="kn">import</span><span class="w"> </span><span class="nn">torch</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">torch.nn</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">nn</span>

<span class="c1"># Method 1: BCELoss (requires sigmoid applied first)</span>
<span class="n">sigmoid</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Sigmoid</span><span class="p">()</span>
<span class="n">bce_loss</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">BCELoss</span><span class="p">()</span>

<span class="n">logits</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([</span><span class="mf">0.5</span><span class="p">,</span> <span class="mf">2.0</span><span class="p">,</span> <span class="o">-</span><span class="mf">1.0</span><span class="p">,</span> <span class="mf">0.0</span><span class="p">])</span>  <span class="c1"># Raw outputs</span>
<span class="n">targets</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([</span><span class="mf">1.0</span><span class="p">,</span> <span class="mf">1.0</span><span class="p">,</span> <span class="mf">0.0</span><span class="p">,</span> <span class="mf">0.0</span><span class="p">])</span>  <span class="c1"># Binary labels</span>

<span class="n">probabilities</span> <span class="o">=</span> <span class="n">sigmoid</span><span class="p">(</span><span class="n">logits</span><span class="p">)</span>
<span class="n">loss</span> <span class="o">=</span> <span class="n">bce_loss</span><span class="p">(</span><span class="n">probabilities</span><span class="p">,</span> <span class="n">targets</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;BCE Loss: </span><span class="si">{</span><span class="n">loss</span><span class="o">.</span><span class="n">item</span><span class="p">()</span><span class="si">:</span><span class="s2">.4f</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>

<span class="c1"># Method 2: BCEWithLogitsLoss (numerically stable, combines sigmoid + BCE)</span>
<span class="n">bce_with_logits</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">BCEWithLogitsLoss</span><span class="p">()</span>
<span class="n">loss_stable</span> <span class="o">=</span> <span class="n">bce_with_logits</span><span class="p">(</span><span class="n">logits</span><span class="p">,</span> <span class="n">targets</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;BCE with Logits: </span><span class="si">{</span><span class="n">loss_stable</span><span class="o">.</span><span class="n">item</span><span class="p">()</span><span class="si">:</span><span class="s2">.4f</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>

<span class="c1"># Manual implementation</span>
<span class="k">def</span><span class="w"> </span><span class="nf">bce_manual</span><span class="p">(</span><span class="n">pred_probs</span><span class="p">,</span> <span class="n">target</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Manual BCE (expects probabilities)&quot;&quot;&quot;</span>
    <span class="n">epsilon</span> <span class="o">=</span> <span class="mf">1e-7</span>  <span class="c1"># For numerical stability</span>
    <span class="n">pred_probs</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">clamp</span><span class="p">(</span><span class="n">pred_probs</span><span class="p">,</span> <span class="n">epsilon</span><span class="p">,</span> <span class="mi">1</span> <span class="o">-</span> <span class="n">epsilon</span><span class="p">)</span>
    <span class="k">return</span> <span class="o">-</span><span class="n">torch</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span>
        <span class="n">target</span> <span class="o">*</span> <span class="n">torch</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="n">pred_probs</span><span class="p">)</span> <span class="o">+</span>
        <span class="p">(</span><span class="mi">1</span> <span class="o">-</span> <span class="n">target</span><span class="p">)</span> <span class="o">*</span> <span class="n">torch</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="mi">1</span> <span class="o">-</span> <span class="n">pred_probs</span><span class="p">)</span>
    <span class="p">)</span>

<span class="n">loss_manual</span> <span class="o">=</span> <span class="n">bce_manual</span><span class="p">(</span><span class="n">probabilities</span><span class="p">,</span> <span class="n">targets</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Manual BCE: </span><span class="si">{</span><span class="n">loss_manual</span><span class="o">.</span><span class="n">item</span><span class="p">()</span><span class="si">:</span><span class="s2">.4f</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
</code></pre></div>

<p><strong>Binary Classification Example:</strong></p>
<div class="highlight"><pre><span></span><code><span class="k">class</span><span class="w"> </span><span class="nc">BinaryClassifier</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="k">def</span><span class="w"> </span><span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">input_dim</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">fc1</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">input_dim</span><span class="p">,</span> <span class="mi">64</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">fc2</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="mi">64</span><span class="p">,</span> <span class="mi">32</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">fc3</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="mi">32</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>  <span class="c1"># Single output</span>

    <span class="k">def</span><span class="w"> </span><span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
        <span class="n">x</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">relu</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">fc1</span><span class="p">(</span><span class="n">x</span><span class="p">))</span>
        <span class="n">x</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">relu</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">fc2</span><span class="p">(</span><span class="n">x</span><span class="p">))</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">fc3</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>  <span class="c1"># Raw logit</span>

<span class="c1"># Training setup</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">BinaryClassifier</span><span class="p">(</span><span class="n">input_dim</span><span class="o">=</span><span class="mi">10</span><span class="p">)</span>
<span class="n">criterion</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">BCEWithLogitsLoss</span><span class="p">()</span>
<span class="n">optimizer</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">optim</span><span class="o">.</span><span class="n">Adam</span><span class="p">(</span><span class="n">model</span><span class="o">.</span><span class="n">parameters</span><span class="p">(),</span> <span class="n">lr</span><span class="o">=</span><span class="mf">0.001</span><span class="p">)</span>

<span class="c1"># Dummy data</span>
<span class="n">inputs</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">32</span><span class="p">,</span> <span class="mi">10</span><span class="p">)</span>  <span class="c1"># Batch of 32</span>
<span class="n">targets</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randint</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="p">(</span><span class="mi">32</span><span class="p">,</span> <span class="mi">1</span><span class="p">))</span><span class="o">.</span><span class="n">float</span><span class="p">()</span>

<span class="c1"># Training step</span>
<span class="n">optimizer</span><span class="o">.</span><span class="n">zero_grad</span><span class="p">()</span>
<span class="n">outputs</span> <span class="o">=</span> <span class="n">model</span><span class="p">(</span><span class="n">inputs</span><span class="p">)</span>
<span class="n">loss</span> <span class="o">=</span> <span class="n">criterion</span><span class="p">(</span><span class="n">outputs</span><span class="p">,</span> <span class="n">targets</span><span class="p">)</span>
<span class="n">loss</span><span class="o">.</span><span class="n">backward</span><span class="p">()</span>
<span class="n">optimizer</span><span class="o">.</span><span class="n">step</span><span class="p">()</span>

<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Training Loss: </span><span class="si">{</span><span class="n">loss</span><span class="o">.</span><span class="n">item</span><span class="p">()</span><span class="si">:</span><span class="s2">.4f</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
</code></pre></div>

<p><strong>When to Use:</strong>
- Binary classification (spam/not spam, cat/dog)
- Multi-label classification (each label is independent)
- Sigmoid output activation</p>
<h3 id="32-cross-entropy-loss-multi-class">3.2 Cross-Entropy Loss (Multi-Class)<a class="header-link" href="#32-cross-entropy-loss-multi-class" title="Permanent link">&para;</a></h3>
<p><strong>Formula:</strong></p>
<div class="highlight"><pre><span></span><code><span class="nv">CE</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="o">-</span><span class="ss">(</span><span class="mi">1</span><span class="o">/</span><span class="nv">n</span><span class="ss">)</span><span class="w"> </span>Œ£·µ¢<span class="w"> </span>Œ£‚±º<span class="w"> </span><span class="nv">y</span>·µ¢‚±º<span class="w"> </span><span class="nv">log</span><span class="ss">(</span>≈∑·µ¢‚±º<span class="ss">)</span>

<span class="nv">where</span>:
<span class="o">-</span><span class="w"> </span><span class="nv">y</span>·µ¢‚±º<span class="w"> </span><span class="nv">is</span><span class="w"> </span><span class="mi">1</span><span class="w"> </span><span class="k">if</span><span class="w"> </span><span class="nv">sample</span><span class="w"> </span><span class="nv">i</span><span class="w"> </span><span class="nv">is</span><span class="w"> </span><span class="nv">class</span><span class="w"> </span><span class="nv">j</span>,<span class="w"> </span><span class="k">else</span><span class="w"> </span><span class="mi">0</span>
<span class="o">-</span><span class="w"> </span>≈∑·µ¢‚±º<span class="w"> </span><span class="nv">is</span><span class="w"> </span><span class="nv">predicted</span><span class="w"> </span><span class="nv">probability</span><span class="w"> </span><span class="k">for</span><span class="w"> </span><span class="nv">class</span><span class="w"> </span><span class="nv">j</span>
</code></pre></div>

<p><strong>Properties:</strong>
- For multi-class classification (K &gt; 2 classes)
- Requires softmax activation
- PyTorch's <code>CrossEntropyLoss</code> combines softmax + NLLLoss</p>
<p><strong>PyTorch Implementation:</strong></p>
<div class="highlight"><pre><span></span><code><span class="c1"># Method 1: CrossEntropyLoss (combines log_softmax + NLLLoss)</span>
<span class="n">ce_loss</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">CrossEntropyLoss</span><span class="p">()</span>

<span class="n">logits</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([</span>
    <span class="p">[</span><span class="mf">2.0</span><span class="p">,</span> <span class="mf">1.0</span><span class="p">,</span> <span class="mf">0.1</span><span class="p">],</span>  <span class="c1"># Sample 1</span>
    <span class="p">[</span><span class="mf">0.5</span><span class="p">,</span> <span class="mf">2.5</span><span class="p">,</span> <span class="mf">0.3</span><span class="p">],</span>  <span class="c1"># Sample 2</span>
    <span class="p">[</span><span class="mf">0.1</span><span class="p">,</span> <span class="mf">0.2</span><span class="p">,</span> <span class="mf">3.0</span><span class="p">],</span>  <span class="c1"># Sample 3</span>
<span class="p">])</span>
<span class="n">targets</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">])</span>  <span class="c1"># Class indices</span>

<span class="n">loss</span> <span class="o">=</span> <span class="n">ce_loss</span><span class="p">(</span><span class="n">logits</span><span class="p">,</span> <span class="n">targets</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;CrossEntropy Loss: </span><span class="si">{</span><span class="n">loss</span><span class="o">.</span><span class="n">item</span><span class="p">()</span><span class="si">:</span><span class="s2">.4f</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>

<span class="c1"># Method 2: Manual with softmax + NLLLoss</span>
<span class="n">log_softmax</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">LogSoftmax</span><span class="p">(</span><span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
<span class="n">nll_loss</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">NLLLoss</span><span class="p">()</span>

<span class="n">log_probs</span> <span class="o">=</span> <span class="n">log_softmax</span><span class="p">(</span><span class="n">logits</span><span class="p">)</span>
<span class="n">loss_manual</span> <span class="o">=</span> <span class="n">nll_loss</span><span class="p">(</span><span class="n">log_probs</span><span class="p">,</span> <span class="n">targets</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Manual CE (LogSoftmax + NLL): </span><span class="si">{</span><span class="n">loss_manual</span><span class="o">.</span><span class="n">item</span><span class="p">()</span><span class="si">:</span><span class="s2">.4f</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>

<span class="c1"># Method 3: From scratch</span>
<span class="k">def</span><span class="w"> </span><span class="nf">cross_entropy_manual</span><span class="p">(</span><span class="n">logits</span><span class="p">,</span> <span class="n">targets</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Manual cross-entropy implementation&quot;&quot;&quot;</span>
    <span class="c1"># Compute log softmax</span>
    <span class="n">max_logits</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">max</span><span class="p">(</span><span class="n">logits</span><span class="p">,</span> <span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">keepdim</span><span class="o">=</span><span class="kc">True</span><span class="p">)[</span><span class="mi">0</span><span class="p">]</span>
    <span class="n">exp_logits</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="n">logits</span> <span class="o">-</span> <span class="n">max_logits</span><span class="p">)</span>  <span class="c1"># Numerical stability</span>
    <span class="n">log_probs</span> <span class="o">=</span> <span class="p">(</span><span class="n">logits</span> <span class="o">-</span> <span class="n">max_logits</span><span class="p">)</span> <span class="o">-</span> <span class="n">torch</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">exp_logits</span><span class="p">,</span> <span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">keepdim</span><span class="o">=</span><span class="kc">True</span><span class="p">))</span>

    <span class="c1"># Gather log probabilities for correct classes</span>
    <span class="n">batch_size</span> <span class="o">=</span> <span class="n">logits</span><span class="o">.</span><span class="n">size</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>
    <span class="n">loss</span> <span class="o">=</span> <span class="o">-</span><span class="n">log_probs</span><span class="p">[</span><span class="nb">range</span><span class="p">(</span><span class="n">batch_size</span><span class="p">),</span> <span class="n">targets</span><span class="p">]</span><span class="o">.</span><span class="n">mean</span><span class="p">()</span>
    <span class="k">return</span> <span class="n">loss</span>

<span class="n">loss_scratch</span> <span class="o">=</span> <span class="n">cross_entropy_manual</span><span class="p">(</span><span class="n">logits</span><span class="p">,</span> <span class="n">targets</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;From Scratch CE: </span><span class="si">{</span><span class="n">loss_scratch</span><span class="o">.</span><span class="n">item</span><span class="p">()</span><span class="si">:</span><span class="s2">.4f</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
</code></pre></div>

<p><strong>Multi-Class Classification Example:</strong></p>
<div class="highlight"><pre><span></span><code><span class="k">class</span><span class="w"> </span><span class="nc">MultiClassClassifier</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="k">def</span><span class="w"> </span><span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">input_dim</span><span class="p">,</span> <span class="n">num_classes</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">fc1</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">input_dim</span><span class="p">,</span> <span class="mi">128</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">fc2</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="mi">128</span><span class="p">,</span> <span class="mi">64</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">fc3</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="mi">64</span><span class="p">,</span> <span class="n">num_classes</span><span class="p">)</span>

    <span class="k">def</span><span class="w"> </span><span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
        <span class="n">x</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">relu</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">fc1</span><span class="p">(</span><span class="n">x</span><span class="p">))</span>
        <span class="n">x</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">relu</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">fc2</span><span class="p">(</span><span class="n">x</span><span class="p">))</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">fc3</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>  <span class="c1"># Raw logits (no softmax)</span>

<span class="c1"># Training setup</span>
<span class="n">num_classes</span> <span class="o">=</span> <span class="mi">10</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">MultiClassClassifier</span><span class="p">(</span><span class="n">input_dim</span><span class="o">=</span><span class="mi">20</span><span class="p">,</span> <span class="n">num_classes</span><span class="o">=</span><span class="n">num_classes</span><span class="p">)</span>
<span class="n">criterion</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">CrossEntropyLoss</span><span class="p">()</span>
<span class="n">optimizer</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">optim</span><span class="o">.</span><span class="n">Adam</span><span class="p">(</span><span class="n">model</span><span class="o">.</span><span class="n">parameters</span><span class="p">(),</span> <span class="n">lr</span><span class="o">=</span><span class="mf">0.001</span><span class="p">)</span>

<span class="c1"># Dummy data</span>
<span class="n">inputs</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">64</span><span class="p">,</span> <span class="mi">20</span><span class="p">)</span>  <span class="c1"># Batch of 64</span>
<span class="n">targets</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randint</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">num_classes</span><span class="p">,</span> <span class="p">(</span><span class="mi">64</span><span class="p">,))</span>  <span class="c1"># Class indices</span>

<span class="c1"># Training step</span>
<span class="n">optimizer</span><span class="o">.</span><span class="n">zero_grad</span><span class="p">()</span>
<span class="n">outputs</span> <span class="o">=</span> <span class="n">model</span><span class="p">(</span><span class="n">inputs</span><span class="p">)</span>
<span class="n">loss</span> <span class="o">=</span> <span class="n">criterion</span><span class="p">(</span><span class="n">outputs</span><span class="p">,</span> <span class="n">targets</span><span class="p">)</span>
<span class="n">loss</span><span class="o">.</span><span class="n">backward</span><span class="p">()</span>
<span class="n">optimizer</span><span class="o">.</span><span class="n">step</span><span class="p">()</span>

<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Training Loss: </span><span class="si">{</span><span class="n">loss</span><span class="o">.</span><span class="n">item</span><span class="p">()</span><span class="si">:</span><span class="s2">.4f</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>

<span class="c1"># Inference</span>
<span class="k">with</span> <span class="n">torch</span><span class="o">.</span><span class="n">no_grad</span><span class="p">():</span>
    <span class="n">test_input</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">20</span><span class="p">)</span>
    <span class="n">logits</span> <span class="o">=</span> <span class="n">model</span><span class="p">(</span><span class="n">test_input</span><span class="p">)</span>
    <span class="n">probs</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">softmax</span><span class="p">(</span><span class="n">logits</span><span class="p">,</span> <span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
    <span class="n">predicted_class</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">argmax</span><span class="p">(</span><span class="n">probs</span><span class="p">,</span> <span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Predicted Class: </span><span class="si">{</span><span class="n">predicted_class</span><span class="o">.</span><span class="n">item</span><span class="p">()</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Class Probabilities: </span><span class="si">{</span><span class="n">probs</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">tolist</span><span class="p">()</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
</code></pre></div>

<h3 id="33-label-smoothing">3.3 Label Smoothing<a class="header-link" href="#33-label-smoothing" title="Permanent link">&para;</a></h3>
<p><strong>Concept:</strong>
Instead of hard labels (0 or 1), use soft labels:</p>
<div class="highlight"><pre><span></span><code>y_smooth = y(1 - Œµ) + Œµ/K

where:
<span class="k">-</span> Œµ is smoothing parameter (e.g., 0.1)
<span class="k">-</span> K is number of classes
</code></pre></div>

<p><strong>Benefits:</strong>
- Prevents overconfidence
- Better generalization
- Regularization effect</p>
<p><strong>PyTorch Implementation:</strong></p>
<div class="highlight"><pre><span></span><code><span class="k">class</span><span class="w"> </span><span class="nc">LabelSmoothingCrossEntropy</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="k">def</span><span class="w"> </span><span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">epsilon</span><span class="o">=</span><span class="mf">0.1</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">epsilon</span> <span class="o">=</span> <span class="n">epsilon</span>

    <span class="k">def</span><span class="w"> </span><span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">logits</span><span class="p">,</span> <span class="n">targets</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Args:</span>
<span class="sd">            logits: (batch_size, num_classes)</span>
<span class="sd">            targets: (batch_size,) class indices</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">num_classes</span> <span class="o">=</span> <span class="n">logits</span><span class="o">.</span><span class="n">size</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span>
        <span class="n">log_probs</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">log_softmax</span><span class="p">(</span><span class="n">logits</span><span class="p">,</span> <span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>

        <span class="c1"># Create smooth targets</span>
        <span class="k">with</span> <span class="n">torch</span><span class="o">.</span><span class="n">no_grad</span><span class="p">():</span>
            <span class="n">true_dist</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">zeros_like</span><span class="p">(</span><span class="n">log_probs</span><span class="p">)</span>
            <span class="n">true_dist</span><span class="o">.</span><span class="n">fill_</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">epsilon</span> <span class="o">/</span> <span class="p">(</span><span class="n">num_classes</span> <span class="o">-</span> <span class="mi">1</span><span class="p">))</span>
            <span class="n">true_dist</span><span class="o">.</span><span class="n">scatter_</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">targets</span><span class="o">.</span><span class="n">unsqueeze</span><span class="p">(</span><span class="mi">1</span><span class="p">),</span> <span class="mf">1.0</span> <span class="o">-</span> <span class="bp">self</span><span class="o">.</span><span class="n">epsilon</span><span class="p">)</span>

        <span class="k">return</span> <span class="n">torch</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="o">-</span><span class="n">true_dist</span> <span class="o">*</span> <span class="n">log_probs</span><span class="p">,</span> <span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">))</span>

<span class="c1"># Example usage</span>
<span class="n">criterion</span> <span class="o">=</span> <span class="n">LabelSmoothingCrossEntropy</span><span class="p">(</span><span class="n">epsilon</span><span class="o">=</span><span class="mf">0.1</span><span class="p">)</span>

<span class="n">logits</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">4</span><span class="p">,</span> <span class="mi">5</span><span class="p">)</span>  <span class="c1"># 4 samples, 5 classes</span>
<span class="n">targets</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([</span><span class="mi">0</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">4</span><span class="p">])</span>

<span class="n">loss</span> <span class="o">=</span> <span class="n">criterion</span><span class="p">(</span><span class="n">logits</span><span class="p">,</span> <span class="n">targets</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Label Smoothing CE: </span><span class="si">{</span><span class="n">loss</span><span class="o">.</span><span class="n">item</span><span class="p">()</span><span class="si">:</span><span class="s2">.4f</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>

<span class="c1"># Compare with standard CE</span>
<span class="n">standard_ce</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">CrossEntropyLoss</span><span class="p">()</span>
<span class="n">loss_standard</span> <span class="o">=</span> <span class="n">standard_ce</span><span class="p">(</span><span class="n">logits</span><span class="p">,</span> <span class="n">targets</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Standard CE: </span><span class="si">{</span><span class="n">loss_standard</span><span class="o">.</span><span class="n">item</span><span class="p">()</span><span class="si">:</span><span class="s2">.4f</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
</code></pre></div>

<p><strong>When to Use:</strong>
- Image classification (ImageNet, CIFAR)
- Preventing overconfidence
- Model calibration</p>
<h3 id="34-focal-loss">3.4 Focal Loss<a class="header-link" href="#34-focal-loss" title="Permanent link">&para;</a></h3>
<p><strong>Formula:</strong></p>
<div class="highlight"><pre><span></span><code><span class="nv">FL</span><span class="ss">(</span><span class="nv">p</span>‚Çú<span class="ss">)</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="o">-</span>Œ±‚Çú<span class="ss">(</span><span class="mi">1</span><span class="w"> </span><span class="o">-</span><span class="w"> </span><span class="nv">p</span>‚Çú<span class="ss">)</span><span class="o">^</span>Œ≥<span class="w"> </span><span class="nv">log</span><span class="ss">(</span><span class="nv">p</span>‚Çú<span class="ss">)</span>

<span class="nv">where</span>:
<span class="o">-</span><span class="w"> </span><span class="nv">p</span>‚Çú<span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="nv">p</span><span class="w"> </span><span class="k">if</span><span class="w"> </span><span class="nv">y</span><span class="o">=</span><span class="mi">1</span>,<span class="w"> </span><span class="k">else</span><span class="w"> </span><span class="mi">1</span><span class="o">-</span><span class="nv">p</span>
<span class="o">-</span><span class="w"> </span>Œ±<span class="w"> </span><span class="nv">balances</span><span class="w"> </span><span class="nv">class</span><span class="w"> </span><span class="nv">frequencies</span>
<span class="o">-</span><span class="w"> </span>Œ≥<span class="w"> </span><span class="nv">focuses</span><span class="w"> </span><span class="nv">on</span><span class="w"> </span><span class="nv">hard</span><span class="w"> </span><span class="nv">examples</span><span class="w"> </span><span class="ss">(</span><span class="nv">typically</span><span class="w"> </span><span class="mi">2</span><span class="ss">)</span>
</code></pre></div>

<p><strong>Motivation:</strong>
- Addresses class imbalance (e.g., 1:1000 ratio in object detection)
- Down-weights easy examples, focuses on hard negatives
- Introduced in RetinaNet paper</p>
<p><strong>PyTorch Implementation:</strong></p>
<div class="highlight"><pre><span></span><code><span class="k">class</span><span class="w"> </span><span class="nc">FocalLoss</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="k">def</span><span class="w"> </span><span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.25</span><span class="p">,</span> <span class="n">gamma</span><span class="o">=</span><span class="mf">2.0</span><span class="p">,</span> <span class="n">reduction</span><span class="o">=</span><span class="s1">&#39;mean&#39;</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Args:</span>
<span class="sd">            alpha: Weighting factor for class imbalance (default 0.25)</span>
<span class="sd">            gamma: Focusing parameter (default 2.0)</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">alpha</span> <span class="o">=</span> <span class="n">alpha</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">gamma</span> <span class="o">=</span> <span class="n">gamma</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">reduction</span> <span class="o">=</span> <span class="n">reduction</span>

    <span class="k">def</span><span class="w"> </span><span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">logits</span><span class="p">,</span> <span class="n">targets</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Args:</span>
<span class="sd">            logits: (N, C) raw predictions</span>
<span class="sd">            targets: (N,) class indices</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">ce_loss</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">functional</span><span class="o">.</span><span class="n">cross_entropy</span><span class="p">(</span><span class="n">logits</span><span class="p">,</span> <span class="n">targets</span><span class="p">,</span> <span class="n">reduction</span><span class="o">=</span><span class="s1">&#39;none&#39;</span><span class="p">)</span>
        <span class="n">p</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="o">-</span><span class="n">ce_loss</span><span class="p">)</span>  <span class="c1"># Probability of correct class</span>

        <span class="c1"># Focal loss formula</span>
        <span class="n">focal_weight</span> <span class="o">=</span> <span class="p">(</span><span class="mi">1</span> <span class="o">-</span> <span class="n">p</span><span class="p">)</span> <span class="o">**</span> <span class="bp">self</span><span class="o">.</span><span class="n">gamma</span>
        <span class="n">focal_loss</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">alpha</span> <span class="o">*</span> <span class="n">focal_weight</span> <span class="o">*</span> <span class="n">ce_loss</span>

        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">reduction</span> <span class="o">==</span> <span class="s1">&#39;mean&#39;</span><span class="p">:</span>
            <span class="k">return</span> <span class="n">focal_loss</span><span class="o">.</span><span class="n">mean</span><span class="p">()</span>
        <span class="k">elif</span> <span class="bp">self</span><span class="o">.</span><span class="n">reduction</span> <span class="o">==</span> <span class="s1">&#39;sum&#39;</span><span class="p">:</span>
            <span class="k">return</span> <span class="n">focal_loss</span><span class="o">.</span><span class="n">sum</span><span class="p">()</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="k">return</span> <span class="n">focal_loss</span>

<span class="c1"># Binary Focal Loss variant</span>
<span class="k">class</span><span class="w"> </span><span class="nc">BinaryFocalLoss</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="k">def</span><span class="w"> </span><span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.25</span><span class="p">,</span> <span class="n">gamma</span><span class="o">=</span><span class="mf">2.0</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">alpha</span> <span class="o">=</span> <span class="n">alpha</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">gamma</span> <span class="o">=</span> <span class="n">gamma</span>

    <span class="k">def</span><span class="w"> </span><span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">logits</span><span class="p">,</span> <span class="n">targets</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Args:</span>
<span class="sd">            logits: (N,) or (N, 1) raw predictions</span>
<span class="sd">            targets: (N,) or (N, 1) binary labels {0, 1}</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">bce_loss</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">functional</span><span class="o">.</span><span class="n">binary_cross_entropy_with_logits</span><span class="p">(</span>
            <span class="n">logits</span><span class="p">,</span> <span class="n">targets</span><span class="p">,</span> <span class="n">reduction</span><span class="o">=</span><span class="s1">&#39;none&#39;</span>
        <span class="p">)</span>
        <span class="n">p</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">sigmoid</span><span class="p">(</span><span class="n">logits</span><span class="p">)</span>
        <span class="n">p_t</span> <span class="o">=</span> <span class="n">p</span> <span class="o">*</span> <span class="n">targets</span> <span class="o">+</span> <span class="p">(</span><span class="mi">1</span> <span class="o">-</span> <span class="n">p</span><span class="p">)</span> <span class="o">*</span> <span class="p">(</span><span class="mi">1</span> <span class="o">-</span> <span class="n">targets</span><span class="p">)</span>

        <span class="n">alpha_t</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">alpha</span> <span class="o">*</span> <span class="n">targets</span> <span class="o">+</span> <span class="p">(</span><span class="mi">1</span> <span class="o">-</span> <span class="bp">self</span><span class="o">.</span><span class="n">alpha</span><span class="p">)</span> <span class="o">*</span> <span class="p">(</span><span class="mi">1</span> <span class="o">-</span> <span class="n">targets</span><span class="p">)</span>
        <span class="n">focal_weight</span> <span class="o">=</span> <span class="p">(</span><span class="mi">1</span> <span class="o">-</span> <span class="n">p_t</span><span class="p">)</span> <span class="o">**</span> <span class="bp">self</span><span class="o">.</span><span class="n">gamma</span>

        <span class="n">focal_loss</span> <span class="o">=</span> <span class="n">alpha_t</span> <span class="o">*</span> <span class="n">focal_weight</span> <span class="o">*</span> <span class="n">bce_loss</span>
        <span class="k">return</span> <span class="n">focal_loss</span><span class="o">.</span><span class="n">mean</span><span class="p">()</span>

<span class="c1"># Example: Imbalanced dataset</span>
<span class="n">num_classes</span> <span class="o">=</span> <span class="mi">3</span>
<span class="n">focal_loss</span> <span class="o">=</span> <span class="n">FocalLoss</span><span class="p">(</span><span class="n">alpha</span><span class="o">=</span><span class="mf">0.25</span><span class="p">,</span> <span class="n">gamma</span><span class="o">=</span><span class="mf">2.0</span><span class="p">)</span>

<span class="c1"># Simulate imbalanced batch (mostly class 0)</span>
<span class="n">logits</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">100</span><span class="p">,</span> <span class="n">num_classes</span><span class="p">)</span>
<span class="n">targets</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">cat</span><span class="p">([</span>
    <span class="n">torch</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="mi">80</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">long</span><span class="p">),</span>   <span class="c1"># 80% class 0</span>
    <span class="n">torch</span><span class="o">.</span><span class="n">ones</span><span class="p">(</span><span class="mi">15</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">long</span><span class="p">),</span>    <span class="c1"># 15% class 1</span>
    <span class="n">torch</span><span class="o">.</span><span class="n">full</span><span class="p">((</span><span class="mi">5</span><span class="p">,),</span> <span class="mi">2</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">long</span><span class="p">)</span>  <span class="c1"># 5% class 2</span>
<span class="p">])</span>

<span class="n">loss_focal</span> <span class="o">=</span> <span class="n">focal_loss</span><span class="p">(</span><span class="n">logits</span><span class="p">,</span> <span class="n">targets</span><span class="p">)</span>
<span class="n">loss_ce</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">CrossEntropyLoss</span><span class="p">()(</span><span class="n">logits</span><span class="p">,</span> <span class="n">targets</span><span class="p">)</span>

<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Focal Loss: </span><span class="si">{</span><span class="n">loss_focal</span><span class="o">.</span><span class="n">item</span><span class="p">()</span><span class="si">:</span><span class="s2">.4f</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;CE Loss: </span><span class="si">{</span><span class="n">loss_ce</span><span class="o">.</span><span class="n">item</span><span class="p">()</span><span class="si">:</span><span class="s2">.4f</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
</code></pre></div>

<p><strong>Focal Loss Visualization:</strong></p>
<div class="highlight"><pre><span></span><code><span class="kn">import</span><span class="w"> </span><span class="nn">matplotlib.pyplot</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">plt</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">numpy</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">np</span>

<span class="k">def</span><span class="w"> </span><span class="nf">visualize_focal_loss</span><span class="p">():</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Visualize how focal loss down-weights easy examples&quot;&quot;&quot;</span>
    <span class="n">p</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="mf">0.01</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">100</span><span class="p">)</span>  <span class="c1"># Probability of correct class</span>

    <span class="c1"># Standard CE</span>
    <span class="n">ce</span> <span class="o">=</span> <span class="o">-</span><span class="n">np</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="n">p</span><span class="p">)</span>

    <span class="c1"># Focal loss with different Œ≥</span>
    <span class="n">fl_gamma_0</span> <span class="o">=</span> <span class="n">ce</span>  <span class="c1"># Œ≥=0 is same as CE</span>
    <span class="n">fl_gamma_1</span> <span class="o">=</span> <span class="p">(</span><span class="mi">1</span> <span class="o">-</span> <span class="n">p</span><span class="p">)</span> <span class="o">*</span> <span class="n">ce</span>
    <span class="n">fl_gamma_2</span> <span class="o">=</span> <span class="p">(</span><span class="mi">1</span> <span class="o">-</span> <span class="n">p</span><span class="p">)</span><span class="o">**</span><span class="mi">2</span> <span class="o">*</span> <span class="n">ce</span>
    <span class="n">fl_gamma_5</span> <span class="o">=</span> <span class="p">(</span><span class="mi">1</span> <span class="o">-</span> <span class="n">p</span><span class="p">)</span><span class="o">**</span><span class="mi">5</span> <span class="o">*</span> <span class="n">ce</span>

    <span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">10</span><span class="p">,</span> <span class="mi">6</span><span class="p">))</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">p</span><span class="p">,</span> <span class="n">ce</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s1">&#39;CE (Œ≥=0)&#39;</span><span class="p">,</span> <span class="n">linewidth</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">p</span><span class="p">,</span> <span class="n">fl_gamma_1</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s1">&#39;FL (Œ≥=1)&#39;</span><span class="p">,</span> <span class="n">linewidth</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">p</span><span class="p">,</span> <span class="n">fl_gamma_2</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s1">&#39;FL (Œ≥=2)&#39;</span><span class="p">,</span> <span class="n">linewidth</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">p</span><span class="p">,</span> <span class="n">fl_gamma_5</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s1">&#39;FL (Œ≥=5)&#39;</span><span class="p">,</span> <span class="n">linewidth</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>

    <span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s1">&#39;Probability of Correct Class (p)&#39;</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">12</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s1">&#39;Loss&#39;</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">12</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s1">&#39;Focal Loss: Down-weighting Easy Examples&#39;</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">14</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">legend</span><span class="p">(</span><span class="n">fontsize</span><span class="o">=</span><span class="mi">11</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">grid</span><span class="p">(</span><span class="kc">True</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.3</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">xlim</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">ylim</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">5</span><span class="p">)</span>

    <span class="c1"># Annotate easy vs hard examples</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">axvline</span><span class="p">(</span><span class="n">x</span><span class="o">=</span><span class="mf">0.5</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s1">&#39;red&#39;</span><span class="p">,</span> <span class="n">linestyle</span><span class="o">=</span><span class="s1">&#39;--&#39;</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.5</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">text</span><span class="p">(</span><span class="mf">0.25</span><span class="p">,</span> <span class="mf">4.5</span><span class="p">,</span> <span class="s1">&#39;Hard Examples</span><span class="se">\n</span><span class="s1">(low confidence)&#39;</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">10</span><span class="p">,</span> <span class="n">ha</span><span class="o">=</span><span class="s1">&#39;center&#39;</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">text</span><span class="p">(</span><span class="mf">0.75</span><span class="p">,</span> <span class="mf">4.5</span><span class="p">,</span> <span class="s1">&#39;Easy Examples</span><span class="se">\n</span><span class="s1">(high confidence)&#39;</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">10</span><span class="p">,</span> <span class="n">ha</span><span class="o">=</span><span class="s1">&#39;center&#39;</span><span class="p">)</span>

    <span class="n">plt</span><span class="o">.</span><span class="n">tight_layout</span><span class="p">()</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">savefig</span><span class="p">(</span><span class="s1">&#39;focal_loss.png&#39;</span><span class="p">,</span> <span class="n">dpi</span><span class="o">=</span><span class="mi">150</span><span class="p">,</span> <span class="n">bbox_inches</span><span class="o">=</span><span class="s1">&#39;tight&#39;</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>

<span class="n">visualize_focal_loss</span><span class="p">()</span>
</code></pre></div>

<p><strong>When to Use:</strong>
- Object detection (RetinaNet, FCOS)
- Imbalanced classification (fraud detection, medical diagnosis)
- When you have many easy negatives</p>
<h3 id="35-bce-vs-cross-entropy">3.5 BCE vs Cross-Entropy<a class="header-link" href="#35-bce-vs-cross-entropy" title="Permanent link">&para;</a></h3>
<p><strong>Decision Guide:</strong></p>
<table>
<thead>
<tr>
<th>Task</th>
<th>Loss</th>
<th>Activation</th>
<th>Notes</th>
</tr>
</thead>
<tbody>
<tr>
<td><strong>Binary classification</strong></td>
<td><code>BCEWithLogitsLoss</code></td>
<td>None (included)</td>
<td>2 classes, mutually exclusive</td>
</tr>
<tr>
<td><strong>Multi-class classification</strong></td>
<td><code>CrossEntropyLoss</code></td>
<td>None (included)</td>
<td>K classes, mutually exclusive</td>
</tr>
<tr>
<td><strong>Multi-label classification</strong></td>
<td><code>BCEWithLogitsLoss</code></td>
<td>None (included)</td>
<td>Multiple independent labels</td>
</tr>
<tr>
<td><strong>Imbalanced classification</strong></td>
<td><code>FocalLoss</code></td>
<td>Softmax</td>
<td>Class imbalance</td>
</tr>
</tbody>
</table>
<p><strong>Example: Multi-Label Classification:</strong></p>
<div class="highlight"><pre><span></span><code><span class="c1"># Multi-label: Each sample can belong to multiple classes</span>
<span class="k">class</span><span class="w"> </span><span class="nc">MultiLabelClassifier</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="k">def</span><span class="w"> </span><span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">input_dim</span><span class="p">,</span> <span class="n">num_labels</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">fc1</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">input_dim</span><span class="p">,</span> <span class="mi">128</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">fc2</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="mi">128</span><span class="p">,</span> <span class="n">num_labels</span><span class="p">)</span>

    <span class="k">def</span><span class="w"> </span><span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
        <span class="n">x</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">relu</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">fc1</span><span class="p">(</span><span class="n">x</span><span class="p">))</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">fc2</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>  <span class="c1"># Raw logits</span>

<span class="n">model</span> <span class="o">=</span> <span class="n">MultiLabelClassifier</span><span class="p">(</span><span class="n">input_dim</span><span class="o">=</span><span class="mi">50</span><span class="p">,</span> <span class="n">num_labels</span><span class="o">=</span><span class="mi">5</span><span class="p">)</span>
<span class="n">criterion</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">BCEWithLogitsLoss</span><span class="p">()</span>  <span class="c1"># Use BCE, not CE!</span>

<span class="c1"># Multi-label targets (sample can have multiple 1s)</span>
<span class="n">inputs</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">32</span><span class="p">,</span> <span class="mi">50</span><span class="p">)</span>
<span class="n">targets</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([</span>
    <span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">],</span>  <span class="c1"># Sample 1: classes 0, 2, 4</span>
    <span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">],</span>  <span class="c1"># Sample 2: classes 1, 2</span>
    <span class="c1"># ... more samples</span>
<span class="p">])</span><span class="o">.</span><span class="n">float</span><span class="p">()</span>

<span class="n">outputs</span> <span class="o">=</span> <span class="n">model</span><span class="p">(</span><span class="n">inputs</span><span class="p">[:</span><span class="mi">2</span><span class="p">])</span>
<span class="n">loss</span> <span class="o">=</span> <span class="n">criterion</span><span class="p">(</span><span class="n">outputs</span><span class="p">,</span> <span class="n">targets</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Multi-Label Loss: </span><span class="si">{</span><span class="n">loss</span><span class="o">.</span><span class="n">item</span><span class="p">()</span><span class="si">:</span><span class="s2">.4f</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
</code></pre></div>

<hr />
<h2 id="4-ranking-and-metric-learning-losses">4. Ranking and Metric Learning Losses<a class="header-link" href="#4-ranking-and-metric-learning-losses" title="Permanent link">&para;</a></h2>
<p>These losses learn embeddings where similar items are close and dissimilar items are far apart.</p>
<h3 id="41-contrastive-loss">4.1 Contrastive Loss<a class="header-link" href="#41-contrastive-loss" title="Permanent link">&para;</a></h3>
<p><strong>Formula:</strong></p>
<div class="highlight"><pre><span></span><code>L = (1/2) <span class="gs">* [y *</span> d¬≤ + (1-y) * max(0, m - d)¬≤]

where:
<span class="k">-</span> d = ||f(x‚ÇÅ) - f(x‚ÇÇ)||‚ÇÇ (Euclidean distance)
<span class="k">-</span> y = 1 if similar, 0 if dissimilar
<span class="k">-</span> m is margin (e.g., 1.0)
</code></pre></div>

<p><strong>Use Case:</strong>
- Siamese networks
- Face verification
- Signature verification</p>
<p><strong>PyTorch Implementation:</strong></p>
<div class="highlight"><pre><span></span><code><span class="k">class</span><span class="w"> </span><span class="nc">ContrastiveLoss</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="k">def</span><span class="w"> </span><span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">margin</span><span class="o">=</span><span class="mf">1.0</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">margin</span> <span class="o">=</span> <span class="n">margin</span>

    <span class="k">def</span><span class="w"> </span><span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">output1</span><span class="p">,</span> <span class="n">output2</span><span class="p">,</span> <span class="n">label</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Args:</span>
<span class="sd">            output1: (N, embedding_dim) embeddings from first input</span>
<span class="sd">            output2: (N, embedding_dim) embeddings from second input</span>
<span class="sd">            label: (N,) 1 if similar, 0 if dissimilar</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">euclidean_distance</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">functional</span><span class="o">.</span><span class="n">pairwise_distance</span><span class="p">(</span><span class="n">output1</span><span class="p">,</span> <span class="n">output2</span><span class="p">)</span>

        <span class="n">loss_contrastive</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span>
            <span class="n">label</span> <span class="o">*</span> <span class="n">torch</span><span class="o">.</span><span class="n">pow</span><span class="p">(</span><span class="n">euclidean_distance</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span> <span class="o">+</span>
            <span class="p">(</span><span class="mi">1</span> <span class="o">-</span> <span class="n">label</span><span class="p">)</span> <span class="o">*</span> <span class="n">torch</span><span class="o">.</span><span class="n">pow</span><span class="p">(</span>
                <span class="n">torch</span><span class="o">.</span><span class="n">clamp</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">margin</span> <span class="o">-</span> <span class="n">euclidean_distance</span><span class="p">,</span> <span class="nb">min</span><span class="o">=</span><span class="mf">0.0</span><span class="p">),</span> <span class="mi">2</span>
            <span class="p">)</span>
        <span class="p">)</span>
        <span class="k">return</span> <span class="n">loss_contrastive</span>

<span class="c1"># Siamese Network Example</span>
<span class="k">class</span><span class="w"> </span><span class="nc">SiameseNetwork</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="k">def</span><span class="w"> </span><span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">input_dim</span><span class="p">,</span> <span class="n">embedding_dim</span><span class="o">=</span><span class="mi">128</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">fc</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Sequential</span><span class="p">(</span>
            <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">input_dim</span><span class="p">,</span> <span class="mi">256</span><span class="p">),</span>
            <span class="n">nn</span><span class="o">.</span><span class="n">ReLU</span><span class="p">(),</span>
            <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="mi">256</span><span class="p">,</span> <span class="n">embedding_dim</span><span class="p">)</span>
        <span class="p">)</span>

    <span class="k">def</span><span class="w"> </span><span class="nf">forward_one</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">fc</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>

    <span class="k">def</span><span class="w"> </span><span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x1</span><span class="p">,</span> <span class="n">x2</span><span class="p">):</span>
        <span class="n">out1</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">forward_one</span><span class="p">(</span><span class="n">x1</span><span class="p">)</span>
        <span class="n">out2</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">forward_one</span><span class="p">(</span><span class="n">x2</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">out1</span><span class="p">,</span> <span class="n">out2</span>

<span class="c1"># Training</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">SiameseNetwork</span><span class="p">(</span><span class="n">input_dim</span><span class="o">=</span><span class="mi">784</span><span class="p">,</span> <span class="n">embedding_dim</span><span class="o">=</span><span class="mi">128</span><span class="p">)</span>
<span class="n">criterion</span> <span class="o">=</span> <span class="n">ContrastiveLoss</span><span class="p">(</span><span class="n">margin</span><span class="o">=</span><span class="mf">1.0</span><span class="p">)</span>
<span class="n">optimizer</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">optim</span><span class="o">.</span><span class="n">Adam</span><span class="p">(</span><span class="n">model</span><span class="o">.</span><span class="n">parameters</span><span class="p">(),</span> <span class="n">lr</span><span class="o">=</span><span class="mf">0.001</span><span class="p">)</span>

<span class="c1"># Dummy data</span>
<span class="n">x1</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">32</span><span class="p">,</span> <span class="mi">784</span><span class="p">)</span>
<span class="n">x2</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">32</span><span class="p">,</span> <span class="mi">784</span><span class="p">)</span>
<span class="n">labels</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randint</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="p">(</span><span class="mi">32</span><span class="p">,))</span><span class="o">.</span><span class="n">float</span><span class="p">()</span>  <span class="c1"># 1=similar, 0=dissimilar</span>

<span class="c1"># Training step</span>
<span class="n">optimizer</span><span class="o">.</span><span class="n">zero_grad</span><span class="p">()</span>
<span class="n">output1</span><span class="p">,</span> <span class="n">output2</span> <span class="o">=</span> <span class="n">model</span><span class="p">(</span><span class="n">x1</span><span class="p">,</span> <span class="n">x2</span><span class="p">)</span>
<span class="n">loss</span> <span class="o">=</span> <span class="n">criterion</span><span class="p">(</span><span class="n">output1</span><span class="p">,</span> <span class="n">output2</span><span class="p">,</span> <span class="n">labels</span><span class="p">)</span>
<span class="n">loss</span><span class="o">.</span><span class="n">backward</span><span class="p">()</span>
<span class="n">optimizer</span><span class="o">.</span><span class="n">step</span><span class="p">()</span>

<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Contrastive Loss: </span><span class="si">{</span><span class="n">loss</span><span class="o">.</span><span class="n">item</span><span class="p">()</span><span class="si">:</span><span class="s2">.4f</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
</code></pre></div>

<h3 id="42-triplet-loss">4.2 Triplet Loss<a class="header-link" href="#42-triplet-loss" title="Permanent link">&para;</a></h3>
<p><strong>Formula:</strong></p>
<div class="highlight"><pre><span></span><code>L = max(0, d(a, p) - d(a, n) + margin)

where:
- a: anchor
- p: positive (same class as anchor)
- n: negative (different class)
- d(x, y) = ||f(x) - f(y)||‚ÇÇ
</code></pre></div>

<p><strong>Mining Strategies:</strong>
- <strong>Hard negatives</strong>: max d(a, p) - d(a, n)
- <strong>Semi-hard negatives</strong>: d(a, p) &lt; d(a, n) &lt; d(a, p) + margin
- <strong>Batch-all</strong>: All valid triplets in batch</p>
<p><strong>PyTorch Implementation:</strong></p>
<div class="highlight"><pre><span></span><code><span class="k">class</span><span class="w"> </span><span class="nc">TripletLoss</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="k">def</span><span class="w"> </span><span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">margin</span><span class="o">=</span><span class="mf">1.0</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">margin</span> <span class="o">=</span> <span class="n">margin</span>

    <span class="k">def</span><span class="w"> </span><span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">anchor</span><span class="p">,</span> <span class="n">positive</span><span class="p">,</span> <span class="n">negative</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Args:</span>
<span class="sd">            anchor: (N, embedding_dim)</span>
<span class="sd">            positive: (N, embedding_dim)</span>
<span class="sd">            negative: (N, embedding_dim)</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">distance_positive</span> <span class="o">=</span> <span class="p">(</span><span class="n">anchor</span> <span class="o">-</span> <span class="n">positive</span><span class="p">)</span><span class="o">.</span><span class="n">pow</span><span class="p">(</span><span class="mi">2</span><span class="p">)</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span>
        <span class="n">distance_negative</span> <span class="o">=</span> <span class="p">(</span><span class="n">anchor</span> <span class="o">-</span> <span class="n">negative</span><span class="p">)</span><span class="o">.</span><span class="n">pow</span><span class="p">(</span><span class="mi">2</span><span class="p">)</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span>

        <span class="n">losses</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">relu</span><span class="p">(</span><span class="n">distance_positive</span> <span class="o">-</span> <span class="n">distance_negative</span> <span class="o">+</span> <span class="bp">self</span><span class="o">.</span><span class="n">margin</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">losses</span><span class="o">.</span><span class="n">mean</span><span class="p">()</span>

<span class="c1"># Online Triplet Mining</span>
<span class="k">class</span><span class="w"> </span><span class="nc">OnlineTripletLoss</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="k">def</span><span class="w"> </span><span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">margin</span><span class="o">=</span><span class="mf">1.0</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">margin</span> <span class="o">=</span> <span class="n">margin</span>

    <span class="k">def</span><span class="w"> </span><span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">embeddings</span><span class="p">,</span> <span class="n">labels</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Args:</span>
<span class="sd">            embeddings: (N, embedding_dim)</span>
<span class="sd">            labels: (N,) class labels</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="c1"># Compute pairwise distances</span>
        <span class="n">pairwise_dist</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">cdist</span><span class="p">(</span><span class="n">embeddings</span><span class="p">,</span> <span class="n">embeddings</span><span class="p">,</span> <span class="n">p</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>

        <span class="c1"># For each anchor, get hardest positive and negative</span>
        <span class="n">batch_size</span> <span class="o">=</span> <span class="n">embeddings</span><span class="o">.</span><span class="n">size</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>
        <span class="n">triplet_loss</span> <span class="o">=</span> <span class="mi">0</span>
        <span class="n">num_triplets</span> <span class="o">=</span> <span class="mi">0</span>

        <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">batch_size</span><span class="p">):</span>
            <span class="c1"># Positive mask: same class as anchor</span>
            <span class="n">pos_mask</span> <span class="o">=</span> <span class="n">labels</span> <span class="o">==</span> <span class="n">labels</span><span class="p">[</span><span class="n">i</span><span class="p">]</span>
            <span class="n">pos_mask</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">=</span> <span class="kc">False</span>  <span class="c1"># Exclude anchor itself</span>

            <span class="c1"># Negative mask: different class</span>
            <span class="n">neg_mask</span> <span class="o">=</span> <span class="n">labels</span> <span class="o">!=</span> <span class="n">labels</span><span class="p">[</span><span class="n">i</span><span class="p">]</span>

            <span class="k">if</span> <span class="n">pos_mask</span><span class="o">.</span><span class="n">any</span><span class="p">()</span> <span class="ow">and</span> <span class="n">neg_mask</span><span class="o">.</span><span class="n">any</span><span class="p">():</span>
                <span class="c1"># Hardest positive</span>
                <span class="n">hardest_positive_dist</span> <span class="o">=</span> <span class="n">pairwise_dist</span><span class="p">[</span><span class="n">i</span><span class="p">][</span><span class="n">pos_mask</span><span class="p">]</span><span class="o">.</span><span class="n">max</span><span class="p">()</span>

                <span class="c1"># Hardest negative (closest negative)</span>
                <span class="n">hardest_negative_dist</span> <span class="o">=</span> <span class="n">pairwise_dist</span><span class="p">[</span><span class="n">i</span><span class="p">][</span><span class="n">neg_mask</span><span class="p">]</span><span class="o">.</span><span class="n">min</span><span class="p">()</span>

                <span class="n">loss</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">relu</span><span class="p">(</span>
                    <span class="n">hardest_positive_dist</span> <span class="o">-</span> <span class="n">hardest_negative_dist</span> <span class="o">+</span> <span class="bp">self</span><span class="o">.</span><span class="n">margin</span>
                <span class="p">)</span>
                <span class="n">triplet_loss</span> <span class="o">+=</span> <span class="n">loss</span>
                <span class="n">num_triplets</span> <span class="o">+=</span> <span class="mi">1</span>

        <span class="k">return</span> <span class="n">triplet_loss</span> <span class="o">/</span> <span class="nb">max</span><span class="p">(</span><span class="n">num_triplets</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>

<span class="c1"># Example usage</span>
<span class="n">embedding_dim</span> <span class="o">=</span> <span class="mi">128</span>
<span class="n">criterion</span> <span class="o">=</span> <span class="n">TripletLoss</span><span class="p">(</span><span class="n">margin</span><span class="o">=</span><span class="mf">1.0</span><span class="p">)</span>
<span class="n">online_criterion</span> <span class="o">=</span> <span class="n">OnlineTripletLoss</span><span class="p">(</span><span class="n">margin</span><span class="o">=</span><span class="mf">1.0</span><span class="p">)</span>

<span class="c1"># Method 1: Pre-mined triplets</span>
<span class="n">anchor</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">32</span><span class="p">,</span> <span class="n">embedding_dim</span><span class="p">)</span>
<span class="n">positive</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">32</span><span class="p">,</span> <span class="n">embedding_dim</span><span class="p">)</span>
<span class="n">negative</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">32</span><span class="p">,</span> <span class="n">embedding_dim</span><span class="p">)</span>

<span class="n">loss</span> <span class="o">=</span> <span class="n">criterion</span><span class="p">(</span><span class="n">anchor</span><span class="p">,</span> <span class="n">positive</span><span class="p">,</span> <span class="n">negative</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Triplet Loss: </span><span class="si">{</span><span class="n">loss</span><span class="o">.</span><span class="n">item</span><span class="p">()</span><span class="si">:</span><span class="s2">.4f</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>

<span class="c1"># Method 2: Online mining</span>
<span class="n">embeddings</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">64</span><span class="p">,</span> <span class="n">embedding_dim</span><span class="p">)</span>
<span class="n">labels</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randint</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">10</span><span class="p">,</span> <span class="p">(</span><span class="mi">64</span><span class="p">,))</span>  <span class="c1"># 10 classes</span>

<span class="n">loss_online</span> <span class="o">=</span> <span class="n">online_criterion</span><span class="p">(</span><span class="n">embeddings</span><span class="p">,</span> <span class="n">labels</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Online Triplet Loss: </span><span class="si">{</span><span class="n">loss_online</span><span class="o">.</span><span class="n">item</span><span class="p">()</span><span class="si">:</span><span class="s2">.4f</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
</code></pre></div>

<p><strong>Training Example:</strong></p>
<div class="highlight"><pre><span></span><code><span class="k">class</span><span class="w"> </span><span class="nc">EmbeddingNet</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="k">def</span><span class="w"> </span><span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">input_dim</span><span class="p">,</span> <span class="n">embedding_dim</span><span class="o">=</span><span class="mi">128</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">fc</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Sequential</span><span class="p">(</span>
            <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">input_dim</span><span class="p">,</span> <span class="mi">256</span><span class="p">),</span>
            <span class="n">nn</span><span class="o">.</span><span class="n">ReLU</span><span class="p">(),</span>
            <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="mi">256</span><span class="p">,</span> <span class="mi">128</span><span class="p">),</span>
            <span class="n">nn</span><span class="o">.</span><span class="n">ReLU</span><span class="p">(),</span>
            <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="mi">128</span><span class="p">,</span> <span class="n">embedding_dim</span><span class="p">)</span>
        <span class="p">)</span>

    <span class="k">def</span><span class="w"> </span><span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">fc</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>

<span class="c1"># Training loop</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">EmbeddingNet</span><span class="p">(</span><span class="n">input_dim</span><span class="o">=</span><span class="mi">784</span><span class="p">,</span> <span class="n">embedding_dim</span><span class="o">=</span><span class="mi">128</span><span class="p">)</span>
<span class="n">criterion</span> <span class="o">=</span> <span class="n">OnlineTripletLoss</span><span class="p">(</span><span class="n">margin</span><span class="o">=</span><span class="mf">1.0</span><span class="p">)</span>
<span class="n">optimizer</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">optim</span><span class="o">.</span><span class="n">Adam</span><span class="p">(</span><span class="n">model</span><span class="o">.</span><span class="n">parameters</span><span class="p">(),</span> <span class="n">lr</span><span class="o">=</span><span class="mf">0.001</span><span class="p">)</span>

<span class="k">for</span> <span class="n">epoch</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">10</span><span class="p">):</span>
    <span class="c1"># Simulate batch</span>
    <span class="n">inputs</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">64</span><span class="p">,</span> <span class="mi">784</span><span class="p">)</span>
    <span class="n">labels</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randint</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">10</span><span class="p">,</span> <span class="p">(</span><span class="mi">64</span><span class="p">,))</span>

    <span class="n">optimizer</span><span class="o">.</span><span class="n">zero_grad</span><span class="p">()</span>
    <span class="n">embeddings</span> <span class="o">=</span> <span class="n">model</span><span class="p">(</span><span class="n">inputs</span><span class="p">)</span>
    <span class="n">loss</span> <span class="o">=</span> <span class="n">criterion</span><span class="p">(</span><span class="n">embeddings</span><span class="p">,</span> <span class="n">labels</span><span class="p">)</span>
    <span class="n">loss</span><span class="o">.</span><span class="n">backward</span><span class="p">()</span>
    <span class="n">optimizer</span><span class="o">.</span><span class="n">step</span><span class="p">()</span>

    <span class="k">if</span> <span class="n">epoch</span> <span class="o">%</span> <span class="mi">2</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
        <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Epoch </span><span class="si">{</span><span class="n">epoch</span><span class="si">}</span><span class="s2">, Loss: </span><span class="si">{</span><span class="n">loss</span><span class="o">.</span><span class="n">item</span><span class="p">()</span><span class="si">:</span><span class="s2">.4f</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
</code></pre></div>

<p><strong>When to Use:</strong>
- Face recognition (FaceNet)
- Person re-identification
- Image retrieval</p>
<h3 id="43-infonce-nt-xent-loss">4.3 InfoNCE / NT-Xent Loss<a class="header-link" href="#43-infonce-nt-xent-loss" title="Permanent link">&para;</a></h3>
<p><strong>Formula:</strong></p>
<div class="highlight"><pre><span></span><code>L = -log [exp(sim(z_i, z_j)/œÑ) / Œ£‚Çñ exp(sim(z_i, z_k)/œÑ)]

where:
<span class="k">-</span> z_i, z_j are positive pair embeddings
<span class="k">-</span> œÑ is temperature parameter (e.g., 0.07)
<span class="k">-</span> sim(u, v) = u¬∑v / (||u|| ||v||) (cosine similarity)
</code></pre></div>

<p><strong>Use Case:</strong>
- Self-supervised learning (SimCLR, MoCo)
- Contrastive language-image pre-training (CLIP)</p>
<p><strong>PyTorch Implementation:</strong></p>
<div class="highlight"><pre><span></span><code><span class="k">class</span><span class="w"> </span><span class="nc">InfoNCELoss</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="k">def</span><span class="w"> </span><span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">temperature</span><span class="o">=</span><span class="mf">0.07</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">temperature</span> <span class="o">=</span> <span class="n">temperature</span>

    <span class="k">def</span><span class="w"> </span><span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">z_i</span><span class="p">,</span> <span class="n">z_j</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Args:</span>
<span class="sd">            z_i: (N, embedding_dim) embeddings of view 1</span>
<span class="sd">            z_j: (N, embedding_dim) embeddings of view 2</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">batch_size</span> <span class="o">=</span> <span class="n">z_i</span><span class="o">.</span><span class="n">size</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>

        <span class="c1"># Normalize embeddings</span>
        <span class="n">z_i</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">functional</span><span class="o">.</span><span class="n">normalize</span><span class="p">(</span><span class="n">z_i</span><span class="p">,</span> <span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
        <span class="n">z_j</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">functional</span><span class="o">.</span><span class="n">normalize</span><span class="p">(</span><span class="n">z_j</span><span class="p">,</span> <span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>

        <span class="c1"># Compute similarity matrix</span>
        <span class="n">representations</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">cat</span><span class="p">([</span><span class="n">z_i</span><span class="p">,</span> <span class="n">z_j</span><span class="p">],</span> <span class="n">dim</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>  <span class="c1"># (2N, dim)</span>
        <span class="n">similarity_matrix</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">mm</span><span class="p">(</span><span class="n">representations</span><span class="p">,</span> <span class="n">representations</span><span class="o">.</span><span class="n">T</span><span class="p">)</span>  <span class="c1"># (2N, 2N)</span>

        <span class="c1"># Create labels: positive pairs are at (i, N+i) and (N+i, i)</span>
        <span class="n">labels</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">cat</span><span class="p">([</span>
            <span class="n">torch</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="n">batch_size</span><span class="p">)</span> <span class="o">+</span> <span class="n">batch_size</span><span class="p">,</span>
            <span class="n">torch</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="n">batch_size</span><span class="p">)</span>
        <span class="p">],</span> <span class="n">dim</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">z_i</span><span class="o">.</span><span class="n">device</span><span class="p">)</span>

        <span class="c1"># Mask to remove self-similarity</span>
        <span class="n">mask</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">eye</span><span class="p">(</span><span class="mi">2</span> <span class="o">*</span> <span class="n">batch_size</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">bool</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="n">z_i</span><span class="o">.</span><span class="n">device</span><span class="p">)</span>
        <span class="n">similarity_matrix</span> <span class="o">=</span> <span class="n">similarity_matrix</span><span class="p">[</span><span class="o">~</span><span class="n">mask</span><span class="p">]</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="mi">2</span> <span class="o">*</span> <span class="n">batch_size</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">)</span>

        <span class="c1"># Compute loss</span>
        <span class="n">similarity_matrix</span> <span class="o">=</span> <span class="n">similarity_matrix</span> <span class="o">/</span> <span class="bp">self</span><span class="o">.</span><span class="n">temperature</span>
        <span class="n">loss</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">functional</span><span class="o">.</span><span class="n">cross_entropy</span><span class="p">(</span><span class="n">similarity_matrix</span><span class="p">,</span> <span class="n">labels</span><span class="p">)</span>

        <span class="k">return</span> <span class="n">loss</span>

<span class="c1"># Simplified NT-Xent (for SimCLR)</span>
<span class="k">class</span><span class="w"> </span><span class="nc">NTXentLoss</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="k">def</span><span class="w"> </span><span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">temperature</span><span class="o">=</span><span class="mf">0.5</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">temperature</span> <span class="o">=</span> <span class="n">temperature</span>

    <span class="k">def</span><span class="w"> </span><span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">z_i</span><span class="p">,</span> <span class="n">z_j</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Simplified version&quot;&quot;&quot;</span>
        <span class="n">batch_size</span> <span class="o">=</span> <span class="n">z_i</span><span class="o">.</span><span class="n">size</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>

        <span class="c1"># L2 normalize</span>
        <span class="n">z_i</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">functional</span><span class="o">.</span><span class="n">normalize</span><span class="p">(</span><span class="n">z_i</span><span class="p">,</span> <span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
        <span class="n">z_j</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">functional</span><span class="o">.</span><span class="n">normalize</span><span class="p">(</span><span class="n">z_j</span><span class="p">,</span> <span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>

        <span class="c1"># Positive similarity</span>
        <span class="n">pos_sim</span> <span class="o">=</span> <span class="p">(</span><span class="n">z_i</span> <span class="o">*</span> <span class="n">z_j</span><span class="p">)</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span> <span class="o">/</span> <span class="bp">self</span><span class="o">.</span><span class="n">temperature</span>  <span class="c1"># (N,)</span>

        <span class="c1"># All similarities</span>
        <span class="n">z</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">cat</span><span class="p">([</span><span class="n">z_i</span><span class="p">,</span> <span class="n">z_j</span><span class="p">],</span> <span class="n">dim</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>  <span class="c1"># (2N, dim)</span>
        <span class="n">sim_matrix</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">mm</span><span class="p">(</span><span class="n">z</span><span class="p">,</span> <span class="n">z</span><span class="o">.</span><span class="n">T</span><span class="p">)</span> <span class="o">/</span> <span class="bp">self</span><span class="o">.</span><span class="n">temperature</span>  <span class="c1"># (2N, 2N)</span>

        <span class="c1"># Remove diagonal</span>
        <span class="n">sim_matrix</span><span class="o">.</span><span class="n">fill_diagonal_</span><span class="p">(</span><span class="o">-</span><span class="nb">float</span><span class="p">(</span><span class="s1">&#39;inf&#39;</span><span class="p">))</span>

        <span class="c1"># Compute loss for i -&gt; j</span>
        <span class="n">pos_sim_expanded</span> <span class="o">=</span> <span class="n">pos_sim</span><span class="o">.</span><span class="n">unsqueeze</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span>  <span class="c1"># (N, 1)</span>
        <span class="n">negatives_i</span> <span class="o">=</span> <span class="n">sim_matrix</span><span class="p">[:</span><span class="n">batch_size</span><span class="p">]</span>  <span class="c1"># (N, 2N)</span>
        <span class="n">logits_i</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">cat</span><span class="p">([</span><span class="n">pos_sim_expanded</span><span class="p">,</span> <span class="n">negatives_i</span><span class="p">],</span> <span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>  <span class="c1"># (N, 2N+1)</span>
        <span class="n">labels</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">batch_size</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">long</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="n">z_i</span><span class="o">.</span><span class="n">device</span><span class="p">)</span>

        <span class="n">loss</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">functional</span><span class="o">.</span><span class="n">cross_entropy</span><span class="p">(</span><span class="n">logits_i</span><span class="p">,</span> <span class="n">labels</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">loss</span>

<span class="c1"># Example usage</span>
<span class="n">criterion</span> <span class="o">=</span> <span class="n">InfoNCELoss</span><span class="p">(</span><span class="n">temperature</span><span class="o">=</span><span class="mf">0.07</span><span class="p">)</span>

<span class="c1"># Simulate augmented views</span>
<span class="n">z_i</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">128</span><span class="p">,</span> <span class="mi">256</span><span class="p">)</span>  <span class="c1"># View 1 embeddings</span>
<span class="n">z_j</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">128</span><span class="p">,</span> <span class="mi">256</span><span class="p">)</span>  <span class="c1"># View 2 embeddings</span>

<span class="n">loss</span> <span class="o">=</span> <span class="n">criterion</span><span class="p">(</span><span class="n">z_i</span><span class="p">,</span> <span class="n">z_j</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;InfoNCE Loss: </span><span class="si">{</span><span class="n">loss</span><span class="o">.</span><span class="n">item</span><span class="p">()</span><span class="si">:</span><span class="s2">.4f</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
</code></pre></div>

<p><strong>When to Use:</strong>
- Self-supervised pre-training (SimCLR)
- Vision-language models (CLIP)
- Contrastive learning</p>
<hr />
<h2 id="5-segmentation-and-detection-losses">5. Segmentation and Detection Losses<a class="header-link" href="#5-segmentation-and-detection-losses" title="Permanent link">&para;</a></h2>
<h3 id="51-dice-loss">5.1 Dice Loss<a class="header-link" href="#51-dice-loss" title="Permanent link">&para;</a></h3>
<p><strong>Formula:</strong></p>
<div class="highlight"><pre><span></span><code>Dice Loss = 1 - (2 * |X ‚à© Y|) / (|X| + |Y|)

where:
<span class="k">-</span> X is predicted segmentation
<span class="k">-</span> Y is ground truth
</code></pre></div>

<p><strong>Properties:</strong>
- Handles class imbalance (background vs foreground)
- Differentiable approximation of IoU
- Range: [0, 1]</p>
<p><strong>PyTorch Implementation:</strong></p>
<div class="highlight"><pre><span></span><code><span class="k">class</span><span class="w"> </span><span class="nc">DiceLoss</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="k">def</span><span class="w"> </span><span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">smooth</span><span class="o">=</span><span class="mf">1.0</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Args:</span>
<span class="sd">            smooth: Smoothing constant to avoid division by zero</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">smooth</span> <span class="o">=</span> <span class="n">smooth</span>

    <span class="k">def</span><span class="w"> </span><span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">pred</span><span class="p">,</span> <span class="n">target</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Args:</span>
<span class="sd">            pred: (N, C, H, W) predicted probabilities</span>
<span class="sd">            target: (N, C, H, W) one-hot encoded ground truth</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">pred_flat</span> <span class="o">=</span> <span class="n">pred</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">)</span>
        <span class="n">target_flat</span> <span class="o">=</span> <span class="n">target</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">)</span>

        <span class="n">intersection</span> <span class="o">=</span> <span class="p">(</span><span class="n">pred_flat</span> <span class="o">*</span> <span class="n">target_flat</span><span class="p">)</span><span class="o">.</span><span class="n">sum</span><span class="p">()</span>
        <span class="n">dice_score</span> <span class="o">=</span> <span class="p">(</span><span class="mf">2.</span> <span class="o">*</span> <span class="n">intersection</span> <span class="o">+</span> <span class="bp">self</span><span class="o">.</span><span class="n">smooth</span><span class="p">)</span> <span class="o">/</span> <span class="p">(</span>
            <span class="n">pred_flat</span><span class="o">.</span><span class="n">sum</span><span class="p">()</span> <span class="o">+</span> <span class="n">target_flat</span><span class="o">.</span><span class="n">sum</span><span class="p">()</span> <span class="o">+</span> <span class="bp">self</span><span class="o">.</span><span class="n">smooth</span>
        <span class="p">)</span>

        <span class="k">return</span> <span class="mi">1</span> <span class="o">-</span> <span class="n">dice_score</span>

<span class="c1"># Multi-class Dice Loss</span>
<span class="k">class</span><span class="w"> </span><span class="nc">MultiClassDiceLoss</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="k">def</span><span class="w"> </span><span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">smooth</span><span class="o">=</span><span class="mf">1.0</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">smooth</span> <span class="o">=</span> <span class="n">smooth</span>

    <span class="k">def</span><span class="w"> </span><span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">pred</span><span class="p">,</span> <span class="n">target</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Args:</span>
<span class="sd">            pred: (N, C, H, W) logits</span>
<span class="sd">            target: (N, H, W) class indices</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">num_classes</span> <span class="o">=</span> <span class="n">pred</span><span class="o">.</span><span class="n">size</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span>
        <span class="n">pred_softmax</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">softmax</span><span class="p">(</span><span class="n">pred</span><span class="p">,</span> <span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>

        <span class="c1"># Convert target to one-hot</span>
        <span class="n">target_one_hot</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">functional</span><span class="o">.</span><span class="n">one_hot</span><span class="p">(</span><span class="n">target</span><span class="p">,</span> <span class="n">num_classes</span><span class="p">)</span>
        <span class="n">target_one_hot</span> <span class="o">=</span> <span class="n">target_one_hot</span><span class="o">.</span><span class="n">permute</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span><span class="o">.</span><span class="n">float</span><span class="p">()</span>

        <span class="n">total_loss</span> <span class="o">=</span> <span class="mi">0</span>
        <span class="k">for</span> <span class="n">c</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">num_classes</span><span class="p">):</span>
            <span class="n">pred_c</span> <span class="o">=</span> <span class="n">pred_softmax</span><span class="p">[:,</span> <span class="n">c</span><span class="p">]</span>
            <span class="n">target_c</span> <span class="o">=</span> <span class="n">target_one_hot</span><span class="p">[:,</span> <span class="n">c</span><span class="p">]</span>

            <span class="n">intersection</span> <span class="o">=</span> <span class="p">(</span><span class="n">pred_c</span> <span class="o">*</span> <span class="n">target_c</span><span class="p">)</span><span class="o">.</span><span class="n">sum</span><span class="p">()</span>
            <span class="n">dice</span> <span class="o">=</span> <span class="p">(</span><span class="mf">2.</span> <span class="o">*</span> <span class="n">intersection</span> <span class="o">+</span> <span class="bp">self</span><span class="o">.</span><span class="n">smooth</span><span class="p">)</span> <span class="o">/</span> <span class="p">(</span>
                <span class="n">pred_c</span><span class="o">.</span><span class="n">sum</span><span class="p">()</span> <span class="o">+</span> <span class="n">target_c</span><span class="o">.</span><span class="n">sum</span><span class="p">()</span> <span class="o">+</span> <span class="bp">self</span><span class="o">.</span><span class="n">smooth</span>
            <span class="p">)</span>
            <span class="n">total_loss</span> <span class="o">+=</span> <span class="mi">1</span> <span class="o">-</span> <span class="n">dice</span>

        <span class="k">return</span> <span class="n">total_loss</span> <span class="o">/</span> <span class="n">num_classes</span>

<span class="c1"># Example usage</span>
<span class="n">batch_size</span><span class="p">,</span> <span class="n">num_classes</span><span class="p">,</span> <span class="n">H</span><span class="p">,</span> <span class="n">W</span> <span class="o">=</span> <span class="mi">4</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">256</span><span class="p">,</span> <span class="mi">256</span>

<span class="c1"># Binary segmentation</span>
<span class="n">pred_binary</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">sigmoid</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="n">batch_size</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="n">H</span><span class="p">,</span> <span class="n">W</span><span class="p">))</span>
<span class="n">target_binary</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randint</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="p">(</span><span class="n">batch_size</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="n">H</span><span class="p">,</span> <span class="n">W</span><span class="p">))</span><span class="o">.</span><span class="n">float</span><span class="p">()</span>

<span class="n">dice_loss</span> <span class="o">=</span> <span class="n">DiceLoss</span><span class="p">()</span>
<span class="n">loss_binary</span> <span class="o">=</span> <span class="n">dice_loss</span><span class="p">(</span><span class="n">pred_binary</span><span class="p">,</span> <span class="n">target_binary</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Binary Dice Loss: </span><span class="si">{</span><span class="n">loss_binary</span><span class="o">.</span><span class="n">item</span><span class="p">()</span><span class="si">:</span><span class="s2">.4f</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>

<span class="c1"># Multi-class segmentation</span>
<span class="n">pred_multi</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="n">batch_size</span><span class="p">,</span> <span class="n">num_classes</span><span class="p">,</span> <span class="n">H</span><span class="p">,</span> <span class="n">W</span><span class="p">)</span>
<span class="n">target_multi</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randint</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">num_classes</span><span class="p">,</span> <span class="p">(</span><span class="n">batch_size</span><span class="p">,</span> <span class="n">H</span><span class="p">,</span> <span class="n">W</span><span class="p">))</span>

<span class="n">dice_multi</span> <span class="o">=</span> <span class="n">MultiClassDiceLoss</span><span class="p">()</span>
<span class="n">loss_multi</span> <span class="o">=</span> <span class="n">dice_multi</span><span class="p">(</span><span class="n">pred_multi</span><span class="p">,</span> <span class="n">target_multi</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Multi-class Dice Loss: </span><span class="si">{</span><span class="n">loss_multi</span><span class="o">.</span><span class="n">item</span><span class="p">()</span><span class="si">:</span><span class="s2">.4f</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
</code></pre></div>

<h3 id="52-iou-loss-giou-loss">5.2 IoU Loss / GIoU Loss<a class="header-link" href="#52-iou-loss-giou-loss" title="Permanent link">&para;</a></h3>
<p><strong>IoU (Intersection over Union):</strong></p>
<div class="highlight"><pre><span></span><code>IoU = Area(Intersection) / Area(Union)
</code></pre></div>

<p><strong>GIoU (Generalized IoU):</strong></p>
<div class="highlight"><pre><span></span><code>GIoU = IoU - |C \ (A ‚à™ B)| / |C|

where C is smallest box enclosing A and B
</code></pre></div>

<p><strong>PyTorch Implementation:</strong></p>
<div class="highlight"><pre><span></span><code><span class="k">def</span><span class="w"> </span><span class="nf">iou_loss</span><span class="p">(</span><span class="n">pred_boxes</span><span class="p">,</span> <span class="n">target_boxes</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Args:</span>
<span class="sd">        pred_boxes: (N, 4) [x1, y1, x2, y2]</span>
<span class="sd">        target_boxes: (N, 4) [x1, y1, x2, y2]</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="c1"># Intersection coordinates</span>
    <span class="n">x1_inter</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">max</span><span class="p">(</span><span class="n">pred_boxes</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">],</span> <span class="n">target_boxes</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">])</span>
    <span class="n">y1_inter</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">max</span><span class="p">(</span><span class="n">pred_boxes</span><span class="p">[:,</span> <span class="mi">1</span><span class="p">],</span> <span class="n">target_boxes</span><span class="p">[:,</span> <span class="mi">1</span><span class="p">])</span>
    <span class="n">x2_inter</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">min</span><span class="p">(</span><span class="n">pred_boxes</span><span class="p">[:,</span> <span class="mi">2</span><span class="p">],</span> <span class="n">target_boxes</span><span class="p">[:,</span> <span class="mi">2</span><span class="p">])</span>
    <span class="n">y2_inter</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">min</span><span class="p">(</span><span class="n">pred_boxes</span><span class="p">[:,</span> <span class="mi">3</span><span class="p">],</span> <span class="n">target_boxes</span><span class="p">[:,</span> <span class="mi">3</span><span class="p">])</span>

    <span class="c1"># Intersection area</span>
    <span class="n">inter_area</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">clamp</span><span class="p">(</span><span class="n">x2_inter</span> <span class="o">-</span> <span class="n">x1_inter</span><span class="p">,</span> <span class="nb">min</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span> <span class="o">*</span> \
                 <span class="n">torch</span><span class="o">.</span><span class="n">clamp</span><span class="p">(</span><span class="n">y2_inter</span> <span class="o">-</span> <span class="n">y1_inter</span><span class="p">,</span> <span class="nb">min</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>

    <span class="c1"># Union area</span>
    <span class="n">pred_area</span> <span class="o">=</span> <span class="p">(</span><span class="n">pred_boxes</span><span class="p">[:,</span> <span class="mi">2</span><span class="p">]</span> <span class="o">-</span> <span class="n">pred_boxes</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">])</span> <span class="o">*</span> \
                <span class="p">(</span><span class="n">pred_boxes</span><span class="p">[:,</span> <span class="mi">3</span><span class="p">]</span> <span class="o">-</span> <span class="n">pred_boxes</span><span class="p">[:,</span> <span class="mi">1</span><span class="p">])</span>
    <span class="n">target_area</span> <span class="o">=</span> <span class="p">(</span><span class="n">target_boxes</span><span class="p">[:,</span> <span class="mi">2</span><span class="p">]</span> <span class="o">-</span> <span class="n">target_boxes</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">])</span> <span class="o">*</span> \
                  <span class="p">(</span><span class="n">target_boxes</span><span class="p">[:,</span> <span class="mi">3</span><span class="p">]</span> <span class="o">-</span> <span class="n">target_boxes</span><span class="p">[:,</span> <span class="mi">1</span><span class="p">])</span>
    <span class="n">union_area</span> <span class="o">=</span> <span class="n">pred_area</span> <span class="o">+</span> <span class="n">target_area</span> <span class="o">-</span> <span class="n">inter_area</span>

    <span class="c1"># IoU</span>
    <span class="n">iou</span> <span class="o">=</span> <span class="n">inter_area</span> <span class="o">/</span> <span class="p">(</span><span class="n">union_area</span> <span class="o">+</span> <span class="mf">1e-6</span><span class="p">)</span>

    <span class="k">return</span> <span class="mi">1</span> <span class="o">-</span> <span class="n">iou</span><span class="o">.</span><span class="n">mean</span><span class="p">()</span>

<span class="k">def</span><span class="w"> </span><span class="nf">giou_loss</span><span class="p">(</span><span class="n">pred_boxes</span><span class="p">,</span> <span class="n">target_boxes</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Generalized IoU Loss&quot;&quot;&quot;</span>
    <span class="c1"># Intersection</span>
    <span class="n">x1_inter</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">max</span><span class="p">(</span><span class="n">pred_boxes</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">],</span> <span class="n">target_boxes</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">])</span>
    <span class="n">y1_inter</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">max</span><span class="p">(</span><span class="n">pred_boxes</span><span class="p">[:,</span> <span class="mi">1</span><span class="p">],</span> <span class="n">target_boxes</span><span class="p">[:,</span> <span class="mi">1</span><span class="p">])</span>
    <span class="n">x2_inter</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">min</span><span class="p">(</span><span class="n">pred_boxes</span><span class="p">[:,</span> <span class="mi">2</span><span class="p">],</span> <span class="n">target_boxes</span><span class="p">[:,</span> <span class="mi">2</span><span class="p">])</span>
    <span class="n">y2_inter</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">min</span><span class="p">(</span><span class="n">pred_boxes</span><span class="p">[:,</span> <span class="mi">3</span><span class="p">],</span> <span class="n">target_boxes</span><span class="p">[:,</span> <span class="mi">3</span><span class="p">])</span>

    <span class="n">inter_area</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">clamp</span><span class="p">(</span><span class="n">x2_inter</span> <span class="o">-</span> <span class="n">x1_inter</span><span class="p">,</span> <span class="nb">min</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span> <span class="o">*</span> \
                 <span class="n">torch</span><span class="o">.</span><span class="n">clamp</span><span class="p">(</span><span class="n">y2_inter</span> <span class="o">-</span> <span class="n">y1_inter</span><span class="p">,</span> <span class="nb">min</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>

    <span class="c1"># Union</span>
    <span class="n">pred_area</span> <span class="o">=</span> <span class="p">(</span><span class="n">pred_boxes</span><span class="p">[:,</span> <span class="mi">2</span><span class="p">]</span> <span class="o">-</span> <span class="n">pred_boxes</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">])</span> <span class="o">*</span> \
                <span class="p">(</span><span class="n">pred_boxes</span><span class="p">[:,</span> <span class="mi">3</span><span class="p">]</span> <span class="o">-</span> <span class="n">pred_boxes</span><span class="p">[:,</span> <span class="mi">1</span><span class="p">])</span>
    <span class="n">target_area</span> <span class="o">=</span> <span class="p">(</span><span class="n">target_boxes</span><span class="p">[:,</span> <span class="mi">2</span><span class="p">]</span> <span class="o">-</span> <span class="n">target_boxes</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">])</span> <span class="o">*</span> \
                  <span class="p">(</span><span class="n">target_boxes</span><span class="p">[:,</span> <span class="mi">3</span><span class="p">]</span> <span class="o">-</span> <span class="n">target_boxes</span><span class="p">[:,</span> <span class="mi">1</span><span class="p">])</span>
    <span class="n">union_area</span> <span class="o">=</span> <span class="n">pred_area</span> <span class="o">+</span> <span class="n">target_area</span> <span class="o">-</span> <span class="n">inter_area</span>

    <span class="c1"># IoU</span>
    <span class="n">iou</span> <span class="o">=</span> <span class="n">inter_area</span> <span class="o">/</span> <span class="p">(</span><span class="n">union_area</span> <span class="o">+</span> <span class="mf">1e-6</span><span class="p">)</span>

    <span class="c1"># Enclosing box</span>
    <span class="n">x1_c</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">min</span><span class="p">(</span><span class="n">pred_boxes</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">],</span> <span class="n">target_boxes</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">])</span>
    <span class="n">y1_c</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">min</span><span class="p">(</span><span class="n">pred_boxes</span><span class="p">[:,</span> <span class="mi">1</span><span class="p">],</span> <span class="n">target_boxes</span><span class="p">[:,</span> <span class="mi">1</span><span class="p">])</span>
    <span class="n">x2_c</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">max</span><span class="p">(</span><span class="n">pred_boxes</span><span class="p">[:,</span> <span class="mi">2</span><span class="p">],</span> <span class="n">target_boxes</span><span class="p">[:,</span> <span class="mi">2</span><span class="p">])</span>
    <span class="n">y2_c</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">max</span><span class="p">(</span><span class="n">pred_boxes</span><span class="p">[:,</span> <span class="mi">3</span><span class="p">],</span> <span class="n">target_boxes</span><span class="p">[:,</span> <span class="mi">3</span><span class="p">])</span>

    <span class="n">enclosing_area</span> <span class="o">=</span> <span class="p">(</span><span class="n">x2_c</span> <span class="o">-</span> <span class="n">x1_c</span><span class="p">)</span> <span class="o">*</span> <span class="p">(</span><span class="n">y2_c</span> <span class="o">-</span> <span class="n">y1_c</span><span class="p">)</span>

    <span class="c1"># GIoU</span>
    <span class="n">giou</span> <span class="o">=</span> <span class="n">iou</span> <span class="o">-</span> <span class="p">(</span><span class="n">enclosing_area</span> <span class="o">-</span> <span class="n">union_area</span><span class="p">)</span> <span class="o">/</span> <span class="p">(</span><span class="n">enclosing_area</span> <span class="o">+</span> <span class="mf">1e-6</span><span class="p">)</span>

    <span class="k">return</span> <span class="mi">1</span> <span class="o">-</span> <span class="n">giou</span><span class="o">.</span><span class="n">mean</span><span class="p">()</span>

<span class="c1"># Example usage</span>
<span class="n">pred_boxes</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([</span>
    <span class="p">[</span><span class="mi">10</span><span class="p">,</span> <span class="mi">10</span><span class="p">,</span> <span class="mi">50</span><span class="p">,</span> <span class="mi">50</span><span class="p">],</span>
    <span class="p">[</span><span class="mi">20</span><span class="p">,</span> <span class="mi">20</span><span class="p">,</span> <span class="mi">60</span><span class="p">,</span> <span class="mi">60</span><span class="p">]</span>
<span class="p">])</span><span class="o">.</span><span class="n">float</span><span class="p">()</span>

<span class="n">target_boxes</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([</span>
    <span class="p">[</span><span class="mi">15</span><span class="p">,</span> <span class="mi">15</span><span class="p">,</span> <span class="mi">55</span><span class="p">,</span> <span class="mi">55</span><span class="p">],</span>
    <span class="p">[</span><span class="mi">25</span><span class="p">,</span> <span class="mi">25</span><span class="p">,</span> <span class="mi">65</span><span class="p">,</span> <span class="mi">65</span><span class="p">]</span>
<span class="p">])</span><span class="o">.</span><span class="n">float</span><span class="p">()</span>

<span class="n">loss_iou</span> <span class="o">=</span> <span class="n">iou_loss</span><span class="p">(</span><span class="n">pred_boxes</span><span class="p">,</span> <span class="n">target_boxes</span><span class="p">)</span>
<span class="n">loss_giou</span> <span class="o">=</span> <span class="n">giou_loss</span><span class="p">(</span><span class="n">pred_boxes</span><span class="p">,</span> <span class="n">target_boxes</span><span class="p">)</span>

<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;IoU Loss: </span><span class="si">{</span><span class="n">loss_iou</span><span class="o">.</span><span class="n">item</span><span class="p">()</span><span class="si">:</span><span class="s2">.4f</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;GIoU Loss: </span><span class="si">{</span><span class="n">loss_giou</span><span class="o">.</span><span class="n">item</span><span class="p">()</span><span class="si">:</span><span class="s2">.4f</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
</code></pre></div>

<h3 id="53-combined-losses">5.3 Combined Losses<a class="header-link" href="#53-combined-losses" title="Permanent link">&para;</a></h3>
<p><strong>Example: CE + Dice for Segmentation:</strong></p>
<div class="highlight"><pre><span></span><code><span class="k">class</span><span class="w"> </span><span class="nc">CombinedLoss</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="k">def</span><span class="w"> </span><span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">ce_weight</span><span class="o">=</span><span class="mf">0.5</span><span class="p">,</span> <span class="n">dice_weight</span><span class="o">=</span><span class="mf">0.5</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">ce_weight</span> <span class="o">=</span> <span class="n">ce_weight</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">dice_weight</span> <span class="o">=</span> <span class="n">dice_weight</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">ce</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">CrossEntropyLoss</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">dice</span> <span class="o">=</span> <span class="n">MultiClassDiceLoss</span><span class="p">()</span>

    <span class="k">def</span><span class="w"> </span><span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">pred</span><span class="p">,</span> <span class="n">target</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Args:</span>
<span class="sd">            pred: (N, C, H, W) logits</span>
<span class="sd">            target: (N, H, W) class indices</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">ce_loss</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">ce</span><span class="p">(</span><span class="n">pred</span><span class="p">,</span> <span class="n">target</span><span class="p">)</span>
        <span class="n">dice_loss</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">dice</span><span class="p">(</span><span class="n">pred</span><span class="p">,</span> <span class="n">target</span><span class="p">)</span>

        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">ce_weight</span> <span class="o">*</span> <span class="n">ce_loss</span> <span class="o">+</span> <span class="bp">self</span><span class="o">.</span><span class="n">dice_weight</span> <span class="o">*</span> <span class="n">dice_loss</span>

<span class="c1"># Example</span>
<span class="n">criterion</span> <span class="o">=</span> <span class="n">CombinedLoss</span><span class="p">(</span><span class="n">ce_weight</span><span class="o">=</span><span class="mf">0.5</span><span class="p">,</span> <span class="n">dice_weight</span><span class="o">=</span><span class="mf">0.5</span><span class="p">)</span>
<span class="n">pred</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">128</span><span class="p">,</span> <span class="mi">128</span><span class="p">)</span>
<span class="n">target</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randint</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">128</span><span class="p">,</span> <span class="mi">128</span><span class="p">))</span>

<span class="n">loss</span> <span class="o">=</span> <span class="n">criterion</span><span class="p">(</span><span class="n">pred</span><span class="p">,</span> <span class="n">target</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Combined Loss (CE + Dice): </span><span class="si">{</span><span class="n">loss</span><span class="o">.</span><span class="n">item</span><span class="p">()</span><span class="si">:</span><span class="s2">.4f</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
</code></pre></div>

<hr />
<h2 id="6-generative-model-losses">6. Generative Model Losses<a class="header-link" href="#6-generative-model-losses" title="Permanent link">&para;</a></h2>
<h3 id="61-adversarial-loss-gan">6.1 Adversarial Loss (GAN)<a class="header-link" href="#61-adversarial-loss-gan" title="Permanent link">&para;</a></h3>
<p><strong>Minimax GAN:</strong></p>
<div class="highlight"><pre><span></span><code>min_G max_D V(D,G) = E[log D(x)] + E[log(1 - D(G(z)))]
</code></pre></div>

<p><strong>PyTorch Implementation:</strong></p>
<div class="highlight"><pre><span></span><code><span class="c1"># Standard GAN loss</span>
<span class="k">def</span><span class="w"> </span><span class="nf">gan_loss_discriminator</span><span class="p">(</span><span class="n">real_output</span><span class="p">,</span> <span class="n">fake_output</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Discriminator loss&quot;&quot;&quot;</span>
    <span class="n">real_loss</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">functional</span><span class="o">.</span><span class="n">binary_cross_entropy_with_logits</span><span class="p">(</span>
        <span class="n">real_output</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">ones_like</span><span class="p">(</span><span class="n">real_output</span><span class="p">)</span>
    <span class="p">)</span>
    <span class="n">fake_loss</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">functional</span><span class="o">.</span><span class="n">binary_cross_entropy_with_logits</span><span class="p">(</span>
        <span class="n">fake_output</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">zeros_like</span><span class="p">(</span><span class="n">fake_output</span><span class="p">)</span>
    <span class="p">)</span>
    <span class="k">return</span> <span class="n">real_loss</span> <span class="o">+</span> <span class="n">fake_loss</span>

<span class="k">def</span><span class="w"> </span><span class="nf">gan_loss_generator</span><span class="p">(</span><span class="n">fake_output</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Generator loss (non-saturating)&quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="n">nn</span><span class="o">.</span><span class="n">functional</span><span class="o">.</span><span class="n">binary_cross_entropy_with_logits</span><span class="p">(</span>
        <span class="n">fake_output</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">ones_like</span><span class="p">(</span><span class="n">fake_output</span><span class="p">)</span>
    <span class="p">)</span>

<span class="c1"># Wasserstein GAN loss</span>
<span class="k">def</span><span class="w"> </span><span class="nf">wgan_loss_discriminator</span><span class="p">(</span><span class="n">real_output</span><span class="p">,</span> <span class="n">fake_output</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;WGAN discriminator loss&quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="o">-</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">real_output</span><span class="p">)</span> <span class="o">-</span> <span class="n">torch</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">fake_output</span><span class="p">))</span>

<span class="k">def</span><span class="w"> </span><span class="nf">wgan_loss_generator</span><span class="p">(</span><span class="n">fake_output</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;WGAN generator loss&quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="o">-</span><span class="n">torch</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">fake_output</span><span class="p">)</span>

<span class="c1"># Example usage</span>
<span class="n">real_output</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">32</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>  <span class="c1"># Discriminator output for real images</span>
<span class="n">fake_output</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">32</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>  <span class="c1"># Discriminator output for fake images</span>

<span class="c1"># Standard GAN</span>
<span class="n">d_loss</span> <span class="o">=</span> <span class="n">gan_loss_discriminator</span><span class="p">(</span><span class="n">real_output</span><span class="p">,</span> <span class="n">fake_output</span><span class="p">)</span>
<span class="n">g_loss</span> <span class="o">=</span> <span class="n">gan_loss_generator</span><span class="p">(</span><span class="n">fake_output</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;GAN D Loss: </span><span class="si">{</span><span class="n">d_loss</span><span class="o">.</span><span class="n">item</span><span class="p">()</span><span class="si">:</span><span class="s2">.4f</span><span class="si">}</span><span class="s2">, G Loss: </span><span class="si">{</span><span class="n">g_loss</span><span class="o">.</span><span class="n">item</span><span class="p">()</span><span class="si">:</span><span class="s2">.4f</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>

<span class="c1"># WGAN</span>
<span class="n">d_loss_wgan</span> <span class="o">=</span> <span class="n">wgan_loss_discriminator</span><span class="p">(</span><span class="n">real_output</span><span class="p">,</span> <span class="n">fake_output</span><span class="p">)</span>
<span class="n">g_loss_wgan</span> <span class="o">=</span> <span class="n">wgan_loss_generator</span><span class="p">(</span><span class="n">fake_output</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;WGAN D Loss: </span><span class="si">{</span><span class="n">d_loss_wgan</span><span class="o">.</span><span class="n">item</span><span class="p">()</span><span class="si">:</span><span class="s2">.4f</span><span class="si">}</span><span class="s2">, G Loss: </span><span class="si">{</span><span class="n">g_loss_wgan</span><span class="o">.</span><span class="n">item</span><span class="p">()</span><span class="si">:</span><span class="s2">.4f</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
</code></pre></div>

<h3 id="62-vae-loss">6.2 VAE Loss<a class="header-link" href="#62-vae-loss" title="Permanent link">&para;</a></h3>
<p><strong>Formula:</strong></p>
<div class="highlight"><pre><span></span><code><span class="n">L</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">Reconstruction</span><span class="w"> </span><span class="n">Loss</span><span class="w"> </span><span class="o">+</span><span class="w"> </span><span class="n">KL</span><span class="w"> </span><span class="n">Divergence</span>
<span class="w">  </span><span class="o">=</span><span class="w"> </span><span class="n">E</span><span class="p">[</span><span class="nb">log</span><span class="w"> </span><span class="n">p</span><span class="p">(</span><span class="n">x</span><span class="o">|</span><span class="n">z</span><span class="p">)]</span><span class="w"> </span><span class="o">+</span><span class="w"> </span><span class="n">KL</span><span class="p">(</span><span class="n">q</span><span class="p">(</span><span class="n">z</span><span class="o">|</span><span class="n">x</span><span class="p">)</span><span class="w"> </span><span class="o">||</span><span class="w"> </span><span class="n">p</span><span class="p">(</span><span class="n">z</span><span class="p">))</span>
</code></pre></div>

<p><strong>PyTorch Implementation:</strong></p>
<div class="highlight"><pre><span></span><code><span class="k">def</span><span class="w"> </span><span class="nf">vae_loss</span><span class="p">(</span><span class="n">recon_x</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">mu</span><span class="p">,</span> <span class="n">logvar</span><span class="p">,</span> <span class="n">beta</span><span class="o">=</span><span class="mf">1.0</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Args:</span>
<span class="sd">        recon_x: Reconstructed input</span>
<span class="sd">        x: Original input</span>
<span class="sd">        mu: Mean of latent distribution</span>
<span class="sd">        logvar: Log variance of latent distribution</span>
<span class="sd">        beta: Weight for KL term (Œ≤-VAE)</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="c1"># Reconstruction loss (BCE for binary images, MSE for continuous)</span>
    <span class="n">recon_loss</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">functional</span><span class="o">.</span><span class="n">binary_cross_entropy</span><span class="p">(</span>
        <span class="n">recon_x</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">reduction</span><span class="o">=</span><span class="s1">&#39;sum&#39;</span>
    <span class="p">)</span>

    <span class="c1"># KL divergence: -0.5 * Œ£(1 + log(œÉ¬≤) - Œº¬≤ - œÉ¬≤)</span>
    <span class="n">kl_div</span> <span class="o">=</span> <span class="o">-</span><span class="mf">0.5</span> <span class="o">*</span> <span class="n">torch</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="mi">1</span> <span class="o">+</span> <span class="n">logvar</span> <span class="o">-</span> <span class="n">mu</span><span class="o">.</span><span class="n">pow</span><span class="p">(</span><span class="mi">2</span><span class="p">)</span> <span class="o">-</span> <span class="n">logvar</span><span class="o">.</span><span class="n">exp</span><span class="p">())</span>

    <span class="k">return</span> <span class="n">recon_loss</span> <span class="o">+</span> <span class="n">beta</span> <span class="o">*</span> <span class="n">kl_div</span>

<span class="c1"># Example VAE</span>
<span class="k">class</span><span class="w"> </span><span class="nc">VAE</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="k">def</span><span class="w"> </span><span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">input_dim</span><span class="p">,</span> <span class="n">hidden_dim</span><span class="p">,</span> <span class="n">latent_dim</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
        <span class="c1"># Encoder</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">fc1</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">input_dim</span><span class="p">,</span> <span class="n">hidden_dim</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">fc_mu</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">hidden_dim</span><span class="p">,</span> <span class="n">latent_dim</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">fc_logvar</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">hidden_dim</span><span class="p">,</span> <span class="n">latent_dim</span><span class="p">)</span>

        <span class="c1"># Decoder</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">fc3</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">latent_dim</span><span class="p">,</span> <span class="n">hidden_dim</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">fc4</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">hidden_dim</span><span class="p">,</span> <span class="n">input_dim</span><span class="p">)</span>

    <span class="k">def</span><span class="w"> </span><span class="nf">encode</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
        <span class="n">h</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">relu</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">fc1</span><span class="p">(</span><span class="n">x</span><span class="p">))</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">fc_mu</span><span class="p">(</span><span class="n">h</span><span class="p">),</span> <span class="bp">self</span><span class="o">.</span><span class="n">fc_logvar</span><span class="p">(</span><span class="n">h</span><span class="p">)</span>

    <span class="k">def</span><span class="w"> </span><span class="nf">reparameterize</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">mu</span><span class="p">,</span> <span class="n">logvar</span><span class="p">):</span>
        <span class="n">std</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="mf">0.5</span> <span class="o">*</span> <span class="n">logvar</span><span class="p">)</span>
        <span class="n">eps</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn_like</span><span class="p">(</span><span class="n">std</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">mu</span> <span class="o">+</span> <span class="n">eps</span> <span class="o">*</span> <span class="n">std</span>

    <span class="k">def</span><span class="w"> </span><span class="nf">decode</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">z</span><span class="p">):</span>
        <span class="n">h</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">relu</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">fc3</span><span class="p">(</span><span class="n">z</span><span class="p">))</span>
        <span class="k">return</span> <span class="n">torch</span><span class="o">.</span><span class="n">sigmoid</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">fc4</span><span class="p">(</span><span class="n">h</span><span class="p">))</span>

    <span class="k">def</span><span class="w"> </span><span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
        <span class="n">mu</span><span class="p">,</span> <span class="n">logvar</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">encode</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="n">z</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">reparameterize</span><span class="p">(</span><span class="n">mu</span><span class="p">,</span> <span class="n">logvar</span><span class="p">)</span>
        <span class="n">recon_x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">decode</span><span class="p">(</span><span class="n">z</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">recon_x</span><span class="p">,</span> <span class="n">mu</span><span class="p">,</span> <span class="n">logvar</span>

<span class="c1"># Training</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">VAE</span><span class="p">(</span><span class="n">input_dim</span><span class="o">=</span><span class="mi">784</span><span class="p">,</span> <span class="n">hidden_dim</span><span class="o">=</span><span class="mi">400</span><span class="p">,</span> <span class="n">latent_dim</span><span class="o">=</span><span class="mi">20</span><span class="p">)</span>
<span class="n">optimizer</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">optim</span><span class="o">.</span><span class="n">Adam</span><span class="p">(</span><span class="n">model</span><span class="o">.</span><span class="n">parameters</span><span class="p">(),</span> <span class="n">lr</span><span class="o">=</span><span class="mf">0.001</span><span class="p">)</span>

<span class="n">x</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">64</span><span class="p">,</span> <span class="mi">784</span><span class="p">)</span><span class="o">.</span><span class="n">sigmoid</span><span class="p">()</span>  <span class="c1"># Dummy data</span>
<span class="n">recon_x</span><span class="p">,</span> <span class="n">mu</span><span class="p">,</span> <span class="n">logvar</span> <span class="o">=</span> <span class="n">model</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>

<span class="n">loss</span> <span class="o">=</span> <span class="n">vae_loss</span><span class="p">(</span><span class="n">recon_x</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">mu</span><span class="p">,</span> <span class="n">logvar</span><span class="p">,</span> <span class="n">beta</span><span class="o">=</span><span class="mf">1.0</span><span class="p">)</span>
<span class="n">optimizer</span><span class="o">.</span><span class="n">zero_grad</span><span class="p">()</span>
<span class="n">loss</span><span class="o">.</span><span class="n">backward</span><span class="p">()</span>
<span class="n">optimizer</span><span class="o">.</span><span class="n">step</span><span class="p">()</span>

<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;VAE Loss: </span><span class="si">{</span><span class="n">loss</span><span class="o">.</span><span class="n">item</span><span class="p">()</span><span class="si">:</span><span class="s2">.4f</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
</code></pre></div>

<h3 id="63-perceptual-loss">6.3 Perceptual Loss<a class="header-link" href="#63-perceptual-loss" title="Permanent link">&para;</a></h3>
<p><strong>Concept:</strong>
Instead of pixel-wise comparison, use feature-wise comparison from a pretrained network (e.g., VGG).</p>
<p><strong>PyTorch Implementation:</strong></p>
<div class="highlight"><pre><span></span><code><span class="kn">import</span><span class="w"> </span><span class="nn">torchvision.models</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">models</span>

<span class="k">class</span><span class="w"> </span><span class="nc">PerceptualLoss</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="k">def</span><span class="w"> </span><span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">layers</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;relu1_2&#39;</span><span class="p">,</span> <span class="s1">&#39;relu2_2&#39;</span><span class="p">,</span> <span class="s1">&#39;relu3_3&#39;</span><span class="p">]):</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
        <span class="c1"># Load pretrained VGG16</span>
        <span class="n">vgg</span> <span class="o">=</span> <span class="n">models</span><span class="o">.</span><span class="n">vgg16</span><span class="p">(</span><span class="n">pretrained</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span><span class="o">.</span><span class="n">features</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">feature_extractor</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">ModuleDict</span><span class="p">()</span>

        <span class="c1"># Map layer names to indices</span>
        <span class="n">layer_map</span> <span class="o">=</span> <span class="p">{</span>
            <span class="s1">&#39;relu1_2&#39;</span><span class="p">:</span> <span class="mi">4</span><span class="p">,</span> <span class="s1">&#39;relu2_2&#39;</span><span class="p">:</span> <span class="mi">9</span><span class="p">,</span> <span class="s1">&#39;relu3_3&#39;</span><span class="p">:</span> <span class="mi">16</span><span class="p">,</span>
            <span class="s1">&#39;relu4_3&#39;</span><span class="p">:</span> <span class="mi">23</span><span class="p">,</span> <span class="s1">&#39;relu5_3&#39;</span><span class="p">:</span> <span class="mi">30</span>
        <span class="p">}</span>

        <span class="k">for</span> <span class="n">name</span> <span class="ow">in</span> <span class="n">layers</span><span class="p">:</span>
            <span class="n">idx</span> <span class="o">=</span> <span class="n">layer_map</span><span class="p">[</span><span class="n">name</span><span class="p">]</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">feature_extractor</span><span class="p">[</span><span class="n">name</span><span class="p">]</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Sequential</span><span class="p">(</span><span class="o">*</span><span class="nb">list</span><span class="p">(</span><span class="n">vgg</span><span class="p">[:</span><span class="n">idx</span><span class="o">+</span><span class="mi">1</span><span class="p">]))</span>

        <span class="c1"># Freeze parameters</span>
        <span class="k">for</span> <span class="n">param</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">parameters</span><span class="p">():</span>
            <span class="n">param</span><span class="o">.</span><span class="n">requires_grad</span> <span class="o">=</span> <span class="kc">False</span>

    <span class="k">def</span><span class="w"> </span><span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">pred</span><span class="p">,</span> <span class="n">target</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Args:</span>
<span class="sd">            pred: (N, 3, H, W) predicted image</span>
<span class="sd">            target: (N, 3, H, W) target image</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">loss</span> <span class="o">=</span> <span class="mi">0</span>
        <span class="k">for</span> <span class="n">name</span><span class="p">,</span> <span class="n">extractor</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">feature_extractor</span><span class="o">.</span><span class="n">items</span><span class="p">():</span>
            <span class="n">pred_features</span> <span class="o">=</span> <span class="n">extractor</span><span class="p">(</span><span class="n">pred</span><span class="p">)</span>
            <span class="n">target_features</span> <span class="o">=</span> <span class="n">extractor</span><span class="p">(</span><span class="n">target</span><span class="p">)</span>
            <span class="n">loss</span> <span class="o">+=</span> <span class="n">nn</span><span class="o">.</span><span class="n">functional</span><span class="o">.</span><span class="n">mse_loss</span><span class="p">(</span><span class="n">pred_features</span><span class="p">,</span> <span class="n">target_features</span><span class="p">)</span>

        <span class="k">return</span> <span class="n">loss</span> <span class="o">/</span> <span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">feature_extractor</span><span class="p">)</span>

<span class="c1"># Example usage (requires torchvision)</span>
<span class="c1"># perceptual_loss = PerceptualLoss()</span>
<span class="c1"># pred_img = torch.randn(4, 3, 224, 224)</span>
<span class="c1"># target_img = torch.randn(4, 3, 224, 224)</span>
<span class="c1"># loss = perceptual_loss(pred_img, target_img)</span>
<span class="c1"># print(f&quot;Perceptual Loss: {loss.item():.4f}&quot;)</span>
</code></pre></div>

<p><strong>When to Use:</strong>
- Style transfer
- Super-resolution
- Image-to-image translation</p>
<hr />
<h2 id="7-advanced-topics">7. Advanced Topics<a class="header-link" href="#7-advanced-topics" title="Permanent link">&para;</a></h2>
<h3 id="71-multi-task-loss-weighting">7.1 Multi-Task Loss Weighting<a class="header-link" href="#71-multi-task-loss-weighting" title="Permanent link">&para;</a></h3>
<p><strong>Problem:</strong>
When training on multiple tasks simultaneously, how to balance the losses?</p>
<p><strong>Method 1: Manual Weighting</strong></p>
<div class="highlight"><pre><span></span><code><span class="n">total_loss</span> <span class="o">=</span> <span class="n">w1</span> <span class="o">*</span> <span class="n">task1_loss</span> <span class="o">+</span> <span class="n">w2</span> <span class="o">*</span> <span class="n">task2_loss</span> <span class="o">+</span> <span class="n">w3</span> <span class="o">*</span> <span class="n">task3_loss</span>
</code></pre></div>

<p><strong>Method 2: Uncertainty Weighting</strong></p>
<p>Based on "Multi-Task Learning Using Uncertainty to Weigh Losses" (Kendall et al., 2018).</p>
<div class="highlight"><pre><span></span><code><span class="k">class</span><span class="w"> </span><span class="nc">MultiTaskLoss</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="k">def</span><span class="w"> </span><span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">num_tasks</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
        <span class="c1"># Learnable log variance for each task</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">log_vars</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Parameter</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">num_tasks</span><span class="p">))</span>

    <span class="k">def</span><span class="w"> </span><span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">losses</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Args:</span>
<span class="sd">            losses: List of losses for each task</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">total_loss</span> <span class="o">=</span> <span class="mi">0</span>
        <span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">loss</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">losses</span><span class="p">):</span>
            <span class="n">precision</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="o">-</span><span class="bp">self</span><span class="o">.</span><span class="n">log_vars</span><span class="p">[</span><span class="n">i</span><span class="p">])</span>
            <span class="n">total_loss</span> <span class="o">+=</span> <span class="n">precision</span> <span class="o">*</span> <span class="n">loss</span> <span class="o">+</span> <span class="bp">self</span><span class="o">.</span><span class="n">log_vars</span><span class="p">[</span><span class="n">i</span><span class="p">]</span>

        <span class="k">return</span> <span class="n">total_loss</span>

<span class="c1"># Example</span>
<span class="n">mtl</span> <span class="o">=</span> <span class="n">MultiTaskLoss</span><span class="p">(</span><span class="n">num_tasks</span><span class="o">=</span><span class="mi">3</span><span class="p">)</span>
<span class="n">optimizer</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">optim</span><span class="o">.</span><span class="n">Adam</span><span class="p">(</span><span class="n">mtl</span><span class="o">.</span><span class="n">parameters</span><span class="p">(),</span> <span class="n">lr</span><span class="o">=</span><span class="mf">0.01</span><span class="p">)</span>

<span class="c1"># Simulate task losses</span>
<span class="n">task_losses</span> <span class="o">=</span> <span class="p">[</span>
    <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="mf">2.5</span><span class="p">),</span>  <span class="c1"># Task 1</span>
    <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="mf">0.8</span><span class="p">),</span>  <span class="c1"># Task 2</span>
    <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="mf">1.2</span><span class="p">),</span>  <span class="c1"># Task 3</span>
<span class="p">]</span>

<span class="n">total_loss</span> <span class="o">=</span> <span class="n">mtl</span><span class="p">(</span><span class="n">task_losses</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Multi-task Loss: </span><span class="si">{</span><span class="n">total_loss</span><span class="o">.</span><span class="n">item</span><span class="p">()</span><span class="si">:</span><span class="s2">.4f</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Learned weights: </span><span class="si">{</span><span class="n">torch</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="o">-</span><span class="n">mtl</span><span class="o">.</span><span class="n">log_vars</span><span class="p">)</span><span class="o">.</span><span class="n">detach</span><span class="p">()</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
</code></pre></div>

<p><strong>Method 3: GradNorm</strong></p>
<p>Balances task losses by normalizing gradients.</p>
<div class="highlight"><pre><span></span><code><span class="k">class</span><span class="w"> </span><span class="nc">GradNorm</span><span class="p">:</span>
    <span class="k">def</span><span class="w"> </span><span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">model</span><span class="p">,</span> <span class="n">num_tasks</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">1.5</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Args:</span>
<span class="sd">            model: Shared network</span>
<span class="sd">            num_tasks: Number of tasks</span>
<span class="sd">            alpha: Restoring force (typically 1.5)</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">model</span> <span class="o">=</span> <span class="n">model</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">num_tasks</span> <span class="o">=</span> <span class="n">num_tasks</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">alpha</span> <span class="o">=</span> <span class="n">alpha</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">weights</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Parameter</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">ones</span><span class="p">(</span><span class="n">num_tasks</span><span class="p">))</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">initial_losses</span> <span class="o">=</span> <span class="kc">None</span>

    <span class="k">def</span><span class="w"> </span><span class="nf">compute_weights</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">losses</span><span class="p">,</span> <span class="n">shared_params</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Update task weights based on gradient norms&quot;&quot;&quot;</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">initial_losses</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">initial_losses</span> <span class="o">=</span> <span class="n">losses</span><span class="o">.</span><span class="n">detach</span><span class="p">()</span>

        <span class="c1"># Compute weighted loss</span>
        <span class="n">weighted_losses</span> <span class="o">=</span> <span class="n">losses</span> <span class="o">*</span> <span class="bp">self</span><span class="o">.</span><span class="n">weights</span>
        <span class="n">total_loss</span> <span class="o">=</span> <span class="n">weighted_losses</span><span class="o">.</span><span class="n">sum</span><span class="p">()</span>

        <span class="c1"># Compute gradients</span>
        <span class="n">total_loss</span><span class="o">.</span><span class="n">backward</span><span class="p">(</span><span class="n">retain_graph</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>

        <span class="c1"># Get gradient norms for shared layers</span>
        <span class="n">grad_norms</span> <span class="o">=</span> <span class="p">[]</span>
        <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">num_tasks</span><span class="p">):</span>
            <span class="c1"># ... compute grad norm for task i</span>
            <span class="k">pass</span>

        <span class="c1"># Update weights (simplified version)</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">weights</span>

<span class="c1"># Note: Full GradNorm implementation requires careful gradient manipulation</span>
</code></pre></div>

<h3 id="72-curriculum-loss">7.2 Curriculum Loss<a class="header-link" href="#72-curriculum-loss" title="Permanent link">&para;</a></h3>
<p><strong>Concept:</strong>
Start training on easy examples, gradually increase difficulty.</p>
<div class="highlight"><pre><span></span><code><span class="k">class</span><span class="w"> </span><span class="nc">CurriculumLoss</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="k">def</span><span class="w"> </span><span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">base_criterion</span><span class="p">,</span> <span class="n">total_epochs</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">base_criterion</span> <span class="o">=</span> <span class="n">base_criterion</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">total_epochs</span> <span class="o">=</span> <span class="n">total_epochs</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">current_epoch</span> <span class="o">=</span> <span class="mi">0</span>

    <span class="k">def</span><span class="w"> </span><span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">pred</span><span class="p">,</span> <span class="n">target</span><span class="p">,</span> <span class="n">difficulty</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Args:</span>
<span class="sd">            pred: Predictions</span>
<span class="sd">            target: Ground truth</span>
<span class="sd">            difficulty: (N,) difficulty score for each sample [0, 1]</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="c1"># Compute base loss</span>
        <span class="n">losses</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">base_criterion</span><span class="p">(</span><span class="n">pred</span><span class="p">,</span> <span class="n">target</span><span class="p">)</span>

        <span class="c1"># Curriculum weight: easier samples first</span>
        <span class="n">progress</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">current_epoch</span> <span class="o">/</span> <span class="bp">self</span><span class="o">.</span><span class="n">total_epochs</span>
        <span class="n">threshold</span> <span class="o">=</span> <span class="n">progress</span>  <span class="c1"># Gradually increase difficulty threshold</span>

        <span class="n">weights</span> <span class="o">=</span> <span class="p">(</span><span class="n">difficulty</span> <span class="o">&lt;=</span> <span class="n">threshold</span><span class="p">)</span><span class="o">.</span><span class="n">float</span><span class="p">()</span>
        <span class="n">weights</span> <span class="o">=</span> <span class="n">weights</span> <span class="o">/</span> <span class="p">(</span><span class="n">weights</span><span class="o">.</span><span class="n">sum</span><span class="p">()</span> <span class="o">+</span> <span class="mf">1e-6</span><span class="p">)</span> <span class="o">*</span> <span class="n">weights</span><span class="o">.</span><span class="n">size</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>

        <span class="k">return</span> <span class="p">(</span><span class="n">losses</span> <span class="o">*</span> <span class="n">weights</span><span class="p">)</span><span class="o">.</span><span class="n">sum</span><span class="p">()</span>

    <span class="k">def</span><span class="w"> </span><span class="nf">step_epoch</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">current_epoch</span> <span class="o">+=</span> <span class="mi">1</span>

<span class="c1"># Example usage</span>
<span class="n">criterion</span> <span class="o">=</span> <span class="n">CurriculumLoss</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">CrossEntropyLoss</span><span class="p">(</span><span class="n">reduction</span><span class="o">=</span><span class="s1">&#39;none&#39;</span><span class="p">),</span> <span class="n">total_epochs</span><span class="o">=</span><span class="mi">100</span><span class="p">)</span>

<span class="k">for</span> <span class="n">epoch</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">100</span><span class="p">):</span>
    <span class="n">pred</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">32</span><span class="p">,</span> <span class="mi">10</span><span class="p">)</span>
    <span class="n">target</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randint</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">10</span><span class="p">,</span> <span class="p">(</span><span class="mi">32</span><span class="p">,))</span>
    <span class="n">difficulty</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">rand</span><span class="p">(</span><span class="mi">32</span><span class="p">)</span>  <span class="c1"># Random difficulty scores</span>

    <span class="n">loss</span> <span class="o">=</span> <span class="n">criterion</span><span class="p">(</span><span class="n">pred</span><span class="p">,</span> <span class="n">target</span><span class="p">,</span> <span class="n">difficulty</span><span class="p">)</span>
    <span class="c1"># ... backward, optimize ...</span>

    <span class="n">criterion</span><span class="o">.</span><span class="n">step_epoch</span><span class="p">()</span>
</code></pre></div>

<h3 id="73-custom-loss-functions">7.3 Custom Loss Functions<a class="header-link" href="#73-custom-loss-functions" title="Permanent link">&para;</a></h3>
<p><strong>Template for Custom Losses:</strong></p>
<div class="highlight"><pre><span></span><code><span class="k">class</span><span class="w"> </span><span class="nc">CustomLoss</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="k">def</span><span class="w"> </span><span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">param1</span><span class="p">,</span> <span class="n">param2</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">param1</span> <span class="o">=</span> <span class="n">param1</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">param2</span> <span class="o">=</span> <span class="n">param2</span>

    <span class="k">def</span><span class="w"> </span><span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">pred</span><span class="p">,</span> <span class="n">target</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Args:</span>
<span class="sd">            pred: Model predictions</span>
<span class="sd">            target: Ground truth</span>

<span class="sd">        Returns:</span>
<span class="sd">            loss: Scalar tensor</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="c1"># Implement your loss computation</span>
        <span class="n">loss</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">mean</span><span class="p">((</span><span class="n">pred</span> <span class="o">-</span> <span class="n">target</span><span class="p">)</span> <span class="o">**</span> <span class="mi">2</span><span class="p">)</span>  <span class="c1"># Example</span>
        <span class="k">return</span> <span class="n">loss</span>

<span class="c1"># Example: Asymmetric Loss (penalize overestimation more than underestimation)</span>
<span class="k">class</span><span class="w"> </span><span class="nc">AsymmetricMSELoss</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="k">def</span><span class="w"> </span><span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">over_penalty</span><span class="o">=</span><span class="mf">2.0</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">over_penalty</span> <span class="o">=</span> <span class="n">over_penalty</span>

    <span class="k">def</span><span class="w"> </span><span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">pred</span><span class="p">,</span> <span class="n">target</span><span class="p">):</span>
        <span class="n">error</span> <span class="o">=</span> <span class="n">pred</span> <span class="o">-</span> <span class="n">target</span>

        <span class="c1"># Penalize overestimation more</span>
        <span class="n">loss</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">where</span><span class="p">(</span>
            <span class="n">error</span> <span class="o">&gt;</span> <span class="mi">0</span><span class="p">,</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">over_penalty</span> <span class="o">*</span> <span class="n">error</span> <span class="o">**</span> <span class="mi">2</span><span class="p">,</span>
            <span class="n">error</span> <span class="o">**</span> <span class="mi">2</span>
        <span class="p">)</span>

        <span class="k">return</span> <span class="n">loss</span><span class="o">.</span><span class="n">mean</span><span class="p">()</span>

<span class="c1"># Usage</span>
<span class="n">asymmetric_loss</span> <span class="o">=</span> <span class="n">AsymmetricMSELoss</span><span class="p">(</span><span class="n">over_penalty</span><span class="o">=</span><span class="mf">2.0</span><span class="p">)</span>
<span class="n">pred</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([</span><span class="mf">2.5</span><span class="p">,</span> <span class="mf">1.0</span><span class="p">,</span> <span class="mf">3.0</span><span class="p">])</span>
<span class="n">target</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([</span><span class="mf">2.0</span><span class="p">,</span> <span class="mf">2.0</span><span class="p">,</span> <span class="mf">2.0</span><span class="p">])</span>

<span class="n">loss</span> <span class="o">=</span> <span class="n">asymmetric_loss</span><span class="p">(</span><span class="n">pred</span><span class="p">,</span> <span class="n">target</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Asymmetric Loss: </span><span class="si">{</span><span class="n">loss</span><span class="o">.</span><span class="n">item</span><span class="p">()</span><span class="si">:</span><span class="s2">.4f</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
</code></pre></div>

<h3 id="74-numerical-stability-tips">7.4 Numerical Stability Tips<a class="header-link" href="#74-numerical-stability-tips" title="Permanent link">&para;</a></h3>
<p><strong>Problem 1: Log-Sum-Exp Trick</strong></p>
<div class="highlight"><pre><span></span><code><span class="c1"># Numerically unstable (can overflow)</span>
<span class="k">def</span><span class="w"> </span><span class="nf">unstable_softmax</span><span class="p">(</span><span class="n">x</span><span class="p">):</span>
    <span class="k">return</span> <span class="n">torch</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="n">x</span><span class="p">)</span> <span class="o">/</span> <span class="n">torch</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="n">x</span><span class="p">))</span>

<span class="c1"># Stable version</span>
<span class="k">def</span><span class="w"> </span><span class="nf">stable_softmax</span><span class="p">(</span><span class="n">x</span><span class="p">):</span>
    <span class="n">x_max</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">max</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
    <span class="n">exp_x</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="n">x</span> <span class="o">-</span> <span class="n">x_max</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">exp_x</span> <span class="o">/</span> <span class="n">torch</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">exp_x</span><span class="p">)</span>

<span class="c1"># Example</span>
<span class="n">x</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([</span><span class="mf">1000.0</span><span class="p">,</span> <span class="mf">1001.0</span><span class="p">,</span> <span class="mf">1002.0</span><span class="p">])</span>
<span class="c1"># unstable_softmax(x)  # Would cause overflow</span>
<span class="n">stable</span> <span class="o">=</span> <span class="n">stable_softmax</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Stable Softmax: </span><span class="si">{</span><span class="n">stable</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
</code></pre></div>

<p><strong>Problem 2: Avoiding log(0)</strong></p>
<div class="highlight"><pre><span></span><code><span class="c1"># Bad: Can produce NaN</span>
<span class="n">loss</span> <span class="o">=</span> <span class="o">-</span><span class="n">torch</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="n">pred</span><span class="p">)</span>

<span class="c1"># Good: Add small epsilon</span>
<span class="n">epsilon</span> <span class="o">=</span> <span class="mf">1e-7</span>
<span class="n">loss</span> <span class="o">=</span> <span class="o">-</span><span class="n">torch</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="n">pred</span> <span class="o">+</span> <span class="n">epsilon</span><span class="p">)</span>

<span class="c1"># Better: Use clamp</span>
<span class="n">loss</span> <span class="o">=</span> <span class="o">-</span><span class="n">torch</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">clamp</span><span class="p">(</span><span class="n">pred</span><span class="p">,</span> <span class="nb">min</span><span class="o">=</span><span class="n">epsilon</span><span class="p">))</span>

<span class="c1"># Best: Use built-in stable versions</span>
<span class="n">loss</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">functional</span><span class="o">.</span><span class="n">binary_cross_entropy_with_logits</span><span class="p">(</span><span class="n">logits</span><span class="p">,</span> <span class="n">target</span><span class="p">)</span>
</code></pre></div>

<p><strong>Problem 3: Gradient Clipping</strong></p>
<div class="highlight"><pre><span></span><code><span class="c1"># Prevent exploding gradients</span>
<span class="k">for</span> <span class="n">param</span> <span class="ow">in</span> <span class="n">model</span><span class="o">.</span><span class="n">parameters</span><span class="p">():</span>
    <span class="k">if</span> <span class="n">param</span><span class="o">.</span><span class="n">grad</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">utils</span><span class="o">.</span><span class="n">clip_grad_norm_</span><span class="p">(</span><span class="n">param</span><span class="p">,</span> <span class="n">max_norm</span><span class="o">=</span><span class="mf">1.0</span><span class="p">)</span>

<span class="c1"># Or clip by value</span>
<span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">utils</span><span class="o">.</span><span class="n">clip_grad_value_</span><span class="p">(</span><span class="n">model</span><span class="o">.</span><span class="n">parameters</span><span class="p">(),</span> <span class="n">clip_value</span><span class="o">=</span><span class="mf">0.5</span><span class="p">)</span>
</code></pre></div>

<hr />
<h2 id="8-practical-guide">8. Practical Guide<a class="header-link" href="#8-practical-guide" title="Permanent link">&para;</a></h2>
<h3 id="81-decision-tree-for-loss-selection">8.1 Decision Tree for Loss Selection<a class="header-link" href="#81-decision-tree-for-loss-selection" title="Permanent link">&para;</a></h3>
<div class="highlight"><pre><span></span><code><span class="n">Start</span>
<span class="w">  </span><span class="err">‚îÇ</span>
<span class="w">  </span><span class="err">‚îú‚îÄ</span><span class="w"> </span><span class="n">Task</span><span class="p">:</span><span class="w"> </span><span class="n">Regression</span><span class="err">?</span>
<span class="w">  </span><span class="err">‚îÇ</span><span class="w">    </span><span class="err">‚îÇ</span>
<span class="w">  </span><span class="err">‚îÇ</span><span class="w">    </span><span class="err">‚îú‚îÄ</span><span class="w"> </span><span class="n">Clean</span><span class="w"> </span><span class="n">data</span><span class="p">,</span><span class="w"> </span><span class="n">no</span><span class="w"> </span><span class="n">outliers</span><span class="w"> </span><span class="err">‚Üí</span><span class="w"> </span><span class="n">MSE</span>
<span class="w">  </span><span class="err">‚îÇ</span><span class="w">    </span><span class="err">‚îú‚îÄ</span><span class="w"> </span><span class="n">Data</span><span class="w"> </span><span class="n">with</span><span class="w"> </span><span class="n">outliers</span><span class="w"> </span><span class="err">‚Üí</span><span class="w"> </span><span class="n">MAE</span><span class="w"> </span><span class="ow">or</span><span class="w"> </span><span class="n">Huber</span>
<span class="w">  </span><span class="err">‚îÇ</span><span class="w">    </span><span class="err">‚îî‚îÄ</span><span class="w"> </span><span class="n">Need</span><span class="w"> </span><span class="n">smooth</span><span class="w"> </span><span class="n">gradients</span><span class="w"> </span><span class="err">‚Üí</span><span class="w"> </span><span class="n">Huber</span><span class="w"> </span><span class="ow">or</span><span class="w"> </span><span class="n">Log</span><span class="o">-</span><span class="n">Cosh</span>
<span class="w">  </span><span class="err">‚îÇ</span>
<span class="w">  </span><span class="err">‚îú‚îÄ</span><span class="w"> </span><span class="n">Task</span><span class="p">:</span><span class="w"> </span><span class="n">Classification</span><span class="err">?</span>
<span class="w">  </span><span class="err">‚îÇ</span><span class="w">    </span><span class="err">‚îÇ</span>
<span class="w">  </span><span class="err">‚îÇ</span><span class="w">    </span><span class="err">‚îú‚îÄ</span><span class="w"> </span><span class="n">Binary</span><span class="w"> </span><span class="n">classification</span><span class="w"> </span><span class="err">‚Üí</span><span class="w"> </span><span class="n">BCEWithLogitsLoss</span>
<span class="w">  </span><span class="err">‚îÇ</span><span class="w">    </span><span class="err">‚îú‚îÄ</span><span class="w"> </span><span class="n">Multi</span><span class="o">-</span><span class="k">class</span><span class="w"> </span><span class="p">(</span><span class="n">mutually</span><span class="w"> </span><span class="n">exclusive</span><span class="p">)</span><span class="w"> </span><span class="err">‚Üí</span><span class="w"> </span><span class="n">CrossEntropyLoss</span>
<span class="w">  </span><span class="err">‚îÇ</span><span class="w">    </span><span class="err">‚îú‚îÄ</span><span class="w"> </span><span class="n">Multi</span><span class="o">-</span><span class="n">label</span><span class="w"> </span><span class="p">(</span><span class="n">independent</span><span class="w"> </span><span class="n">labels</span><span class="p">)</span><span class="w"> </span><span class="err">‚Üí</span><span class="w"> </span><span class="n">BCEWithLogitsLoss</span>
<span class="w">  </span><span class="err">‚îÇ</span><span class="w">    </span><span class="err">‚îî‚îÄ</span><span class="w"> </span><span class="n">Class</span><span class="w"> </span><span class="n">imbalance</span><span class="w"> </span><span class="err">‚Üí</span><span class="w"> </span><span class="n">FocalLoss</span><span class="w"> </span><span class="ow">or</span><span class="w"> </span><span class="n">weighted</span><span class="w"> </span><span class="n">CE</span>
<span class="w">  </span><span class="err">‚îÇ</span>
<span class="w">  </span><span class="err">‚îú‚îÄ</span><span class="w"> </span><span class="n">Task</span><span class="p">:</span><span class="w"> </span><span class="n">Segmentation</span><span class="err">?</span>
<span class="w">  </span><span class="err">‚îÇ</span><span class="w">    </span><span class="err">‚îÇ</span>
<span class="w">  </span><span class="err">‚îÇ</span><span class="w">    </span><span class="err">‚îú‚îÄ</span><span class="w"> </span><span class="n">Small</span><span class="w"> </span><span class="n">objects</span><span class="w"> </span><span class="err">‚Üí</span><span class="w"> </span><span class="n">DiceLoss</span>
<span class="w">  </span><span class="err">‚îÇ</span><span class="w">    </span><span class="err">‚îú‚îÄ</span><span class="w"> </span><span class="n">Class</span><span class="w"> </span><span class="n">imbalance</span><span class="w"> </span><span class="err">‚Üí</span><span class="w"> </span><span class="n">DiceLoss</span><span class="w"> </span><span class="ow">or</span><span class="w"> </span><span class="n">FocalLoss</span>
<span class="w">  </span><span class="err">‚îÇ</span><span class="w">    </span><span class="err">‚îî‚îÄ</span><span class="w"> </span><span class="n">Balanced</span><span class="w"> </span><span class="n">data</span><span class="w"> </span><span class="err">‚Üí</span><span class="w"> </span><span class="n">CrossEntropyLoss</span><span class="w"> </span><span class="ow">or</span><span class="w"> </span><span class="n">CE</span><span class="w"> </span><span class="o">+</span><span class="w"> </span><span class="n">Dice</span>
<span class="w">  </span><span class="err">‚îÇ</span>
<span class="w">  </span><span class="err">‚îú‚îÄ</span><span class="w"> </span><span class="n">Task</span><span class="p">:</span><span class="w"> </span><span class="nb nb-Type">Object</span><span class="w"> </span><span class="n">Detection</span><span class="err">?</span>
<span class="w">  </span><span class="err">‚îÇ</span><span class="w">    </span><span class="err">‚îÇ</span>
<span class="w">  </span><span class="err">‚îÇ</span><span class="w">    </span><span class="err">‚îú‚îÄ</span><span class="w"> </span><span class="n">Classification</span><span class="w"> </span><span class="n">head</span><span class="w"> </span><span class="err">‚Üí</span><span class="w"> </span><span class="n">CrossEntropyLoss</span><span class="w"> </span><span class="ow">or</span><span class="w"> </span><span class="n">FocalLoss</span>
<span class="w">  </span><span class="err">‚îÇ</span><span class="w">    </span><span class="err">‚îî‚îÄ</span><span class="w"> </span><span class="n">Bounding</span><span class="w"> </span><span class="n">box</span><span class="w"> </span><span class="n">regression</span><span class="w"> </span><span class="err">‚Üí</span><span class="w"> </span><span class="n">IoU</span><span class="w"> </span><span class="n">Loss</span><span class="p">,</span><span class="w"> </span><span class="n">GIoU</span><span class="w"> </span><span class="n">Loss</span><span class="p">,</span><span class="w"> </span><span class="ow">or</span><span class="w"> </span><span class="n">Smooth</span><span class="w"> </span><span class="n">L1</span>
<span class="w">  </span><span class="err">‚îÇ</span>
<span class="w">  </span><span class="err">‚îú‚îÄ</span><span class="w"> </span><span class="n">Task</span><span class="p">:</span><span class="w"> </span><span class="n">Metric</span><span class="w"> </span><span class="n">Learning</span><span class="err">?</span>
<span class="w">  </span><span class="err">‚îÇ</span><span class="w">    </span><span class="err">‚îÇ</span>
<span class="w">  </span><span class="err">‚îÇ</span><span class="w">    </span><span class="err">‚îú‚îÄ</span><span class="w"> </span><span class="n">Pair</span><span class="w"> </span><span class="n">verification</span><span class="w"> </span><span class="err">‚Üí</span><span class="w"> </span><span class="n">ContrastiveLoss</span>
<span class="w">  </span><span class="err">‚îÇ</span><span class="w">    </span><span class="err">‚îú‚îÄ</span><span class="w"> </span><span class="n">Triplet</span><span class="w"> </span><span class="n">comparison</span><span class="w"> </span><span class="err">‚Üí</span><span class="w"> </span><span class="n">TripletLoss</span>
<span class="w">  </span><span class="err">‚îÇ</span><span class="w">    </span><span class="err">‚îî‚îÄ</span><span class="w"> </span><span class="n">Self</span><span class="o">-</span><span class="n">supervised</span><span class="w"> </span><span class="err">‚Üí</span><span class="w"> </span><span class="n">InfoNCE</span><span class="w"> </span><span class="p">(</span><span class="n">NT</span><span class="o">-</span><span class="n">Xent</span><span class="p">)</span>
<span class="w">  </span><span class="err">‚îÇ</span>
<span class="w">  </span><span class="err">‚îî‚îÄ</span><span class="w"> </span><span class="n">Task</span><span class="p">:</span><span class="w"> </span><span class="n">Generative</span><span class="w"> </span><span class="n">Model</span><span class="err">?</span>
<span class="w">       </span><span class="err">‚îÇ</span>
<span class="w">       </span><span class="err">‚îú‚îÄ</span><span class="w"> </span><span class="n">GAN</span><span class="w"> </span><span class="err">‚Üí</span><span class="w"> </span><span class="n">Adversarial</span><span class="w"> </span><span class="n">Loss</span><span class="w"> </span><span class="p">(</span><span class="n">BCE</span><span class="w"> </span><span class="ow">or</span><span class="w"> </span><span class="n">Wasserstein</span><span class="p">)</span>
<span class="w">       </span><span class="err">‚îú‚îÄ</span><span class="w"> </span><span class="n">VAE</span><span class="w"> </span><span class="err">‚Üí</span><span class="w"> </span><span class="n">Reconstruction</span><span class="w"> </span><span class="n">Loss</span><span class="w"> </span><span class="o">+</span><span class="w"> </span><span class="n">KL</span><span class="w"> </span><span class="n">Divergence</span>
<span class="w">       </span><span class="err">‚îî‚îÄ</span><span class="w"> </span><span class="n">Image</span><span class="w"> </span><span class="n">translation</span><span class="w"> </span><span class="err">‚Üí</span><span class="w"> </span><span class="n">Perceptual</span><span class="w"> </span><span class="n">Loss</span><span class="w"> </span><span class="o">+</span><span class="w"> </span><span class="n">L1</span><span class="o">/</span><span class="n">L2</span>
</code></pre></div>

<h3 id="82-comparison-table">8.2 Comparison Table<a class="header-link" href="#82-comparison-table" title="Permanent link">&para;</a></h3>
<table>
<thead>
<tr>
<th>Task</th>
<th>Loss Function</th>
<th>Activation</th>
<th>Notes</th>
</tr>
</thead>
<tbody>
<tr>
<td><strong>Regression</strong></td>
<td>MSE</td>
<td>None</td>
<td>Clean data</td>
</tr>
<tr>
<td></td>
<td>MAE</td>
<td>None</td>
<td>Outlier-robust</td>
</tr>
<tr>
<td></td>
<td>Huber</td>
<td>None</td>
<td>Balance L1/L2</td>
</tr>
<tr>
<td><strong>Binary Classification</strong></td>
<td>BCEWithLogitsLoss</td>
<td>None (internal sigmoid)</td>
<td>Numerically stable</td>
</tr>
<tr>
<td><strong>Multi-Class</strong></td>
<td>CrossEntropyLoss</td>
<td>None (internal softmax)</td>
<td>Mutually exclusive</td>
</tr>
<tr>
<td><strong>Multi-Label</strong></td>
<td>BCEWithLogitsLoss</td>
<td>None (internal sigmoid)</td>
<td>Independent labels</td>
</tr>
<tr>
<td><strong>Imbalanced Classification</strong></td>
<td>FocalLoss</td>
<td>Softmax</td>
<td>Addresses imbalance</td>
</tr>
<tr>
<td><strong>Segmentation</strong></td>
<td>DiceLoss</td>
<td>Softmax</td>
<td>Handles class imbalance</td>
</tr>
<tr>
<td></td>
<td>CE + Dice</td>
<td>Softmax</td>
<td>Combined approach</td>
</tr>
<tr>
<td><strong>Detection (bbox)</strong></td>
<td>IoU / GIoU Loss</td>
<td>None</td>
<td>Bounding box regression</td>
</tr>
<tr>
<td><strong>Detection (class)</strong></td>
<td>FocalLoss</td>
<td>Softmax</td>
<td>Handles easy negatives</td>
</tr>
<tr>
<td><strong>Face Verification</strong></td>
<td>ContrastiveLoss</td>
<td>None</td>
<td>Siamese networks</td>
</tr>
<tr>
<td><strong>Face Recognition</strong></td>
<td>TripletLoss</td>
<td>None</td>
<td>Triplet mining</td>
</tr>
<tr>
<td><strong>Self-Supervised</strong></td>
<td>InfoNCE</td>
<td>None</td>
<td>Contrastive learning</td>
</tr>
<tr>
<td><strong>GAN</strong></td>
<td>BCELoss (adversarial)</td>
<td>Sigmoid</td>
<td>Minimax game</td>
</tr>
<tr>
<td><strong>VAE</strong></td>
<td>BCE/MSE + KL</td>
<td>Sigmoid/None</td>
<td>Reconstruction + regularization</td>
</tr>
</tbody>
</table>
<h3 id="83-common-pitfalls">8.3 Common Pitfalls<a class="header-link" href="#83-common-pitfalls" title="Permanent link">&para;</a></h3>
<p><strong>1. Using wrong reduction</strong></p>
<div class="highlight"><pre><span></span><code><span class="c1"># Bad: Default reduction=&#39;mean&#39; might not be what you want</span>
<span class="n">criterion</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">CrossEntropyLoss</span><span class="p">()</span>

<span class="c1"># Good: Explicit reduction</span>
<span class="n">criterion</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">CrossEntropyLoss</span><span class="p">(</span><span class="n">reduction</span><span class="o">=</span><span class="s1">&#39;mean&#39;</span><span class="p">)</span>  <span class="c1"># or &#39;sum&#39;, &#39;none&#39;</span>
</code></pre></div>

<p><strong>2. Forgetting to apply activation</strong></p>
<div class="highlight"><pre><span></span><code><span class="c1"># Bad: Using BCELoss with raw logits</span>
<span class="n">pred</span> <span class="o">=</span> <span class="n">model</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>  <span class="c1"># Raw logits</span>
<span class="n">loss</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">BCELoss</span><span class="p">()(</span><span class="n">pred</span><span class="p">,</span> <span class="n">target</span><span class="p">)</span>  <span class="c1"># Wrong! BCELoss expects probabilities</span>

<span class="c1"># Good: Use BCEWithLogitsLoss</span>
<span class="n">loss</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">BCEWithLogitsLoss</span><span class="p">()(</span><span class="n">pred</span><span class="p">,</span> <span class="n">target</span><span class="p">)</span>

<span class="c1"># Or apply sigmoid first</span>
<span class="n">loss</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">BCELoss</span><span class="p">()(</span><span class="n">torch</span><span class="o">.</span><span class="n">sigmoid</span><span class="p">(</span><span class="n">pred</span><span class="p">),</span> <span class="n">target</span><span class="p">)</span>
</code></pre></div>

<p><strong>3. Wrong target format</strong></p>
<div class="highlight"><pre><span></span><code><span class="c1"># CrossEntropyLoss expects class indices, not one-hot</span>
<span class="c1"># Bad</span>
<span class="n">target</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">],</span> <span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">]])</span>  <span class="c1"># One-hot</span>
<span class="n">loss</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">CrossEntropyLoss</span><span class="p">()(</span><span class="n">pred</span><span class="p">,</span> <span class="n">target</span><span class="p">)</span>  <span class="c1"># Error!</span>

<span class="c1"># Good</span>
<span class="n">target</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">])</span>  <span class="c1"># Class indices</span>
<span class="n">loss</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">CrossEntropyLoss</span><span class="p">()(</span><span class="n">pred</span><span class="p">,</span> <span class="n">target</span><span class="p">)</span>
</code></pre></div>

<p><strong>4. Not handling class imbalance</strong></p>
<div class="highlight"><pre><span></span><code><span class="c1"># Bad: Ignoring class imbalance (e.g., 95% class 0, 5% class 1)</span>
<span class="n">criterion</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">CrossEntropyLoss</span><span class="p">()</span>

<span class="c1"># Good: Use class weights</span>
<span class="n">class_weights</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([</span><span class="mf">1.0</span><span class="p">,</span> <span class="mf">19.0</span><span class="p">])</span>  <span class="c1"># Weight minority class higher</span>
<span class="n">criterion</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">CrossEntropyLoss</span><span class="p">(</span><span class="n">weight</span><span class="o">=</span><span class="n">class_weights</span><span class="p">)</span>

<span class="c1"># Or use FocalLoss</span>
<span class="n">criterion</span> <span class="o">=</span> <span class="n">FocalLoss</span><span class="p">(</span><span class="n">alpha</span><span class="o">=</span><span class="mf">0.25</span><span class="p">,</span> <span class="n">gamma</span><span class="o">=</span><span class="mf">2.0</span><span class="p">)</span>
</code></pre></div>

<p><strong>5. Incorrect loss scaling</strong></p>
<div class="highlight"><pre><span></span><code><span class="c1"># Bad: Losses of different magnitudes</span>
<span class="n">total_loss</span> <span class="o">=</span> <span class="n">loss1</span> <span class="o">+</span> <span class="n">loss2</span>  <span class="c1"># loss1 ~ 0.01, loss2 ~ 100.0</span>

<span class="c1"># Good: Normalize or weight appropriately</span>
<span class="n">total_loss</span> <span class="o">=</span> <span class="mi">100</span> <span class="o">*</span> <span class="n">loss1</span> <span class="o">+</span> <span class="n">loss2</span>
<span class="c1"># Or use learnable weights</span>
<span class="n">total_loss</span> <span class="o">=</span> <span class="n">w1</span> <span class="o">*</span> <span class="n">loss1</span> <span class="o">+</span> <span class="n">w2</span> <span class="o">*</span> <span class="n">loss2</span>
</code></pre></div>

<h3 id="84-debugging-tips">8.4 Debugging Tips<a class="header-link" href="#84-debugging-tips" title="Permanent link">&para;</a></h3>
<p><strong>1. Check loss values</strong></p>
<div class="highlight"><pre><span></span><code><span class="c1"># Monitor loss statistics</span>
<span class="k">def</span><span class="w"> </span><span class="nf">check_loss</span><span class="p">(</span><span class="n">loss</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="s2">&quot;Loss&quot;</span><span class="p">):</span>
    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="n">name</span><span class="si">}</span><span class="s2">: </span><span class="si">{</span><span class="n">loss</span><span class="o">.</span><span class="n">item</span><span class="p">()</span><span class="si">:</span><span class="s2">.4f</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
    <span class="k">assert</span> <span class="ow">not</span> <span class="n">torch</span><span class="o">.</span><span class="n">isnan</span><span class="p">(</span><span class="n">loss</span><span class="p">),</span> <span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="n">name</span><span class="si">}</span><span class="s2"> is NaN!&quot;</span>
    <span class="k">assert</span> <span class="ow">not</span> <span class="n">torch</span><span class="o">.</span><span class="n">isinf</span><span class="p">(</span><span class="n">loss</span><span class="p">),</span> <span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="n">name</span><span class="si">}</span><span class="s2"> is Inf!&quot;</span>
    <span class="k">assert</span> <span class="n">loss</span> <span class="o">&gt;=</span> <span class="mi">0</span><span class="p">,</span> <span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="n">name</span><span class="si">}</span><span class="s2"> is negative!&quot;</span>

<span class="c1"># Use in training</span>
<span class="n">loss</span> <span class="o">=</span> <span class="n">criterion</span><span class="p">(</span><span class="n">pred</span><span class="p">,</span> <span class="n">target</span><span class="p">)</span>
<span class="n">check_loss</span><span class="p">(</span><span class="n">loss</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="s2">&quot;Training Loss&quot;</span><span class="p">)</span>
</code></pre></div>

<p><strong>2. Visualize loss landscape</strong></p>
<div class="highlight"><pre><span></span><code><span class="kn">import</span><span class="w"> </span><span class="nn">matplotlib.pyplot</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">plt</span>

<span class="c1"># Track loss over training</span>
<span class="n">losses</span> <span class="o">=</span> <span class="p">[]</span>
<span class="k">for</span> <span class="n">epoch</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">num_epochs</span><span class="p">):</span>
    <span class="c1"># ... training ...</span>
    <span class="n">losses</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">loss</span><span class="o">.</span><span class="n">item</span><span class="p">())</span>

<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">losses</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s1">&#39;Epoch&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s1">&#39;Loss&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s1">&#39;Training Loss Curve&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">yscale</span><span class="p">(</span><span class="s1">&#39;log&#39;</span><span class="p">)</span>  <span class="c1"># Use log scale if loss varies widely</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</code></pre></div>

<p><strong>3. Compare with baseline</strong></p>
<div class="highlight"><pre><span></span><code><span class="c1"># Sanity check: random predictions should give expected loss</span>
<span class="c1"># For CrossEntropyLoss with C classes: expected loss ‚âà log(C)</span>

<span class="n">num_classes</span> <span class="o">=</span> <span class="mi">10</span>
<span class="n">random_pred</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">100</span><span class="p">,</span> <span class="n">num_classes</span><span class="p">)</span>
<span class="n">random_target</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randint</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">num_classes</span><span class="p">,</span> <span class="p">(</span><span class="mi">100</span><span class="p">,))</span>

<span class="n">loss</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">CrossEntropyLoss</span><span class="p">()(</span><span class="n">random_pred</span><span class="p">,</span> <span class="n">random_target</span><span class="p">)</span>
<span class="n">expected</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="n">num_classes</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">float</span><span class="p">))</span>

<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Random Loss: </span><span class="si">{</span><span class="n">loss</span><span class="o">.</span><span class="n">item</span><span class="p">()</span><span class="si">:</span><span class="s2">.4f</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Expected: </span><span class="si">{</span><span class="n">expected</span><span class="o">.</span><span class="n">item</span><span class="p">()</span><span class="si">:</span><span class="s2">.4f</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>  <span class="c1"># Should be ‚âà 2.3026 for 10 classes</span>
</code></pre></div>

<p><strong>4. Gradient flow check</strong></p>
<div class="highlight"><pre><span></span><code><span class="c1"># Check if gradients are flowing</span>
<span class="k">def</span><span class="w"> </span><span class="nf">check_gradients</span><span class="p">(</span><span class="n">model</span><span class="p">):</span>
    <span class="k">for</span> <span class="n">name</span><span class="p">,</span> <span class="n">param</span> <span class="ow">in</span> <span class="n">model</span><span class="o">.</span><span class="n">named_parameters</span><span class="p">():</span>
        <span class="k">if</span> <span class="n">param</span><span class="o">.</span><span class="n">grad</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">grad_norm</span> <span class="o">=</span> <span class="n">param</span><span class="o">.</span><span class="n">grad</span><span class="o">.</span><span class="n">norm</span><span class="p">()</span><span class="o">.</span><span class="n">item</span><span class="p">()</span>
            <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="n">name</span><span class="si">}</span><span class="s2">: grad norm = </span><span class="si">{</span><span class="n">grad_norm</span><span class="si">:</span><span class="s2">.6f</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
            <span class="k">if</span> <span class="n">grad_norm</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
                <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;  WARNING: Zero gradient!&quot;</span><span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="n">name</span><span class="si">}</span><span class="s2">: No gradient!&quot;</span><span class="p">)</span>

<span class="c1"># After backward</span>
<span class="n">loss</span><span class="o">.</span><span class="n">backward</span><span class="p">()</span>
<span class="n">check_gradients</span><span class="p">(</span><span class="n">model</span><span class="p">)</span>
</code></pre></div>

<h3 id="85-loss-function-impact-on-convergence">8.5 Loss Function Impact on Convergence<a class="header-link" href="#85-loss-function-impact-on-convergence" title="Permanent link">&para;</a></h3>
<p><strong>Example: Comparing Convergence Speed</strong></p>
<div class="highlight"><pre><span></span><code><span class="kn">import</span><span class="w"> </span><span class="nn">matplotlib.pyplot</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">plt</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">torch</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">torch.nn</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">nn</span>

<span class="c1"># Simple model</span>
<span class="k">class</span><span class="w"> </span><span class="nc">SimpleNet</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="k">def</span><span class="w"> </span><span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">fc</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="mi">10</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>

    <span class="k">def</span><span class="w"> </span><span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">fc</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>

<span class="k">def</span><span class="w"> </span><span class="nf">train_with_loss</span><span class="p">(</span><span class="n">criterion</span><span class="p">,</span> <span class="n">num_epochs</span><span class="o">=</span><span class="mi">100</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Train and return loss history&quot;&quot;&quot;</span>
    <span class="n">model</span> <span class="o">=</span> <span class="n">SimpleNet</span><span class="p">()</span>
    <span class="n">optimizer</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">optim</span><span class="o">.</span><span class="n">SGD</span><span class="p">(</span><span class="n">model</span><span class="o">.</span><span class="n">parameters</span><span class="p">(),</span> <span class="n">lr</span><span class="o">=</span><span class="mf">0.01</span><span class="p">)</span>

    <span class="c1"># Dummy data</span>
    <span class="n">X</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">1000</span><span class="p">,</span> <span class="mi">10</span><span class="p">)</span>
    <span class="n">y</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">1000</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>

    <span class="n">losses</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="k">for</span> <span class="n">epoch</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">num_epochs</span><span class="p">):</span>
        <span class="n">optimizer</span><span class="o">.</span><span class="n">zero_grad</span><span class="p">()</span>
        <span class="n">pred</span> <span class="o">=</span> <span class="n">model</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>
        <span class="n">loss</span> <span class="o">=</span> <span class="n">criterion</span><span class="p">(</span><span class="n">pred</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>
        <span class="n">loss</span><span class="o">.</span><span class="n">backward</span><span class="p">()</span>
        <span class="n">optimizer</span><span class="o">.</span><span class="n">step</span><span class="p">()</span>
        <span class="n">losses</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">loss</span><span class="o">.</span><span class="n">item</span><span class="p">())</span>

    <span class="k">return</span> <span class="n">losses</span>

<span class="c1"># Compare different losses</span>
<span class="n">mse_losses</span> <span class="o">=</span> <span class="n">train_with_loss</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">MSELoss</span><span class="p">())</span>
<span class="n">mae_losses</span> <span class="o">=</span> <span class="n">train_with_loss</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">L1Loss</span><span class="p">())</span>
<span class="n">huber_losses</span> <span class="o">=</span> <span class="n">train_with_loss</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">SmoothL1Loss</span><span class="p">())</span>

<span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">10</span><span class="p">,</span> <span class="mi">6</span><span class="p">))</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">mse_losses</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s1">&#39;MSE&#39;</span><span class="p">,</span> <span class="n">linewidth</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">mae_losses</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s1">&#39;MAE&#39;</span><span class="p">,</span> <span class="n">linewidth</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">huber_losses</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s1">&#39;Huber&#39;</span><span class="p">,</span> <span class="n">linewidth</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s1">&#39;Epoch&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s1">&#39;Loss&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s1">&#39;Convergence Speed Comparison&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">legend</span><span class="p">()</span>
<span class="n">plt</span><span class="o">.</span><span class="n">grid</span><span class="p">(</span><span class="kc">True</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.3</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">yscale</span><span class="p">(</span><span class="s1">&#39;log&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">savefig</span><span class="p">(</span><span class="s1">&#39;convergence_comparison.png&#39;</span><span class="p">,</span> <span class="n">dpi</span><span class="o">=</span><span class="mi">150</span><span class="p">,</span> <span class="n">bbox_inches</span><span class="o">=</span><span class="s1">&#39;tight&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</code></pre></div>

<hr />
<h2 id="exercises">Exercises<a class="header-link" href="#exercises" title="Permanent link">&para;</a></h2>
<h3 id="exercise-1-implement-tversky-loss">Exercise 1: Implement Tversky Loss<a class="header-link" href="#exercise-1-implement-tversky-loss" title="Permanent link">&para;</a></h3>
<p>The Tversky loss is a generalization of Dice loss for segmentation, with controllable false positive/negative trade-off:</p>
<div class="highlight"><pre><span></span><code>Tversky = (TP) / (TP + Œ±*FP + Œ≤*FN)

where:
<span class="k">-</span> TP = true positives
<span class="k">-</span> FP = false positives
<span class="k">-</span> FN = false negatives
<span class="k">-</span> Œ±, Œ≤ control the trade-off (typically Œ± + Œ≤ = 1)
</code></pre></div>

<p><strong>Task:</strong>
1. Implement <code>TverskyLoss</code> as a PyTorch <code>nn.Module</code>
2. Test with Œ±=0.3, Œ≤=0.7 (focus on reducing false negatives)
3. Compare with DiceLoss on a binary segmentation task</p>
<p><strong>Starter Code:</strong></p>
<div class="highlight"><pre><span></span><code><span class="k">class</span><span class="w"> </span><span class="nc">TverskyLoss</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="k">def</span><span class="w"> </span><span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.5</span><span class="p">,</span> <span class="n">beta</span><span class="o">=</span><span class="mf">0.5</span><span class="p">,</span> <span class="n">smooth</span><span class="o">=</span><span class="mf">1.0</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
        <span class="c1"># TODO: Initialize parameters</span>
        <span class="k">pass</span>

    <span class="k">def</span><span class="w"> </span><span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">pred</span><span class="p">,</span> <span class="n">target</span><span class="p">):</span>
        <span class="c1"># TODO: Implement Tversky loss</span>
        <span class="c1"># Hint: Calculate TP, FP, FN</span>
        <span class="k">pass</span>
</code></pre></div>

<h3 id="exercise-2-multi-scale-perceptual-loss">Exercise 2: Multi-Scale Perceptual Loss<a class="header-link" href="#exercise-2-multi-scale-perceptual-loss" title="Permanent link">&para;</a></h3>
<p>Implement a perceptual loss that compares features at multiple scales (different layers of a pretrained network).</p>
<p><strong>Task:</strong>
1. Extract features from layers <code>['relu2_2', 'relu3_3', 'relu4_3']</code> of VGG16
2. Compute MSE between predicted and target features at each layer
3. Combine losses with learnable weights
4. Test on image reconstruction task</p>
<p><strong>Questions:</strong>
- How do different layers affect the loss?
- What happens if you only use early layers vs. only deep layers?</p>
<h3 id="exercise-3-adaptive-loss-balancing">Exercise 3: Adaptive Loss Balancing<a class="header-link" href="#exercise-3-adaptive-loss-balancing" title="Permanent link">&para;</a></h3>
<p>Implement an adaptive loss balancing scheme for a multi-task learning scenario.</p>
<p><strong>Task:</strong>
You're training a model for autonomous driving with three tasks:
1. Semantic segmentation (CrossEntropyLoss)
2. Depth estimation (L1Loss)
3. Object detection (FocalLoss)</p>
<p>Implement:
1. Uncertainty-based weighting (Section 7.1)
2. Track the learned weights over training
3. Visualize how weights change as training progresses</p>
<p><strong>Starter Code:</strong></p>
<div class="highlight"><pre><span></span><code><span class="k">class</span><span class="w"> </span><span class="nc">MultiTaskModel</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="k">def</span><span class="w"> </span><span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
        <span class="c1"># TODO: Define three task heads</span>
        <span class="k">pass</span>

    <span class="k">def</span><span class="w"> </span><span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
        <span class="c1"># TODO: Return predictions for all three tasks</span>
        <span class="k">pass</span>

<span class="c1"># Training loop</span>
<span class="k">for</span> <span class="n">epoch</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">num_epochs</span><span class="p">):</span>
    <span class="c1"># TODO:</span>
    <span class="c1"># 1. Forward pass</span>
    <span class="c1"># 2. Compute three losses</span>
    <span class="c1"># 3. Apply uncertainty weighting</span>
    <span class="c1"># 4. Backward and optimize</span>
    <span class="k">pass</span>
</code></pre></div>

<p><strong>Questions:</strong>
- Which task gets the highest weight? Why?
- How quickly do weights stabilize?
- What happens if you initialize weights differently?</p>
<hr />
<h2 id="references">References<a class="header-link" href="#references" title="Permanent link">&para;</a></h2>
<ol>
<li><strong>Loss Functions Survey:</strong></li>
<li>
<p>Janocha, K., &amp; Czarnecki, W. M. (2017). "On Loss Functions for Deep Neural Networks in Classification"</p>
</li>
<li>
<p><strong>Regression Losses:</strong></p>
</li>
<li>
<p>Huber, P. J. (1964). "Robust Estimation of a Location Parameter"</p>
</li>
<li>
<p><strong>Classification Losses:</strong></p>
</li>
<li>Lin, T. Y., et al. (2017). "Focal Loss for Dense Object Detection" (RetinaNet)</li>
<li>
<p>M√ºller, R., et al. (2019). "When Does Label Smoothing Help?"</p>
</li>
<li>
<p><strong>Metric Learning:</strong></p>
</li>
<li>Schroff, F., et al. (2015). "FaceNet: A Unified Embedding for Face Recognition and Clustering" (Triplet Loss)</li>
<li>Chopra, S., et al. (2005). "Learning a Similarity Metric Discriminatively" (Contrastive Loss)</li>
<li>
<p>Chen, T., et al. (2020). "A Simple Framework for Contrastive Learning of Visual Representations" (SimCLR, NT-Xent)</p>
</li>
<li>
<p><strong>Segmentation Losses:</strong></p>
</li>
<li>Milletari, F., et al. (2016). "V-Net: Fully Convolutional Neural Networks for Volumetric Medical Image Segmentation" (Dice Loss)</li>
<li>
<p>Rezatofighi, H., et al. (2019). "Generalized Intersection over Union" (GIoU)</p>
</li>
<li>
<p><strong>Generative Models:</strong></p>
</li>
<li>Goodfellow, I., et al. (2014). "Generative Adversarial Networks"</li>
<li>Kingma, D. P., &amp; Welling, M. (2013). "Auto-Encoding Variational Bayes"</li>
<li>
<p>Johnson, J., et al. (2016). "Perceptual Losses for Real-Time Style Transfer and Super-Resolution"</p>
</li>
<li>
<p><strong>Multi-Task Learning:</strong></p>
</li>
<li>Kendall, A., et al. (2018). "Multi-Task Learning Using Uncertainty to Weigh Losses for Scene Geometry and Semantics"</li>
<li>
<p>Chen, Z., et al. (2018). "GradNorm: Gradient Normalization for Adaptive Loss Balancing in Deep Multitask Networks"</p>
</li>
<li>
<p><strong>PyTorch Documentation:</strong></p>
</li>
<li>https://pytorch.org/docs/stable/nn.html#loss-functions</li>
<li>
<p>https://pytorch.org/tutorials/beginner/basics/optimization_tutorial.html</p>
</li>
<li>
<p><strong>Additional Resources:</strong></p>
</li>
<li>Murphy, K. P. (2022). "Probabilistic Machine Learning: An Introduction" (Chapter on Loss Functions)</li>
<li>Goodfellow, I., et al. (2016). "Deep Learning" (Chapter 8: Optimization for Training Deep Models)</li>
</ol>
    </div>

    
<div class="lesson-toolbar lesson-toolbar--bottom">
    <div class="toolbar-nav toolbar-nav--prev">
        
        <a href="/study/en/Deep_Learning/23_Training_Optimization.html" class="nav-prev">
            <span class="nav-label">Previous</span>
            <span class="nav-title">23. Training Optimization</span>
        </a>
        
    </div>
    <div class="toolbar-actions">
        <button class="btn btn-copy-link" data-action="copy-link" title="Copy link">
            <span class="icon">üîó</span>
            <span class="text">Copy link</span>
        </button>
        <a href="/study/en/Deep_Learning/" class="btn btn-topic-link" title="Back to topic list">
            <span class="icon">üìã</span>
            <span class="text">Topic list</span>
        </a>
    </div>
    <div class="toolbar-nav toolbar-nav--next">
        
        <a href="/study/en/Deep_Learning/25_Optimizers.html" class="nav-next">
            <span class="nav-label">Next</span>
            <span class="nav-title">25. Optimizers</span>
        </a>
        
    </div>
</div>


    <div class="toolbar-keyboard-hint">
        <kbd>&larr;</kbd> <kbd>&rarr;</kbd> to navigate between lessons
    </div>
</article>

<!-- Scroll to top floating button -->
<button class="scroll-to-top" id="scroll-to-top" title="Scroll to top" aria-label="Scroll to top">‚Üë</button>

            </div>
        </main>
    </div>

    <script src="/study/static/js/app.js"></script>
    <script>
        function switchLanguage(newLang) {
            const path = window.location.pathname;
            const base = '/study';
            const currentLang = 'en';
            const newPath = path.replace(base + '/' + currentLang + '/', base + '/' + newLang + '/');
            window.location.href = newPath + window.location.search;
        }
    </script>
    
<script>
    document.addEventListener('DOMContentLoaded', function() {
        // Copy link - independent feedback per button
        document.querySelectorAll('[data-action="copy-link"]').forEach(function(btn) {
            btn.addEventListener('click', function() {
                navigator.clipboard.writeText(window.location.href);
                var textEl = this.querySelector('.text');
                var originalText = textEl.textContent;
                textEl.textContent = 'Copied!';
                setTimeout(function() { textEl.textContent = originalText; }, 2000);
            });
        });

        // Add copy buttons to code blocks
        document.querySelectorAll('.lesson-content pre').forEach(function(pre) {
            var copyBtn = document.createElement('button');
            copyBtn.className = 'code-copy-btn';
            copyBtn.textContent = 'Copy';
            copyBtn.addEventListener('click', function() {
                var code = pre.querySelector('code');
                var text = code ? code.textContent : pre.textContent;
                navigator.clipboard.writeText(text);
                copyBtn.textContent = 'Copied!';
                setTimeout(function() { copyBtn.textContent = 'Copy'; }, 2000);
            });
            pre.style.position = 'relative';
            pre.appendChild(copyBtn);
        });

        // Scroll to top button
        var scrollBtn = document.getElementById('scroll-to-top');
        window.addEventListener('scroll', function() {
            scrollBtn.classList.toggle('visible', window.scrollY > 300);
        });
        scrollBtn.addEventListener('click', function() {
            window.scrollTo({ top: 0, behavior: 'smooth' });
        });

        // Keyboard shortcuts: left/right arrows for lesson navigation
        document.addEventListener('keydown', function(e) {
            var tag = document.activeElement.tagName;
            if (tag === 'INPUT' || tag === 'TEXTAREA' || tag === 'SELECT') return;

            if (e.key === 'ArrowLeft') {
                var prevLink = document.querySelector('.nav-prev');
                if (prevLink) prevLink.click();
            } else if (e.key === 'ArrowRight') {
                var nextLink = document.querySelector('.nav-next');
                if (nextLink) nextLink.click();
            }
        });
    });
</script>

</body>
</html>
{% endraw %}