{% raw %}
<!DOCTYPE html>
<html lang="en" data-theme="light">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>25. Optimizers - Study Materials</title>
    <link rel="stylesheet" href="/study/static/css/style.css">
    <link rel="stylesheet" href="/study/static/css/highlight.css">
    <!-- KaTeX for LaTeX math rendering -->
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.11/dist/katex.min.css">
    <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.11/dist/katex.min.js"></script>
    <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.11/dist/contrib/auto-render.min.js"
        onload="renderMathInElement(document.body, {
            delimiters: [
                {left: '$$', right: '$$', display: true},
                {left: '$', right: '$', display: false}
            ],
            throwOnError: false
        });"></script>
    
</head>
<body>
    <div class="app-container">
        <!-- Sidebar -->
        <aside class="sidebar" id="sidebar">
            <div class="sidebar-header">
                <a href="/study/en/" class="logo">Study Materials</a>
                <button class="sidebar-toggle" id="sidebar-toggle" aria-label="Toggle sidebar">
                    <span></span>
                </button>
            </div>

            <nav class="sidebar-nav">
                <a href="/study/en/" class="nav-item ">
                    <span class="nav-icon">üè†</span>
                    <span class="nav-text">Home</span>
                </a>
                <a href="/study/examples/" class="nav-item ">
                    <span class="nav-icon">üíª</span>
                    <span class="nav-text">Examples</span>
                </a>
            </nav>

            <div class="sidebar-search">
                <form action="/study/en/search.html" method="get" id="search-form">
                    <input type="search" name="q" placeholder="Search..." id="search-sidebar-input">
                </form>
            </div>

            <!-- Language Selector -->
            <div class="sidebar-lang">
                <select id="lang-select" class="lang-selector" onchange="switchLanguage(this.value)">
                    
                    <option value="en" selected>
                        English
                    </option>
                    
                    <option value="ko" >
                        ÌïúÍµ≠Ïñ¥
                    </option>
                    
                </select>
            </div>

            <div class="sidebar-footer">
                <button class="theme-toggle" id="theme-toggle" aria-label="Toggle theme">
                    <span class="theme-icon light">‚òÄÔ∏è</span>
                    <span class="theme-icon dark">üåô</span>
                </button>
            </div>
        </aside>

        <!-- Main Content -->
        <main class="main-content">
            <header class="main-header">
                <button class="menu-toggle" id="menu-toggle" aria-label="Open menu">
                    <span></span>
                </button>
                
<nav class="breadcrumb">
    <a href="/study/en/">Topics</a>
    <span class="separator">/</span>
    <a href="/study/en/Deep_Learning/">Deep Learning</a>
    <span class="separator">/</span>
    <span class="current">25. Optimizers</span>
</nav>

            </header>

            <div class="content">
                
<article class="lesson-article">
    <header class="lesson-header">
        <h1>25. Optimizers</h1>
    </header>

    
<div class="lesson-toolbar lesson-toolbar--top">
    <div class="toolbar-nav toolbar-nav--prev">
        
        <a href="/study/en/Deep_Learning/24_Loss_Functions.html" class="nav-prev">
            <span class="nav-label">Previous</span>
            <span class="nav-title">24. Loss Functions</span>
        </a>
        
    </div>
    <div class="toolbar-actions">
        <button class="btn btn-copy-link" data-action="copy-link" title="Copy link">
            <span class="icon">üîó</span>
            <span class="text">Copy link</span>
        </button>
        <a href="/study/en/Deep_Learning/" class="btn btn-topic-link" title="Back to topic list">
            <span class="icon">üìã</span>
            <span class="text">Topic list</span>
        </a>
    </div>
    <div class="toolbar-nav toolbar-nav--next">
        
        <a href="/study/en/Deep_Learning/26_Normalization_Layers.html" class="nav-next">
            <span class="nav-label">Next</span>
            <span class="nav-title">26. Normalization Layers</span>
        </a>
        
    </div>
</div>


    
    <nav class="toc" id="toc">
        <h2>Table of Contents</h2>
        <div class="toc">
<ul>
<li><a href="#learning-objectives">Learning Objectives</a></li>
<li><a href="#1-gradient-descent-fundamentals">1. Gradient Descent Fundamentals</a><ul>
<li><a href="#11-variants-of-gradient-descent">1.1 Variants of Gradient Descent</a></li>
<li><a href="#12-the-optimization-landscape">1.2 The Optimization Landscape</a></li>
</ul>
</li>
<li><a href="#2-classic-optimizers">2. Classic Optimizers</a><ul>
<li><a href="#21-stochastic-gradient-descent-sgd">2.1 Stochastic Gradient Descent (SGD)</a></li>
<li><a href="#22-sgd-with-momentum">2.2 SGD with Momentum</a></li>
<li><a href="#23-nesterov-accelerated-gradient-nag">2.3 Nesterov Accelerated Gradient (NAG)</a></li>
</ul>
</li>
<li><a href="#3-adaptive-learning-rate-methods">3. Adaptive Learning Rate Methods</a><ul>
<li><a href="#31-adagrad">3.1 Adagrad</a></li>
<li><a href="#32-rmsprop">3.2 RMSprop</a></li>
<li><a href="#33-adam-adaptive-moment-estimation">3.3 Adam (Adaptive Moment Estimation)</a></li>
<li><a href="#34-adamw-adam-with-decoupled-weight-decay">3.4 AdamW (Adam with Decoupled Weight Decay)</a></li>
</ul>
</li>
<li><a href="#4-modern-optimizers">4. Modern Optimizers</a><ul>
<li><a href="#41-lamb-layer-wise-adaptive-moments-optimizer-for-batch-training">4.1 LAMB (Layer-wise Adaptive Moments optimizer for Batch training)</a></li>
<li><a href="#42-adafactor">4.2 Adafactor</a></li>
<li><a href="#43-lion-evolved-sign-momentum">4.3 Lion (Evolved Sign Momentum)</a></li>
<li><a href="#44-8-bit-adam-bitsandbytes">4.4 8-bit Adam (bitsandbytes)</a></li>
<li><a href="#45-sophia-second-order-clipped-stochastic-optimization">4.5 Sophia (Second-order Clipped Stochastic Optimization)</a></li>
<li><a href="#46-optimizer-comparison-table">4.6 Optimizer Comparison Table</a></li>
</ul>
</li>
<li><a href="#5-learning-rate-schedulers">5. Learning Rate Schedulers</a><ul>
<li><a href="#51-step-based-schedulers">5.1 Step-based Schedulers</a></li>
<li><a href="#52-exponential-decay">5.2 Exponential Decay</a></li>
<li><a href="#53-cosine-annealing">5.3 Cosine Annealing</a></li>
<li><a href="#54-onecyclelr-super-convergence">5.4 OneCycleLR (Super-convergence)</a></li>
<li><a href="#55-linear-warmup-cosine-decay-transformer-standard">5.5 Linear Warmup + Cosine Decay (Transformer Standard)</a></li>
<li><a href="#56-reducelronplateau">5.6 ReduceLROnPlateau</a></li>
<li><a href="#57-custom-scheduler">5.7 Custom Scheduler</a></li>
<li><a href="#58-scheduler-visualization-suite">5.8 Scheduler Visualization Suite</a></li>
</ul>
</li>
<li><a href="#6-practical-techniques">6. Practical Techniques</a><ul>
<li><a href="#61-learning-rate-finder-lr-range-test">6.1 Learning Rate Finder (LR Range Test)</a></li>
<li><a href="#62-gradient-clipping">6.2 Gradient Clipping</a></li>
<li><a href="#63-gradient-accumulation">6.3 Gradient Accumulation</a></li>
<li><a href="#64-mixed-precision-training">6.4 Mixed Precision Training</a></li>
<li><a href="#65-per-parameter-group-learning-rates">6.5 Per-Parameter Group Learning Rates</a></li>
</ul>
</li>
<li><a href="#7-choosing-the-right-optimizer">7. Choosing the Right Optimizer</a><ul>
<li><a href="#71-decision-guide">7.1 Decision Guide</a></li>
<li><a href="#72-sgd-vs-adam-debate">7.2 SGD vs Adam Debate</a></li>
<li><a href="#73-common-recipes">7.3 Common Recipes</a></li>
<li><a href="#74-debugging-optimization-issues">7.4 Debugging Optimization Issues</a></li>
</ul>
</li>
<li><a href="#exercises">Exercises</a><ul>
<li><a href="#exercise-1-implement-and-compare-optimizers">Exercise 1: Implement and Compare Optimizers</a></li>
<li><a href="#exercise-2-scheduler-ablation-study">Exercise 2: Scheduler Ablation Study</a></li>
<li><a href="#exercise-3-large-batch-training-simulation">Exercise 3: Large-Batch Training Simulation</a></li>
</ul>
</li>
<li><a href="#references">References</a></li>
</ul>
</div>

    </nav>
    

    <div class="lesson-content markdown-body">
        <p><a href="./24_Loss_Functions.md">Previous: Loss Functions</a> | <a href="./26_Normalization_Layers.md">Next: Normalization Layers</a></p>
<hr />
<h1 id="25-optimizers">25. Optimizers<a class="header-link" href="#25-optimizers" title="Permanent link">&para;</a></h1>
<h2 id="learning-objectives">Learning Objectives<a class="header-link" href="#learning-objectives" title="Permanent link">&para;</a></h2>
<ul>
<li>Understand gradient descent variants and the optimization landscape in deep learning</li>
<li>Master classic optimizers (SGD, Momentum, Nesterov) and adaptive methods (Adagrad, RMSprop, Adam, AdamW)</li>
<li>Explore modern optimizers (LAMB, Adafactor, Lion, 8-bit Adam) for large-scale training</li>
<li>Implement learning rate schedulers (cosine annealing, OneCycleLR, warmup strategies)</li>
<li>Apply practical optimization techniques (gradient clipping, accumulation, per-parameter groups)</li>
<li>Choose the right optimizer and hyperparameters for different architectures and tasks</li>
</ul>
<p><strong>Difficulty</strong>: ‚≠ê‚≠ê‚≠ê</p>
<hr />
<h2 id="1-gradient-descent-fundamentals">1. Gradient Descent Fundamentals<a class="header-link" href="#1-gradient-descent-fundamentals" title="Permanent link">&para;</a></h2>
<h3 id="11-variants-of-gradient-descent">1.1 Variants of Gradient Descent<a class="header-link" href="#11-variants-of-gradient-descent" title="Permanent link">&para;</a></h3>
<p><strong>Batch Gradient Descent</strong> computes the gradient using the entire training dataset:</p>
<p>$$
\theta_{t+1} = \theta_t - \eta \nabla_\theta \mathcal{L}(\theta_t)
$$</p>
<p>where $\mathcal{L}$ is the average loss over all training examples.</p>
<p><strong>Stochastic Gradient Descent (SGD)</strong> uses a single random example:</p>
<p>$$
\theta_{t+1} = \theta_t - \eta \nabla_\theta \ell(x_i, y_i; \theta_t)
$$</p>
<p><strong>Mini-batch Gradient Descent</strong> is the practical middle ground:</p>
<p>$$
\theta_{t+1} = \theta_t - \eta \nabla_\theta \frac{1}{|\mathcal{B}|} \sum_{(x,y) \in \mathcal{B}} \ell(x, y; \theta_t)
$$</p>
<div class="highlight"><pre><span></span><code><span class="kn">import</span><span class="w"> </span><span class="nn">torch</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">torch.nn</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">nn</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">torch.optim</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">optim</span>

<span class="c1"># Example: Different GD variants</span>
<span class="k">def</span><span class="w"> </span><span class="nf">train_epoch_batch_gd</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">train_loader</span><span class="p">,</span> <span class="n">criterion</span><span class="p">,</span> <span class="n">optimizer</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Batch GD: accumulate gradients over entire epoch&quot;&quot;&quot;</span>
    <span class="n">model</span><span class="o">.</span><span class="n">train</span><span class="p">()</span>
    <span class="n">optimizer</span><span class="o">.</span><span class="n">zero_grad</span><span class="p">()</span>
    <span class="n">total_loss</span> <span class="o">=</span> <span class="mi">0</span>

    <span class="k">for</span> <span class="n">X</span><span class="p">,</span> <span class="n">y</span> <span class="ow">in</span> <span class="n">train_loader</span><span class="p">:</span>
        <span class="n">output</span> <span class="o">=</span> <span class="n">model</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>
        <span class="n">loss</span> <span class="o">=</span> <span class="n">criterion</span><span class="p">(</span><span class="n">output</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>
        <span class="n">loss</span><span class="o">.</span><span class="n">backward</span><span class="p">()</span>  <span class="c1"># Accumulate gradients</span>
        <span class="n">total_loss</span> <span class="o">+=</span> <span class="n">loss</span><span class="o">.</span><span class="n">item</span><span class="p">()</span>

    <span class="n">optimizer</span><span class="o">.</span><span class="n">step</span><span class="p">()</span>  <span class="c1"># Single update after full pass</span>
    <span class="k">return</span> <span class="n">total_loss</span> <span class="o">/</span> <span class="nb">len</span><span class="p">(</span><span class="n">train_loader</span><span class="p">)</span>

<span class="k">def</span><span class="w"> </span><span class="nf">train_epoch_sgd</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">train_loader</span><span class="p">,</span> <span class="n">criterion</span><span class="p">,</span> <span class="n">optimizer</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;SGD: update after each batch&quot;&quot;&quot;</span>
    <span class="n">model</span><span class="o">.</span><span class="n">train</span><span class="p">()</span>
    <span class="n">total_loss</span> <span class="o">=</span> <span class="mi">0</span>

    <span class="k">for</span> <span class="n">X</span><span class="p">,</span> <span class="n">y</span> <span class="ow">in</span> <span class="n">train_loader</span><span class="p">:</span>
        <span class="n">optimizer</span><span class="o">.</span><span class="n">zero_grad</span><span class="p">()</span>
        <span class="n">output</span> <span class="o">=</span> <span class="n">model</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>
        <span class="n">loss</span> <span class="o">=</span> <span class="n">criterion</span><span class="p">(</span><span class="n">output</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>
        <span class="n">loss</span><span class="o">.</span><span class="n">backward</span><span class="p">()</span>
        <span class="n">optimizer</span><span class="o">.</span><span class="n">step</span><span class="p">()</span>  <span class="c1"># Update after each batch</span>
        <span class="n">total_loss</span> <span class="o">+=</span> <span class="n">loss</span><span class="o">.</span><span class="n">item</span><span class="p">()</span>

    <span class="k">return</span> <span class="n">total_loss</span> <span class="o">/</span> <span class="nb">len</span><span class="p">(</span><span class="n">train_loader</span><span class="p">)</span>

<span class="k">def</span><span class="w"> </span><span class="nf">train_epoch_mini_batch</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">train_loader</span><span class="p">,</span> <span class="n">criterion</span><span class="p">,</span> <span class="n">optimizer</span><span class="p">,</span> <span class="n">accumulation_steps</span><span class="o">=</span><span class="mi">4</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Mini-batch with gradient accumulation&quot;&quot;&quot;</span>
    <span class="n">model</span><span class="o">.</span><span class="n">train</span><span class="p">()</span>
    <span class="n">optimizer</span><span class="o">.</span><span class="n">zero_grad</span><span class="p">()</span>
    <span class="n">total_loss</span> <span class="o">=</span> <span class="mi">0</span>

    <span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">train_loader</span><span class="p">):</span>
        <span class="n">output</span> <span class="o">=</span> <span class="n">model</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>
        <span class="n">loss</span> <span class="o">=</span> <span class="n">criterion</span><span class="p">(</span><span class="n">output</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span> <span class="o">/</span> <span class="n">accumulation_steps</span>
        <span class="n">loss</span><span class="o">.</span><span class="n">backward</span><span class="p">()</span>

        <span class="k">if</span> <span class="p">(</span><span class="n">i</span> <span class="o">+</span> <span class="mi">1</span><span class="p">)</span> <span class="o">%</span> <span class="n">accumulation_steps</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
            <span class="n">optimizer</span><span class="o">.</span><span class="n">step</span><span class="p">()</span>
            <span class="n">optimizer</span><span class="o">.</span><span class="n">zero_grad</span><span class="p">()</span>

        <span class="n">total_loss</span> <span class="o">+=</span> <span class="n">loss</span><span class="o">.</span><span class="n">item</span><span class="p">()</span> <span class="o">*</span> <span class="n">accumulation_steps</span>

    <span class="k">return</span> <span class="n">total_loss</span> <span class="o">/</span> <span class="nb">len</span><span class="p">(</span><span class="n">train_loader</span><span class="p">)</span>
</code></pre></div>

<h3 id="12-the-optimization-landscape">1.2 The Optimization Landscape<a class="header-link" href="#12-the-optimization-landscape" title="Permanent link">&para;</a></h3>
<p>Deep neural networks create highly non-convex loss surfaces with:</p>
<ul>
<li><strong>Local minima</strong>: Points where gradient is zero but not global minimum</li>
<li><strong>Saddle points</strong>: Zero gradient but not minimum (very common in high dimensions)</li>
<li><strong>Plateaus</strong>: Flat regions where gradient is near zero</li>
<li><strong>Ravines</strong>: Steep in some directions, flat in others</li>
</ul>
<div class="highlight"><pre><span></span><code><span class="n">Loss</span><span class="w"> </span><span class="n">Surface</span><span class="w"> </span><span class="n">Visualization</span><span class="o">:</span>

<span class="w">           </span><span class="n">Global</span><span class="w"> </span><span class="n">Minimum</span>
<span class="w">                </span><span class="o">|</span>
<span class="w">    </span>‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ñº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
<span class="w">    </span><span class="o">|</span><span class="w">                        </span><span class="o">|</span>
<span class="w">    </span><span class="o">|</span><span class="w">  </span>‚ï±‚ï≤<span class="w">      </span>‚ï±‚ï≤<span class="w">      </span>‚ï±‚ï≤<span class="w">   </span><span class="o">|</span><span class="w">  </span><span class="o">&lt;-</span><span class="w"> </span><span class="n">Local</span><span class="w"> </span><span class="n">Minima</span>
<span class="w">    </span><span class="o">|</span><span class="w"> </span>‚ï±<span class="w">  </span>‚ï≤<span class="w">    </span>‚ï±<span class="w">  </span>‚ï≤<span class="w">    </span>‚ï±<span class="w">  </span>‚ï≤<span class="w">  </span><span class="o">|</span>
<span class="w">    </span><span class="o">|</span>‚ï±<span class="w">    </span>‚ï≤__‚ï±<span class="w">    </span>‚ï≤__‚ï±<span class="w">    </span>‚ï≤<span class="w"> </span><span class="o">|</span>
<span class="w">    </span><span class="o">|</span><span class="w">      </span>‚ñ≤<span class="w">       </span>‚ñ≤<span class="w">        </span>‚ñº<span class="o">|</span><span class="w">  </span><span class="o">&lt;-</span><span class="w"> </span><span class="n">Saddle</span><span class="w"> </span><span class="n">Point</span>
<span class="w">    </span><span class="o">|</span><span class="w">  </span><span class="n">Plateau</span><span class="w">  </span><span class="n">Local</span><span class="w"> </span><span class="n">Min</span><span class="w">    </span><span class="o">|</span>
<span class="w">    </span>‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò

<span class="n">Gradient</span><span class="w"> </span><span class="n">Behavior</span><span class="o">:</span>
<span class="o">-</span><span class="w"> </span><span class="n">At</span><span class="w"> </span><span class="n">local</span><span class="w"> </span><span class="n">minima</span><span class="o">:</span><span class="w"> </span>‚àá<span class="n">L</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="m">0</span><span class="p">,</span><span class="w"> </span><span class="n">all</span><span class="w"> </span><span class="n">eigenvalues</span><span class="w"> </span><span class="o">&gt;</span><span class="w"> </span><span class="m">0</span>
<span class="o">-</span><span class="w"> </span><span class="n">At</span><span class="w"> </span><span class="n">saddle</span><span class="w"> </span><span class="n">points</span><span class="o">:</span><span class="w"> </span>‚àá<span class="n">L</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="m">0</span><span class="p">,</span><span class="w"> </span><span class="n">some</span><span class="w"> </span><span class="n">eigenvalues</span><span class="w"> </span><span class="o">&lt;</span><span class="w"> </span><span class="m">0</span>
<span class="o">-</span><span class="w"> </span><span class="n">On</span><span class="w"> </span><span class="n">plateaus</span><span class="o">:</span><span class="w"> </span>‚àá<span class="n">L</span><span class="w"> </span>‚âà<span class="w"> </span><span class="m">0</span><span class="p">,</span><span class="w"> </span><span class="n">very</span><span class="w"> </span><span class="n">slow</span><span class="w"> </span><span class="n">progress</span>
</code></pre></div>

<p><strong>Why optimization still works despite non-convexity:</strong></p>
<ol>
<li>High-dimensional spaces have exponentially more saddle points than local minima</li>
<li>Local minima often have similar loss values to global minimum</li>
<li>Modern optimizers (with momentum) can escape saddle points</li>
<li>Overparameterization creates many good solutions</li>
</ol>
<div class="highlight"><pre><span></span><code><span class="kn">import</span><span class="w"> </span><span class="nn">numpy</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">np</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">matplotlib.pyplot</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">plt</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">mpl_toolkits.mplot3d</span><span class="w"> </span><span class="kn">import</span> <span class="n">Axes3D</span>

<span class="k">def</span><span class="w"> </span><span class="nf">visualize_loss_surface</span><span class="p">():</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Visualize a toy non-convex loss surface&quot;&quot;&quot;</span>
    <span class="n">x</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="o">-</span><span class="mi">3</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">100</span><span class="p">)</span>
    <span class="n">y</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="o">-</span><span class="mi">3</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">100</span><span class="p">)</span>
    <span class="n">X</span><span class="p">,</span> <span class="n">Y</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">meshgrid</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>

    <span class="c1"># Non-convex function with multiple minima</span>
    <span class="n">Z</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">sin</span><span class="p">(</span><span class="n">X</span><span class="o">**</span><span class="mi">2</span> <span class="o">+</span> <span class="n">Y</span><span class="o">**</span><span class="mi">2</span><span class="p">)</span> <span class="o">/</span> <span class="p">(</span><span class="n">X</span><span class="o">**</span><span class="mi">2</span> <span class="o">+</span> <span class="n">Y</span><span class="o">**</span><span class="mi">2</span> <span class="o">+</span> <span class="mi">1</span><span class="p">)</span> <span class="o">+</span> <span class="mf">0.1</span> <span class="o">*</span> <span class="p">(</span><span class="n">X</span><span class="o">**</span><span class="mi">2</span> <span class="o">+</span> <span class="n">Y</span><span class="o">**</span><span class="mi">2</span><span class="p">)</span>

    <span class="n">fig</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">12</span><span class="p">,</span> <span class="mi">5</span><span class="p">))</span>

    <span class="c1"># 3D surface</span>
    <span class="n">ax1</span> <span class="o">=</span> <span class="n">fig</span><span class="o">.</span><span class="n">add_subplot</span><span class="p">(</span><span class="mi">121</span><span class="p">,</span> <span class="n">projection</span><span class="o">=</span><span class="s1">&#39;3d&#39;</span><span class="p">)</span>
    <span class="n">ax1</span><span class="o">.</span><span class="n">plot_surface</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">Y</span><span class="p">,</span> <span class="n">Z</span><span class="p">,</span> <span class="n">cmap</span><span class="o">=</span><span class="s1">&#39;viridis&#39;</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.8</span><span class="p">)</span>
    <span class="n">ax1</span><span class="o">.</span><span class="n">set_title</span><span class="p">(</span><span class="s1">&#39;Loss Surface (3D)&#39;</span><span class="p">)</span>
    <span class="n">ax1</span><span class="o">.</span><span class="n">set_xlabel</span><span class="p">(</span><span class="s1">&#39;Œ∏‚ÇÅ&#39;</span><span class="p">)</span>
    <span class="n">ax1</span><span class="o">.</span><span class="n">set_ylabel</span><span class="p">(</span><span class="s1">&#39;Œ∏‚ÇÇ&#39;</span><span class="p">)</span>
    <span class="n">ax1</span><span class="o">.</span><span class="n">set_zlabel</span><span class="p">(</span><span class="s1">&#39;Loss&#39;</span><span class="p">)</span>

    <span class="c1"># Contour plot</span>
    <span class="n">ax2</span> <span class="o">=</span> <span class="n">fig</span><span class="o">.</span><span class="n">add_subplot</span><span class="p">(</span><span class="mi">122</span><span class="p">)</span>
    <span class="n">contour</span> <span class="o">=</span> <span class="n">ax2</span><span class="o">.</span><span class="n">contour</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">Y</span><span class="p">,</span> <span class="n">Z</span><span class="p">,</span> <span class="n">levels</span><span class="o">=</span><span class="mi">20</span><span class="p">,</span> <span class="n">cmap</span><span class="o">=</span><span class="s1">&#39;viridis&#39;</span><span class="p">)</span>
    <span class="n">ax2</span><span class="o">.</span><span class="n">clabel</span><span class="p">(</span><span class="n">contour</span><span class="p">,</span> <span class="n">inline</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">8</span><span class="p">)</span>
    <span class="n">ax2</span><span class="o">.</span><span class="n">set_title</span><span class="p">(</span><span class="s1">&#39;Loss Surface (Contour)&#39;</span><span class="p">)</span>
    <span class="n">ax2</span><span class="o">.</span><span class="n">set_xlabel</span><span class="p">(</span><span class="s1">&#39;Œ∏‚ÇÅ&#39;</span><span class="p">)</span>
    <span class="n">ax2</span><span class="o">.</span><span class="n">set_ylabel</span><span class="p">(</span><span class="s1">&#39;Œ∏‚ÇÇ&#39;</span><span class="p">)</span>

    <span class="n">plt</span><span class="o">.</span><span class="n">tight_layout</span><span class="p">()</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">savefig</span><span class="p">(</span><span class="s1">&#39;loss_surface.png&#39;</span><span class="p">,</span> <span class="n">dpi</span><span class="o">=</span><span class="mi">150</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">close</span><span class="p">()</span>

<span class="c1"># Simulate optimization trajectory</span>
<span class="k">def</span><span class="w"> </span><span class="nf">optimize_trajectory</span><span class="p">(</span><span class="n">optimizer_fn</span><span class="p">,</span> <span class="n">steps</span><span class="o">=</span><span class="mi">100</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Track optimizer path on loss surface&quot;&quot;&quot;</span>
    <span class="n">trajectory</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="n">theta</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([</span><span class="mf">2.5</span><span class="p">,</span> <span class="mf">2.5</span><span class="p">],</span> <span class="n">requires_grad</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
    <span class="n">optimizer</span> <span class="o">=</span> <span class="n">optimizer_fn</span><span class="p">([</span><span class="n">theta</span><span class="p">])</span>

    <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">steps</span><span class="p">):</span>
        <span class="n">optimizer</span><span class="o">.</span><span class="n">zero_grad</span><span class="p">()</span>

        <span class="c1"># Loss function (same as above)</span>
        <span class="n">loss</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">sin</span><span class="p">(</span><span class="n">theta</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">**</span><span class="mi">2</span> <span class="o">+</span> <span class="n">theta</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">**</span><span class="mi">2</span><span class="p">)</span> <span class="o">/</span> <span class="p">(</span><span class="n">theta</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">**</span><span class="mi">2</span> <span class="o">+</span> <span class="n">theta</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">**</span><span class="mi">2</span> <span class="o">+</span> <span class="mi">1</span><span class="p">)</span> <span class="o">+</span> \
               <span class="mf">0.1</span> <span class="o">*</span> <span class="p">(</span><span class="n">theta</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">**</span><span class="mi">2</span> <span class="o">+</span> <span class="n">theta</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">**</span><span class="mi">2</span><span class="p">)</span>
        <span class="n">loss</span><span class="o">.</span><span class="n">backward</span><span class="p">()</span>

        <span class="n">trajectory</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">theta</span><span class="o">.</span><span class="n">detach</span><span class="p">()</span><span class="o">.</span><span class="n">clone</span><span class="p">()</span><span class="o">.</span><span class="n">numpy</span><span class="p">())</span>
        <span class="n">optimizer</span><span class="o">.</span><span class="n">step</span><span class="p">()</span>

    <span class="k">return</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">trajectory</span><span class="p">)</span>

<span class="c1"># Compare trajectories</span>
<span class="n">sgd_traj</span> <span class="o">=</span> <span class="n">optimize_trajectory</span><span class="p">(</span><span class="k">lambda</span> <span class="n">p</span><span class="p">:</span> <span class="n">optim</span><span class="o">.</span><span class="n">SGD</span><span class="p">(</span><span class="n">p</span><span class="p">,</span> <span class="n">lr</span><span class="o">=</span><span class="mf">0.1</span><span class="p">))</span>
<span class="n">momentum_traj</span> <span class="o">=</span> <span class="n">optimize_trajectory</span><span class="p">(</span><span class="k">lambda</span> <span class="n">p</span><span class="p">:</span> <span class="n">optim</span><span class="o">.</span><span class="n">SGD</span><span class="p">(</span><span class="n">p</span><span class="p">,</span> <span class="n">lr</span><span class="o">=</span><span class="mf">0.1</span><span class="p">,</span> <span class="n">momentum</span><span class="o">=</span><span class="mf">0.9</span><span class="p">))</span>
<span class="n">adam_traj</span> <span class="o">=</span> <span class="n">optimize_trajectory</span><span class="p">(</span><span class="k">lambda</span> <span class="n">p</span><span class="p">:</span> <span class="n">optim</span><span class="o">.</span><span class="n">Adam</span><span class="p">(</span><span class="n">p</span><span class="p">,</span> <span class="n">lr</span><span class="o">=</span><span class="mf">0.1</span><span class="p">))</span>
</code></pre></div>

<hr />
<h2 id="2-classic-optimizers">2. Classic Optimizers<a class="header-link" href="#2-classic-optimizers" title="Permanent link">&para;</a></h2>
<h3 id="21-stochastic-gradient-descent-sgd">2.1 Stochastic Gradient Descent (SGD)<a class="header-link" href="#21-stochastic-gradient-descent-sgd" title="Permanent link">&para;</a></h3>
<p>Basic SGD update rule:</p>
<p>$$
\theta_{t+1} = \theta_t - \eta \nabla_\theta \mathcal{L}(\theta_t)
$$</p>
<p><strong>Problems with vanilla SGD:</strong>
- Oscillation in ravines (steep in some directions)
- Slow convergence in flat regions
- Difficulty escaping saddle points
- Same learning rate for all parameters</p>
<h3 id="22-sgd-with-momentum">2.2 SGD with Momentum<a class="header-link" href="#22-sgd-with-momentum" title="Permanent link">&para;</a></h3>
<p>Momentum accelerates in consistent directions and dampens oscillations:</p>
<p>$$
\begin{align}
v_t &= \beta v_{t-1} + \nabla_\theta \mathcal{L}(\theta_t) \\
\theta_{t+1} &= \theta_t - \eta v_t
\end{align}
$$</p>
<p>where $\beta \in [0, 1)$ is the momentum coefficient (typically 0.9).</p>
<p><strong>Physical analogy</strong>: A ball rolling down a hill accumulates velocity.</p>
<div class="highlight"><pre><span></span><code><span class="n">Momentum</span><span class="w"> </span><span class="n">Effect</span><span class="o">:</span>

<span class="n">Without</span><span class="w"> </span><span class="n">Momentum</span><span class="o">:</span><span class="w">        </span><span class="n">With</span><span class="w"> </span><span class="n">Momentum</span><span class="o">:</span>
<span class="w">     </span>‚Üì<span class="w">                       </span>‚Üì
<span class="w">   </span>‚Üô<span class="w"> </span>‚Üò<span class="w">                    </span>‚Üô<span class="w">  </span>‚Üò
<span class="w">  </span>‚Üô<span class="w">   </span>‚Üò<span class="w">                 </span>‚Üô<span class="w">     </span>‚Üò
<span class="w"> </span>‚Üô<span class="w">  </span>‚Üì<span class="w">  </span>‚Üò<span class="w">              </span>‚Üô<span class="w">    </span>‚Üì<span class="w">   </span>‚Üò
‚Üô<span class="w">   </span>‚Üì<span class="w">   </span>‚Üò<span class="w">           </span>‚Üô<span class="w">      </span>‚Üì<span class="w">    </span>‚Üò<span class="w">  </span><span class="o">&lt;-</span><span class="w"> </span><span class="n">Smoother</span><span class="p">,</span><span class="w"> </span><span class="n">faster</span>
<span class="w">   </span><span class="n">zigzag</span><span class="w">            </span><span class="n">straighter</span><span class="w"> </span><span class="n">path</span>
</code></pre></div>

<p><strong>Manual implementation:</strong></p>
<div class="highlight"><pre><span></span><code><span class="k">class</span><span class="w"> </span><span class="nc">SGDMomentum</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Manual implementation of SGD with momentum&quot;&quot;&quot;</span>
    <span class="k">def</span><span class="w"> </span><span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">params</span><span class="p">,</span> <span class="n">lr</span><span class="o">=</span><span class="mf">0.01</span><span class="p">,</span> <span class="n">momentum</span><span class="o">=</span><span class="mf">0.9</span><span class="p">,</span> <span class="n">weight_decay</span><span class="o">=</span><span class="mi">0</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">params</span> <span class="o">=</span> <span class="nb">list</span><span class="p">(</span><span class="n">params</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">lr</span> <span class="o">=</span> <span class="n">lr</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">momentum</span> <span class="o">=</span> <span class="n">momentum</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">weight_decay</span> <span class="o">=</span> <span class="n">weight_decay</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">velocities</span> <span class="o">=</span> <span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">zeros_like</span><span class="p">(</span><span class="n">p</span><span class="p">)</span> <span class="k">for</span> <span class="n">p</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">params</span><span class="p">]</span>

    <span class="k">def</span><span class="w"> </span><span class="nf">zero_grad</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="k">for</span> <span class="n">p</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">params</span><span class="p">:</span>
            <span class="k">if</span> <span class="n">p</span><span class="o">.</span><span class="n">grad</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
                <span class="n">p</span><span class="o">.</span><span class="n">grad</span><span class="o">.</span><span class="n">zero_</span><span class="p">()</span>

    <span class="k">def</span><span class="w"> </span><span class="nf">step</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="k">with</span> <span class="n">torch</span><span class="o">.</span><span class="n">no_grad</span><span class="p">():</span>
            <span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">p</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">params</span><span class="p">):</span>
                <span class="k">if</span> <span class="n">p</span><span class="o">.</span><span class="n">grad</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
                    <span class="k">continue</span>

                <span class="c1"># Gradient with weight decay</span>
                <span class="n">grad</span> <span class="o">=</span> <span class="n">p</span><span class="o">.</span><span class="n">grad</span>
                <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">weight_decay</span> <span class="o">!=</span> <span class="mi">0</span><span class="p">:</span>
                    <span class="n">grad</span> <span class="o">=</span> <span class="n">grad</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">p</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">weight_decay</span><span class="p">)</span>

                <span class="c1"># Momentum update</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">velocities</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">momentum</span> <span class="o">*</span> <span class="bp">self</span><span class="o">.</span><span class="n">velocities</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">+</span> <span class="n">grad</span>
                <span class="n">p</span><span class="o">.</span><span class="n">add_</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">velocities</span><span class="p">[</span><span class="n">i</span><span class="p">],</span> <span class="n">alpha</span><span class="o">=-</span><span class="bp">self</span><span class="o">.</span><span class="n">lr</span><span class="p">)</span>

<span class="c1"># Test implementation</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="mi">10</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
<span class="n">criterion</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">MSELoss</span><span class="p">()</span>

<span class="c1"># Custom optimizer</span>
<span class="n">optimizer</span> <span class="o">=</span> <span class="n">SGDMomentum</span><span class="p">(</span><span class="n">model</span><span class="o">.</span><span class="n">parameters</span><span class="p">(),</span> <span class="n">lr</span><span class="o">=</span><span class="mf">0.01</span><span class="p">,</span> <span class="n">momentum</span><span class="o">=</span><span class="mf">0.9</span><span class="p">)</span>

<span class="c1"># PyTorch equivalent</span>
<span class="n">optimizer_torch</span> <span class="o">=</span> <span class="n">optim</span><span class="o">.</span><span class="n">SGD</span><span class="p">(</span><span class="n">model</span><span class="o">.</span><span class="n">parameters</span><span class="p">(),</span> <span class="n">lr</span><span class="o">=</span><span class="mf">0.01</span><span class="p">,</span> <span class="n">momentum</span><span class="o">=</span><span class="mf">0.9</span><span class="p">)</span>

<span class="c1"># Both should produce identical results</span>
<span class="n">X</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">32</span><span class="p">,</span> <span class="mi">10</span><span class="p">)</span>
<span class="n">y</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">32</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>

<span class="c1"># Custom</span>
<span class="n">optimizer</span><span class="o">.</span><span class="n">zero_grad</span><span class="p">()</span>
<span class="n">loss</span> <span class="o">=</span> <span class="n">criterion</span><span class="p">(</span><span class="n">model</span><span class="p">(</span><span class="n">X</span><span class="p">),</span> <span class="n">y</span><span class="p">)</span>
<span class="n">loss</span><span class="o">.</span><span class="n">backward</span><span class="p">()</span>
<span class="n">optimizer</span><span class="o">.</span><span class="n">step</span><span class="p">()</span>

<span class="c1"># Check velocity is being maintained</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Velocity norm: </span><span class="si">{</span><span class="n">torch</span><span class="o">.</span><span class="n">norm</span><span class="p">(</span><span class="n">optimizer</span><span class="o">.</span><span class="n">velocities</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span><span class="si">:</span><span class="s2">.6f</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
</code></pre></div>

<h3 id="23-nesterov-accelerated-gradient-nag">2.3 Nesterov Accelerated Gradient (NAG)<a class="header-link" href="#23-nesterov-accelerated-gradient-nag" title="Permanent link">&para;</a></h3>
<p>Nesterov momentum "looks ahead" before computing the gradient:</p>
<p>$$
\begin{align}
v_t &= \beta v_{t-1} + \nabla_\theta \mathcal{L}(\theta_t - \beta v_{t-1}) \\
\theta_{t+1} &= \theta_t - \eta v_t
\end{align}
$$</p>
<p><strong>Intuition</strong>: Compute gradient at the "look-ahead" position, not current position.</p>
<div class="highlight"><pre><span></span><code><span class="c1"># PyTorch SGD with Nesterov momentum</span>
<span class="n">optimizer_sgd</span> <span class="o">=</span> <span class="n">optim</span><span class="o">.</span><span class="n">SGD</span><span class="p">(</span><span class="n">model</span><span class="o">.</span><span class="n">parameters</span><span class="p">(),</span> <span class="n">lr</span><span class="o">=</span><span class="mf">0.01</span><span class="p">,</span> <span class="n">momentum</span><span class="o">=</span><span class="mf">0.9</span><span class="p">)</span>
<span class="n">optimizer_nesterov</span> <span class="o">=</span> <span class="n">optim</span><span class="o">.</span><span class="n">SGD</span><span class="p">(</span><span class="n">model</span><span class="o">.</span><span class="n">parameters</span><span class="p">(),</span> <span class="n">lr</span><span class="o">=</span><span class="mf">0.01</span><span class="p">,</span> <span class="n">momentum</span><span class="o">=</span><span class="mf">0.9</span><span class="p">,</span> <span class="n">nesterov</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>

<span class="c1"># Comparison on a simple problem</span>
<span class="k">def</span><span class="w"> </span><span class="nf">compare_sgd_variants</span><span class="p">():</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Compare vanilla SGD, momentum, and Nesterov&quot;&quot;&quot;</span>
    <span class="n">torch</span><span class="o">.</span><span class="n">manual_seed</span><span class="p">(</span><span class="mi">42</span><span class="p">)</span>

    <span class="c1"># Simple 2D problem</span>
    <span class="n">target</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([</span><span class="mf">1.0</span><span class="p">,</span> <span class="mf">1.0</span><span class="p">])</span>

    <span class="n">results</span> <span class="o">=</span> <span class="p">{}</span>
    <span class="k">for</span> <span class="n">name</span><span class="p">,</span> <span class="n">opt_fn</span> <span class="ow">in</span> <span class="p">[</span>
        <span class="p">(</span><span class="s1">&#39;SGD&#39;</span><span class="p">,</span> <span class="k">lambda</span> <span class="n">p</span><span class="p">:</span> <span class="n">optim</span><span class="o">.</span><span class="n">SGD</span><span class="p">(</span><span class="n">p</span><span class="p">,</span> <span class="n">lr</span><span class="o">=</span><span class="mf">0.1</span><span class="p">)),</span>
        <span class="p">(</span><span class="s1">&#39;Momentum&#39;</span><span class="p">,</span> <span class="k">lambda</span> <span class="n">p</span><span class="p">:</span> <span class="n">optim</span><span class="o">.</span><span class="n">SGD</span><span class="p">(</span><span class="n">p</span><span class="p">,</span> <span class="n">lr</span><span class="o">=</span><span class="mf">0.1</span><span class="p">,</span> <span class="n">momentum</span><span class="o">=</span><span class="mf">0.9</span><span class="p">)),</span>
        <span class="p">(</span><span class="s1">&#39;Nesterov&#39;</span><span class="p">,</span> <span class="k">lambda</span> <span class="n">p</span><span class="p">:</span> <span class="n">optim</span><span class="o">.</span><span class="n">SGD</span><span class="p">(</span><span class="n">p</span><span class="p">,</span> <span class="n">lr</span><span class="o">=</span><span class="mf">0.1</span><span class="p">,</span> <span class="n">momentum</span><span class="o">=</span><span class="mf">0.9</span><span class="p">,</span> <span class="n">nesterov</span><span class="o">=</span><span class="kc">True</span><span class="p">))</span>
    <span class="p">]:</span>
        <span class="n">theta</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([</span><span class="mf">0.0</span><span class="p">,</span> <span class="mf">0.0</span><span class="p">],</span> <span class="n">requires_grad</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
        <span class="n">optimizer</span> <span class="o">=</span> <span class="n">opt_fn</span><span class="p">([</span><span class="n">theta</span><span class="p">])</span>
        <span class="n">losses</span> <span class="o">=</span> <span class="p">[]</span>

        <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">50</span><span class="p">):</span>
            <span class="n">optimizer</span><span class="o">.</span><span class="n">zero_grad</span><span class="p">()</span>
            <span class="n">loss</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">sum</span><span class="p">((</span><span class="n">theta</span> <span class="o">-</span> <span class="n">target</span><span class="p">)</span><span class="o">**</span><span class="mi">2</span><span class="p">)</span>
            <span class="n">loss</span><span class="o">.</span><span class="n">backward</span><span class="p">()</span>
            <span class="n">optimizer</span><span class="o">.</span><span class="n">step</span><span class="p">()</span>
            <span class="n">losses</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">loss</span><span class="o">.</span><span class="n">item</span><span class="p">())</span>

        <span class="n">results</span><span class="p">[</span><span class="n">name</span><span class="p">]</span> <span class="o">=</span> <span class="n">losses</span>

    <span class="c1"># Plot convergence</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">10</span><span class="p">,</span> <span class="mi">6</span><span class="p">))</span>
    <span class="k">for</span> <span class="n">name</span><span class="p">,</span> <span class="n">losses</span> <span class="ow">in</span> <span class="n">results</span><span class="o">.</span><span class="n">items</span><span class="p">():</span>
        <span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">losses</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="n">name</span><span class="p">,</span> <span class="n">linewidth</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s1">&#39;Iteration&#39;</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s1">&#39;Loss&#39;</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s1">&#39;SGD Variants Convergence Comparison&#39;</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">legend</span><span class="p">()</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">yscale</span><span class="p">(</span><span class="s1">&#39;log&#39;</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">grid</span><span class="p">(</span><span class="kc">True</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.3</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">savefig</span><span class="p">(</span><span class="s1">&#39;sgd_comparison.png&#39;</span><span class="p">,</span> <span class="n">dpi</span><span class="o">=</span><span class="mi">150</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">close</span><span class="p">()</span>

<span class="n">compare_sgd_variants</span><span class="p">()</span>
</code></pre></div>

<hr />
<h2 id="3-adaptive-learning-rate-methods">3. Adaptive Learning Rate Methods<a class="header-link" href="#3-adaptive-learning-rate-methods" title="Permanent link">&para;</a></h2>
<h3 id="31-adagrad">3.1 Adagrad<a class="header-link" href="#31-adagrad" title="Permanent link">&para;</a></h3>
<p>Adagrad adapts learning rates per-parameter based on historical gradients:</p>
<p>$$
\begin{align}
g_t &= \nabla_\theta \mathcal{L}(\theta_t) \\
G_t &= G_{t-1} + g_t \odot g_t \\
\theta_{t+1} &= \theta_t - \frac{\eta}{\sqrt{G_t + \epsilon}} \odot g_t
\end{align}
$$</p>
<p>where $G_t$ accumulates squared gradients element-wise.</p>
<p><strong>Advantages:</strong>
- Automatic learning rate annealing
- Larger updates for infrequent features
- Works well for sparse data</p>
<p><strong>Disadvantages:</strong>
- Learning rate monotonically decreases
- May stop learning too early</p>
<div class="highlight"><pre><span></span><code><span class="c1"># PyTorch Adagrad</span>
<span class="n">optimizer</span> <span class="o">=</span> <span class="n">optim</span><span class="o">.</span><span class="n">Adagrad</span><span class="p">(</span><span class="n">model</span><span class="o">.</span><span class="n">parameters</span><span class="p">(),</span> <span class="n">lr</span><span class="o">=</span><span class="mf">0.01</span><span class="p">,</span> <span class="n">eps</span><span class="o">=</span><span class="mf">1e-8</span><span class="p">)</span>

<span class="c1"># Adagrad is useful for NLP with sparse features</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">torch.nn.functional</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">F</span>

<span class="k">class</span><span class="w"> </span><span class="nc">WordEmbeddingModel</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="k">def</span><span class="w"> </span><span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">vocab_size</span><span class="p">,</span> <span class="n">embed_dim</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">embedding</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Embedding</span><span class="p">(</span><span class="n">vocab_size</span><span class="p">,</span> <span class="n">embed_dim</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">fc</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">embed_dim</span><span class="p">,</span> <span class="n">vocab_size</span><span class="p">)</span>

    <span class="k">def</span><span class="w"> </span><span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
        <span class="n">embedded</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">embedding</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">fc</span><span class="p">(</span><span class="n">embedded</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">))</span>

<span class="n">model</span> <span class="o">=</span> <span class="n">WordEmbeddingModel</span><span class="p">(</span><span class="n">vocab_size</span><span class="o">=</span><span class="mi">10000</span><span class="p">,</span> <span class="n">embed_dim</span><span class="o">=</span><span class="mi">128</span><span class="p">)</span>
<span class="n">optimizer</span> <span class="o">=</span> <span class="n">optim</span><span class="o">.</span><span class="n">Adagrad</span><span class="p">(</span><span class="n">model</span><span class="o">.</span><span class="n">parameters</span><span class="p">(),</span> <span class="n">lr</span><span class="o">=</span><span class="mf">0.01</span><span class="p">)</span>  <span class="c1"># Good for sparse embeddings</span>
</code></pre></div>

<h3 id="32-rmsprop">3.2 RMSprop<a class="header-link" href="#32-rmsprop" title="Permanent link">&para;</a></h3>
<p>RMSprop fixes Adagrad's aggressive learning rate decay using exponential moving average:</p>
<p>$$
\begin{align}
g_t &= \nabla_\theta \mathcal{L}(\theta_t) \\
E[g^2]_t &= \beta E[g^2]_{t-1} + (1-\beta) g_t \odot g_t \\
\theta_{t+1} &= \theta_t - \frac{\eta}{\sqrt{E[g^2]_t + \epsilon}} \odot g_t
\end{align}
$$</p>
<p>where $\beta \in [0, 1)$ (typically 0.9).</p>
<div class="highlight"><pre><span></span><code><span class="c1"># PyTorch RMSprop</span>
<span class="n">optimizer</span> <span class="o">=</span> <span class="n">optim</span><span class="o">.</span><span class="n">RMSprop</span><span class="p">(</span><span class="n">model</span><span class="o">.</span><span class="n">parameters</span><span class="p">(),</span> <span class="n">lr</span><span class="o">=</span><span class="mf">0.001</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.9</span><span class="p">,</span> <span class="n">eps</span><span class="o">=</span><span class="mf">1e-8</span><span class="p">)</span>

<span class="c1"># RMSprop is popular for RNNs</span>
<span class="k">class</span><span class="w"> </span><span class="nc">SimpleRNN</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="k">def</span><span class="w"> </span><span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">input_size</span><span class="p">,</span> <span class="n">hidden_size</span><span class="p">,</span> <span class="n">output_size</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">rnn</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">RNN</span><span class="p">(</span><span class="n">input_size</span><span class="p">,</span> <span class="n">hidden_size</span><span class="p">,</span> <span class="n">batch_first</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">fc</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">hidden_size</span><span class="p">,</span> <span class="n">output_size</span><span class="p">)</span>

    <span class="k">def</span><span class="w"> </span><span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
        <span class="n">out</span><span class="p">,</span> <span class="n">_</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">rnn</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">fc</span><span class="p">(</span><span class="n">out</span><span class="p">[:,</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="p">:])</span>

<span class="n">rnn_model</span> <span class="o">=</span> <span class="n">SimpleRNN</span><span class="p">(</span><span class="n">input_size</span><span class="o">=</span><span class="mi">10</span><span class="p">,</span> <span class="n">hidden_size</span><span class="o">=</span><span class="mi">64</span><span class="p">,</span> <span class="n">output_size</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
<span class="n">optimizer</span> <span class="o">=</span> <span class="n">optim</span><span class="o">.</span><span class="n">RMSprop</span><span class="p">(</span><span class="n">rnn_model</span><span class="o">.</span><span class="n">parameters</span><span class="p">(),</span> <span class="n">lr</span><span class="o">=</span><span class="mf">0.001</span><span class="p">)</span>  <span class="c1"># Good for RNNs</span>
</code></pre></div>

<h3 id="33-adam-adaptive-moment-estimation">3.3 Adam (Adaptive Moment Estimation)<a class="header-link" href="#33-adam-adaptive-moment-estimation" title="Permanent link">&para;</a></h3>
<p>Adam combines momentum and RMSprop, maintaining both first and second moment estimates:</p>
<p>$$
\begin{align}
g_t &= \nabla_\theta \mathcal{L}(\theta_t) \\
m_t &= \beta_1 m_{t-1} + (1-\beta_1) g_t \quad \text{(first moment)} \\
v_t &= \beta_2 v_{t-1} + (1-\beta_2) g_t \odot g_t \quad \text{(second moment)} \\
\hat{m}_t &= \frac{m_t}{1-\beta_1^t} \quad \text{(bias correction)} \\
\hat{v}_t &= \frac{v_t}{1-\beta_2^t} \quad \text{(bias correction)} \\
\theta_{t+1} &= \theta_t - \frac{\eta}{\sqrt{\hat{v}_t} + \epsilon} \hat{m}_t
\end{align}
$$</p>
<p><strong>Default hyperparameters</strong> (work well in practice):
- $\beta_1 = 0.9$ (momentum)
- $\beta_2 = 0.999$ (RMSprop decay)
- $\epsilon = 10^{-8}$</p>
<p><strong>Manual implementation:</strong></p>
<div class="highlight"><pre><span></span><code><span class="k">class</span><span class="w"> </span><span class="nc">AdamOptimizer</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Manual implementation of Adam optimizer&quot;&quot;&quot;</span>
    <span class="k">def</span><span class="w"> </span><span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">params</span><span class="p">,</span> <span class="n">lr</span><span class="o">=</span><span class="mf">0.001</span><span class="p">,</span> <span class="n">betas</span><span class="o">=</span><span class="p">(</span><span class="mf">0.9</span><span class="p">,</span> <span class="mf">0.999</span><span class="p">),</span> <span class="n">eps</span><span class="o">=</span><span class="mf">1e-8</span><span class="p">,</span> <span class="n">weight_decay</span><span class="o">=</span><span class="mi">0</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">params</span> <span class="o">=</span> <span class="nb">list</span><span class="p">(</span><span class="n">params</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">lr</span> <span class="o">=</span> <span class="n">lr</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">beta1</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">beta2</span> <span class="o">=</span> <span class="n">betas</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">eps</span> <span class="o">=</span> <span class="n">eps</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">weight_decay</span> <span class="o">=</span> <span class="n">weight_decay</span>

        <span class="c1"># Initialize moment estimates</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">m</span> <span class="o">=</span> <span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">zeros_like</span><span class="p">(</span><span class="n">p</span><span class="p">)</span> <span class="k">for</span> <span class="n">p</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">params</span><span class="p">]</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">v</span> <span class="o">=</span> <span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">zeros_like</span><span class="p">(</span><span class="n">p</span><span class="p">)</span> <span class="k">for</span> <span class="n">p</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">params</span><span class="p">]</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">t</span> <span class="o">=</span> <span class="mi">0</span>

    <span class="k">def</span><span class="w"> </span><span class="nf">zero_grad</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="k">for</span> <span class="n">p</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">params</span><span class="p">:</span>
            <span class="k">if</span> <span class="n">p</span><span class="o">.</span><span class="n">grad</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
                <span class="n">p</span><span class="o">.</span><span class="n">grad</span><span class="o">.</span><span class="n">zero_</span><span class="p">()</span>

    <span class="k">def</span><span class="w"> </span><span class="nf">step</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">t</span> <span class="o">+=</span> <span class="mi">1</span>

        <span class="k">with</span> <span class="n">torch</span><span class="o">.</span><span class="n">no_grad</span><span class="p">():</span>
            <span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">p</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">params</span><span class="p">):</span>
                <span class="k">if</span> <span class="n">p</span><span class="o">.</span><span class="n">grad</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
                    <span class="k">continue</span>

                <span class="n">grad</span> <span class="o">=</span> <span class="n">p</span><span class="o">.</span><span class="n">grad</span>

                <span class="c1"># Weight decay (L2 regularization)</span>
                <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">weight_decay</span> <span class="o">!=</span> <span class="mi">0</span><span class="p">:</span>
                    <span class="n">grad</span> <span class="o">=</span> <span class="n">grad</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">p</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">weight_decay</span><span class="p">)</span>

                <span class="c1"># Update biased first moment estimate</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">m</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">beta1</span> <span class="o">*</span> <span class="bp">self</span><span class="o">.</span><span class="n">m</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">+</span> <span class="p">(</span><span class="mi">1</span> <span class="o">-</span> <span class="bp">self</span><span class="o">.</span><span class="n">beta1</span><span class="p">)</span> <span class="o">*</span> <span class="n">grad</span>

                <span class="c1"># Update biased second raw moment estimate</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">v</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">beta2</span> <span class="o">*</span> <span class="bp">self</span><span class="o">.</span><span class="n">v</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">+</span> <span class="p">(</span><span class="mi">1</span> <span class="o">-</span> <span class="bp">self</span><span class="o">.</span><span class="n">beta2</span><span class="p">)</span> <span class="o">*</span> <span class="n">grad</span><span class="o">.</span><span class="n">pow</span><span class="p">(</span><span class="mi">2</span><span class="p">)</span>

                <span class="c1"># Compute bias-corrected first moment estimate</span>
                <span class="n">m_hat</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">m</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">/</span> <span class="p">(</span><span class="mi">1</span> <span class="o">-</span> <span class="bp">self</span><span class="o">.</span><span class="n">beta1</span> <span class="o">**</span> <span class="bp">self</span><span class="o">.</span><span class="n">t</span><span class="p">)</span>

                <span class="c1"># Compute bias-corrected second raw moment estimate</span>
                <span class="n">v_hat</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">v</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">/</span> <span class="p">(</span><span class="mi">1</span> <span class="o">-</span> <span class="bp">self</span><span class="o">.</span><span class="n">beta2</span> <span class="o">**</span> <span class="bp">self</span><span class="o">.</span><span class="n">t</span><span class="p">)</span>

                <span class="c1"># Update parameters</span>
                <span class="n">p</span><span class="o">.</span><span class="n">add_</span><span class="p">(</span><span class="n">m_hat</span> <span class="o">/</span> <span class="p">(</span><span class="n">v_hat</span><span class="o">.</span><span class="n">sqrt</span><span class="p">()</span> <span class="o">+</span> <span class="bp">self</span><span class="o">.</span><span class="n">eps</span><span class="p">),</span> <span class="n">alpha</span><span class="o">=-</span><span class="bp">self</span><span class="o">.</span><span class="n">lr</span><span class="p">)</span>

<span class="c1"># Verify against PyTorch implementation</span>
<span class="k">def</span><span class="w"> </span><span class="nf">test_adam_implementation</span><span class="p">():</span>
    <span class="n">torch</span><span class="o">.</span><span class="n">manual_seed</span><span class="p">(</span><span class="mi">42</span><span class="p">)</span>

    <span class="c1"># Create two identical models</span>
    <span class="n">model1</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Sequential</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="mi">10</span><span class="p">,</span> <span class="mi">5</span><span class="p">),</span> <span class="n">nn</span><span class="o">.</span><span class="n">ReLU</span><span class="p">(),</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="mi">5</span><span class="p">,</span> <span class="mi">1</span><span class="p">))</span>
    <span class="n">model2</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Sequential</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="mi">10</span><span class="p">,</span> <span class="mi">5</span><span class="p">),</span> <span class="n">nn</span><span class="o">.</span><span class="n">ReLU</span><span class="p">(),</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="mi">5</span><span class="p">,</span> <span class="mi">1</span><span class="p">))</span>
    <span class="n">model2</span><span class="o">.</span><span class="n">load_state_dict</span><span class="p">(</span><span class="n">model1</span><span class="o">.</span><span class="n">state_dict</span><span class="p">())</span>

    <span class="c1"># Use both optimizers</span>
    <span class="n">opt_custom</span> <span class="o">=</span> <span class="n">AdamOptimizer</span><span class="p">(</span><span class="n">model1</span><span class="o">.</span><span class="n">parameters</span><span class="p">(),</span> <span class="n">lr</span><span class="o">=</span><span class="mf">0.001</span><span class="p">)</span>
    <span class="n">opt_torch</span> <span class="o">=</span> <span class="n">optim</span><span class="o">.</span><span class="n">Adam</span><span class="p">(</span><span class="n">model2</span><span class="o">.</span><span class="n">parameters</span><span class="p">(),</span> <span class="n">lr</span><span class="o">=</span><span class="mf">0.001</span><span class="p">)</span>

    <span class="n">criterion</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">MSELoss</span><span class="p">()</span>

    <span class="c1"># Run a few steps</span>
    <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">10</span><span class="p">):</span>
        <span class="n">X</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">32</span><span class="p">,</span> <span class="mi">10</span><span class="p">)</span>
        <span class="n">y</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">32</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>

        <span class="c1"># Custom Adam</span>
        <span class="n">opt_custom</span><span class="o">.</span><span class="n">zero_grad</span><span class="p">()</span>
        <span class="n">loss1</span> <span class="o">=</span> <span class="n">criterion</span><span class="p">(</span><span class="n">model1</span><span class="p">(</span><span class="n">X</span><span class="p">),</span> <span class="n">y</span><span class="p">)</span>
        <span class="n">loss1</span><span class="o">.</span><span class="n">backward</span><span class="p">()</span>
        <span class="n">opt_custom</span><span class="o">.</span><span class="n">step</span><span class="p">()</span>

        <span class="c1"># PyTorch Adam</span>
        <span class="n">opt_torch</span><span class="o">.</span><span class="n">zero_grad</span><span class="p">()</span>
        <span class="n">loss2</span> <span class="o">=</span> <span class="n">criterion</span><span class="p">(</span><span class="n">model2</span><span class="p">(</span><span class="n">X</span><span class="p">),</span> <span class="n">y</span><span class="p">)</span>
        <span class="n">loss2</span><span class="o">.</span><span class="n">backward</span><span class="p">()</span>
        <span class="n">opt_torch</span><span class="o">.</span><span class="n">step</span><span class="p">()</span>

    <span class="c1"># Check parameters are close (may have small numerical differences)</span>
    <span class="k">for</span> <span class="n">p1</span><span class="p">,</span> <span class="n">p2</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">(</span><span class="n">model1</span><span class="o">.</span><span class="n">parameters</span><span class="p">(),</span> <span class="n">model2</span><span class="o">.</span><span class="n">parameters</span><span class="p">()):</span>
        <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Param diff: </span><span class="si">{</span><span class="n">torch</span><span class="o">.</span><span class="n">max</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">abs</span><span class="p">(</span><span class="n">p1</span><span class="w"> </span><span class="o">-</span><span class="w"> </span><span class="n">p2</span><span class="p">))</span><span class="o">.</span><span class="n">item</span><span class="p">()</span><span class="si">:</span><span class="s2">.2e</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>

<span class="n">test_adam_implementation</span><span class="p">()</span>

<span class="c1"># PyTorch Adam</span>
<span class="n">optimizer</span> <span class="o">=</span> <span class="n">optim</span><span class="o">.</span><span class="n">Adam</span><span class="p">(</span><span class="n">model</span><span class="o">.</span><span class="n">parameters</span><span class="p">(),</span> <span class="n">lr</span><span class="o">=</span><span class="mf">0.001</span><span class="p">,</span> <span class="n">betas</span><span class="o">=</span><span class="p">(</span><span class="mf">0.9</span><span class="p">,</span> <span class="mf">0.999</span><span class="p">),</span> <span class="n">eps</span><span class="o">=</span><span class="mf">1e-8</span><span class="p">)</span>
</code></pre></div>

<h3 id="34-adamw-adam-with-decoupled-weight-decay">3.4 AdamW (Adam with Decoupled Weight Decay)<a class="header-link" href="#34-adamw-adam-with-decoupled-weight-decay" title="Permanent link">&para;</a></h3>
<p><strong>Key insight</strong>: L2 regularization and weight decay are equivalent in SGD but not in Adam!</p>
<p><strong>L2 regularization</strong> (traditional):
$$
\mathcal{L}_{\text{total}} = \mathcal{L} + \frac{\lambda}{2} ||\theta||^2
$$</p>
<p><strong>Weight decay</strong> (decoupled):
$$
\theta_{t+1} = (1 - \eta \lambda) \theta_t - \frac{\eta}{\sqrt{\hat{v}_t} + \epsilon} \hat{m}_t
$$</p>
<p>AdamW applies weight decay directly to parameters, not through gradient:</p>
<div class="highlight"><pre><span></span><code><span class="c1"># Adam with L2 regularization (traditional)</span>
<span class="n">optimizer_adam</span> <span class="o">=</span> <span class="n">optim</span><span class="o">.</span><span class="n">Adam</span><span class="p">(</span><span class="n">model</span><span class="o">.</span><span class="n">parameters</span><span class="p">(),</span> <span class="n">lr</span><span class="o">=</span><span class="mf">0.001</span><span class="p">,</span> <span class="n">weight_decay</span><span class="o">=</span><span class="mf">0.01</span><span class="p">)</span>

<span class="c1"># AdamW with decoupled weight decay (better!)</span>
<span class="n">optimizer_adamw</span> <span class="o">=</span> <span class="n">optim</span><span class="o">.</span><span class="n">AdamW</span><span class="p">(</span><span class="n">model</span><span class="o">.</span><span class="n">parameters</span><span class="p">(),</span> <span class="n">lr</span><span class="o">=</span><span class="mf">0.001</span><span class="p">,</span> <span class="n">weight_decay</span><span class="o">=</span><span class="mf">0.01</span><span class="p">)</span>

<span class="c1"># Visualize the difference</span>
<span class="k">def</span><span class="w"> </span><span class="nf">compare_adam_adamw</span><span class="p">():</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Show difference between Adam and AdamW&quot;&quot;&quot;</span>
    <span class="n">torch</span><span class="o">.</span><span class="n">manual_seed</span><span class="p">(</span><span class="mi">42</span><span class="p">)</span>

    <span class="c1"># Simple overparameterized model</span>
    <span class="n">model_adam</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Sequential</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="mi">10</span><span class="p">,</span> <span class="mi">100</span><span class="p">),</span> <span class="n">nn</span><span class="o">.</span><span class="n">ReLU</span><span class="p">(),</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="mi">100</span><span class="p">,</span> <span class="mi">1</span><span class="p">))</span>
    <span class="n">model_adamw</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Sequential</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="mi">10</span><span class="p">,</span> <span class="mi">100</span><span class="p">),</span> <span class="n">nn</span><span class="o">.</span><span class="n">ReLU</span><span class="p">(),</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="mi">100</span><span class="p">,</span> <span class="mi">1</span><span class="p">))</span>
    <span class="n">model_adamw</span><span class="o">.</span><span class="n">load_state_dict</span><span class="p">(</span><span class="n">model_adam</span><span class="o">.</span><span class="n">state_dict</span><span class="p">())</span>

    <span class="n">opt_adam</span> <span class="o">=</span> <span class="n">optim</span><span class="o">.</span><span class="n">Adam</span><span class="p">(</span><span class="n">model_adam</span><span class="o">.</span><span class="n">parameters</span><span class="p">(),</span> <span class="n">lr</span><span class="o">=</span><span class="mf">0.001</span><span class="p">,</span> <span class="n">weight_decay</span><span class="o">=</span><span class="mf">0.01</span><span class="p">)</span>
    <span class="n">opt_adamw</span> <span class="o">=</span> <span class="n">optim</span><span class="o">.</span><span class="n">AdamW</span><span class="p">(</span><span class="n">model_adamw</span><span class="o">.</span><span class="n">parameters</span><span class="p">(),</span> <span class="n">lr</span><span class="o">=</span><span class="mf">0.001</span><span class="p">,</span> <span class="n">weight_decay</span><span class="o">=</span><span class="mf">0.01</span><span class="p">)</span>

    <span class="n">criterion</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">MSELoss</span><span class="p">()</span>

    <span class="c1"># Track weight norms</span>
    <span class="n">adam_norms</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="n">adamw_norms</span> <span class="o">=</span> <span class="p">[]</span>

    <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">100</span><span class="p">):</span>
        <span class="n">X</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">32</span><span class="p">,</span> <span class="mi">10</span><span class="p">)</span>
        <span class="n">y</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">32</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>

        <span class="c1"># Adam</span>
        <span class="n">opt_adam</span><span class="o">.</span><span class="n">zero_grad</span><span class="p">()</span>
        <span class="n">loss</span> <span class="o">=</span> <span class="n">criterion</span><span class="p">(</span><span class="n">model_adam</span><span class="p">(</span><span class="n">X</span><span class="p">),</span> <span class="n">y</span><span class="p">)</span>
        <span class="n">loss</span><span class="o">.</span><span class="n">backward</span><span class="p">()</span>
        <span class="n">opt_adam</span><span class="o">.</span><span class="n">step</span><span class="p">()</span>

        <span class="c1"># AdamW</span>
        <span class="n">opt_adamw</span><span class="o">.</span><span class="n">zero_grad</span><span class="p">()</span>
        <span class="n">loss</span> <span class="o">=</span> <span class="n">criterion</span><span class="p">(</span><span class="n">model_adamw</span><span class="p">(</span><span class="n">X</span><span class="p">),</span> <span class="n">y</span><span class="p">)</span>
        <span class="n">loss</span><span class="o">.</span><span class="n">backward</span><span class="p">()</span>
        <span class="n">opt_adamw</span><span class="o">.</span><span class="n">step</span><span class="p">()</span>

        <span class="c1"># Record weight norms</span>
        <span class="n">adam_norms</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">norm</span><span class="p">(</span><span class="n">model_adam</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">weight</span><span class="p">)</span><span class="o">.</span><span class="n">item</span><span class="p">())</span>
        <span class="n">adamw_norms</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">norm</span><span class="p">(</span><span class="n">model_adamw</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">weight</span><span class="p">)</span><span class="o">.</span><span class="n">item</span><span class="p">())</span>

    <span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">10</span><span class="p">,</span> <span class="mi">6</span><span class="p">))</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">adam_norms</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s1">&#39;Adam (L2 reg)&#39;</span><span class="p">,</span> <span class="n">linewidth</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">adamw_norms</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s1">&#39;AdamW (decoupled)&#39;</span><span class="p">,</span> <span class="n">linewidth</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s1">&#39;Iteration&#39;</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s1">&#39;Weight Norm&#39;</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s1">&#39;Adam vs AdamW: Weight Regularization&#39;</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">legend</span><span class="p">()</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">grid</span><span class="p">(</span><span class="kc">True</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.3</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">savefig</span><span class="p">(</span><span class="s1">&#39;adam_adamw_comparison.png&#39;</span><span class="p">,</span> <span class="n">dpi</span><span class="o">=</span><span class="mi">150</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">close</span><span class="p">()</span>

<span class="n">compare_adam_adamw</span><span class="p">()</span>
</code></pre></div>

<p><strong>When to use AdamW:</strong>
- Almost always prefer AdamW over Adam when using weight decay
- Especially important for transformers and large models
- Default optimizer for BERT, GPT, and most modern NLP models</p>
<hr />
<h2 id="4-modern-optimizers">4. Modern Optimizers<a class="header-link" href="#4-modern-optimizers" title="Permanent link">&para;</a></h2>
<h3 id="41-lamb-layer-wise-adaptive-moments-optimizer-for-batch-training">4.1 LAMB (Layer-wise Adaptive Moments optimizer for Batch training)<a class="header-link" href="#41-lamb-layer-wise-adaptive-moments-optimizer-for-batch-training" title="Permanent link">&para;</a></h3>
<p>LAMB enables large batch training by using layer-wise adaptation:</p>
<p>$$
r_t = \frac{||\theta_t||}{||\hat{m}_t / (\sqrt{\hat{v}_t} + \epsilon)||} \cdot \frac{\hat{m}_t}{\sqrt{\hat{v}_t} + \epsilon}
$$</p>
<p>$$
\theta_{t+1} = \theta_t - \eta r_t
$$</p>
<p><strong>Use case</strong>: Training BERT with batch size 32K (vs 256 with Adam).</p>
<div class="highlight"><pre><span></span><code><span class="c1"># LAMB is not in PyTorch by default, but available in apex or standalone</span>
<span class="c1"># pip install pytorch-lamb</span>

<span class="k">try</span><span class="p">:</span>
    <span class="kn">from</span><span class="w"> </span><span class="nn">pytorch_lamb</span><span class="w"> </span><span class="kn">import</span> <span class="n">Lamb</span>

    <span class="c1"># LAMB optimizer for large-batch training</span>
    <span class="n">optimizer</span> <span class="o">=</span> <span class="n">Lamb</span><span class="p">(</span>
        <span class="n">model</span><span class="o">.</span><span class="n">parameters</span><span class="p">(),</span>
        <span class="n">lr</span><span class="o">=</span><span class="mf">0.001</span><span class="p">,</span>
        <span class="n">betas</span><span class="o">=</span><span class="p">(</span><span class="mf">0.9</span><span class="p">,</span> <span class="mf">0.999</span><span class="p">),</span>
        <span class="n">eps</span><span class="o">=</span><span class="mf">1e-6</span><span class="p">,</span>
        <span class="n">weight_decay</span><span class="o">=</span><span class="mf">0.01</span><span class="p">,</span>
        <span class="n">bias_correction</span><span class="o">=</span><span class="kc">True</span>
    <span class="p">)</span>

    <span class="c1"># Typical use: scale learning rate with batch size</span>
    <span class="c1"># LR = base_lr * sqrt(batch_size / base_batch_size)</span>
    <span class="n">base_lr</span> <span class="o">=</span> <span class="mf">0.001</span>
    <span class="n">base_batch_size</span> <span class="o">=</span> <span class="mi">256</span>
    <span class="n">large_batch_size</span> <span class="o">=</span> <span class="mi">8192</span>

    <span class="n">scaled_lr</span> <span class="o">=</span> <span class="n">base_lr</span> <span class="o">*</span> <span class="p">(</span><span class="n">large_batch_size</span> <span class="o">/</span> <span class="n">base_batch_size</span><span class="p">)</span> <span class="o">**</span> <span class="mf">0.5</span>
    <span class="n">optimizer</span> <span class="o">=</span> <span class="n">Lamb</span><span class="p">(</span><span class="n">model</span><span class="o">.</span><span class="n">parameters</span><span class="p">(),</span> <span class="n">lr</span><span class="o">=</span><span class="n">scaled_lr</span><span class="p">,</span> <span class="n">weight_decay</span><span class="o">=</span><span class="mf">0.01</span><span class="p">)</span>

<span class="k">except</span> <span class="ne">ImportError</span><span class="p">:</span>
    <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;LAMB not installed. Install with: pip install pytorch-lamb&quot;</span><span class="p">)</span>

<span class="c1"># LARS (Layer-wise Adaptive Rate Scaling) - similar idea for CNNs</span>
<span class="c1"># Used for ImageNet training with large batches</span>
</code></pre></div>

<h3 id="42-adafactor">4.2 Adafactor<a class="header-link" href="#42-adafactor" title="Permanent link">&para;</a></h3>
<p>Adafactor reduces memory by not storing full second moment:</p>
<p><strong>Key idea</strong>: Factor second moment matrix to reduce memory from O(nm) to O(n+m).</p>
<div class="highlight"><pre><span></span><code><span class="c1"># PyTorch doesn&#39;t have built-in Adafactor, but transformers library does</span>
<span class="k">try</span><span class="p">:</span>
    <span class="kn">from</span><span class="w"> </span><span class="nn">transformers</span><span class="w"> </span><span class="kn">import</span> <span class="n">Adafactor</span>

    <span class="c1"># Adafactor: memory-efficient for large models</span>
    <span class="n">optimizer</span> <span class="o">=</span> <span class="n">Adafactor</span><span class="p">(</span>
        <span class="n">model</span><span class="o">.</span><span class="n">parameters</span><span class="p">(),</span>
        <span class="n">lr</span><span class="o">=</span><span class="mf">1e-3</span><span class="p">,</span>
        <span class="n">eps</span><span class="o">=</span><span class="p">(</span><span class="mf">1e-30</span><span class="p">,</span> <span class="mf">1e-3</span><span class="p">),</span>
        <span class="n">clip_threshold</span><span class="o">=</span><span class="mf">1.0</span><span class="p">,</span>
        <span class="n">decay_rate</span><span class="o">=-</span><span class="mf">0.8</span><span class="p">,</span>
        <span class="n">beta1</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
        <span class="n">weight_decay</span><span class="o">=</span><span class="mf">0.0</span><span class="p">,</span>
        <span class="n">relative_step</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
        <span class="n">scale_parameter</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
        <span class="n">warmup_init</span><span class="o">=</span><span class="kc">False</span>
    <span class="p">)</span>

    <span class="c1"># With relative step (learning rate is automatically computed)</span>
    <span class="n">optimizer</span> <span class="o">=</span> <span class="n">Adafactor</span><span class="p">(</span>
        <span class="n">model</span><span class="o">.</span><span class="n">parameters</span><span class="p">(),</span>
        <span class="n">scale_parameter</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
        <span class="n">relative_step</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
        <span class="n">warmup_init</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
        <span class="n">lr</span><span class="o">=</span><span class="kc">None</span>  <span class="c1"># LR is computed automatically</span>
    <span class="p">)</span>

<span class="k">except</span> <span class="ne">ImportError</span><span class="p">:</span>
    <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Adafactor not available. Install with: pip install transformers&quot;</span><span class="p">)</span>
</code></pre></div>

<p><strong>When to use Adafactor:</strong>
- Training very large models (billions of parameters)
- Limited GPU memory
- T5, UL2 models were trained with Adafactor</p>
<h3 id="43-lion-evolved-sign-momentum">4.3 Lion (Evolved Sign Momentum)<a class="header-link" href="#43-lion-evolved-sign-momentum" title="Permanent link">&para;</a></h3>
<p>Lion uses only the sign of gradients, reducing memory and computation:</p>
<p>$$
\begin{align}
c_t &= \beta_1 m_{t-1} + (1-\beta_1) g_t \\
\theta_{t+1} &= \theta_t - \eta \cdot \text{sign}(c_t) \\
m_t &= \beta_2 m_{t-1} + (1-\beta_2) g_t
\end{align}
$$</p>
<p><strong>Advantages:</strong>
- 2x memory efficient (only stores momentum, not second moment)
- Faster computation (no square root operations)
- Competitive or better performance than Adam</p>
<div class="highlight"><pre><span></span><code><span class="c1"># Lion optimizer (pip install lion-pytorch)</span>
<span class="k">try</span><span class="p">:</span>
    <span class="kn">from</span><span class="w"> </span><span class="nn">lion_pytorch</span><span class="w"> </span><span class="kn">import</span> <span class="n">Lion</span>

    <span class="n">optimizer</span> <span class="o">=</span> <span class="n">Lion</span><span class="p">(</span>
        <span class="n">model</span><span class="o">.</span><span class="n">parameters</span><span class="p">(),</span>
        <span class="n">lr</span><span class="o">=</span><span class="mf">1e-4</span><span class="p">,</span>  <span class="c1"># Use ~3-10x smaller LR than Adam</span>
        <span class="n">betas</span><span class="o">=</span><span class="p">(</span><span class="mf">0.9</span><span class="p">,</span> <span class="mf">0.99</span><span class="p">),</span>
        <span class="n">weight_decay</span><span class="o">=</span><span class="mf">0.01</span>
    <span class="p">)</span>

    <span class="c1"># Typical usage: train vision transformer</span>
    <span class="c1"># Note: Lion needs smaller learning rate than Adam</span>

<span class="k">except</span> <span class="ne">ImportError</span><span class="p">:</span>
    <span class="c1"># Manual implementation (simplified)</span>
    <span class="k">class</span><span class="w"> </span><span class="nc">Lion</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">optim</span><span class="o">.</span><span class="n">Optimizer</span><span class="p">):</span>
        <span class="k">def</span><span class="w"> </span><span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">params</span><span class="p">,</span> <span class="n">lr</span><span class="o">=</span><span class="mf">1e-4</span><span class="p">,</span> <span class="n">betas</span><span class="o">=</span><span class="p">(</span><span class="mf">0.9</span><span class="p">,</span> <span class="mf">0.99</span><span class="p">),</span> <span class="n">weight_decay</span><span class="o">=</span><span class="mf">0.0</span><span class="p">):</span>
            <span class="n">defaults</span> <span class="o">=</span> <span class="nb">dict</span><span class="p">(</span><span class="n">lr</span><span class="o">=</span><span class="n">lr</span><span class="p">,</span> <span class="n">betas</span><span class="o">=</span><span class="n">betas</span><span class="p">,</span> <span class="n">weight_decay</span><span class="o">=</span><span class="n">weight_decay</span><span class="p">)</span>
            <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="n">params</span><span class="p">,</span> <span class="n">defaults</span><span class="p">)</span>

        <span class="k">def</span><span class="w"> </span><span class="nf">step</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
            <span class="k">for</span> <span class="n">group</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">param_groups</span><span class="p">:</span>
                <span class="k">for</span> <span class="n">p</span> <span class="ow">in</span> <span class="n">group</span><span class="p">[</span><span class="s1">&#39;params&#39;</span><span class="p">]:</span>
                    <span class="k">if</span> <span class="n">p</span><span class="o">.</span><span class="n">grad</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
                        <span class="k">continue</span>

                    <span class="n">grad</span> <span class="o">=</span> <span class="n">p</span><span class="o">.</span><span class="n">grad</span>
                    <span class="n">state</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">state</span><span class="p">[</span><span class="n">p</span><span class="p">]</span>

                    <span class="c1"># Initialize momentum</span>
                    <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">state</span><span class="p">)</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
                        <span class="n">state</span><span class="p">[</span><span class="s1">&#39;exp_avg&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">zeros_like</span><span class="p">(</span><span class="n">p</span><span class="p">)</span>

                    <span class="n">exp_avg</span> <span class="o">=</span> <span class="n">state</span><span class="p">[</span><span class="s1">&#39;exp_avg&#39;</span><span class="p">]</span>
                    <span class="n">beta1</span><span class="p">,</span> <span class="n">beta2</span> <span class="o">=</span> <span class="n">group</span><span class="p">[</span><span class="s1">&#39;betas&#39;</span><span class="p">]</span>

                    <span class="c1"># Weight decay</span>
                    <span class="k">if</span> <span class="n">group</span><span class="p">[</span><span class="s1">&#39;weight_decay&#39;</span><span class="p">]</span> <span class="o">!=</span> <span class="mi">0</span><span class="p">:</span>
                        <span class="n">p</span><span class="o">.</span><span class="n">mul_</span><span class="p">(</span><span class="mi">1</span> <span class="o">-</span> <span class="n">group</span><span class="p">[</span><span class="s1">&#39;lr&#39;</span><span class="p">]</span> <span class="o">*</span> <span class="n">group</span><span class="p">[</span><span class="s1">&#39;weight_decay&#39;</span><span class="p">])</span>

                    <span class="c1"># Update (sign of interpolated gradient)</span>
                    <span class="n">update</span> <span class="o">=</span> <span class="n">exp_avg</span><span class="o">.</span><span class="n">mul</span><span class="p">(</span><span class="n">beta1</span><span class="p">)</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">grad</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mi">1</span> <span class="o">-</span> <span class="n">beta1</span><span class="p">)</span><span class="o">.</span><span class="n">sign_</span><span class="p">()</span>
                    <span class="n">p</span><span class="o">.</span><span class="n">add_</span><span class="p">(</span><span class="n">update</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=-</span><span class="n">group</span><span class="p">[</span><span class="s1">&#39;lr&#39;</span><span class="p">])</span>

                    <span class="c1"># Update momentum</span>
                    <span class="n">exp_avg</span><span class="o">.</span><span class="n">mul_</span><span class="p">(</span><span class="n">beta2</span><span class="p">)</span><span class="o">.</span><span class="n">add_</span><span class="p">(</span><span class="n">grad</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mi">1</span> <span class="o">-</span> <span class="n">beta2</span><span class="p">)</span>

    <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Using simplified Lion implementation&quot;</span><span class="p">)</span>
</code></pre></div>

<h3 id="44-8-bit-adam-bitsandbytes">4.4 8-bit Adam (bitsandbytes)<a class="header-link" href="#44-8-bit-adam-bitsandbytes" title="Permanent link">&para;</a></h3>
<p>8-bit Adam quantizes optimizer states to reduce memory:</p>
<div class="highlight"><pre><span></span><code><span class="c1"># 8-bit Adam from bitsandbytes</span>
<span class="c1"># pip install bitsandbytes</span>

<span class="k">try</span><span class="p">:</span>
    <span class="kn">import</span><span class="w"> </span><span class="nn">bitsandbytes</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">bnb</span>

    <span class="c1"># 8-bit Adam: same performance, 75% less memory</span>
    <span class="n">optimizer</span> <span class="o">=</span> <span class="n">bnb</span><span class="o">.</span><span class="n">optim</span><span class="o">.</span><span class="n">Adam8bit</span><span class="p">(</span>
        <span class="n">model</span><span class="o">.</span><span class="n">parameters</span><span class="p">(),</span>
        <span class="n">lr</span><span class="o">=</span><span class="mf">1e-3</span><span class="p">,</span>
        <span class="n">betas</span><span class="o">=</span><span class="p">(</span><span class="mf">0.9</span><span class="p">,</span> <span class="mf">0.999</span><span class="p">),</span>
        <span class="n">eps</span><span class="o">=</span><span class="mf">1e-8</span><span class="p">,</span>
        <span class="n">weight_decay</span><span class="o">=</span><span class="mf">0.01</span>
    <span class="p">)</span>

    <span class="c1"># Also available: AdamW8bit, Lion8bit, etc.</span>
    <span class="n">optimizer</span> <span class="o">=</span> <span class="n">bnb</span><span class="o">.</span><span class="n">optim</span><span class="o">.</span><span class="n">AdamW8bit</span><span class="p">(</span><span class="n">model</span><span class="o">.</span><span class="n">parameters</span><span class="p">(),</span> <span class="n">lr</span><span class="o">=</span><span class="mf">1e-3</span><span class="p">)</span>

    <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Using 8-bit Adam - significant memory savings!&quot;</span><span class="p">)</span>

<span class="k">except</span> <span class="ne">ImportError</span><span class="p">:</span>
    <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;bitsandbytes not installed. Install with: pip install bitsandbytes&quot;</span><span class="p">)</span>
    <span class="n">optimizer</span> <span class="o">=</span> <span class="n">optim</span><span class="o">.</span><span class="n">AdamW</span><span class="p">(</span><span class="n">model</span><span class="o">.</span><span class="n">parameters</span><span class="p">(),</span> <span class="n">lr</span><span class="o">=</span><span class="mf">1e-3</span><span class="p">)</span>
</code></pre></div>

<p><strong>When to use 8-bit optimizers:</strong>
- Training large models with limited GPU memory
- Fine-tuning LLMs (e.g., LLaMA-7B on consumer GPU)
- Minimal performance impact (~0.1% difference)</p>
<h3 id="45-sophia-second-order-clipped-stochastic-optimization">4.5 Sophia (Second-order Clipped Stochastic Optimization)<a class="header-link" href="#45-sophia-second-order-clipped-stochastic-optimization" title="Permanent link">&para;</a></h3>
<p>Sophia uses Hessian diagonal information for better curvature adaptation:</p>
<p>$$
\theta_{t+1} = \theta_t - \eta \cdot \frac{m_t}{\max\{h_t, \epsilon\}}
$$</p>
<p>where $h_t$ is an estimate of the Hessian diagonal.</p>
<p><strong>Use case</strong>: Training language models 2x faster than Adam.</p>
<div class="highlight"><pre><span></span><code><span class="c1"># Sophia is experimental and requires custom implementation</span>
<span class="c1"># Available at: https://github.com/Liuhong99/Sophia</span>

<span class="c1"># Typical usage for LLM training:</span>
<span class="c1"># optimizer = SophiaG(model.parameters(), lr=2e-4, rho=0.04, weight_decay=0.1)</span>
</code></pre></div>

<h3 id="46-optimizer-comparison-table">4.6 Optimizer Comparison Table<a class="header-link" href="#46-optimizer-comparison-table" title="Permanent link">&para;</a></h3>
<table>
<thead>
<tr>
<th>Optimizer</th>
<th>Memory</th>
<th>Speed</th>
<th>Use Case</th>
<th>Typical LR</th>
</tr>
</thead>
<tbody>
<tr>
<td>SGD+Momentum</td>
<td>Low</td>
<td>Fast</td>
<td>CNNs, ResNets</td>
<td>0.1 - 0.01</td>
</tr>
<tr>
<td>Adam</td>
<td>High</td>
<td>Medium</td>
<td>General, Transformers</td>
<td>1e-3 - 1e-4</td>
</tr>
<tr>
<td>AdamW</td>
<td>High</td>
<td>Medium</td>
<td>Transformers, Fine-tuning</td>
<td>1e-3 - 1e-4</td>
</tr>
<tr>
<td>LAMB</td>
<td>High</td>
<td>Medium</td>
<td>Large-batch training</td>
<td>1e-2 - 1e-3</td>
</tr>
<tr>
<td>Adafactor</td>
<td>Medium</td>
<td>Slow</td>
<td>Very large models</td>
<td>1e-3 - 1e-2</td>
</tr>
<tr>
<td>Lion</td>
<td>Medium</td>
<td>Fast</td>
<td>Vision Transformers</td>
<td>1e-4 - 1e-5</td>
</tr>
<tr>
<td>8-bit Adam</td>
<td>Low</td>
<td>Medium</td>
<td>Limited GPU memory</td>
<td>1e-3 - 1e-4</td>
</tr>
<tr>
<td>Sophia</td>
<td>High</td>
<td>Medium</td>
<td>LLM pretraining</td>
<td>2e-4 - 5e-4</td>
</tr>
</tbody>
</table>
<hr />
<h2 id="5-learning-rate-schedulers">5. Learning Rate Schedulers<a class="header-link" href="#5-learning-rate-schedulers" title="Permanent link">&para;</a></h2>
<h3 id="51-step-based-schedulers">5.1 Step-based Schedulers<a class="header-link" href="#51-step-based-schedulers" title="Permanent link">&para;</a></h3>
<p><strong>StepLR</strong>: Decay LR by gamma every step_size epochs:</p>
<div class="highlight"><pre><span></span><code><span class="c1"># StepLR: multiply LR by 0.1 every 30 epochs</span>
<span class="n">optimizer</span> <span class="o">=</span> <span class="n">optim</span><span class="o">.</span><span class="n">SGD</span><span class="p">(</span><span class="n">model</span><span class="o">.</span><span class="n">parameters</span><span class="p">(),</span> <span class="n">lr</span><span class="o">=</span><span class="mf">0.1</span><span class="p">)</span>
<span class="n">scheduler</span> <span class="o">=</span> <span class="n">optim</span><span class="o">.</span><span class="n">lr_scheduler</span><span class="o">.</span><span class="n">StepLR</span><span class="p">(</span><span class="n">optimizer</span><span class="p">,</span> <span class="n">step_size</span><span class="o">=</span><span class="mi">30</span><span class="p">,</span> <span class="n">gamma</span><span class="o">=</span><span class="mf">0.1</span><span class="p">)</span>

<span class="c1"># Epoch 0-29: lr=0.1, Epoch 30-59: lr=0.01, Epoch 60+: lr=0.001</span>
<span class="k">for</span> <span class="n">epoch</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">100</span><span class="p">):</span>
    <span class="n">train_one_epoch</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">optimizer</span><span class="p">)</span>
    <span class="n">scheduler</span><span class="o">.</span><span class="n">step</span><span class="p">()</span>  <span class="c1"># Update LR at end of epoch</span>

<span class="c1"># MultiStepLR: decay at specific milestones</span>
<span class="n">scheduler</span> <span class="o">=</span> <span class="n">optim</span><span class="o">.</span><span class="n">lr_scheduler</span><span class="o">.</span><span class="n">MultiStepLR</span><span class="p">(</span><span class="n">optimizer</span><span class="p">,</span> <span class="n">milestones</span><span class="o">=</span><span class="p">[</span><span class="mi">30</span><span class="p">,</span> <span class="mi">80</span><span class="p">],</span> <span class="n">gamma</span><span class="o">=</span><span class="mf">0.1</span><span class="p">)</span>
<span class="c1"># Epoch 0-29: lr=0.1, Epoch 30-79: lr=0.01, Epoch 80+: lr=0.001</span>
</code></pre></div>

<h3 id="52-exponential-decay">5.2 Exponential Decay<a class="header-link" href="#52-exponential-decay" title="Permanent link">&para;</a></h3>
<p><strong>ExponentialLR</strong>: Multiply LR by gamma every epoch:</p>
<p>$$
\eta_t = \eta_0 \cdot \gamma^t
$$</p>
<div class="highlight"><pre><span></span><code><span class="c1"># Exponential decay</span>
<span class="n">optimizer</span> <span class="o">=</span> <span class="n">optim</span><span class="o">.</span><span class="n">Adam</span><span class="p">(</span><span class="n">model</span><span class="o">.</span><span class="n">parameters</span><span class="p">(),</span> <span class="n">lr</span><span class="o">=</span><span class="mf">0.001</span><span class="p">)</span>
<span class="n">scheduler</span> <span class="o">=</span> <span class="n">optim</span><span class="o">.</span><span class="n">lr_scheduler</span><span class="o">.</span><span class="n">ExponentialLR</span><span class="p">(</span><span class="n">optimizer</span><span class="p">,</span> <span class="n">gamma</span><span class="o">=</span><span class="mf">0.95</span><span class="p">)</span>

<span class="c1"># Each epoch: lr *= 0.95</span>
<span class="k">for</span> <span class="n">epoch</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">100</span><span class="p">):</span>
    <span class="n">train_one_epoch</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">optimizer</span><span class="p">)</span>
    <span class="n">scheduler</span><span class="o">.</span><span class="n">step</span><span class="p">()</span>
</code></pre></div>

<h3 id="53-cosine-annealing">5.3 Cosine Annealing<a class="header-link" href="#53-cosine-annealing" title="Permanent link">&para;</a></h3>
<p><strong>CosineAnnealingLR</strong>: Smooth decay following cosine curve:</p>
<p>$$
\eta_t = \eta_{\min} + \frac{1}{2}(\eta_{\max} - \eta_{\min})(1 + \cos(\frac{t}{T}\pi))
$$</p>
<div class="highlight"><pre><span></span><code><span class="c1"># Cosine annealing</span>
<span class="n">optimizer</span> <span class="o">=</span> <span class="n">optim</span><span class="o">.</span><span class="n">AdamW</span><span class="p">(</span><span class="n">model</span><span class="o">.</span><span class="n">parameters</span><span class="p">(),</span> <span class="n">lr</span><span class="o">=</span><span class="mf">0.001</span><span class="p">)</span>
<span class="n">scheduler</span> <span class="o">=</span> <span class="n">optim</span><span class="o">.</span><span class="n">lr_scheduler</span><span class="o">.</span><span class="n">CosineAnnealingLR</span><span class="p">(</span>
    <span class="n">optimizer</span><span class="p">,</span>
    <span class="n">T_max</span><span class="o">=</span><span class="mi">100</span><span class="p">,</span>  <span class="c1"># Total number of epochs</span>
    <span class="n">eta_min</span><span class="o">=</span><span class="mf">1e-6</span>  <span class="c1"># Minimum learning rate</span>
<span class="p">)</span>

<span class="c1"># LR smoothly decays from 0.001 to 1e-6 over 100 epochs</span>

<span class="c1"># Cosine annealing with warm restarts</span>
<span class="n">scheduler</span> <span class="o">=</span> <span class="n">optim</span><span class="o">.</span><span class="n">lr_scheduler</span><span class="o">.</span><span class="n">CosineAnnealingWarmRestarts</span><span class="p">(</span>
    <span class="n">optimizer</span><span class="p">,</span>
    <span class="n">T_0</span><span class="o">=</span><span class="mi">10</span><span class="p">,</span>  <span class="c1"># First restart after 10 epochs</span>
    <span class="n">T_mult</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span>  <span class="c1"># Double period after each restart</span>
    <span class="n">eta_min</span><span class="o">=</span><span class="mf">1e-6</span>
<span class="p">)</span>

<span class="c1"># LR schedule: 10 epochs, restart, 20 epochs, restart, 40 epochs, ...</span>
</code></pre></div>

<p><strong>Visualize cosine annealing:</strong></p>
<div class="highlight"><pre><span></span><code><span class="k">def</span><span class="w"> </span><span class="nf">visualize_cosine_schedule</span><span class="p">():</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Visualize cosine annealing schedule&quot;&quot;&quot;</span>
    <span class="kn">import</span><span class="w"> </span><span class="nn">numpy</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">np</span>

    <span class="n">epochs</span> <span class="o">=</span> <span class="mi">100</span>
    <span class="n">lr_max</span> <span class="o">=</span> <span class="mf">0.001</span>
    <span class="n">lr_min</span> <span class="o">=</span> <span class="mf">1e-6</span>

    <span class="c1"># Standard cosine</span>
    <span class="n">lrs_cosine</span> <span class="o">=</span> <span class="p">[</span>
        <span class="n">lr_min</span> <span class="o">+</span> <span class="mf">0.5</span> <span class="o">*</span> <span class="p">(</span><span class="n">lr_max</span> <span class="o">-</span> <span class="n">lr_min</span><span class="p">)</span> <span class="o">*</span> <span class="p">(</span><span class="mi">1</span> <span class="o">+</span> <span class="n">np</span><span class="o">.</span><span class="n">cos</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">pi</span> <span class="o">*</span> <span class="n">t</span> <span class="o">/</span> <span class="n">epochs</span><span class="p">))</span>
        <span class="k">for</span> <span class="n">t</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">epochs</span><span class="p">)</span>
    <span class="p">]</span>

    <span class="c1"># Cosine with warm restarts (T_0=10, T_mult=2)</span>
    <span class="n">lrs_restart</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="n">t</span> <span class="o">=</span> <span class="mi">0</span>
    <span class="n">T_cur</span> <span class="o">=</span> <span class="mi">10</span>
    <span class="k">for</span> <span class="n">epoch</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">epochs</span><span class="p">):</span>
        <span class="n">lr</span> <span class="o">=</span> <span class="n">lr_min</span> <span class="o">+</span> <span class="mf">0.5</span> <span class="o">*</span> <span class="p">(</span><span class="n">lr_max</span> <span class="o">-</span> <span class="n">lr_min</span><span class="p">)</span> <span class="o">*</span> <span class="p">(</span><span class="mi">1</span> <span class="o">+</span> <span class="n">np</span><span class="o">.</span><span class="n">cos</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">pi</span> <span class="o">*</span> <span class="n">t</span> <span class="o">/</span> <span class="n">T_cur</span><span class="p">))</span>
        <span class="n">lrs_restart</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">lr</span><span class="p">)</span>
        <span class="n">t</span> <span class="o">+=</span> <span class="mi">1</span>
        <span class="k">if</span> <span class="n">t</span> <span class="o">&gt;=</span> <span class="n">T_cur</span><span class="p">:</span>  <span class="c1"># Restart</span>
            <span class="n">t</span> <span class="o">=</span> <span class="mi">0</span>
            <span class="n">T_cur</span> <span class="o">*=</span> <span class="mi">2</span>

    <span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">12</span><span class="p">,</span> <span class="mi">6</span><span class="p">))</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">lrs_cosine</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s1">&#39;Cosine Annealing&#39;</span><span class="p">,</span> <span class="n">linewidth</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">lrs_restart</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s1">&#39;Cosine with Warm Restarts&#39;</span><span class="p">,</span> <span class="n">linewidth</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s1">&#39;Epoch&#39;</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s1">&#39;Learning Rate&#39;</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s1">&#39;Cosine Annealing Schedules&#39;</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">legend</span><span class="p">()</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">yscale</span><span class="p">(</span><span class="s1">&#39;log&#39;</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">grid</span><span class="p">(</span><span class="kc">True</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.3</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">savefig</span><span class="p">(</span><span class="s1">&#39;cosine_schedule.png&#39;</span><span class="p">,</span> <span class="n">dpi</span><span class="o">=</span><span class="mi">150</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">close</span><span class="p">()</span>

<span class="n">visualize_cosine_schedule</span><span class="p">()</span>
</code></pre></div>

<h3 id="54-onecyclelr-super-convergence">5.4 OneCycleLR (Super-convergence)<a class="header-link" href="#54-onecyclelr-super-convergence" title="Permanent link">&para;</a></h3>
<p>OneCycleLR uses a single cycle with warmup, peak, and decay:</p>
<div class="highlight"><pre><span></span><code>LR Schedule:
   max_lr ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ï±‚ï≤
                    ‚ï±    ‚ï≤
                  ‚ï±        ‚ï≤
   div_factor  ‚ï±            ‚ï≤‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ final_div
              ‚Üë               ‚Üë
           warmup          annealing
</code></pre></div>

<div class="highlight"><pre><span></span><code><span class="c1"># OneCycleLR: state-of-the-art for many tasks</span>
<span class="n">optimizer</span> <span class="o">=</span> <span class="n">optim</span><span class="o">.</span><span class="n">SGD</span><span class="p">(</span><span class="n">model</span><span class="o">.</span><span class="n">parameters</span><span class="p">(),</span> <span class="n">lr</span><span class="o">=</span><span class="mf">0.1</span><span class="p">,</span> <span class="n">momentum</span><span class="o">=</span><span class="mf">0.9</span><span class="p">)</span>
<span class="n">scheduler</span> <span class="o">=</span> <span class="n">optim</span><span class="o">.</span><span class="n">lr_scheduler</span><span class="o">.</span><span class="n">OneCycleLR</span><span class="p">(</span>
    <span class="n">optimizer</span><span class="p">,</span>
    <span class="n">max_lr</span><span class="o">=</span><span class="mf">0.1</span><span class="p">,</span>  <span class="c1"># Peak learning rate</span>
    <span class="n">total_steps</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>  <span class="c1"># Will be computed from epochs and steps_per_epoch</span>
    <span class="n">epochs</span><span class="o">=</span><span class="mi">90</span><span class="p">,</span>
    <span class="n">steps_per_epoch</span><span class="o">=</span><span class="nb">len</span><span class="p">(</span><span class="n">train_loader</span><span class="p">),</span>
    <span class="n">pct_start</span><span class="o">=</span><span class="mf">0.3</span><span class="p">,</span>  <span class="c1"># Warmup: 30% of training</span>
    <span class="n">anneal_strategy</span><span class="o">=</span><span class="s1">&#39;cos&#39;</span><span class="p">,</span>  <span class="c1"># &#39;cos&#39; or &#39;linear&#39;</span>
    <span class="n">div_factor</span><span class="o">=</span><span class="mf">25.0</span><span class="p">,</span>  <span class="c1"># Initial LR = max_lr / div_factor</span>
    <span class="n">final_div_factor</span><span class="o">=</span><span class="mf">10000.0</span>  <span class="c1"># Final LR = max_lr / final_div_factor</span>
<span class="p">)</span>

<span class="c1"># Must call scheduler.step() after EVERY batch</span>
<span class="k">for</span> <span class="n">epoch</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">90</span><span class="p">):</span>
    <span class="k">for</span> <span class="n">batch</span> <span class="ow">in</span> <span class="n">train_loader</span><span class="p">:</span>
        <span class="n">optimizer</span><span class="o">.</span><span class="n">zero_grad</span><span class="p">()</span>
        <span class="n">loss</span> <span class="o">=</span> <span class="n">model</span><span class="p">(</span><span class="n">batch</span><span class="p">)</span>
        <span class="n">loss</span><span class="o">.</span><span class="n">backward</span><span class="p">()</span>
        <span class="n">optimizer</span><span class="o">.</span><span class="n">step</span><span class="p">()</span>
        <span class="n">scheduler</span><span class="o">.</span><span class="n">step</span><span class="p">()</span>  <span class="c1"># Update after each batch!</span>

<span class="c1"># Visualize OneCycleLR</span>
<span class="k">def</span><span class="w"> </span><span class="nf">visualize_onecycle</span><span class="p">():</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Visualize OneCycleLR schedule&quot;&quot;&quot;</span>
    <span class="n">model</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="mi">10</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
    <span class="n">optimizer</span> <span class="o">=</span> <span class="n">optim</span><span class="o">.</span><span class="n">SGD</span><span class="p">(</span><span class="n">model</span><span class="o">.</span><span class="n">parameters</span><span class="p">(),</span> <span class="n">lr</span><span class="o">=</span><span class="mf">0.1</span><span class="p">)</span>

    <span class="n">scheduler</span> <span class="o">=</span> <span class="n">optim</span><span class="o">.</span><span class="n">lr_scheduler</span><span class="o">.</span><span class="n">OneCycleLR</span><span class="p">(</span>
        <span class="n">optimizer</span><span class="p">,</span>
        <span class="n">max_lr</span><span class="o">=</span><span class="mf">0.1</span><span class="p">,</span>
        <span class="n">total_steps</span><span class="o">=</span><span class="mi">1000</span><span class="p">,</span>
        <span class="n">pct_start</span><span class="o">=</span><span class="mf">0.3</span><span class="p">,</span>
        <span class="n">anneal_strategy</span><span class="o">=</span><span class="s1">&#39;cos&#39;</span>
    <span class="p">)</span>

    <span class="n">lrs</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">1000</span><span class="p">):</span>
        <span class="n">lrs</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">optimizer</span><span class="o">.</span><span class="n">param_groups</span><span class="p">[</span><span class="mi">0</span><span class="p">][</span><span class="s1">&#39;lr&#39;</span><span class="p">])</span>
        <span class="n">optimizer</span><span class="o">.</span><span class="n">step</span><span class="p">()</span>
        <span class="n">scheduler</span><span class="o">.</span><span class="n">step</span><span class="p">()</span>

    <span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">12</span><span class="p">,</span> <span class="mi">6</span><span class="p">))</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">lrs</span><span class="p">,</span> <span class="n">linewidth</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s1">&#39;Step&#39;</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s1">&#39;Learning Rate&#39;</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s1">&#39;OneCycleLR Schedule&#39;</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">axvline</span><span class="p">(</span><span class="mi">300</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s1">&#39;r&#39;</span><span class="p">,</span> <span class="n">linestyle</span><span class="o">=</span><span class="s1">&#39;--&#39;</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.5</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s1">&#39;End of warmup&#39;</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">legend</span><span class="p">()</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">grid</span><span class="p">(</span><span class="kc">True</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.3</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">savefig</span><span class="p">(</span><span class="s1">&#39;onecycle_schedule.png&#39;</span><span class="p">,</span> <span class="n">dpi</span><span class="o">=</span><span class="mi">150</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">close</span><span class="p">()</span>

<span class="n">visualize_onecycle</span><span class="p">()</span>
</code></pre></div>

<h3 id="55-linear-warmup-cosine-decay-transformer-standard">5.5 Linear Warmup + Cosine Decay (Transformer Standard)<a class="header-link" href="#55-linear-warmup-cosine-decay-transformer-standard" title="Permanent link">&para;</a></h3>
<p>Most transformer models use warmup followed by cosine decay:</p>
<div class="highlight"><pre><span></span><code><span class="kn">import</span><span class="w"> </span><span class="nn">math</span>

<span class="k">class</span><span class="w"> </span><span class="nc">WarmupCosineSchedule</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">optim</span><span class="o">.</span><span class="n">lr_scheduler</span><span class="o">.</span><span class="n">LambdaLR</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Linear warmup + cosine decay (BERT/GPT standard)&quot;&quot;&quot;</span>
    <span class="k">def</span><span class="w"> </span><span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">optimizer</span><span class="p">,</span> <span class="n">warmup_steps</span><span class="p">,</span> <span class="n">total_steps</span><span class="p">,</span> <span class="n">min_lr_ratio</span><span class="o">=</span><span class="mf">0.0</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">warmup_steps</span> <span class="o">=</span> <span class="n">warmup_steps</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">total_steps</span> <span class="o">=</span> <span class="n">total_steps</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">min_lr_ratio</span> <span class="o">=</span> <span class="n">min_lr_ratio</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="n">optimizer</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">lr_lambda</span><span class="p">)</span>

    <span class="k">def</span><span class="w"> </span><span class="nf">lr_lambda</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">step</span><span class="p">):</span>
        <span class="k">if</span> <span class="n">step</span> <span class="o">&lt;</span> <span class="bp">self</span><span class="o">.</span><span class="n">warmup_steps</span><span class="p">:</span>
            <span class="c1"># Linear warmup</span>
            <span class="k">return</span> <span class="nb">float</span><span class="p">(</span><span class="n">step</span><span class="p">)</span> <span class="o">/</span> <span class="nb">float</span><span class="p">(</span><span class="nb">max</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">warmup_steps</span><span class="p">))</span>

        <span class="c1"># Cosine decay</span>
        <span class="n">progress</span> <span class="o">=</span> <span class="nb">float</span><span class="p">(</span><span class="n">step</span> <span class="o">-</span> <span class="bp">self</span><span class="o">.</span><span class="n">warmup_steps</span><span class="p">)</span> <span class="o">/</span> <span class="nb">float</span><span class="p">(</span><span class="nb">max</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">total_steps</span> <span class="o">-</span> <span class="bp">self</span><span class="o">.</span><span class="n">warmup_steps</span><span class="p">))</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">min_lr_ratio</span> <span class="o">+</span> <span class="p">(</span><span class="mi">1</span> <span class="o">-</span> <span class="bp">self</span><span class="o">.</span><span class="n">min_lr_ratio</span><span class="p">)</span> <span class="o">*</span> <span class="mf">0.5</span> <span class="o">*</span> <span class="p">(</span><span class="mf">1.0</span> <span class="o">+</span> <span class="n">math</span><span class="o">.</span><span class="n">cos</span><span class="p">(</span><span class="n">math</span><span class="o">.</span><span class="n">pi</span> <span class="o">*</span> <span class="n">progress</span><span class="p">))</span>

<span class="c1"># Usage for transformer training</span>
<span class="n">optimizer</span> <span class="o">=</span> <span class="n">optim</span><span class="o">.</span><span class="n">AdamW</span><span class="p">(</span><span class="n">model</span><span class="o">.</span><span class="n">parameters</span><span class="p">(),</span> <span class="n">lr</span><span class="o">=</span><span class="mf">5e-4</span><span class="p">,</span> <span class="n">weight_decay</span><span class="o">=</span><span class="mf">0.01</span><span class="p">)</span>
<span class="n">total_steps</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">train_loader</span><span class="p">)</span> <span class="o">*</span> <span class="n">num_epochs</span>
<span class="n">warmup_steps</span> <span class="o">=</span> <span class="nb">int</span><span class="p">(</span><span class="mf">0.1</span> <span class="o">*</span> <span class="n">total_steps</span><span class="p">)</span>  <span class="c1"># 10% warmup</span>

<span class="n">scheduler</span> <span class="o">=</span> <span class="n">WarmupCosineSchedule</span><span class="p">(</span>
    <span class="n">optimizer</span><span class="p">,</span>
    <span class="n">warmup_steps</span><span class="o">=</span><span class="n">warmup_steps</span><span class="p">,</span>
    <span class="n">total_steps</span><span class="o">=</span><span class="n">total_steps</span><span class="p">,</span>
    <span class="n">min_lr_ratio</span><span class="o">=</span><span class="mf">0.1</span>  <span class="c1"># Decay to 10% of peak LR</span>
<span class="p">)</span>

<span class="c1"># Training loop</span>
<span class="k">for</span> <span class="n">epoch</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">num_epochs</span><span class="p">):</span>
    <span class="k">for</span> <span class="n">batch</span> <span class="ow">in</span> <span class="n">train_loader</span><span class="p">:</span>
        <span class="n">optimizer</span><span class="o">.</span><span class="n">zero_grad</span><span class="p">()</span>
        <span class="n">loss</span> <span class="o">=</span> <span class="n">model</span><span class="p">(</span><span class="n">batch</span><span class="p">)</span>
        <span class="n">loss</span><span class="o">.</span><span class="n">backward</span><span class="p">()</span>
        <span class="n">optimizer</span><span class="o">.</span><span class="n">step</span><span class="p">()</span>
        <span class="n">scheduler</span><span class="o">.</span><span class="n">step</span><span class="p">()</span>  <span class="c1"># Update after each batch</span>

<span class="c1"># Visualize</span>
<span class="k">def</span><span class="w"> </span><span class="nf">visualize_warmup_cosine</span><span class="p">():</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Visualize warmup + cosine schedule&quot;&quot;&quot;</span>
    <span class="n">model</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="mi">10</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
    <span class="n">optimizer</span> <span class="o">=</span> <span class="n">optim</span><span class="o">.</span><span class="n">AdamW</span><span class="p">(</span><span class="n">model</span><span class="o">.</span><span class="n">parameters</span><span class="p">(),</span> <span class="n">lr</span><span class="o">=</span><span class="mf">5e-4</span><span class="p">)</span>

    <span class="n">total_steps</span> <span class="o">=</span> <span class="mi">10000</span>
    <span class="n">warmup_steps</span> <span class="o">=</span> <span class="mi">1000</span>
    <span class="n">scheduler</span> <span class="o">=</span> <span class="n">WarmupCosineSchedule</span><span class="p">(</span><span class="n">optimizer</span><span class="p">,</span> <span class="n">warmup_steps</span><span class="p">,</span> <span class="n">total_steps</span><span class="p">,</span> <span class="n">min_lr_ratio</span><span class="o">=</span><span class="mf">0.1</span><span class="p">)</span>

    <span class="n">lrs</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">total_steps</span><span class="p">):</span>
        <span class="n">lrs</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">optimizer</span><span class="o">.</span><span class="n">param_groups</span><span class="p">[</span><span class="mi">0</span><span class="p">][</span><span class="s1">&#39;lr&#39;</span><span class="p">])</span>
        <span class="n">optimizer</span><span class="o">.</span><span class="n">step</span><span class="p">()</span>
        <span class="n">scheduler</span><span class="o">.</span><span class="n">step</span><span class="p">()</span>

    <span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">12</span><span class="p">,</span> <span class="mi">6</span><span class="p">))</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">lrs</span><span class="p">,</span> <span class="n">linewidth</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s1">&#39;Step&#39;</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s1">&#39;Learning Rate&#39;</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s1">&#39;Linear Warmup + Cosine Decay (Transformer Standard)&#39;</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">axvline</span><span class="p">(</span><span class="n">warmup_steps</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s1">&#39;r&#39;</span><span class="p">,</span> <span class="n">linestyle</span><span class="o">=</span><span class="s1">&#39;--&#39;</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.5</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s1">&#39;End of warmup&#39;</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">legend</span><span class="p">()</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">grid</span><span class="p">(</span><span class="kc">True</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.3</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">savefig</span><span class="p">(</span><span class="s1">&#39;warmup_cosine_schedule.png&#39;</span><span class="p">,</span> <span class="n">dpi</span><span class="o">=</span><span class="mi">150</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">close</span><span class="p">()</span>

<span class="n">visualize_warmup_cosine</span><span class="p">()</span>
</code></pre></div>

<h3 id="56-reducelronplateau">5.6 ReduceLROnPlateau<a class="header-link" href="#56-reducelronplateau" title="Permanent link">&para;</a></h3>
<p>Reduce LR when validation metric plateaus:</p>
<div class="highlight"><pre><span></span><code><span class="c1"># ReduceLROnPlateau: data-driven LR reduction</span>
<span class="n">optimizer</span> <span class="o">=</span> <span class="n">optim</span><span class="o">.</span><span class="n">Adam</span><span class="p">(</span><span class="n">model</span><span class="o">.</span><span class="n">parameters</span><span class="p">(),</span> <span class="n">lr</span><span class="o">=</span><span class="mf">0.001</span><span class="p">)</span>
<span class="n">scheduler</span> <span class="o">=</span> <span class="n">optim</span><span class="o">.</span><span class="n">lr_scheduler</span><span class="o">.</span><span class="n">ReduceLROnPlateau</span><span class="p">(</span>
    <span class="n">optimizer</span><span class="p">,</span>
    <span class="n">mode</span><span class="o">=</span><span class="s1">&#39;min&#39;</span><span class="p">,</span>  <span class="c1"># &#39;min&#39; for loss, &#39;max&#39; for accuracy</span>
    <span class="n">factor</span><span class="o">=</span><span class="mf">0.5</span><span class="p">,</span>  <span class="c1"># Multiply LR by 0.5</span>
    <span class="n">patience</span><span class="o">=</span><span class="mi">10</span><span class="p">,</span>  <span class="c1"># Wait 10 epochs before reducing</span>
    <span class="n">verbose</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
    <span class="n">min_lr</span><span class="o">=</span><span class="mf">1e-6</span>
<span class="p">)</span>

<span class="k">for</span> <span class="n">epoch</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">100</span><span class="p">):</span>
    <span class="n">train_loss</span> <span class="o">=</span> <span class="n">train_one_epoch</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">optimizer</span><span class="p">)</span>
    <span class="n">val_loss</span> <span class="o">=</span> <span class="n">validate</span><span class="p">(</span><span class="n">model</span><span class="p">)</span>

    <span class="c1"># Step based on validation loss</span>
    <span class="n">scheduler</span><span class="o">.</span><span class="n">step</span><span class="p">(</span><span class="n">val_loss</span><span class="p">)</span>

    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Epoch </span><span class="si">{</span><span class="n">epoch</span><span class="si">}</span><span class="s2">: LR = </span><span class="si">{</span><span class="n">optimizer</span><span class="o">.</span><span class="n">param_groups</span><span class="p">[</span><span class="mi">0</span><span class="p">][</span><span class="s1">&#39;lr&#39;</span><span class="p">]</span><span class="si">:</span><span class="s2">.6f</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
</code></pre></div>

<h3 id="57-custom-scheduler">5.7 Custom Scheduler<a class="header-link" href="#57-custom-scheduler" title="Permanent link">&para;</a></h3>
<p>Implement custom schedule using LambdaLR:</p>
<div class="highlight"><pre><span></span><code><span class="c1"># Custom schedule: polynomial decay with warmup</span>
<span class="k">def</span><span class="w"> </span><span class="nf">polynomial_decay_with_warmup</span><span class="p">(</span><span class="n">step</span><span class="p">,</span> <span class="n">warmup_steps</span><span class="p">,</span> <span class="n">total_steps</span><span class="p">,</span> <span class="n">power</span><span class="o">=</span><span class="mf">1.0</span><span class="p">):</span>
    <span class="k">if</span> <span class="n">step</span> <span class="o">&lt;</span> <span class="n">warmup_steps</span><span class="p">:</span>
        <span class="k">return</span> <span class="nb">float</span><span class="p">(</span><span class="n">step</span><span class="p">)</span> <span class="o">/</span> <span class="nb">float</span><span class="p">(</span><span class="nb">max</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">warmup_steps</span><span class="p">))</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="n">progress</span> <span class="o">=</span> <span class="p">(</span><span class="n">step</span> <span class="o">-</span> <span class="n">warmup_steps</span><span class="p">)</span> <span class="o">/</span> <span class="p">(</span><span class="n">total_steps</span> <span class="o">-</span> <span class="n">warmup_steps</span><span class="p">)</span>
        <span class="k">return</span> <span class="p">(</span><span class="mi">1</span> <span class="o">-</span> <span class="n">progress</span><span class="p">)</span> <span class="o">**</span> <span class="n">power</span>

<span class="n">optimizer</span> <span class="o">=</span> <span class="n">optim</span><span class="o">.</span><span class="n">AdamW</span><span class="p">(</span><span class="n">model</span><span class="o">.</span><span class="n">parameters</span><span class="p">(),</span> <span class="n">lr</span><span class="o">=</span><span class="mf">5e-4</span><span class="p">)</span>
<span class="n">scheduler</span> <span class="o">=</span> <span class="n">optim</span><span class="o">.</span><span class="n">lr_scheduler</span><span class="o">.</span><span class="n">LambdaLR</span><span class="p">(</span>
    <span class="n">optimizer</span><span class="p">,</span>
    <span class="n">lr_lambda</span><span class="o">=</span><span class="k">lambda</span> <span class="n">step</span><span class="p">:</span> <span class="n">polynomial_decay_with_warmup</span><span class="p">(</span><span class="n">step</span><span class="p">,</span> <span class="mi">1000</span><span class="p">,</span> <span class="mi">10000</span><span class="p">,</span> <span class="n">power</span><span class="o">=</span><span class="mf">2.0</span><span class="p">)</span>
<span class="p">)</span>

<span class="c1"># Or chain multiple schedulers</span>
<span class="n">scheduler1</span> <span class="o">=</span> <span class="n">optim</span><span class="o">.</span><span class="n">lr_scheduler</span><span class="o">.</span><span class="n">LinearLR</span><span class="p">(</span><span class="n">optimizer</span><span class="p">,</span> <span class="n">start_factor</span><span class="o">=</span><span class="mf">0.1</span><span class="p">,</span> <span class="n">total_iters</span><span class="o">=</span><span class="mi">1000</span><span class="p">)</span>
<span class="n">scheduler2</span> <span class="o">=</span> <span class="n">optim</span><span class="o">.</span><span class="n">lr_scheduler</span><span class="o">.</span><span class="n">ExponentialLR</span><span class="p">(</span><span class="n">optimizer</span><span class="p">,</span> <span class="n">gamma</span><span class="o">=</span><span class="mf">0.9</span><span class="p">)</span>
<span class="n">scheduler</span> <span class="o">=</span> <span class="n">optim</span><span class="o">.</span><span class="n">lr_scheduler</span><span class="o">.</span><span class="n">ChainedScheduler</span><span class="p">([</span><span class="n">scheduler1</span><span class="p">,</span> <span class="n">scheduler2</span><span class="p">])</span>
</code></pre></div>

<h3 id="58-scheduler-visualization-suite">5.8 Scheduler Visualization Suite<a class="header-link" href="#58-scheduler-visualization-suite" title="Permanent link">&para;</a></h3>
<div class="highlight"><pre><span></span><code><span class="k">def</span><span class="w"> </span><span class="nf">compare_all_schedulers</span><span class="p">():</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Compare all scheduler types&quot;&quot;&quot;</span>
    <span class="n">model</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="mi">10</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
    <span class="n">total_steps</span> <span class="o">=</span> <span class="mi">1000</span>

    <span class="n">schedulers</span> <span class="o">=</span> <span class="p">{</span>
        <span class="s1">&#39;StepLR&#39;</span><span class="p">:</span> <span class="k">lambda</span> <span class="n">opt</span><span class="p">:</span> <span class="n">optim</span><span class="o">.</span><span class="n">lr_scheduler</span><span class="o">.</span><span class="n">StepLR</span><span class="p">(</span><span class="n">opt</span><span class="p">,</span> <span class="n">step_size</span><span class="o">=</span><span class="mi">300</span><span class="p">,</span> <span class="n">gamma</span><span class="o">=</span><span class="mf">0.1</span><span class="p">),</span>
        <span class="s1">&#39;ExponentialLR&#39;</span><span class="p">:</span> <span class="k">lambda</span> <span class="n">opt</span><span class="p">:</span> <span class="n">optim</span><span class="o">.</span><span class="n">lr_scheduler</span><span class="o">.</span><span class="n">ExponentialLR</span><span class="p">(</span><span class="n">opt</span><span class="p">,</span> <span class="n">gamma</span><span class="o">=</span><span class="mf">0.995</span><span class="p">),</span>
        <span class="s1">&#39;CosineAnnealing&#39;</span><span class="p">:</span> <span class="k">lambda</span> <span class="n">opt</span><span class="p">:</span> <span class="n">optim</span><span class="o">.</span><span class="n">lr_scheduler</span><span class="o">.</span><span class="n">CosineAnnealingLR</span><span class="p">(</span><span class="n">opt</span><span class="p">,</span> <span class="n">T_max</span><span class="o">=</span><span class="n">total_steps</span><span class="p">),</span>
        <span class="s1">&#39;OneCycleLR&#39;</span><span class="p">:</span> <span class="k">lambda</span> <span class="n">opt</span><span class="p">:</span> <span class="n">optim</span><span class="o">.</span><span class="n">lr_scheduler</span><span class="o">.</span><span class="n">OneCycleLR</span><span class="p">(</span><span class="n">opt</span><span class="p">,</span> <span class="n">max_lr</span><span class="o">=</span><span class="mf">0.1</span><span class="p">,</span> <span class="n">total_steps</span><span class="o">=</span><span class="n">total_steps</span><span class="p">),</span>
        <span class="s1">&#39;Warmup+Cosine&#39;</span><span class="p">:</span> <span class="k">lambda</span> <span class="n">opt</span><span class="p">:</span> <span class="n">WarmupCosineSchedule</span><span class="p">(</span><span class="n">opt</span><span class="p">,</span> <span class="n">warmup_steps</span><span class="o">=</span><span class="mi">100</span><span class="p">,</span> <span class="n">total_steps</span><span class="o">=</span><span class="n">total_steps</span><span class="p">),</span>
    <span class="p">}</span>

    <span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">14</span><span class="p">,</span> <span class="mi">8</span><span class="p">))</span>

    <span class="k">for</span> <span class="n">name</span><span class="p">,</span> <span class="n">sched_fn</span> <span class="ow">in</span> <span class="n">schedulers</span><span class="o">.</span><span class="n">items</span><span class="p">():</span>
        <span class="n">optimizer</span> <span class="o">=</span> <span class="n">optim</span><span class="o">.</span><span class="n">SGD</span><span class="p">(</span><span class="n">model</span><span class="o">.</span><span class="n">parameters</span><span class="p">(),</span> <span class="n">lr</span><span class="o">=</span><span class="mf">0.1</span><span class="p">)</span>
        <span class="n">scheduler</span> <span class="o">=</span> <span class="n">sched_fn</span><span class="p">(</span><span class="n">optimizer</span><span class="p">)</span>

        <span class="n">lrs</span> <span class="o">=</span> <span class="p">[]</span>
        <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">total_steps</span><span class="p">):</span>
            <span class="n">lrs</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">optimizer</span><span class="o">.</span><span class="n">param_groups</span><span class="p">[</span><span class="mi">0</span><span class="p">][</span><span class="s1">&#39;lr&#39;</span><span class="p">])</span>
            <span class="n">optimizer</span><span class="o">.</span><span class="n">step</span><span class="p">()</span>
            <span class="n">scheduler</span><span class="o">.</span><span class="n">step</span><span class="p">()</span>

        <span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">lrs</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="n">name</span><span class="p">,</span> <span class="n">linewidth</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>

    <span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s1">&#39;Step&#39;</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s1">&#39;Learning Rate&#39;</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s1">&#39;Learning Rate Scheduler Comparison&#39;</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">legend</span><span class="p">()</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">yscale</span><span class="p">(</span><span class="s1">&#39;log&#39;</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">grid</span><span class="p">(</span><span class="kc">True</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.3</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">savefig</span><span class="p">(</span><span class="s1">&#39;scheduler_comparison.png&#39;</span><span class="p">,</span> <span class="n">dpi</span><span class="o">=</span><span class="mi">150</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">close</span><span class="p">()</span>

<span class="n">compare_all_schedulers</span><span class="p">()</span>
</code></pre></div>

<hr />
<h2 id="6-practical-techniques">6. Practical Techniques<a class="header-link" href="#6-practical-techniques" title="Permanent link">&para;</a></h2>
<h3 id="61-learning-rate-finder-lr-range-test">6.1 Learning Rate Finder (LR Range Test)<a class="header-link" href="#61-learning-rate-finder-lr-range-test" title="Permanent link">&para;</a></h3>
<p>Find optimal learning rate by gradually increasing LR:</p>
<div class="highlight"><pre><span></span><code><span class="k">class</span><span class="w"> </span><span class="nc">LRFinder</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Learning rate range test (Leslie Smith)&quot;&quot;&quot;</span>
    <span class="k">def</span><span class="w"> </span><span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">model</span><span class="p">,</span> <span class="n">optimizer</span><span class="p">,</span> <span class="n">criterion</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="s1">&#39;cuda&#39;</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">model</span> <span class="o">=</span> <span class="n">model</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">optimizer</span> <span class="o">=</span> <span class="n">optimizer</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">criterion</span> <span class="o">=</span> <span class="n">criterion</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">device</span> <span class="o">=</span> <span class="n">device</span>

    <span class="k">def</span><span class="w"> </span><span class="nf">range_test</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">train_loader</span><span class="p">,</span> <span class="n">start_lr</span><span class="o">=</span><span class="mf">1e-7</span><span class="p">,</span> <span class="n">end_lr</span><span class="o">=</span><span class="mi">10</span><span class="p">,</span> <span class="n">num_iter</span><span class="o">=</span><span class="mi">100</span><span class="p">,</span> <span class="n">smooth_f</span><span class="o">=</span><span class="mf">0.05</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Run LR range test&quot;&quot;&quot;</span>
        <span class="c1"># Save initial state</span>
        <span class="n">model_state</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">model</span><span class="o">.</span><span class="n">state_dict</span><span class="p">()</span>
        <span class="n">optim_state</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">optimizer</span><span class="o">.</span><span class="n">state_dict</span><span class="p">()</span>

        <span class="c1"># Update LR</span>
        <span class="n">lrs</span> <span class="o">=</span> <span class="p">[]</span>
        <span class="n">losses</span> <span class="o">=</span> <span class="p">[]</span>
        <span class="n">best_loss</span> <span class="o">=</span> <span class="nb">float</span><span class="p">(</span><span class="s1">&#39;inf&#39;</span><span class="p">)</span>

        <span class="n">lr_mult</span> <span class="o">=</span> <span class="p">(</span><span class="n">end_lr</span> <span class="o">/</span> <span class="n">start_lr</span><span class="p">)</span> <span class="o">**</span> <span class="p">(</span><span class="mi">1</span> <span class="o">/</span> <span class="n">num_iter</span><span class="p">)</span>
        <span class="n">lr</span> <span class="o">=</span> <span class="n">start_lr</span>

        <span class="k">for</span> <span class="n">param_group</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">optimizer</span><span class="o">.</span><span class="n">param_groups</span><span class="p">:</span>
            <span class="n">param_group</span><span class="p">[</span><span class="s1">&#39;lr&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">lr</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">model</span><span class="o">.</span><span class="n">train</span><span class="p">()</span>
        <span class="n">iterator</span> <span class="o">=</span> <span class="nb">iter</span><span class="p">(</span><span class="n">train_loader</span><span class="p">)</span>

        <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">num_iter</span><span class="p">):</span>
            <span class="k">try</span><span class="p">:</span>
                <span class="n">X</span><span class="p">,</span> <span class="n">y</span> <span class="o">=</span> <span class="nb">next</span><span class="p">(</span><span class="n">iterator</span><span class="p">)</span>
            <span class="k">except</span> <span class="ne">StopIteration</span><span class="p">:</span>
                <span class="n">iterator</span> <span class="o">=</span> <span class="nb">iter</span><span class="p">(</span><span class="n">train_loader</span><span class="p">)</span>
                <span class="n">X</span><span class="p">,</span> <span class="n">y</span> <span class="o">=</span> <span class="nb">next</span><span class="p">(</span><span class="n">iterator</span><span class="p">)</span>

            <span class="n">X</span><span class="p">,</span> <span class="n">y</span> <span class="o">=</span> <span class="n">X</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">device</span><span class="p">),</span> <span class="n">y</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">device</span><span class="p">)</span>

            <span class="c1"># Forward pass</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">optimizer</span><span class="o">.</span><span class="n">zero_grad</span><span class="p">()</span>
            <span class="n">output</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">model</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>
            <span class="n">loss</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">criterion</span><span class="p">(</span><span class="n">output</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>

            <span class="c1"># Smooth loss</span>
            <span class="k">if</span> <span class="n">i</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
                <span class="n">avg_loss</span> <span class="o">=</span> <span class="n">loss</span><span class="o">.</span><span class="n">item</span><span class="p">()</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="n">avg_loss</span> <span class="o">=</span> <span class="n">smooth_f</span> <span class="o">*</span> <span class="n">loss</span><span class="o">.</span><span class="n">item</span><span class="p">()</span> <span class="o">+</span> <span class="p">(</span><span class="mi">1</span> <span class="o">-</span> <span class="n">smooth_f</span><span class="p">)</span> <span class="o">*</span> <span class="n">avg_loss</span>

            <span class="c1"># Stop if loss is exploding</span>
            <span class="k">if</span> <span class="n">avg_loss</span> <span class="o">&gt;</span> <span class="mi">4</span> <span class="o">*</span> <span class="n">best_loss</span> <span class="ow">or</span> <span class="n">torch</span><span class="o">.</span><span class="n">isnan</span><span class="p">(</span><span class="n">loss</span><span class="p">):</span>
                <span class="k">break</span>

            <span class="k">if</span> <span class="n">avg_loss</span> <span class="o">&lt;</span> <span class="n">best_loss</span><span class="p">:</span>
                <span class="n">best_loss</span> <span class="o">=</span> <span class="n">avg_loss</span>

            <span class="c1"># Record</span>
            <span class="n">lrs</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">lr</span><span class="p">)</span>
            <span class="n">losses</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">avg_loss</span><span class="p">)</span>

            <span class="c1"># Backward pass</span>
            <span class="n">loss</span><span class="o">.</span><span class="n">backward</span><span class="p">()</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">optimizer</span><span class="o">.</span><span class="n">step</span><span class="p">()</span>

            <span class="c1"># Update LR</span>
            <span class="n">lr</span> <span class="o">*=</span> <span class="n">lr_mult</span>
            <span class="k">for</span> <span class="n">param_group</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">optimizer</span><span class="o">.</span><span class="n">param_groups</span><span class="p">:</span>
                <span class="n">param_group</span><span class="p">[</span><span class="s1">&#39;lr&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">lr</span>

        <span class="c1"># Restore initial state</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">model</span><span class="o">.</span><span class="n">load_state_dict</span><span class="p">(</span><span class="n">model_state</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">optimizer</span><span class="o">.</span><span class="n">load_state_dict</span><span class="p">(</span><span class="n">optim_state</span><span class="p">)</span>

        <span class="k">return</span> <span class="n">lrs</span><span class="p">,</span> <span class="n">losses</span>

    <span class="k">def</span><span class="w"> </span><span class="nf">plot</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">lrs</span><span class="p">,</span> <span class="n">losses</span><span class="p">,</span> <span class="n">skip_start</span><span class="o">=</span><span class="mi">10</span><span class="p">,</span> <span class="n">skip_end</span><span class="o">=</span><span class="mi">5</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Plot LR finder results&quot;&quot;&quot;</span>
        <span class="k">if</span> <span class="n">skip_end</span> <span class="o">&gt;</span> <span class="mi">0</span><span class="p">:</span>
            <span class="n">lrs</span> <span class="o">=</span> <span class="n">lrs</span><span class="p">[</span><span class="n">skip_start</span><span class="p">:</span><span class="o">-</span><span class="n">skip_end</span><span class="p">]</span>
            <span class="n">losses</span> <span class="o">=</span> <span class="n">losses</span><span class="p">[</span><span class="n">skip_start</span><span class="p">:</span><span class="o">-</span><span class="n">skip_end</span><span class="p">]</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">lrs</span> <span class="o">=</span> <span class="n">lrs</span><span class="p">[</span><span class="n">skip_start</span><span class="p">:]</span>
            <span class="n">losses</span> <span class="o">=</span> <span class="n">losses</span><span class="p">[</span><span class="n">skip_start</span><span class="p">:]</span>

        <span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">10</span><span class="p">,</span> <span class="mi">6</span><span class="p">))</span>
        <span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">lrs</span><span class="p">,</span> <span class="n">losses</span><span class="p">,</span> <span class="n">linewidth</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
        <span class="n">plt</span><span class="o">.</span><span class="n">xscale</span><span class="p">(</span><span class="s1">&#39;log&#39;</span><span class="p">)</span>
        <span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s1">&#39;Learning Rate&#39;</span><span class="p">)</span>
        <span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s1">&#39;Loss&#39;</span><span class="p">)</span>
        <span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s1">&#39;LR Range Test&#39;</span><span class="p">)</span>
        <span class="n">plt</span><span class="o">.</span><span class="n">grid</span><span class="p">(</span><span class="kc">True</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.3</span><span class="p">)</span>

        <span class="c1"># Find steepest descent</span>
        <span class="n">min_grad_idx</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">gradient</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">losses</span><span class="p">))</span><span class="o">.</span><span class="n">argmin</span><span class="p">()</span>
        <span class="n">suggested_lr</span> <span class="o">=</span> <span class="n">lrs</span><span class="p">[</span><span class="n">min_grad_idx</span><span class="p">]</span>
        <span class="n">plt</span><span class="o">.</span><span class="n">axvline</span><span class="p">(</span><span class="n">suggested_lr</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s1">&#39;r&#39;</span><span class="p">,</span> <span class="n">linestyle</span><span class="o">=</span><span class="s1">&#39;--&#39;</span><span class="p">,</span>
                    <span class="n">label</span><span class="o">=</span><span class="sa">f</span><span class="s1">&#39;Suggested LR: </span><span class="si">{</span><span class="n">suggested_lr</span><span class="si">:</span><span class="s1">.2e</span><span class="si">}</span><span class="s1">&#39;</span><span class="p">)</span>
        <span class="n">plt</span><span class="o">.</span><span class="n">legend</span><span class="p">()</span>
        <span class="n">plt</span><span class="o">.</span><span class="n">savefig</span><span class="p">(</span><span class="s1">&#39;lr_finder.png&#39;</span><span class="p">,</span> <span class="n">dpi</span><span class="o">=</span><span class="mi">150</span><span class="p">)</span>
        <span class="n">plt</span><span class="o">.</span><span class="n">close</span><span class="p">()</span>

        <span class="k">return</span> <span class="n">suggested_lr</span>

<span class="c1"># Usage</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Sequential</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="mi">10</span><span class="p">,</span> <span class="mi">50</span><span class="p">),</span> <span class="n">nn</span><span class="o">.</span><span class="n">ReLU</span><span class="p">(),</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="mi">50</span><span class="p">,</span> <span class="mi">1</span><span class="p">))</span>
<span class="n">optimizer</span> <span class="o">=</span> <span class="n">optim</span><span class="o">.</span><span class="n">SGD</span><span class="p">(</span><span class="n">model</span><span class="o">.</span><span class="n">parameters</span><span class="p">(),</span> <span class="n">lr</span><span class="o">=</span><span class="mf">0.001</span><span class="p">)</span>
<span class="n">criterion</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">MSELoss</span><span class="p">()</span>

<span class="n">lr_finder</span> <span class="o">=</span> <span class="n">LRFinder</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">optimizer</span><span class="p">,</span> <span class="n">criterion</span><span class="p">)</span>
<span class="n">lrs</span><span class="p">,</span> <span class="n">losses</span> <span class="o">=</span> <span class="n">lr_finder</span><span class="o">.</span><span class="n">range_test</span><span class="p">(</span><span class="n">train_loader</span><span class="p">,</span> <span class="n">start_lr</span><span class="o">=</span><span class="mf">1e-7</span><span class="p">,</span> <span class="n">end_lr</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
<span class="n">suggested_lr</span> <span class="o">=</span> <span class="n">lr_finder</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">lrs</span><span class="p">,</span> <span class="n">losses</span><span class="p">)</span>

<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Suggested learning rate: </span><span class="si">{</span><span class="n">suggested_lr</span><span class="si">:</span><span class="s2">.2e</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
</code></pre></div>

<h3 id="62-gradient-clipping">6.2 Gradient Clipping<a class="header-link" href="#62-gradient-clipping" title="Permanent link">&para;</a></h3>
<p>Prevent gradient explosion by clipping:</p>
<div class="highlight"><pre><span></span><code><span class="c1"># Gradient clipping by norm (most common)</span>
<span class="n">optimizer</span> <span class="o">=</span> <span class="n">optim</span><span class="o">.</span><span class="n">Adam</span><span class="p">(</span><span class="n">model</span><span class="o">.</span><span class="n">parameters</span><span class="p">(),</span> <span class="n">lr</span><span class="o">=</span><span class="mf">0.001</span><span class="p">)</span>

<span class="k">for</span> <span class="n">epoch</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">num_epochs</span><span class="p">):</span>
    <span class="k">for</span> <span class="n">X</span><span class="p">,</span> <span class="n">y</span> <span class="ow">in</span> <span class="n">train_loader</span><span class="p">:</span>
        <span class="n">optimizer</span><span class="o">.</span><span class="n">zero_grad</span><span class="p">()</span>
        <span class="n">output</span> <span class="o">=</span> <span class="n">model</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>
        <span class="n">loss</span> <span class="o">=</span> <span class="n">criterion</span><span class="p">(</span><span class="n">output</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>
        <span class="n">loss</span><span class="o">.</span><span class="n">backward</span><span class="p">()</span>

        <span class="c1"># Clip gradients to max norm of 1.0</span>
        <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">utils</span><span class="o">.</span><span class="n">clip_grad_norm_</span><span class="p">(</span><span class="n">model</span><span class="o">.</span><span class="n">parameters</span><span class="p">(),</span> <span class="n">max_norm</span><span class="o">=</span><span class="mf">1.0</span><span class="p">)</span>

        <span class="n">optimizer</span><span class="o">.</span><span class="n">step</span><span class="p">()</span>

<span class="c1"># Gradient clipping by value</span>
<span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">utils</span><span class="o">.</span><span class="n">clip_grad_value_</span><span class="p">(</span><span class="n">model</span><span class="o">.</span><span class="n">parameters</span><span class="p">(),</span> <span class="n">clip_value</span><span class="o">=</span><span class="mf">0.5</span><span class="p">)</span>

<span class="c1"># Monitor gradient norms</span>
<span class="k">def</span><span class="w"> </span><span class="nf">get_grad_norm</span><span class="p">(</span><span class="n">model</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Compute total gradient norm&quot;&quot;&quot;</span>
    <span class="n">total_norm</span> <span class="o">=</span> <span class="mi">0</span>
    <span class="k">for</span> <span class="n">p</span> <span class="ow">in</span> <span class="n">model</span><span class="o">.</span><span class="n">parameters</span><span class="p">():</span>
        <span class="k">if</span> <span class="n">p</span><span class="o">.</span><span class="n">grad</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">param_norm</span> <span class="o">=</span> <span class="n">p</span><span class="o">.</span><span class="n">grad</span><span class="o">.</span><span class="n">data</span><span class="o">.</span><span class="n">norm</span><span class="p">(</span><span class="mi">2</span><span class="p">)</span>
            <span class="n">total_norm</span> <span class="o">+=</span> <span class="n">param_norm</span><span class="o">.</span><span class="n">item</span><span class="p">()</span> <span class="o">**</span> <span class="mi">2</span>
    <span class="k">return</span> <span class="n">total_norm</span> <span class="o">**</span> <span class="mf">0.5</span>

<span class="c1"># Training with gradient monitoring</span>
<span class="n">grad_norms</span> <span class="o">=</span> <span class="p">[]</span>
<span class="k">for</span> <span class="n">epoch</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">num_epochs</span><span class="p">):</span>
    <span class="k">for</span> <span class="n">X</span><span class="p">,</span> <span class="n">y</span> <span class="ow">in</span> <span class="n">train_loader</span><span class="p">:</span>
        <span class="n">optimizer</span><span class="o">.</span><span class="n">zero_grad</span><span class="p">()</span>
        <span class="n">loss</span> <span class="o">=</span> <span class="n">criterion</span><span class="p">(</span><span class="n">model</span><span class="p">(</span><span class="n">X</span><span class="p">),</span> <span class="n">y</span><span class="p">)</span>
        <span class="n">loss</span><span class="o">.</span><span class="n">backward</span><span class="p">()</span>

        <span class="c1"># Monitor before clipping</span>
        <span class="n">grad_norm_before</span> <span class="o">=</span> <span class="n">get_grad_norm</span><span class="p">(</span><span class="n">model</span><span class="p">)</span>

        <span class="c1"># Clip</span>
        <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">utils</span><span class="o">.</span><span class="n">clip_grad_norm_</span><span class="p">(</span><span class="n">model</span><span class="o">.</span><span class="n">parameters</span><span class="p">(),</span> <span class="n">max_norm</span><span class="o">=</span><span class="mf">1.0</span><span class="p">)</span>

        <span class="n">grad_norm_after</span> <span class="o">=</span> <span class="n">get_grad_norm</span><span class="p">(</span><span class="n">model</span><span class="p">)</span>
        <span class="n">grad_norms</span><span class="o">.</span><span class="n">append</span><span class="p">((</span><span class="n">grad_norm_before</span><span class="p">,</span> <span class="n">grad_norm_after</span><span class="p">))</span>

        <span class="n">optimizer</span><span class="o">.</span><span class="n">step</span><span class="p">()</span>

<span class="c1"># Plot gradient norms</span>
<span class="n">before</span><span class="p">,</span> <span class="n">after</span> <span class="o">=</span> <span class="nb">zip</span><span class="p">(</span><span class="o">*</span><span class="n">grad_norms</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">12</span><span class="p">,</span> <span class="mi">6</span><span class="p">))</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">before</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s1">&#39;Before clipping&#39;</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.7</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">after</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s1">&#39;After clipping&#39;</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.7</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s1">&#39;Step&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s1">&#39;Gradient Norm&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s1">&#39;Gradient Clipping Effect&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">legend</span><span class="p">()</span>
<span class="n">plt</span><span class="o">.</span><span class="n">grid</span><span class="p">(</span><span class="kc">True</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.3</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">savefig</span><span class="p">(</span><span class="s1">&#39;gradient_clipping.png&#39;</span><span class="p">,</span> <span class="n">dpi</span><span class="o">=</span><span class="mi">150</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">close</span><span class="p">()</span>
</code></pre></div>

<h3 id="63-gradient-accumulation">6.3 Gradient Accumulation<a class="header-link" href="#63-gradient-accumulation" title="Permanent link">&para;</a></h3>
<p>Simulate larger batch sizes with limited memory:</p>
<div class="highlight"><pre><span></span><code><span class="c1"># Gradient accumulation</span>
<span class="n">accumulation_steps</span> <span class="o">=</span> <span class="mi">4</span>  <span class="c1"># Effective batch size = batch_size * accumulation_steps</span>

<span class="n">optimizer</span><span class="o">.</span><span class="n">zero_grad</span><span class="p">()</span>

<span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">train_loader</span><span class="p">):</span>
    <span class="c1"># Forward pass</span>
    <span class="n">output</span> <span class="o">=</span> <span class="n">model</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>
    <span class="n">loss</span> <span class="o">=</span> <span class="n">criterion</span><span class="p">(</span><span class="n">output</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>

    <span class="c1"># Normalize loss to account for accumulation</span>
    <span class="n">loss</span> <span class="o">=</span> <span class="n">loss</span> <span class="o">/</span> <span class="n">accumulation_steps</span>
    <span class="n">loss</span><span class="o">.</span><span class="n">backward</span><span class="p">()</span>

    <span class="c1"># Update only after accumulating gradients</span>
    <span class="k">if</span> <span class="p">(</span><span class="n">i</span> <span class="o">+</span> <span class="mi">1</span><span class="p">)</span> <span class="o">%</span> <span class="n">accumulation_steps</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
        <span class="n">optimizer</span><span class="o">.</span><span class="n">step</span><span class="p">()</span>
        <span class="n">optimizer</span><span class="o">.</span><span class="n">zero_grad</span><span class="p">()</span>

<span class="c1"># Complete implementation with proper handling</span>
<span class="k">def</span><span class="w"> </span><span class="nf">train_with_accumulation</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">train_loader</span><span class="p">,</span> <span class="n">optimizer</span><span class="p">,</span> <span class="n">criterion</span><span class="p">,</span>
                            <span class="n">accumulation_steps</span><span class="o">=</span><span class="mi">4</span><span class="p">,</span> <span class="n">clip_norm</span><span class="o">=</span><span class="mf">1.0</span><span class="p">):</span>
    <span class="n">model</span><span class="o">.</span><span class="n">train</span><span class="p">()</span>
    <span class="n">total_loss</span> <span class="o">=</span> <span class="mi">0</span>
    <span class="n">optimizer</span><span class="o">.</span><span class="n">zero_grad</span><span class="p">()</span>

    <span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">train_loader</span><span class="p">):</span>
        <span class="n">X</span><span class="p">,</span> <span class="n">y</span> <span class="o">=</span> <span class="n">X</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">),</span> <span class="n">y</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>

        <span class="c1"># Forward pass</span>
        <span class="n">output</span> <span class="o">=</span> <span class="n">model</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>
        <span class="n">loss</span> <span class="o">=</span> <span class="n">criterion</span><span class="p">(</span><span class="n">output</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span> <span class="o">/</span> <span class="n">accumulation_steps</span>
        <span class="n">loss</span><span class="o">.</span><span class="n">backward</span><span class="p">()</span>

        <span class="c1"># Update parameters</span>
        <span class="k">if</span> <span class="p">(</span><span class="n">i</span> <span class="o">+</span> <span class="mi">1</span><span class="p">)</span> <span class="o">%</span> <span class="n">accumulation_steps</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
            <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">utils</span><span class="o">.</span><span class="n">clip_grad_norm_</span><span class="p">(</span><span class="n">model</span><span class="o">.</span><span class="n">parameters</span><span class="p">(),</span> <span class="n">clip_norm</span><span class="p">)</span>
            <span class="n">optimizer</span><span class="o">.</span><span class="n">step</span><span class="p">()</span>
            <span class="n">optimizer</span><span class="o">.</span><span class="n">zero_grad</span><span class="p">()</span>

        <span class="n">total_loss</span> <span class="o">+=</span> <span class="n">loss</span><span class="o">.</span><span class="n">item</span><span class="p">()</span> <span class="o">*</span> <span class="n">accumulation_steps</span>

    <span class="c1"># Handle remaining gradients</span>
    <span class="k">if</span> <span class="p">(</span><span class="n">i</span> <span class="o">+</span> <span class="mi">1</span><span class="p">)</span> <span class="o">%</span> <span class="n">accumulation_steps</span> <span class="o">!=</span> <span class="mi">0</span><span class="p">:</span>
        <span class="n">optimizer</span><span class="o">.</span><span class="n">step</span><span class="p">()</span>
        <span class="n">optimizer</span><span class="o">.</span><span class="n">zero_grad</span><span class="p">()</span>

    <span class="k">return</span> <span class="n">total_loss</span> <span class="o">/</span> <span class="nb">len</span><span class="p">(</span><span class="n">train_loader</span><span class="p">)</span>
</code></pre></div>

<h3 id="64-mixed-precision-training">6.4 Mixed Precision Training<a class="header-link" href="#64-mixed-precision-training" title="Permanent link">&para;</a></h3>
<p>Combine with optimizers for faster training:</p>
<div class="highlight"><pre><span></span><code><span class="kn">from</span><span class="w"> </span><span class="nn">torch.cuda.amp</span><span class="w"> </span><span class="kn">import</span> <span class="n">GradScaler</span><span class="p">,</span> <span class="n">autocast</span>

<span class="c1"># Mixed precision training setup</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="s1">&#39;cuda&#39;</span><span class="p">)</span>
<span class="n">optimizer</span> <span class="o">=</span> <span class="n">optim</span><span class="o">.</span><span class="n">AdamW</span><span class="p">(</span><span class="n">model</span><span class="o">.</span><span class="n">parameters</span><span class="p">(),</span> <span class="n">lr</span><span class="o">=</span><span class="mf">1e-3</span><span class="p">)</span>
<span class="n">scaler</span> <span class="o">=</span> <span class="n">GradScaler</span><span class="p">()</span>

<span class="k">for</span> <span class="n">epoch</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">num_epochs</span><span class="p">):</span>
    <span class="k">for</span> <span class="n">X</span><span class="p">,</span> <span class="n">y</span> <span class="ow">in</span> <span class="n">train_loader</span><span class="p">:</span>
        <span class="n">X</span><span class="p">,</span> <span class="n">y</span> <span class="o">=</span> <span class="n">X</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="s1">&#39;cuda&#39;</span><span class="p">),</span> <span class="n">y</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="s1">&#39;cuda&#39;</span><span class="p">)</span>

        <span class="n">optimizer</span><span class="o">.</span><span class="n">zero_grad</span><span class="p">()</span>

        <span class="c1"># Forward pass in fp16</span>
        <span class="k">with</span> <span class="n">autocast</span><span class="p">():</span>
            <span class="n">output</span> <span class="o">=</span> <span class="n">model</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>
            <span class="n">loss</span> <span class="o">=</span> <span class="n">criterion</span><span class="p">(</span><span class="n">output</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>

        <span class="c1"># Backward pass with scaling</span>
        <span class="n">scaler</span><span class="o">.</span><span class="n">scale</span><span class="p">(</span><span class="n">loss</span><span class="p">)</span><span class="o">.</span><span class="n">backward</span><span class="p">()</span>

        <span class="c1"># Gradient clipping (unscale first!)</span>
        <span class="n">scaler</span><span class="o">.</span><span class="n">unscale_</span><span class="p">(</span><span class="n">optimizer</span><span class="p">)</span>
        <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">utils</span><span class="o">.</span><span class="n">clip_grad_norm_</span><span class="p">(</span><span class="n">model</span><span class="o">.</span><span class="n">parameters</span><span class="p">(),</span> <span class="n">max_norm</span><span class="o">=</span><span class="mf">1.0</span><span class="p">)</span>

        <span class="c1"># Update with scaled gradients</span>
        <span class="n">scaler</span><span class="o">.</span><span class="n">step</span><span class="p">(</span><span class="n">optimizer</span><span class="p">)</span>
        <span class="n">scaler</span><span class="o">.</span><span class="n">update</span><span class="p">()</span>

<span class="c1"># Mixed precision with gradient accumulation</span>
<span class="n">scaler</span> <span class="o">=</span> <span class="n">GradScaler</span><span class="p">()</span>
<span class="n">accumulation_steps</span> <span class="o">=</span> <span class="mi">4</span>

<span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">train_loader</span><span class="p">):</span>
    <span class="k">with</span> <span class="n">autocast</span><span class="p">():</span>
        <span class="n">output</span> <span class="o">=</span> <span class="n">model</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>
        <span class="n">loss</span> <span class="o">=</span> <span class="n">criterion</span><span class="p">(</span><span class="n">output</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span> <span class="o">/</span> <span class="n">accumulation_steps</span>

    <span class="n">scaler</span><span class="o">.</span><span class="n">scale</span><span class="p">(</span><span class="n">loss</span><span class="p">)</span><span class="o">.</span><span class="n">backward</span><span class="p">()</span>

    <span class="k">if</span> <span class="p">(</span><span class="n">i</span> <span class="o">+</span> <span class="mi">1</span><span class="p">)</span> <span class="o">%</span> <span class="n">accumulation_steps</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
        <span class="n">scaler</span><span class="o">.</span><span class="n">unscale_</span><span class="p">(</span><span class="n">optimizer</span><span class="p">)</span>
        <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">utils</span><span class="o">.</span><span class="n">clip_grad_norm_</span><span class="p">(</span><span class="n">model</span><span class="o">.</span><span class="n">parameters</span><span class="p">(),</span> <span class="mf">1.0</span><span class="p">)</span>
        <span class="n">scaler</span><span class="o">.</span><span class="n">step</span><span class="p">(</span><span class="n">optimizer</span><span class="p">)</span>
        <span class="n">scaler</span><span class="o">.</span><span class="n">update</span><span class="p">()</span>
        <span class="n">optimizer</span><span class="o">.</span><span class="n">zero_grad</span><span class="p">()</span>
</code></pre></div>

<h3 id="65-per-parameter-group-learning-rates">6.5 Per-Parameter Group Learning Rates<a class="header-link" href="#65-per-parameter-group-learning-rates" title="Permanent link">&para;</a></h3>
<p>Different LR for different parts of the model:</p>
<div class="highlight"><pre><span></span><code><span class="c1"># Example: Fine-tuning with different LRs for backbone and head</span>
<span class="k">class</span><span class="w"> </span><span class="nc">TransferModel</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="k">def</span><span class="w"> </span><span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">backbone</span><span class="p">,</span> <span class="n">num_classes</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">backbone</span> <span class="o">=</span> <span class="n">backbone</span>  <span class="c1"># Pretrained</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">head</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="mi">512</span><span class="p">,</span> <span class="n">num_classes</span><span class="p">)</span>  <span class="c1"># New</span>

    <span class="k">def</span><span class="w"> </span><span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
        <span class="n">features</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">backbone</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">head</span><span class="p">(</span><span class="n">features</span><span class="p">)</span>

<span class="c1"># Different learning rates for different parts</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">TransferModel</span><span class="p">(</span><span class="n">pretrained_backbone</span><span class="p">,</span> <span class="n">num_classes</span><span class="o">=</span><span class="mi">10</span><span class="p">)</span>

<span class="n">optimizer</span> <span class="o">=</span> <span class="n">optim</span><span class="o">.</span><span class="n">AdamW</span><span class="p">([</span>
    <span class="p">{</span><span class="s1">&#39;params&#39;</span><span class="p">:</span> <span class="n">model</span><span class="o">.</span><span class="n">backbone</span><span class="o">.</span><span class="n">parameters</span><span class="p">(),</span> <span class="s1">&#39;lr&#39;</span><span class="p">:</span> <span class="mf">1e-5</span><span class="p">},</span>  <span class="c1"># Low LR for backbone</span>
    <span class="p">{</span><span class="s1">&#39;params&#39;</span><span class="p">:</span> <span class="n">model</span><span class="o">.</span><span class="n">head</span><span class="o">.</span><span class="n">parameters</span><span class="p">(),</span> <span class="s1">&#39;lr&#39;</span><span class="p">:</span> <span class="mf">1e-3</span><span class="p">}</span>  <span class="c1"># High LR for head</span>
<span class="p">],</span> <span class="n">weight_decay</span><span class="o">=</span><span class="mf">0.01</span><span class="p">)</span>

<span class="c1"># Can also use different weight decay</span>
<span class="n">optimizer</span> <span class="o">=</span> <span class="n">optim</span><span class="o">.</span><span class="n">AdamW</span><span class="p">([</span>
    <span class="p">{</span><span class="s1">&#39;params&#39;</span><span class="p">:</span> <span class="n">model</span><span class="o">.</span><span class="n">backbone</span><span class="o">.</span><span class="n">parameters</span><span class="p">(),</span> <span class="s1">&#39;lr&#39;</span><span class="p">:</span> <span class="mf">1e-5</span><span class="p">,</span> <span class="s1">&#39;weight_decay&#39;</span><span class="p">:</span> <span class="mf">0.01</span><span class="p">},</span>
    <span class="p">{</span><span class="s1">&#39;params&#39;</span><span class="p">:</span> <span class="n">model</span><span class="o">.</span><span class="n">head</span><span class="o">.</span><span class="n">parameters</span><span class="p">(),</span> <span class="s1">&#39;lr&#39;</span><span class="p">:</span> <span class="mf">1e-3</span><span class="p">,</span> <span class="s1">&#39;weight_decay&#39;</span><span class="p">:</span> <span class="mf">0.0</span><span class="p">}</span>
<span class="p">])</span>

<span class="c1"># Advanced: layer-wise learning rate decay (LLRD)</span>
<span class="k">def</span><span class="w"> </span><span class="nf">get_layer_wise_lr_groups</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">base_lr</span><span class="o">=</span><span class="mf">1e-3</span><span class="p">,</span> <span class="n">decay_rate</span><span class="o">=</span><span class="mf">0.9</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Apply lower LR to earlier layers&quot;&quot;&quot;</span>
    <span class="n">num_layers</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="nb">list</span><span class="p">(</span><span class="n">model</span><span class="o">.</span><span class="n">named_parameters</span><span class="p">()))</span>

    <span class="n">param_groups</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="p">(</span><span class="n">name</span><span class="p">,</span> <span class="n">param</span><span class="p">)</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">model</span><span class="o">.</span><span class="n">named_parameters</span><span class="p">()):</span>
        <span class="c1"># Earlier layers get lower LR</span>
        <span class="n">layer_lr</span> <span class="o">=</span> <span class="n">base_lr</span> <span class="o">*</span> <span class="p">(</span><span class="n">decay_rate</span> <span class="o">**</span> <span class="p">(</span><span class="n">num_layers</span> <span class="o">-</span> <span class="n">i</span> <span class="o">-</span> <span class="mi">1</span><span class="p">))</span>
        <span class="n">param_groups</span><span class="o">.</span><span class="n">append</span><span class="p">({</span><span class="s1">&#39;params&#39;</span><span class="p">:</span> <span class="n">param</span><span class="p">,</span> <span class="s1">&#39;lr&#39;</span><span class="p">:</span> <span class="n">layer_lr</span><span class="p">})</span>

    <span class="k">return</span> <span class="n">param_groups</span>

<span class="c1"># Usage for transformer fine-tuning</span>
<span class="n">param_groups</span> <span class="o">=</span> <span class="n">get_layer_wise_lr_groups</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">base_lr</span><span class="o">=</span><span class="mf">1e-4</span><span class="p">,</span> <span class="n">decay_rate</span><span class="o">=</span><span class="mf">0.95</span><span class="p">)</span>
<span class="n">optimizer</span> <span class="o">=</span> <span class="n">optim</span><span class="o">.</span><span class="n">AdamW</span><span class="p">(</span><span class="n">param_groups</span><span class="p">,</span> <span class="n">weight_decay</span><span class="o">=</span><span class="mf">0.01</span><span class="p">)</span>

<span class="c1"># Discriminative fine-tuning (common in NLP)</span>
<span class="k">def</span><span class="w"> </span><span class="nf">get_discriminative_lr_groups</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">base_lr</span><span class="o">=</span><span class="mf">1e-3</span><span class="p">,</span> <span class="n">lr_mult</span><span class="o">=</span><span class="mf">2.6</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Higher LR for later layers&quot;&quot;&quot;</span>
    <span class="n">layers</span> <span class="o">=</span> <span class="p">[</span><span class="n">model</span><span class="o">.</span><span class="n">layer1</span><span class="p">,</span> <span class="n">model</span><span class="o">.</span><span class="n">layer2</span><span class="p">,</span> <span class="n">model</span><span class="o">.</span><span class="n">layer3</span><span class="p">,</span> <span class="n">model</span><span class="o">.</span><span class="n">head</span><span class="p">]</span>

    <span class="n">param_groups</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">layer</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">layers</span><span class="p">):</span>
        <span class="n">lr</span> <span class="o">=</span> <span class="n">base_lr</span> <span class="o">*</span> <span class="p">(</span><span class="n">lr_mult</span> <span class="o">**</span> <span class="n">i</span><span class="p">)</span>
        <span class="n">param_groups</span><span class="o">.</span><span class="n">append</span><span class="p">({</span><span class="s1">&#39;params&#39;</span><span class="p">:</span> <span class="n">layer</span><span class="o">.</span><span class="n">parameters</span><span class="p">(),</span> <span class="s1">&#39;lr&#39;</span><span class="p">:</span> <span class="n">lr</span><span class="p">})</span>

    <span class="k">return</span> <span class="n">param_groups</span>
</code></pre></div>

<hr />
<h2 id="7-choosing-the-right-optimizer">7. Choosing the Right Optimizer<a class="header-link" href="#7-choosing-the-right-optimizer" title="Permanent link">&para;</a></h2>
<h3 id="71-decision-guide">7.1 Decision Guide<a class="header-link" href="#71-decision-guide" title="Permanent link">&para;</a></h3>
<div class="highlight"><pre><span></span><code>Optimization Decision Tree:

Are you training from scratch?
‚îÇ
‚îú‚îÄ Yes: What architecture?
‚îÇ  ‚îú‚îÄ CNN (ResNet, EfficientNet)
‚îÇ  ‚îÇ  ‚îî‚îÄ SGD + Momentum (0.9) + Cosine Annealing
‚îÇ  ‚îÇ     LR: 0.1, batch 256
‚îÇ  ‚îÇ
‚îÇ  ‚îú‚îÄ Transformer (BERT, GPT, ViT)
‚îÇ  ‚îÇ  ‚îî‚îÄ AdamW + Linear Warmup (10%) + Cosine Decay
‚îÇ  ‚îÇ     LR: 5e-4, batch 256-512, weight_decay 0.01
‚îÇ  ‚îÇ
‚îÇ  ‚îú‚îÄ GAN
‚îÇ  ‚îÇ  ‚îî‚îÄ Adam (Œ≤‚ÇÅ=0.5, Œ≤‚ÇÇ=0.999) or RMSprop
‚îÇ  ‚îÇ     LR: 2e-4, no momentum, no warmup
‚îÇ  ‚îÇ
‚îÇ  ‚îî‚îÄ RNN/LSTM
‚îÇ     ‚îî‚îÄ AdamW or RMSprop + Gradient Clipping (1.0)
‚îÇ        LR: 1e-3
‚îÇ
‚îî‚îÄ No: Fine-tuning pretrained model?
   ‚îî‚îÄ AdamW + Low LR + Per-parameter groups
      Backbone LR: 1e-5, Head LR: 1e-3
      Linear warmup (5-10%) + Cosine decay
</code></pre></div>

<h3 id="72-sgd-vs-adam-debate">7.2 SGD vs Adam Debate<a class="header-link" href="#72-sgd-vs-adam-debate" title="Permanent link">&para;</a></h3>
<p><strong>When to use SGD + Momentum:</strong></p>
<p>‚úÖ Training CNNs from scratch (ResNet, VGG)
‚úÖ When generalization is critical
‚úÖ You have time for extensive hyperparameter search
‚úÖ Large batch sizes available</p>
<p><strong>Pros:</strong>
- Often better final accuracy (0.5-1% on ImageNet)
- Better generalization to new data
- More stable with large batch sizes</p>
<p><strong>Cons:</strong>
- Requires careful LR tuning
- Needs longer training (90-200 epochs)
- Sensitive to initialization
- Requires warmup for large batches</p>
<p><strong>When to use Adam/AdamW:</strong></p>
<p>‚úÖ Training transformers
‚úÖ Quick prototyping
‚úÖ Fine-tuning pretrained models
‚úÖ Working with limited compute/time
‚úÖ Small batch sizes</p>
<p><strong>Pros:</strong>
- Robust to hyperparameters (works out-of-box)
- Faster convergence (fewer epochs)
- Less sensitive to LR choice
- Good for adaptive problems (NLP, RL)</p>
<p><strong>Cons:</strong>
- May generalize slightly worse on vision
- Higher memory usage
- Can overfit more easily</p>
<div class="highlight"><pre><span></span><code><span class="c1"># Example: Training ResNet on CIFAR-10</span>

<span class="c1"># SGD approach (better accuracy, more tuning)</span>
<span class="k">def</span><span class="w"> </span><span class="nf">train_resnet_sgd</span><span class="p">():</span>
    <span class="n">model</span> <span class="o">=</span> <span class="n">torchvision</span><span class="o">.</span><span class="n">models</span><span class="o">.</span><span class="n">resnet18</span><span class="p">(</span><span class="n">num_classes</span><span class="o">=</span><span class="mi">10</span><span class="p">)</span>

    <span class="n">optimizer</span> <span class="o">=</span> <span class="n">optim</span><span class="o">.</span><span class="n">SGD</span><span class="p">(</span>
        <span class="n">model</span><span class="o">.</span><span class="n">parameters</span><span class="p">(),</span>
        <span class="n">lr</span><span class="o">=</span><span class="mf">0.1</span><span class="p">,</span>
        <span class="n">momentum</span><span class="o">=</span><span class="mf">0.9</span><span class="p">,</span>
        <span class="n">weight_decay</span><span class="o">=</span><span class="mf">5e-4</span><span class="p">,</span>
        <span class="n">nesterov</span><span class="o">=</span><span class="kc">True</span>
    <span class="p">)</span>

    <span class="n">scheduler</span> <span class="o">=</span> <span class="n">optim</span><span class="o">.</span><span class="n">lr_scheduler</span><span class="o">.</span><span class="n">CosineAnnealingLR</span><span class="p">(</span>
        <span class="n">optimizer</span><span class="p">,</span>
        <span class="n">T_max</span><span class="o">=</span><span class="mi">200</span><span class="p">,</span>
        <span class="n">eta_min</span><span class="o">=</span><span class="mf">1e-4</span>
    <span class="p">)</span>

    <span class="c1"># Train for 200 epochs</span>
    <span class="c1"># Expected accuracy: ~95%</span>

<span class="c1"># AdamW approach (faster convergence, easier tuning)</span>
<span class="k">def</span><span class="w"> </span><span class="nf">train_resnet_adamw</span><span class="p">():</span>
    <span class="n">model</span> <span class="o">=</span> <span class="n">torchvision</span><span class="o">.</span><span class="n">models</span><span class="o">.</span><span class="n">resnet18</span><span class="p">(</span><span class="n">num_classes</span><span class="o">=</span><span class="mi">10</span><span class="p">)</span>

    <span class="n">optimizer</span> <span class="o">=</span> <span class="n">optim</span><span class="o">.</span><span class="n">AdamW</span><span class="p">(</span>
        <span class="n">model</span><span class="o">.</span><span class="n">parameters</span><span class="p">(),</span>
        <span class="n">lr</span><span class="o">=</span><span class="mf">1e-3</span><span class="p">,</span>
        <span class="n">weight_decay</span><span class="o">=</span><span class="mf">0.01</span>
    <span class="p">)</span>

    <span class="c1"># Simple schedule</span>
    <span class="n">scheduler</span> <span class="o">=</span> <span class="n">optim</span><span class="o">.</span><span class="n">lr_scheduler</span><span class="o">.</span><span class="n">CosineAnnealingLR</span><span class="p">(</span><span class="n">optimizer</span><span class="p">,</span> <span class="n">T_max</span><span class="o">=</span><span class="mi">100</span><span class="p">)</span>

    <span class="c1"># Train for 100 epochs</span>
    <span class="c1"># Expected accuracy: ~94% (slightly lower but faster)</span>
</code></pre></div>

<h3 id="73-common-recipes">7.3 Common Recipes<a class="header-link" href="#73-common-recipes" title="Permanent link">&para;</a></h3>
<p><strong>Recipe 1: ImageNet Training (ResNet-50)</strong></p>
<div class="highlight"><pre><span></span><code><span class="c1"># Standard recipe for ImageNet</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">torchvision</span><span class="o">.</span><span class="n">models</span><span class="o">.</span><span class="n">resnet50</span><span class="p">(</span><span class="n">pretrained</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
<span class="n">optimizer</span> <span class="o">=</span> <span class="n">optim</span><span class="o">.</span><span class="n">SGD</span><span class="p">(</span>
    <span class="n">model</span><span class="o">.</span><span class="n">parameters</span><span class="p">(),</span>
    <span class="n">lr</span><span class="o">=</span><span class="mf">0.1</span><span class="p">,</span>
    <span class="n">momentum</span><span class="o">=</span><span class="mf">0.9</span><span class="p">,</span>
    <span class="n">weight_decay</span><span class="o">=</span><span class="mf">1e-4</span>
<span class="p">)</span>

<span class="c1"># Warmup for 5 epochs, then cosine decay for 90 epochs</span>
<span class="n">warmup_scheduler</span> <span class="o">=</span> <span class="n">optim</span><span class="o">.</span><span class="n">lr_scheduler</span><span class="o">.</span><span class="n">LinearLR</span><span class="p">(</span>
    <span class="n">optimizer</span><span class="p">,</span>
    <span class="n">start_factor</span><span class="o">=</span><span class="mf">0.1</span><span class="p">,</span>
    <span class="n">total_iters</span><span class="o">=</span><span class="mi">5</span> <span class="o">*</span> <span class="nb">len</span><span class="p">(</span><span class="n">train_loader</span><span class="p">)</span>
<span class="p">)</span>
<span class="n">cosine_scheduler</span> <span class="o">=</span> <span class="n">optim</span><span class="o">.</span><span class="n">lr_scheduler</span><span class="o">.</span><span class="n">CosineAnnealingLR</span><span class="p">(</span>
    <span class="n">optimizer</span><span class="p">,</span>
    <span class="n">T_max</span><span class="o">=</span><span class="mi">90</span> <span class="o">*</span> <span class="nb">len</span><span class="p">(</span><span class="n">train_loader</span><span class="p">),</span>
    <span class="n">eta_min</span><span class="o">=</span><span class="mf">1e-5</span>
<span class="p">)</span>
<span class="n">scheduler</span> <span class="o">=</span> <span class="n">optim</span><span class="o">.</span><span class="n">lr_scheduler</span><span class="o">.</span><span class="n">SequentialLR</span><span class="p">(</span>
    <span class="n">optimizer</span><span class="p">,</span>
    <span class="n">schedulers</span><span class="o">=</span><span class="p">[</span><span class="n">warmup_scheduler</span><span class="p">,</span> <span class="n">cosine_scheduler</span><span class="p">],</span>
    <span class="n">milestones</span><span class="o">=</span><span class="p">[</span><span class="mi">5</span> <span class="o">*</span> <span class="nb">len</span><span class="p">(</span><span class="n">train_loader</span><span class="p">)]</span>
<span class="p">)</span>

<span class="c1"># Batch size 256, 90 epochs total</span>
<span class="c1"># Expected Top-1: 76.2%, Top-5: 93.0%</span>
</code></pre></div>

<p><strong>Recipe 2: BERT Pretraining</strong></p>
<div class="highlight"><pre><span></span><code><span class="c1"># BERT-base configuration</span>
<span class="n">optimizer</span> <span class="o">=</span> <span class="n">optim</span><span class="o">.</span><span class="n">AdamW</span><span class="p">(</span>
    <span class="n">model</span><span class="o">.</span><span class="n">parameters</span><span class="p">(),</span>
    <span class="n">lr</span><span class="o">=</span><span class="mf">1e-4</span><span class="p">,</span>
    <span class="n">betas</span><span class="o">=</span><span class="p">(</span><span class="mf">0.9</span><span class="p">,</span> <span class="mf">0.999</span><span class="p">),</span>
    <span class="n">eps</span><span class="o">=</span><span class="mf">1e-6</span><span class="p">,</span>
    <span class="n">weight_decay</span><span class="o">=</span><span class="mf">0.01</span>
<span class="p">)</span>

<span class="c1"># 10% linear warmup + 90% linear decay</span>
<span class="n">total_steps</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">train_loader</span><span class="p">)</span> <span class="o">*</span> <span class="n">num_epochs</span>
<span class="n">warmup_steps</span> <span class="o">=</span> <span class="nb">int</span><span class="p">(</span><span class="mf">0.1</span> <span class="o">*</span> <span class="n">total_steps</span><span class="p">)</span>

<span class="n">scheduler</span> <span class="o">=</span> <span class="n">optim</span><span class="o">.</span><span class="n">lr_scheduler</span><span class="o">.</span><span class="n">LambdaLR</span><span class="p">(</span>
    <span class="n">optimizer</span><span class="p">,</span>
    <span class="n">lr_lambda</span><span class="o">=</span><span class="k">lambda</span> <span class="n">step</span><span class="p">:</span> <span class="nb">min</span><span class="p">(</span>
        <span class="n">step</span> <span class="o">/</span> <span class="n">warmup_steps</span><span class="p">,</span>
        <span class="p">(</span><span class="n">total_steps</span> <span class="o">-</span> <span class="n">step</span><span class="p">)</span> <span class="o">/</span> <span class="p">(</span><span class="n">total_steps</span> <span class="o">-</span> <span class="n">warmup_steps</span><span class="p">)</span>
    <span class="p">)</span>
<span class="p">)</span>

<span class="c1"># Batch size 256, gradient accumulation 4 (effective 1024)</span>
<span class="c1"># Gradient clipping: 1.0</span>
</code></pre></div>

<p><strong>Recipe 3: Fine-tuning Pretrained Model</strong></p>
<div class="highlight"><pre><span></span><code><span class="c1"># Fine-tuning BERT for classification</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">AutoModel</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="s1">&#39;bert-base-uncased&#39;</span><span class="p">)</span>
<span class="n">classifier</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="mi">768</span><span class="p">,</span> <span class="n">num_classes</span><span class="p">)</span>

<span class="c1"># Different LR for pretrained vs new layers</span>
<span class="n">optimizer</span> <span class="o">=</span> <span class="n">optim</span><span class="o">.</span><span class="n">AdamW</span><span class="p">([</span>
    <span class="p">{</span><span class="s1">&#39;params&#39;</span><span class="p">:</span> <span class="n">model</span><span class="o">.</span><span class="n">parameters</span><span class="p">(),</span> <span class="s1">&#39;lr&#39;</span><span class="p">:</span> <span class="mf">2e-5</span><span class="p">},</span>  <span class="c1"># Very low for pretrained</span>
    <span class="p">{</span><span class="s1">&#39;params&#39;</span><span class="p">:</span> <span class="n">classifier</span><span class="o">.</span><span class="n">parameters</span><span class="p">(),</span> <span class="s1">&#39;lr&#39;</span><span class="p">:</span> <span class="mf">2e-4</span><span class="p">}</span>  <span class="c1"># Higher for new layer</span>
<span class="p">],</span> <span class="n">weight_decay</span><span class="o">=</span><span class="mf">0.01</span><span class="p">)</span>

<span class="c1"># Short warmup + cosine decay</span>
<span class="n">total_steps</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">train_loader</span><span class="p">)</span> <span class="o">*</span> <span class="mi">5</span>  <span class="c1"># 5 epochs</span>
<span class="n">warmup_steps</span> <span class="o">=</span> <span class="nb">int</span><span class="p">(</span><span class="mf">0.1</span> <span class="o">*</span> <span class="n">total_steps</span><span class="p">)</span>

<span class="n">scheduler</span> <span class="o">=</span> <span class="n">WarmupCosineSchedule</span><span class="p">(</span><span class="n">optimizer</span><span class="p">,</span> <span class="n">warmup_steps</span><span class="p">,</span> <span class="n">total_steps</span><span class="p">)</span>

<span class="c1"># Small batch size (16-32), few epochs (3-5)</span>
<span class="c1"># Gradient clipping: 1.0</span>
</code></pre></div>

<p><strong>Recipe 4: GAN Training</strong></p>
<div class="highlight"><pre><span></span><code><span class="c1"># DCGAN recipe</span>
<span class="n">generator</span> <span class="o">=</span> <span class="n">Generator</span><span class="p">()</span>
<span class="n">discriminator</span> <span class="o">=</span> <span class="n">Discriminator</span><span class="p">()</span>

<span class="c1"># Adam with Œ≤‚ÇÅ=0.5 (less momentum than default)</span>
<span class="n">opt_G</span> <span class="o">=</span> <span class="n">optim</span><span class="o">.</span><span class="n">Adam</span><span class="p">(</span><span class="n">generator</span><span class="o">.</span><span class="n">parameters</span><span class="p">(),</span> <span class="n">lr</span><span class="o">=</span><span class="mf">2e-4</span><span class="p">,</span> <span class="n">betas</span><span class="o">=</span><span class="p">(</span><span class="mf">0.5</span><span class="p">,</span> <span class="mf">0.999</span><span class="p">))</span>
<span class="n">opt_D</span> <span class="o">=</span> <span class="n">optim</span><span class="o">.</span><span class="n">Adam</span><span class="p">(</span><span class="n">discriminator</span><span class="o">.</span><span class="n">parameters</span><span class="p">(),</span> <span class="n">lr</span><span class="o">=</span><span class="mf">2e-4</span><span class="p">,</span> <span class="n">betas</span><span class="o">=</span><span class="p">(</span><span class="mf">0.5</span><span class="p">,</span> <span class="mf">0.999</span><span class="p">))</span>

<span class="c1"># No scheduler, no weight decay</span>
<span class="c1"># Alternate training: 1 D step, 1 G step</span>
<span class="c1"># No gradient clipping usually</span>
</code></pre></div>

<p><strong>Recipe 5: Vision Transformer (ViT)</strong></p>
<div class="highlight"><pre><span></span><code><span class="c1"># ViT-B/16 on ImageNet</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">torchvision</span><span class="o">.</span><span class="n">models</span><span class="o">.</span><span class="n">vit_b_16</span><span class="p">(</span><span class="n">pretrained</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>

<span class="n">optimizer</span> <span class="o">=</span> <span class="n">optim</span><span class="o">.</span><span class="n">AdamW</span><span class="p">(</span>
    <span class="n">model</span><span class="o">.</span><span class="n">parameters</span><span class="p">(),</span>
    <span class="n">lr</span><span class="o">=</span><span class="mf">3e-3</span><span class="p">,</span>  <span class="c1"># Higher than BERT!</span>
    <span class="n">weight_decay</span><span class="o">=</span><span class="mf">0.3</span><span class="p">,</span>  <span class="c1"># Stronger regularization</span>
    <span class="n">betas</span><span class="o">=</span><span class="p">(</span><span class="mf">0.9</span><span class="p">,</span> <span class="mf">0.999</span><span class="p">)</span>
<span class="p">)</span>

<span class="c1"># Linear warmup + cosine decay</span>
<span class="n">total_steps</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">train_loader</span><span class="p">)</span> <span class="o">*</span> <span class="mi">300</span>  <span class="c1"># 300 epochs</span>
<span class="n">warmup_steps</span> <span class="o">=</span> <span class="nb">int</span><span class="p">(</span><span class="mf">0.05</span> <span class="o">*</span> <span class="n">total_steps</span><span class="p">)</span>  <span class="c1"># 5% warmup</span>

<span class="n">scheduler</span> <span class="o">=</span> <span class="n">WarmupCosineSchedule</span><span class="p">(</span><span class="n">optimizer</span><span class="p">,</span> <span class="n">warmup_steps</span><span class="p">,</span> <span class="n">total_steps</span><span class="p">,</span> <span class="n">min_lr_ratio</span><span class="o">=</span><span class="mf">0.01</span><span class="p">)</span>

<span class="c1"># Large batch (1024-4096), strong augmentation</span>
<span class="c1"># Gradient clipping: 1.0</span>
</code></pre></div>

<h3 id="74-debugging-optimization-issues">7.4 Debugging Optimization Issues<a class="header-link" href="#74-debugging-optimization-issues" title="Permanent link">&para;</a></h3>
<p><strong>Problem 1: Loss not decreasing</strong></p>
<div class="highlight"><pre><span></span><code><span class="c1"># Checklist:</span>
<span class="c1"># 1. Check learning rate (try LR finder)</span>
<span class="c1"># 2. Verify gradients are flowing</span>
<span class="k">for</span> <span class="n">name</span><span class="p">,</span> <span class="n">param</span> <span class="ow">in</span> <span class="n">model</span><span class="o">.</span><span class="n">named_parameters</span><span class="p">():</span>
    <span class="k">if</span> <span class="n">param</span><span class="o">.</span><span class="n">grad</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
        <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="n">name</span><span class="si">}</span><span class="s2">: grad_norm=</span><span class="si">{</span><span class="n">param</span><span class="o">.</span><span class="n">grad</span><span class="o">.</span><span class="n">norm</span><span class="p">()</span><span class="si">:</span><span class="s2">.6f</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="n">name</span><span class="si">}</span><span class="s2">: NO GRADIENT!&quot;</span><span class="p">)</span>  <span class="c1"># Problem!</span>

<span class="c1"># 3. Check for NaN/Inf</span>
<span class="k">if</span> <span class="n">torch</span><span class="o">.</span><span class="n">isnan</span><span class="p">(</span><span class="n">loss</span><span class="p">)</span> <span class="ow">or</span> <span class="n">torch</span><span class="o">.</span><span class="n">isinf</span><span class="p">(</span><span class="n">loss</span><span class="p">):</span>
    <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Loss is NaN/Inf! Reduce learning rate or check data.&quot;</span><span class="p">)</span>

<span class="c1"># 4. Verify data is changing</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Batch variance: </span><span class="si">{</span><span class="n">X</span><span class="o">.</span><span class="n">var</span><span class="p">()</span><span class="o">.</span><span class="n">item</span><span class="p">()</span><span class="si">:</span><span class="s2">.6f</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>  <span class="c1"># Should be non-zero</span>

<span class="c1"># 5. Try simpler optimizer (SGD) to isolate issue</span>
<span class="n">optimizer</span> <span class="o">=</span> <span class="n">optim</span><span class="o">.</span><span class="n">SGD</span><span class="p">(</span><span class="n">model</span><span class="o">.</span><span class="n">parameters</span><span class="p">(),</span> <span class="n">lr</span><span class="o">=</span><span class="mf">0.001</span><span class="p">)</span>
</code></pre></div>

<p><strong>Problem 2: Loss exploding</strong></p>
<div class="highlight"><pre><span></span><code><span class="c1"># Solutions:</span>
<span class="c1"># 1. Gradient clipping</span>
<span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">utils</span><span class="o">.</span><span class="n">clip_grad_norm_</span><span class="p">(</span><span class="n">model</span><span class="o">.</span><span class="n">parameters</span><span class="p">(),</span> <span class="n">max_norm</span><span class="o">=</span><span class="mf">1.0</span><span class="p">)</span>

<span class="c1"># 2. Lower learning rate</span>
<span class="n">optimizer</span> <span class="o">=</span> <span class="n">optim</span><span class="o">.</span><span class="n">Adam</span><span class="p">(</span><span class="n">model</span><span class="o">.</span><span class="n">parameters</span><span class="p">(),</span> <span class="n">lr</span><span class="o">=</span><span class="mf">1e-4</span><span class="p">)</span>  <span class="c1"># Try 10x lower</span>

<span class="c1"># 3. Check for numerical instability</span>
<span class="c1"># Use float32, avoid log(0), div by 0</span>
<span class="n">loss</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="n">output</span> <span class="o">+</span> <span class="mf">1e-8</span><span class="p">)</span>  <span class="c1"># Add epsilon</span>

<span class="c1"># 4. Use mixed precision</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">torch.cuda.amp</span><span class="w"> </span><span class="kn">import</span> <span class="n">autocast</span>
<span class="k">with</span> <span class="n">autocast</span><span class="p">():</span>
    <span class="n">output</span> <span class="o">=</span> <span class="n">model</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>
</code></pre></div>

<p><strong>Problem 3: Training vs validation divergence</strong></p>
<div class="highlight"><pre><span></span><code><span class="c1"># Solutions:</span>
<span class="c1"># 1. Increase regularization</span>
<span class="n">optimizer</span> <span class="o">=</span> <span class="n">optim</span><span class="o">.</span><span class="n">AdamW</span><span class="p">(</span><span class="n">model</span><span class="o">.</span><span class="n">parameters</span><span class="p">(),</span> <span class="n">lr</span><span class="o">=</span><span class="mf">1e-3</span><span class="p">,</span> <span class="n">weight_decay</span><span class="o">=</span><span class="mf">0.1</span><span class="p">)</span>  <span class="c1"># Higher</span>

<span class="c1"># 2. Add dropout</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Sequential</span><span class="p">(</span>
    <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="mi">100</span><span class="p">,</span> <span class="mi">50</span><span class="p">),</span>
    <span class="n">nn</span><span class="o">.</span><span class="n">ReLU</span><span class="p">(),</span>
    <span class="n">nn</span><span class="o">.</span><span class="n">Dropout</span><span class="p">(</span><span class="mf">0.5</span><span class="p">),</span>  <span class="c1"># Add dropout</span>
    <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="mi">50</span><span class="p">,</span> <span class="mi">10</span><span class="p">)</span>
<span class="p">)</span>

<span class="c1"># 3. Reduce model capacity</span>
<span class="c1"># Use smaller model or fewer layers</span>

<span class="c1"># 4. More data augmentation</span>
<span class="c1"># Stronger augmentation on training data</span>
</code></pre></div>

<p><strong>Problem 4: Slow convergence</strong></p>
<div class="highlight"><pre><span></span><code><span class="c1"># Solutions:</span>
<span class="c1"># 1. Increase learning rate (use LR finder)</span>
<span class="c1"># 2. Switch from SGD to Adam</span>
<span class="n">optimizer</span> <span class="o">=</span> <span class="n">optim</span><span class="o">.</span><span class="n">AdamW</span><span class="p">(</span><span class="n">model</span><span class="o">.</span><span class="n">parameters</span><span class="p">(),</span> <span class="n">lr</span><span class="o">=</span><span class="mf">1e-3</span><span class="p">)</span>

<span class="c1"># 3. Add learning rate warmup</span>
<span class="n">scheduler</span> <span class="o">=</span> <span class="n">WarmupCosineSchedule</span><span class="p">(</span><span class="n">optimizer</span><span class="p">,</span> <span class="n">warmup_steps</span><span class="o">=</span><span class="mi">1000</span><span class="p">,</span> <span class="n">total_steps</span><span class="o">=</span><span class="mi">10000</span><span class="p">)</span>

<span class="c1"># 4. Check batch size (try larger)</span>
<span class="c1"># 5. Verify batch normalization is working</span>
<span class="c1"># 6. Check weight initialization</span>
<span class="k">def</span><span class="w"> </span><span class="nf">init_weights</span><span class="p">(</span><span class="n">m</span><span class="p">):</span>
    <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">m</span><span class="p">,</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">):</span>
        <span class="n">nn</span><span class="o">.</span><span class="n">init</span><span class="o">.</span><span class="n">kaiming_normal_</span><span class="p">(</span><span class="n">m</span><span class="o">.</span><span class="n">weight</span><span class="p">)</span>
        <span class="n">m</span><span class="o">.</span><span class="n">bias</span><span class="o">.</span><span class="n">data</span><span class="o">.</span><span class="n">fill_</span><span class="p">(</span><span class="mf">0.01</span><span class="p">)</span>

<span class="n">model</span><span class="o">.</span><span class="n">apply</span><span class="p">(</span><span class="n">init_weights</span><span class="p">)</span>
</code></pre></div>

<hr />
<h2 id="exercises">Exercises<a class="header-link" href="#exercises" title="Permanent link">&para;</a></h2>
<h3 id="exercise-1-implement-and-compare-optimizers">Exercise 1: Implement and Compare Optimizers<a class="header-link" href="#exercise-1-implement-and-compare-optimizers" title="Permanent link">&para;</a></h3>
<p>Implement RMSprop from scratch and compare its trajectory to SGD and Adam on a 2D toy problem. Visualize optimization paths on a non-convex surface (e.g., Rosenbrock function). Measure convergence speed (iterations to reach loss &lt; 0.01).</p>
<p><strong>Bonus</strong>: Add Nesterov momentum to your RMSprop implementation.</p>
<h3 id="exercise-2-scheduler-ablation-study">Exercise 2: Scheduler Ablation Study<a class="header-link" href="#exercise-2-scheduler-ablation-study" title="Permanent link">&para;</a></h3>
<p>Train a ResNet-18 on CIFAR-10 with five different schedulers: (1) No scheduler, (2) StepLR, (3) CosineAnnealingLR, (4) OneCycleLR, (5) Warmup + Cosine. Use the same optimizer (SGD, momentum=0.9) and initial LR. Plot training curves and report final test accuracy. Which scheduler achieves best accuracy? Which converges fastest?</p>
<p><strong>Bonus</strong>: Use LR finder to automatically determine the best initial LR for OneCycleLR.</p>
<h3 id="exercise-3-large-batch-training-simulation">Exercise 3: Large-Batch Training Simulation<a class="header-link" href="#exercise-3-large-batch-training-simulation" title="Permanent link">&para;</a></h3>
<p>Simulate large-batch training using gradient accumulation. Train a model with effective batch size 1024 using actual batch size 64 and accumulation_steps=16. Compare training time, memory usage, and final accuracy to using batch_size=1024 directly (if GPU memory allows). Implement LARS-style layer-wise LR scaling and show it helps with large batches.</p>
<p><strong>Bonus</strong>: Add mixed precision training and measure speedup.</p>
<hr />
<h2 id="references">References<a class="header-link" href="#references" title="Permanent link">&para;</a></h2>
<ol>
<li><strong>Ruder, S.</strong> (2016). <em>An overview of gradient descent optimization algorithms</em>. arXiv:1609.04747</li>
<li>
<p>Comprehensive survey of SGD, momentum, Adagrad, RMSprop, Adam</p>
</li>
<li>
<p><strong>Kingma, D. P., &amp; Ba, J.</strong> (2015). <em>Adam: A Method for Stochastic Optimization</em>. ICLR 2015</p>
</li>
<li>
<p>Original Adam paper</p>
</li>
<li>
<p><strong>Loshchilov, I., &amp; Hutter, F.</strong> (2019). <em>Decoupled Weight Decay Regularization</em>. ICLR 2019</p>
</li>
<li>
<p>AdamW: fixing weight decay in Adam</p>
</li>
<li>
<p><strong>You, Y., et al.</strong> (2020). <em>Large Batch Optimization for Deep Learning: Training BERT in 76 minutes</em>. ICLR 2020</p>
</li>
<li>
<p>LAMB optimizer</p>
</li>
<li>
<p><strong>Smith, L. N.</strong> (2018). <em>A disciplined approach to neural network hyper-parameters</em>. arXiv:1803.09820</p>
</li>
<li>
<p>Cyclical learning rates, LR range test, super-convergence</p>
</li>
<li>
<p><strong>Chen, X., et al.</strong> (2023). <em>Symbolic Discovery of Optimization Algorithms</em>. arXiv:2302.06675</p>
</li>
<li>
<p>Lion optimizer (Google)</p>
</li>
<li>
<p><strong>Dettmers, T., et al.</strong> (2022). <em>8-bit Optimizers via Block-wise Quantization</em>. ICLR 2022</p>
</li>
<li>
<p>8-bit Adam for memory-efficient training</p>
</li>
<li>
<p><strong>PyTorch Documentation</strong>: https://pytorch.org/docs/stable/optim.html</p>
</li>
<li>
<p>Official optimizer and scheduler documentation</p>
</li>
<li>
<p><strong>Goodfellow, I., et al.</strong> (2016). <em>Deep Learning</em>. MIT Press</p>
</li>
<li>
<p>Chapter 8: Optimization for Training Deep Models</p>
</li>
<li>
<p><strong>Zhang, M., et al.</strong> (2020). <em>Lookahead Optimizer: k steps forward, 1 step back</em>. NeurIPS 2019</p>
<ul>
<li>Lookahead wrapper for any optimizer</li>
</ul>
</li>
</ol>
    </div>

    
<div class="lesson-toolbar lesson-toolbar--bottom">
    <div class="toolbar-nav toolbar-nav--prev">
        
        <a href="/study/en/Deep_Learning/24_Loss_Functions.html" class="nav-prev">
            <span class="nav-label">Previous</span>
            <span class="nav-title">24. Loss Functions</span>
        </a>
        
    </div>
    <div class="toolbar-actions">
        <button class="btn btn-copy-link" data-action="copy-link" title="Copy link">
            <span class="icon">üîó</span>
            <span class="text">Copy link</span>
        </button>
        <a href="/study/en/Deep_Learning/" class="btn btn-topic-link" title="Back to topic list">
            <span class="icon">üìã</span>
            <span class="text">Topic list</span>
        </a>
    </div>
    <div class="toolbar-nav toolbar-nav--next">
        
        <a href="/study/en/Deep_Learning/26_Normalization_Layers.html" class="nav-next">
            <span class="nav-label">Next</span>
            <span class="nav-title">26. Normalization Layers</span>
        </a>
        
    </div>
</div>


    <div class="toolbar-keyboard-hint">
        <kbd>&larr;</kbd> <kbd>&rarr;</kbd> to navigate between lessons
    </div>
</article>

<!-- Scroll to top floating button -->
<button class="scroll-to-top" id="scroll-to-top" title="Scroll to top" aria-label="Scroll to top">‚Üë</button>

            </div>
        </main>
    </div>

    <script src="/study/static/js/app.js"></script>
    <script>
        function switchLanguage(newLang) {
            const path = window.location.pathname;
            const base = '/study';
            const currentLang = 'en';
            const newPath = path.replace(base + '/' + currentLang + '/', base + '/' + newLang + '/');
            window.location.href = newPath + window.location.search;
        }
    </script>
    
<script>
    document.addEventListener('DOMContentLoaded', function() {
        // Copy link - independent feedback per button
        document.querySelectorAll('[data-action="copy-link"]').forEach(function(btn) {
            btn.addEventListener('click', function() {
                navigator.clipboard.writeText(window.location.href);
                var textEl = this.querySelector('.text');
                var originalText = textEl.textContent;
                textEl.textContent = 'Copied!';
                setTimeout(function() { textEl.textContent = originalText; }, 2000);
            });
        });

        // Add copy buttons to code blocks
        document.querySelectorAll('.lesson-content pre').forEach(function(pre) {
            var copyBtn = document.createElement('button');
            copyBtn.className = 'code-copy-btn';
            copyBtn.textContent = 'Copy';
            copyBtn.addEventListener('click', function() {
                var code = pre.querySelector('code');
                var text = code ? code.textContent : pre.textContent;
                navigator.clipboard.writeText(text);
                copyBtn.textContent = 'Copied!';
                setTimeout(function() { copyBtn.textContent = 'Copy'; }, 2000);
            });
            pre.style.position = 'relative';
            pre.appendChild(copyBtn);
        });

        // Scroll to top button
        var scrollBtn = document.getElementById('scroll-to-top');
        window.addEventListener('scroll', function() {
            scrollBtn.classList.toggle('visible', window.scrollY > 300);
        });
        scrollBtn.addEventListener('click', function() {
            window.scrollTo({ top: 0, behavior: 'smooth' });
        });

        // Keyboard shortcuts: left/right arrows for lesson navigation
        document.addEventListener('keydown', function(e) {
            var tag = document.activeElement.tagName;
            if (tag === 'INPUT' || tag === 'TEXTAREA' || tag === 'SELECT') return;

            if (e.key === 'ArrowLeft') {
                var prevLink = document.querySelector('.nav-prev');
                if (prevLink) prevLink.click();
            } else if (e.key === 'ArrowRight') {
                var nextLink = document.querySelector('.nav-next');
                if (nextLink) nextLink.click();
            }
        });
    });
</script>

</body>
</html>
{% endraw %}