{% raw %}
<!DOCTYPE html>
<html lang="en" data-theme="light">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>26. Normalization Layers - Study Materials</title>
    <link rel="stylesheet" href="/study/static/css/style.css">
    <link rel="stylesheet" href="/study/static/css/highlight.css">
    <!-- KaTeX for LaTeX math rendering -->
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.11/dist/katex.min.css">
    <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.11/dist/katex.min.js"></script>
    <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.11/dist/contrib/auto-render.min.js"
        onload="renderMathInElement(document.body, {
            delimiters: [
                {left: '$$', right: '$$', display: true},
                {left: '$', right: '$', display: false}
            ],
            throwOnError: false
        });"></script>
    
</head>
<body>
    <div class="app-container">
        <!-- Sidebar -->
        <aside class="sidebar" id="sidebar">
            <div class="sidebar-header">
                <a href="/study/en/" class="logo">Study Materials</a>
                <button class="sidebar-toggle" id="sidebar-toggle" aria-label="Toggle sidebar">
                    <span></span>
                </button>
            </div>

            <nav class="sidebar-nav">
                <a href="/study/en/" class="nav-item ">
                    <span class="nav-icon">ğŸ </span>
                    <span class="nav-text">Home</span>
                </a>
                <a href="/study/examples/" class="nav-item ">
                    <span class="nav-icon">ğŸ’»</span>
                    <span class="nav-text">Examples</span>
                </a>
            </nav>

            <div class="sidebar-search">
                <form action="/study/en/search.html" method="get" id="search-form">
                    <input type="search" name="q" placeholder="Search..." id="search-sidebar-input">
                </form>
            </div>

            <!-- Language Selector -->
            <div class="sidebar-lang">
                <select id="lang-select" class="lang-selector" onchange="switchLanguage(this.value)">
                    
                    <option value="en" selected>
                        English
                    </option>
                    
                    <option value="ko" >
                        í•œêµ­ì–´
                    </option>
                    
                </select>
            </div>

            <div class="sidebar-footer">
                <button class="theme-toggle" id="theme-toggle" aria-label="Toggle theme">
                    <span class="theme-icon light">â˜€ï¸</span>
                    <span class="theme-icon dark">ğŸŒ™</span>
                </button>
            </div>
        </aside>

        <!-- Main Content -->
        <main class="main-content">
            <header class="main-header">
                <button class="menu-toggle" id="menu-toggle" aria-label="Open menu">
                    <span></span>
                </button>
                
<nav class="breadcrumb">
    <a href="/study/en/">Topics</a>
    <span class="separator">/</span>
    <a href="/study/en/Deep_Learning/">Deep Learning</a>
    <span class="separator">/</span>
    <span class="current">26. Normalization Layers</span>
</nav>

            </header>

            <div class="content">
                
<article class="lesson-article">
    <header class="lesson-header">
        <h1>26. Normalization Layers</h1>
    </header>

    
<div class="lesson-toolbar lesson-toolbar--top">
    <div class="toolbar-nav toolbar-nav--prev">
        
        <a href="/study/en/Deep_Learning/25_Optimizers.html" class="nav-prev">
            <span class="nav-label">Previous</span>
            <span class="nav-title">25. Optimizers</span>
        </a>
        
    </div>
    <div class="toolbar-actions">
        <button class="btn btn-copy-link" data-action="copy-link" title="Copy link">
            <span class="icon">ğŸ”—</span>
            <span class="text">Copy link</span>
        </button>
        <a href="/study/en/Deep_Learning/" class="btn btn-topic-link" title="Back to topic list">
            <span class="icon">ğŸ“‹</span>
            <span class="text">Topic list</span>
        </a>
    </div>
    <div class="toolbar-nav toolbar-nav--next">
        
        <a href="/study/en/Deep_Learning/27_TensorBoard.html" class="nav-next">
            <span class="nav-label">Next</span>
            <span class="nav-title">27. TensorBoard Visualization</span>
        </a>
        
    </div>
</div>


    
    <nav class="toc" id="toc">
        <h2>Table of Contents</h2>
        <div class="toc">
<ul>
<li><a href="#learning-objectives">Learning Objectives</a></li>
<li><a href="#1-why-normalization">1. Why Normalization?</a><ul>
<li><a href="#11-the-problem-internal-covariate-shift">1.1 The Problem: Internal Covariate Shift</a></li>
<li><a href="#12-modern-understanding-loss-landscape-smoothing">1.2 Modern Understanding: Loss Landscape Smoothing</a></li>
<li><a href="#13-normalization-axes">1.3 Normalization Axes</a></li>
</ul>
</li>
<li><a href="#2-batch-normalization">2. Batch Normalization</a><ul>
<li><a href="#21-core-concept">2.1 Core Concept</a></li>
<li><a href="#22-training-vs-inference-mode">2.2 Training vs Inference Mode</a></li>
<li><a href="#23-where-to-place-batchnorm">2.3 Where to Place BatchNorm?</a></li>
<li><a href="#24-pytorch-batchnorm">2.4 PyTorch BatchNorm</a></li>
<li><a href="#25-manual-implementation">2.5 Manual Implementation</a></li>
<li><a href="#26-limitations-of-batchnorm">2.6 Limitations of BatchNorm</a></li>
</ul>
</li>
<li><a href="#3-layer-normalization">3. Layer Normalization</a><ul>
<li><a href="#31-core-concept">3.1 Core Concept</a></li>
<li><a href="#32-why-layernorm-for-transformers">3.2 Why LayerNorm for Transformers?</a></li>
<li><a href="#33-pytorch-layernorm">3.3 PyTorch LayerNorm</a></li>
<li><a href="#34-manual-implementation">3.4 Manual Implementation</a></li>
<li><a href="#35-use-cases">3.5 Use Cases</a></li>
</ul>
</li>
<li><a href="#4-group-normalization">4. Group Normalization</a><ul>
<li><a href="#41-core-concept">4.1 Core Concept</a></li>
<li><a href="#42-formula">4.2 Formula</a></li>
<li><a href="#43-pytorch-groupnorm">4.3 PyTorch GroupNorm</a></li>
<li><a href="#44-choosing-number-of-groups">4.4 Choosing Number of Groups</a></li>
<li><a href="#45-use-cases">4.5 Use Cases</a></li>
</ul>
</li>
<li><a href="#5-instance-normalization">5. Instance Normalization</a><ul>
<li><a href="#51-core-concept">5.1 Core Concept</a></li>
<li><a href="#52-formula">5.2 Formula</a></li>
<li><a href="#53-pytorch-instancenorm">5.3 PyTorch InstanceNorm</a></li>
<li><a href="#54-why-instance-normalization">5.4 Why Instance Normalization?</a></li>
<li><a href="#55-use-cases">5.5 Use Cases</a></li>
</ul>
</li>
<li><a href="#6-rmsnorm-root-mean-square-normalization">6. RMSNorm (Root Mean Square Normalization)</a><ul>
<li><a href="#61-core-concept">6.1 Core Concept</a></li>
<li><a href="#62-why-rmsnorm">6.2 Why RMSNorm?</a></li>
<li><a href="#63-manual-implementation">6.3 Manual Implementation</a></li>
<li><a href="#64-comparison-with-layernorm">6.4 Comparison with LayerNorm</a></li>
<li><a href="#65-rmsnorm-in-llama">6.5 RMSNorm in LLaMA</a></li>
<li><a href="#66-use-cases">6.6 Use Cases</a></li>
</ul>
</li>
<li><a href="#7-other-normalization-techniques">7. Other Normalization Techniques</a><ul>
<li><a href="#71-weight-normalization">7.1 Weight Normalization</a></li>
<li><a href="#72-spectral-normalization">7.2 Spectral Normalization</a></li>
<li><a href="#73-adaptive-instance-normalization-adain">7.3 Adaptive Instance Normalization (AdaIN)</a></li>
<li><a href="#74-comparison-table">7.4 Comparison Table</a></li>
</ul>
</li>
<li><a href="#8-comprehensive-comparison">8. Comprehensive Comparison</a><ul>
<li><a href="#81-visual-comparison">8.1 Visual Comparison</a></li>
<li><a href="#82-when-to-use-which">8.2 When to Use Which?</a><ul>
<li><a href="#decision-tree">Decision Tree</a></li>
<li><a href="#architecture-specific-recommendations">Architecture-Specific Recommendations</a></li>
</ul>
</li>
<li><a href="#83-performance-benchmarks">8.3 Performance Benchmarks</a></li>
</ul>
</li>
<li><a href="#9-practical-tips">9. Practical Tips</a><ul>
<li><a href="#91-initialization-of-and">9.1 Initialization of Î³ and Î²</a></li>
<li><a href="#92-interaction-with-weight-initialization">9.2 Interaction with Weight Initialization</a></li>
<li><a href="#93-normalization-and-learning-rate">9.3 Normalization and Learning Rate</a></li>
<li><a href="#94-common-bugs-and-pitfalls">9.4 Common Bugs and Pitfalls</a><ul>
<li><a href="#bug-1-forgetting-modeleval">Bug 1: Forgetting model.eval()</a></li>
<li><a href="#bug-2-wrong-dimension-ordering">Bug 2: Wrong dimension ordering</a></li>
<li><a href="#bug-3-groupnorm-with-incompatible-channels">Bug 3: GroupNorm with incompatible channels</a></li>
<li><a href="#bug-4-batchnorm-with-batch_size-1">Bug 4: BatchNorm with batch_size = 1</a></li>
<li><a href="#bug-5-mixing-frozen-and-trainable-batchnorm">Bug 5: Mixing frozen and trainable BatchNorm</a></li>
</ul>
</li>
<li><a href="#95-best-practices-checklist">9.5 Best Practices Checklist</a></li>
</ul>
</li>
<li><a href="#exercises">Exercises</a><ul>
<li><a href="#exercise-1-implement-and-compare-normalization-methods">Exercise 1: Implement and Compare Normalization Methods</a></li>
<li><a href="#exercise-2-rmsnorm-vs-layernorm-in-transformers">Exercise 2: RMSNorm vs LayerNorm in Transformers</a></li>
<li><a href="#exercise-3-adaptive-instance-normalization-for-style-transfer">Exercise 3: Adaptive Instance Normalization for Style Transfer</a></li>
</ul>
</li>
<li><a href="#references">References</a></li>
</ul>
</div>

    </nav>
    

    <div class="lesson-content markdown-body">
        <p><a href="./25_Optimizers.md">Previous: Optimizers</a> | <a href="./27_TensorBoard.md">Next: TensorBoard Visualization</a></p>
<hr />
<h1 id="26-normalization-layers">26. Normalization Layers<a class="header-link" href="#26-normalization-layers" title="Permanent link">&para;</a></h1>
<h2 id="learning-objectives">Learning Objectives<a class="header-link" href="#learning-objectives" title="Permanent link">&para;</a></h2>
<ul>
<li>Understand the motivation for normalization in deep learning and how it smooths the loss landscape</li>
<li>Master Batch Normalization, Layer Normalization, Group Normalization, and their use cases</li>
<li>Learn RMSNorm and why it's preferred in modern large language models</li>
<li>Implement normalization layers from scratch and understand their computational implications</li>
<li>Apply the right normalization technique based on architecture and batch size constraints</li>
</ul>
<hr />
<h2 id="1-why-normalization">1. Why Normalization?<a class="header-link" href="#1-why-normalization" title="Permanent link">&para;</a></h2>
<h3 id="11-the-problem-internal-covariate-shift">1.1 The Problem: Internal Covariate Shift<a class="header-link" href="#11-the-problem-internal-covariate-shift" title="Permanent link">&para;</a></h3>
<p><strong>Internal Covariate Shift</strong> refers to the change in the distribution of network activations during training. As parameters in earlier layers change, the inputs to later layers shift, forcing them to continuously adapt.</p>
<p><strong>Original motivation</strong> (Ioffe &amp; Szegedy, 2015):
- Stabilize activation distributions across layers
- Allow each layer to learn on a more stable input distribution
- Enable higher learning rates without divergence</p>
<h3 id="12-modern-understanding-loss-landscape-smoothing">1.2 Modern Understanding: Loss Landscape Smoothing<a class="header-link" href="#12-modern-understanding-loss-landscape-smoothing" title="Permanent link">&para;</a></h3>
<p>Recent research (Santurkar et al., 2018) shows normalization's primary benefit is <strong>smoothing the loss landscape</strong>:</p>
<div class="highlight"><pre><span></span><code><span class="n">Without</span><span class="w"> </span><span class="nl">Normalization:</span><span class="w">          </span><span class="n">With</span><span class="w"> </span><span class="nl">Normalization:</span>

<span class="w">    </span><span class="o">|</span><span class="err">\</span><span class="w">                              </span><span class="o">/</span><span class="se">\</span>
<span class="w">    </span><span class="o">|</span><span class="w"> </span><span class="err">\</span><span class="w">        </span><span class="o">/</span><span class="err">\</span><span class="w">                  </span><span class="o">/</span><span class="w">  </span><span class="se">\</span>
<span class="w">    </span><span class="o">|</span><span class="w">  </span><span class="err">\</span><span class="w">  </span><span class="o">/</span><span class="err">\</span><span class="w">  </span><span class="o">/</span><span class="w">  </span><span class="err">\</span><span class="w">                </span><span class="o">/</span><span class="w">    </span><span class="se">\</span>
<span class="w">    </span><span class="o">|</span><span class="n">___\/</span><span class="w">  </span><span class="n">\/____\__</span><span class="w">           </span><span class="o">/</span><span class="n">______\_____</span>

<span class="w">    </span><span class="n">Rough</span><span class="p">,</span><span class="w"> </span><span class="n">irregular</span><span class="w">             </span><span class="n">Smoother</span><span class="p">,</span><span class="w"> </span><span class="n">more</span><span class="w"> </span><span class="n">predictable</span>
<span class="w">    </span><span class="n">gradients</span><span class="w">                    </span><span class="n">gradients</span>
</code></pre></div>

<p><strong>Benefits</strong>:
1. <strong>Faster convergence</strong> â€” smoother gradients allow larger steps
2. <strong>Higher learning rates</strong> â€” reduced risk of divergence
3. <strong>Regularization effect</strong> â€” noise from batch statistics acts as implicit regularization
4. <strong>Reduced sensitivity to initialization</strong> â€” less dependence on careful weight initialization</p>
<h3 id="13-normalization-axes">1.3 Normalization Axes<a class="header-link" href="#13-normalization-axes" title="Permanent link">&para;</a></h3>
<p>Different normalization methods normalize across different dimensions:</p>
<div class="highlight"><pre><span></span><code><span class="nv">Input</span><span class="w"> </span><span class="nv">tensor</span><span class="w"> </span><span class="nv">shape</span>:<span class="w"> </span><span class="ss">(</span><span class="nv">N</span>,<span class="w"> </span><span class="nv">C</span>,<span class="w"> </span><span class="nv">H</span>,<span class="w"> </span><span class="nv">W</span><span class="ss">)</span>
<span class="nv">N</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="nv">batch</span><span class="w"> </span><span class="nv">size</span>
<span class="nv">C</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="nv">channels</span>
<span class="nv">H</span>,<span class="w"> </span><span class="nv">W</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="nv">spatial</span><span class="w"> </span><span class="nv">dimensions</span>

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚<span class="w">  </span><span class="nv">Batch</span><span class="w"> </span><span class="nv">Norm</span>:<span class="w">     </span><span class="nv">normalize</span><span class="w"> </span><span class="nv">across</span><span class="w"> </span><span class="nv">N</span><span class="w">     </span><span class="k">for</span><span class="w"> </span><span class="nv">each</span><span class="w"> </span><span class="ss">(</span><span class="nv">C</span>,<span class="w"> </span><span class="nv">H</span>,<span class="w"> </span><span class="nv">W</span><span class="ss">)</span><span class="w">  </span>â”‚
â”‚<span class="w">  </span><span class="nv">Layer</span><span class="w"> </span><span class="nv">Norm</span>:<span class="w">     </span><span class="nv">normalize</span><span class="w"> </span><span class="nv">across</span><span class="w"> </span><span class="nv">C</span>,<span class="nv">H</span>,<span class="nv">W</span><span class="w"> </span><span class="k">for</span><span class="w"> </span><span class="nv">each</span><span class="w"> </span><span class="nv">N</span><span class="w">          </span>â”‚
â”‚<span class="w">  </span><span class="nv">Instance</span><span class="w"> </span><span class="nv">Norm</span>:<span class="w">  </span><span class="nv">normalize</span><span class="w"> </span><span class="nv">across</span><span class="w"> </span><span class="nv">H</span>,<span class="nv">W</span><span class="w">   </span><span class="k">for</span><span class="w"> </span><span class="nv">each</span><span class="w"> </span><span class="ss">(</span><span class="nv">N</span>,<span class="w"> </span><span class="nv">C</span><span class="ss">)</span><span class="w">     </span>â”‚
â”‚<span class="w">  </span><span class="nv">Group</span><span class="w"> </span><span class="nv">Norm</span>:<span class="w">     </span><span class="nv">normalize</span><span class="w"> </span><span class="nv">across</span><span class="w"> </span><span class="nv">C</span><span class="o">/</span><span class="nv">G</span>,<span class="nv">H</span>,<span class="nv">W</span><span class="w"> </span><span class="k">for</span><span class="w"> </span><span class="nv">each</span><span class="w"> </span><span class="ss">(</span><span class="nv">N</span>,<span class="w"> </span><span class="nv">G</span><span class="ss">)</span><span class="w">   </span>â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
</code></pre></div>

<hr />
<h2 id="2-batch-normalization">2. Batch Normalization<a class="header-link" href="#2-batch-normalization" title="Permanent link">&para;</a></h2>
<h3 id="21-core-concept">2.1 Core Concept<a class="header-link" href="#21-core-concept" title="Permanent link">&para;</a></h3>
<p><strong>Batch Normalization</strong> (BatchNorm) normalizes activations across the batch dimension for each feature independently.</p>
<p><strong>Algorithm</strong>:</p>
<div class="highlight"><pre><span></span><code><span class="n">Input</span><span class="o">:</span><span class="w"> </span><span class="n">mini</span><span class="o">-</span><span class="n">batch</span><span class="w"> </span><span class="n">B</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="o">{</span><span class="n">xâ‚</span><span class="o">,</span><span class="w"> </span><span class="o">...,</span><span class="w"> </span><span class="n">xâ‚˜</span><span class="o">}</span>
<span class="n">Parameters</span><span class="o">:</span><span class="w"> </span><span class="err">Î³</span><span class="w"> </span><span class="o">(</span><span class="n">scale</span><span class="o">),</span><span class="w"> </span><span class="err">Î²</span><span class="w"> </span><span class="o">(</span><span class="n">shift</span><span class="o">)</span><span class="w"> </span><span class="err">â€”</span><span class="w"> </span><span class="n">learnable</span>

<span class="mi">1</span><span class="o">.</span><span class="w"> </span><span class="n">Calculate</span><span class="w"> </span><span class="n">batch</span><span class="w"> </span><span class="n">statistics</span><span class="o">:</span>
<span class="w">   </span><span class="err">Î¼</span><span class="n">_B</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="o">(</span><span class="mi">1</span><span class="o">/</span><span class="n">m</span><span class="o">)</span><span class="w"> </span><span class="err">Î£</span><span class="w"> </span><span class="n">xáµ¢</span><span class="w">                    </span><span class="err">#</span><span class="w"> </span><span class="n">mean</span>
<span class="w">   </span><span class="err">ÏƒÂ²</span><span class="n">_B</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="o">(</span><span class="mi">1</span><span class="o">/</span><span class="n">m</span><span class="o">)</span><span class="w"> </span><span class="err">Î£</span><span class="w"> </span><span class="o">(</span><span class="n">xáµ¢</span><span class="w"> </span><span class="o">-</span><span class="w"> </span><span class="err">Î¼</span><span class="n">_B</span><span class="o">)</span><span class="err">Â²</span><span class="w">          </span><span class="err">#</span><span class="w"> </span><span class="n">variance</span>

<span class="mi">2</span><span class="o">.</span><span class="w"> </span><span class="n">Normalize</span><span class="o">:</span>
<span class="w">   </span><span class="n">x</span><span class="err">Ì‚áµ¢</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="o">(</span><span class="n">xáµ¢</span><span class="w"> </span><span class="o">-</span><span class="w"> </span><span class="err">Î¼</span><span class="n">_B</span><span class="o">)</span><span class="w"> </span><span class="o">/</span><span class="w"> </span><span class="err">âˆš</span><span class="o">(</span><span class="err">ÏƒÂ²</span><span class="n">_B</span><span class="w"> </span><span class="o">+</span><span class="w"> </span><span class="err">Îµ</span><span class="o">)</span><span class="w">       </span><span class="err">#</span><span class="w"> </span><span class="err">Îµ</span><span class="w"> </span><span class="k">for</span><span class="w"> </span><span class="n">numerical</span><span class="w"> </span><span class="n">stability</span>

<span class="mi">3</span><span class="o">.</span><span class="w"> </span><span class="n">Scale</span><span class="w"> </span><span class="n">and</span><span class="w"> </span><span class="n">shift</span><span class="o">:</span>
<span class="w">   </span><span class="n">yáµ¢</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="err">Î³</span><span class="w"> </span><span class="n">x</span><span class="err">Ì‚áµ¢</span><span class="w"> </span><span class="o">+</span><span class="w"> </span><span class="err">Î²</span><span class="w">                        </span><span class="err">#</span><span class="w"> </span><span class="n">learnable</span><span class="w"> </span><span class="n">transformation</span>
</code></pre></div>

<p><strong>Why scale and shift?</strong> The network can learn to undo normalization if needed (e.g., <code>Î³ = âˆšÏƒÂ²</code>, <code>Î² = Î¼</code> recovers the original distribution).</p>
<h3 id="22-training-vs-inference-mode">2.2 Training vs Inference Mode<a class="header-link" href="#22-training-vs-inference-mode" title="Permanent link">&para;</a></h3>
<p><strong>Training</strong>:
- Use batch statistics (Î¼_B, ÏƒÂ²_B)
- Update running estimates for inference:
  <code>running_mean = momentum Ã— running_mean + (1 - momentum) Ã— Î¼_B
  running_var = momentum Ã— running_var + (1 - momentum) Ã— ÏƒÂ²_B</code></p>
<p><strong>Inference</strong>:
- Use running statistics (fixed)
- No dependence on current batch</p>
<div class="highlight"><pre><span></span><code><span class="kn">import</span><span class="w"> </span><span class="nn">torch</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">torch.nn</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">nn</span>

<span class="c1"># Training mode</span>
<span class="n">bn</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">BatchNorm2d</span><span class="p">(</span><span class="mi">64</span><span class="p">)</span>
<span class="n">bn</span><span class="o">.</span><span class="n">train</span><span class="p">()</span>
<span class="n">out</span> <span class="o">=</span> <span class="n">bn</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>  <span class="c1"># Uses batch statistics</span>

<span class="c1"># Inference mode</span>
<span class="n">bn</span><span class="o">.</span><span class="n">eval</span><span class="p">()</span>
<span class="n">out</span> <span class="o">=</span> <span class="n">bn</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>  <span class="c1"># Uses running statistics</span>
</code></pre></div>

<h3 id="23-where-to-place-batchnorm">2.3 Where to Place BatchNorm?<a class="header-link" href="#23-where-to-place-batchnorm" title="Permanent link">&para;</a></h3>
<p><strong>Option 1: After activation</strong> (original paper)</p>
<div class="highlight"><pre><span></span><code>Linear/Conv â†’ Activation â†’ BatchNorm
</code></pre></div>

<p><strong>Option 2: Before activation</strong> (common practice)</p>
<div class="highlight"><pre><span></span><code>Linear/Conv â†’ BatchNorm â†’ Activation
</code></pre></div>

<p><strong>Modern consensus</strong>: Before activation works better in practice, especially with ReLU.</p>
<h3 id="24-pytorch-batchnorm">2.4 PyTorch BatchNorm<a class="header-link" href="#24-pytorch-batchnorm" title="Permanent link">&para;</a></h3>
<div class="highlight"><pre><span></span><code><span class="kn">import</span><span class="w"> </span><span class="nn">torch.nn</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">nn</span>

<span class="c1"># For fully connected layers (1D)</span>
<span class="n">bn1d</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">BatchNorm1d</span><span class="p">(</span><span class="n">num_features</span><span class="o">=</span><span class="mi">128</span><span class="p">)</span>

<span class="c1"># For convolutional layers (2D)</span>
<span class="n">bn2d</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">BatchNorm2d</span><span class="p">(</span><span class="n">num_features</span><span class="o">=</span><span class="mi">64</span><span class="p">)</span>

<span class="c1"># For 3D convolutions (video, volumetric data)</span>
<span class="n">bn3d</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">BatchNorm3d</span><span class="p">(</span><span class="n">num_features</span><span class="o">=</span><span class="mi">32</span><span class="p">)</span>

<span class="c1"># Example CNN block</span>
<span class="k">class</span><span class="w"> </span><span class="nc">ConvBlock</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="k">def</span><span class="w"> </span><span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">in_channels</span><span class="p">,</span> <span class="n">out_channels</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">conv</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Conv2d</span><span class="p">(</span><span class="n">in_channels</span><span class="p">,</span> <span class="n">out_channels</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="n">padding</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">bn</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">BatchNorm2d</span><span class="p">(</span><span class="n">out_channels</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">relu</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">ReLU</span><span class="p">(</span><span class="n">inplace</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>

    <span class="k">def</span><span class="w"> </span><span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
        <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">conv</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">bn</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">relu</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">x</span>

<span class="c1"># Usage</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">ConvBlock</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="mi">64</span><span class="p">)</span>
<span class="n">x</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">32</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">224</span><span class="p">,</span> <span class="mi">224</span><span class="p">)</span>  <span class="c1"># (N, C, H, W)</span>
<span class="n">out</span> <span class="o">=</span> <span class="n">model</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">out</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>  <span class="c1"># torch.Size([32, 64, 224, 224])</span>
</code></pre></div>

<h3 id="25-manual-implementation">2.5 Manual Implementation<a class="header-link" href="#25-manual-implementation" title="Permanent link">&para;</a></h3>
<div class="highlight"><pre><span></span><code><span class="kn">import</span><span class="w"> </span><span class="nn">torch</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">torch.nn</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">nn</span>

<span class="k">class</span><span class="w"> </span><span class="nc">BatchNorm2dManual</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="k">def</span><span class="w"> </span><span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">num_features</span><span class="p">,</span> <span class="n">eps</span><span class="o">=</span><span class="mf">1e-5</span><span class="p">,</span> <span class="n">momentum</span><span class="o">=</span><span class="mf">0.1</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">eps</span> <span class="o">=</span> <span class="n">eps</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">momentum</span> <span class="o">=</span> <span class="n">momentum</span>

        <span class="c1"># Learnable parameters</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">gamma</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Parameter</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">ones</span><span class="p">(</span><span class="n">num_features</span><span class="p">))</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">beta</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Parameter</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">num_features</span><span class="p">))</span>

        <span class="c1"># Running statistics (not updated by gradient descent)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">register_buffer</span><span class="p">(</span><span class="s1">&#39;running_mean&#39;</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">num_features</span><span class="p">))</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">register_buffer</span><span class="p">(</span><span class="s1">&#39;running_var&#39;</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">ones</span><span class="p">(</span><span class="n">num_features</span><span class="p">))</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">register_buffer</span><span class="p">(</span><span class="s1">&#39;num_batches_tracked&#39;</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">long</span><span class="p">))</span>

    <span class="k">def</span><span class="w"> </span><span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
        <span class="c1"># x shape: (N, C, H, W)</span>

        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">training</span><span class="p">:</span>
            <span class="c1"># Calculate batch statistics</span>
            <span class="c1"># Mean and var across (N, H, W) for each channel C</span>
            <span class="n">mean</span> <span class="o">=</span> <span class="n">x</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">dim</span><span class="o">=</span><span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">],</span> <span class="n">keepdim</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>  <span class="c1"># Shape: (C,)</span>
            <span class="n">var</span> <span class="o">=</span> <span class="n">x</span><span class="o">.</span><span class="n">var</span><span class="p">(</span><span class="n">dim</span><span class="o">=</span><span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">],</span> <span class="n">unbiased</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">keepdim</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>  <span class="c1"># Shape: (C,)</span>

            <span class="c1"># Update running statistics</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">running_mean</span> <span class="o">=</span> <span class="p">(</span><span class="mi">1</span> <span class="o">-</span> <span class="bp">self</span><span class="o">.</span><span class="n">momentum</span><span class="p">)</span> <span class="o">*</span> <span class="bp">self</span><span class="o">.</span><span class="n">running_mean</span> <span class="o">+</span> <span class="bp">self</span><span class="o">.</span><span class="n">momentum</span> <span class="o">*</span> <span class="n">mean</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">running_var</span> <span class="o">=</span> <span class="p">(</span><span class="mi">1</span> <span class="o">-</span> <span class="bp">self</span><span class="o">.</span><span class="n">momentum</span><span class="p">)</span> <span class="o">*</span> <span class="bp">self</span><span class="o">.</span><span class="n">running_var</span> <span class="o">+</span> <span class="bp">self</span><span class="o">.</span><span class="n">momentum</span> <span class="o">*</span> <span class="n">var</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">num_batches_tracked</span> <span class="o">+=</span> <span class="mi">1</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="c1"># Use running statistics</span>
            <span class="n">mean</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">running_mean</span>
            <span class="n">var</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">running_var</span>

        <span class="c1"># Normalize</span>
        <span class="c1"># Reshape for broadcasting: (1, C, 1, 1)</span>
        <span class="n">mean</span> <span class="o">=</span> <span class="n">mean</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
        <span class="n">var</span> <span class="o">=</span> <span class="n">var</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
        <span class="n">gamma</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">gamma</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
        <span class="n">beta</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">beta</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>

        <span class="n">x_norm</span> <span class="o">=</span> <span class="p">(</span><span class="n">x</span> <span class="o">-</span> <span class="n">mean</span><span class="p">)</span> <span class="o">/</span> <span class="n">torch</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="n">var</span> <span class="o">+</span> <span class="bp">self</span><span class="o">.</span><span class="n">eps</span><span class="p">)</span>
        <span class="n">out</span> <span class="o">=</span> <span class="n">gamma</span> <span class="o">*</span> <span class="n">x_norm</span> <span class="o">+</span> <span class="n">beta</span>

        <span class="k">return</span> <span class="n">out</span>

<span class="c1"># Test</span>
<span class="n">manual_bn</span> <span class="o">=</span> <span class="n">BatchNorm2dManual</span><span class="p">(</span><span class="mi">64</span><span class="p">)</span>
<span class="n">pytorch_bn</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">BatchNorm2d</span><span class="p">(</span><span class="mi">64</span><span class="p">)</span>

<span class="n">x</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">32</span><span class="p">,</span> <span class="mi">64</span><span class="p">,</span> <span class="mi">16</span><span class="p">,</span> <span class="mi">16</span><span class="p">)</span>

<span class="c1"># Training mode</span>
<span class="n">manual_bn</span><span class="o">.</span><span class="n">train</span><span class="p">()</span>
<span class="n">pytorch_bn</span><span class="o">.</span><span class="n">train</span><span class="p">()</span>
<span class="n">out_manual</span> <span class="o">=</span> <span class="n">manual_bn</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
<span class="n">out_pytorch</span> <span class="o">=</span> <span class="n">pytorch_bn</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>

<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Output shape: </span><span class="si">{</span><span class="n">out_manual</span><span class="o">.</span><span class="n">shape</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Mean close to 0: </span><span class="si">{</span><span class="n">out_manual</span><span class="o">.</span><span class="n">mean</span><span class="p">()</span><span class="o">.</span><span class="n">item</span><span class="p">()</span><span class="si">:</span><span class="s2">.6f</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Std close to 1: </span><span class="si">{</span><span class="n">out_manual</span><span class="o">.</span><span class="n">std</span><span class="p">()</span><span class="o">.</span><span class="n">item</span><span class="p">()</span><span class="si">:</span><span class="s2">.6f</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
</code></pre></div>

<h3 id="26-limitations-of-batchnorm">2.6 Limitations of BatchNorm<a class="header-link" href="#26-limitations-of-batchnorm" title="Permanent link">&para;</a></h3>
<p><strong>1. Batch size dependency</strong>
- Small batches â†’ noisy statistics â†’ poor performance
- Batch size &lt; 8 is problematic</p>
<p><strong>2. Sequence models (RNNs)</strong>
- Different sequence lengths in a batch
- Hard to apply across time dimension</p>
<p><strong>3. Distributed training</strong>
- Each GPU has a different batch
- Sync BatchNorm needed (expensive)</p>
<p><strong>4. Online learning</strong>
- Single sample at a time
- No batch statistics available</p>
<hr />
<h2 id="3-layer-normalization">3. Layer Normalization<a class="header-link" href="#3-layer-normalization" title="Permanent link">&para;</a></h2>
<h3 id="31-core-concept">3.1 Core Concept<a class="header-link" href="#31-core-concept" title="Permanent link">&para;</a></h3>
<p><strong>Layer Normalization</strong> (LayerNorm) normalizes across all features for each sample independently, making it batch-independent.</p>
<div class="highlight"><pre><span></span><code><span class="n">BatchNorm</span><span class="o">:</span><span class="w">  </span><span class="n">normalize</span><span class="w"> </span><span class="n">across</span><span class="w"> </span><span class="n">samples</span><span class="w">    </span><span class="k">for</span><span class="w"> </span><span class="k">each</span><span class="w"> </span><span class="n">feature</span>
<span class="n">LayerNorm</span><span class="o">:</span><span class="w">  </span><span class="n">normalize</span><span class="w"> </span><span class="n">across</span><span class="w"> </span><span class="n">features</span><span class="w">   </span><span class="k">for</span><span class="w"> </span><span class="k">each</span><span class="w"> </span><span class="n">sample</span>
</code></pre></div>

<p><strong>Formula</strong>:</p>
<div class="highlight"><pre><span></span><code><span class="n">For</span><span class="w"> </span><span class="n">each</span><span class="w"> </span><span class="n">sample</span><span class="w"> </span><span class="n">x</span><span class="w"> </span><span class="ow">in</span><span class="w"> </span><span class="n">batch</span><span class="p">:</span>
<span class="w">  </span><span class="err">Î¼</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="p">(</span><span class="mi">1</span><span class="o">/</span><span class="n">D</span><span class="p">)</span><span class="w"> </span><span class="err">Î£</span><span class="w"> </span><span class="n">xáµ¢</span><span class="w">                        </span><span class="c1"># mean across features</span>
<span class="w">  </span><span class="err">ÏƒÂ²</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="p">(</span><span class="mi">1</span><span class="o">/</span><span class="n">D</span><span class="p">)</span><span class="w"> </span><span class="err">Î£</span><span class="w"> </span><span class="p">(</span><span class="n">xáµ¢</span><span class="w"> </span><span class="o">-</span><span class="w"> </span><span class="err">Î¼</span><span class="p">)</span><span class="err">Â²</span><span class="w">                </span><span class="c1"># variance across features</span>
<span class="w">  </span><span class="n">x</span><span class="err">Ì‚áµ¢</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="p">(</span><span class="n">xáµ¢</span><span class="w"> </span><span class="o">-</span><span class="w"> </span><span class="err">Î¼</span><span class="p">)</span><span class="w"> </span><span class="o">/</span><span class="w"> </span><span class="err">âˆš</span><span class="p">(</span><span class="err">ÏƒÂ²</span><span class="w"> </span><span class="o">+</span><span class="w"> </span><span class="err">Îµ</span><span class="p">)</span><span class="w">             </span><span class="c1"># normalize</span>
<span class="w">  </span><span class="n">yáµ¢</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="err">Î³</span><span class="w"> </span><span class="n">x</span><span class="err">Ì‚áµ¢</span><span class="w"> </span><span class="o">+</span><span class="w"> </span><span class="err">Î²</span><span class="w">                         </span><span class="c1"># scale and shift</span>
</code></pre></div>

<h3 id="32-why-layernorm-for-transformers">3.2 Why LayerNorm for Transformers?<a class="header-link" href="#32-why-layernorm-for-transformers" title="Permanent link">&para;</a></h3>
<p><strong>Advantages</strong>:
1. <strong>Batch-independent</strong> â€” works with batch size = 1
2. <strong>Sequence-length independent</strong> â€” each position normalized the same way
3. <strong>Deterministic at inference</strong> â€” no running statistics needed</p>
<p><strong>Transformer architecture</strong>:</p>
<div class="highlight"><pre><span></span><code>â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Pre-Norm (modern):                     â”‚
â”‚    x â†’ LayerNorm â†’ Attention â†’ Add(x)   â”‚
â”‚    x â†’ LayerNorm â†’ FFN â†’ Add(x)         â”‚
â”‚                                          â”‚
â”‚  Post-Norm (original):                  â”‚
â”‚    x â†’ Attention â†’ Add(x) â†’ LayerNorm   â”‚
â”‚    x â†’ FFN â†’ Add(x) â†’ LayerNorm         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
</code></pre></div>

<p><strong>Pre-Norm vs Post-Norm</strong>:
- <strong>Pre-Norm</strong>: Better gradient flow, easier to train, used in GPT, LLaMA
- <strong>Post-Norm</strong>: Original Transformer design, slightly better performance with careful tuning</p>
<h3 id="33-pytorch-layernorm">3.3 PyTorch LayerNorm<a class="header-link" href="#33-pytorch-layernorm" title="Permanent link">&para;</a></h3>
<div class="highlight"><pre><span></span><code><span class="kn">import</span><span class="w"> </span><span class="nn">torch</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">torch.nn</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">nn</span>

<span class="c1"># LayerNorm for Transformers</span>
<span class="n">ln</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">LayerNorm</span><span class="p">(</span><span class="mi">512</span><span class="p">)</span>  <span class="c1"># d_model = 512</span>

<span class="c1"># Example: Self-Attention with Pre-Norm</span>
<span class="k">class</span><span class="w"> </span><span class="nc">TransformerBlock</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="k">def</span><span class="w"> </span><span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">d_model</span><span class="p">,</span> <span class="n">num_heads</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">ln1</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">LayerNorm</span><span class="p">(</span><span class="n">d_model</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">attn</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">MultiheadAttention</span><span class="p">(</span><span class="n">d_model</span><span class="p">,</span> <span class="n">num_heads</span><span class="p">,</span> <span class="n">batch_first</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">ln2</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">LayerNorm</span><span class="p">(</span><span class="n">d_model</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">ffn</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Sequential</span><span class="p">(</span>
            <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">d_model</span><span class="p">,</span> <span class="mi">4</span> <span class="o">*</span> <span class="n">d_model</span><span class="p">),</span>
            <span class="n">nn</span><span class="o">.</span><span class="n">GELU</span><span class="p">(),</span>
            <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="mi">4</span> <span class="o">*</span> <span class="n">d_model</span><span class="p">,</span> <span class="n">d_model</span><span class="p">)</span>
        <span class="p">)</span>

    <span class="k">def</span><span class="w"> </span><span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
        <span class="c1"># Pre-Norm</span>
        <span class="n">x</span> <span class="o">=</span> <span class="n">x</span> <span class="o">+</span> <span class="bp">self</span><span class="o">.</span><span class="n">attn</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">ln1</span><span class="p">(</span><span class="n">x</span><span class="p">),</span> <span class="bp">self</span><span class="o">.</span><span class="n">ln1</span><span class="p">(</span><span class="n">x</span><span class="p">),</span> <span class="bp">self</span><span class="o">.</span><span class="n">ln1</span><span class="p">(</span><span class="n">x</span><span class="p">))[</span><span class="mi">0</span><span class="p">]</span>
        <span class="n">x</span> <span class="o">=</span> <span class="n">x</span> <span class="o">+</span> <span class="bp">self</span><span class="o">.</span><span class="n">ffn</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">ln2</span><span class="p">(</span><span class="n">x</span><span class="p">))</span>
        <span class="k">return</span> <span class="n">x</span>

<span class="c1"># Usage</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">TransformerBlock</span><span class="p">(</span><span class="n">d_model</span><span class="o">=</span><span class="mi">512</span><span class="p">,</span> <span class="n">num_heads</span><span class="o">=</span><span class="mi">8</span><span class="p">)</span>
<span class="n">x</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">32</span><span class="p">,</span> <span class="mi">100</span><span class="p">,</span> <span class="mi">512</span><span class="p">)</span>  <span class="c1"># (batch, seq_len, d_model)</span>
<span class="n">out</span> <span class="o">=</span> <span class="n">model</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">out</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>  <span class="c1"># torch.Size([32, 100, 512])</span>
</code></pre></div>

<h3 id="34-manual-implementation">3.4 Manual Implementation<a class="header-link" href="#34-manual-implementation" title="Permanent link">&para;</a></h3>
<div class="highlight"><pre><span></span><code><span class="kn">import</span><span class="w"> </span><span class="nn">torch</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">torch.nn</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">nn</span>

<span class="k">class</span><span class="w"> </span><span class="nc">LayerNormManual</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="k">def</span><span class="w"> </span><span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">normalized_shape</span><span class="p">,</span> <span class="n">eps</span><span class="o">=</span><span class="mf">1e-5</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">eps</span> <span class="o">=</span> <span class="n">eps</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">normalized_shape</span> <span class="o">=</span> <span class="n">normalized_shape</span>

        <span class="c1"># Learnable parameters</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">gamma</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Parameter</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">ones</span><span class="p">(</span><span class="n">normalized_shape</span><span class="p">))</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">beta</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Parameter</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">normalized_shape</span><span class="p">))</span>

    <span class="k">def</span><span class="w"> </span><span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
        <span class="c1"># x shape: (N, ..., normalized_shape)</span>
        <span class="c1"># E.g., (N, seq_len, d_model) for Transformers</span>

        <span class="c1"># Calculate mean and variance across the last dimensions</span>
        <span class="c1"># Keep dims for broadcasting</span>
        <span class="n">dims</span> <span class="o">=</span> <span class="nb">list</span><span class="p">(</span><span class="nb">range</span><span class="p">(</span><span class="o">-</span><span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">gamma</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span> <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">normalized_shape</span><span class="p">,</span> <span class="nb">tuple</span><span class="p">)</span>
                          <span class="k">else</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">))</span>

        <span class="n">mean</span> <span class="o">=</span> <span class="n">x</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">dim</span><span class="o">=-</span><span class="mi">1</span><span class="p">,</span> <span class="n">keepdim</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
        <span class="n">var</span> <span class="o">=</span> <span class="n">x</span><span class="o">.</span><span class="n">var</span><span class="p">(</span><span class="n">dim</span><span class="o">=-</span><span class="mi">1</span><span class="p">,</span> <span class="n">unbiased</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">keepdim</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>

        <span class="c1"># Normalize</span>
        <span class="n">x_norm</span> <span class="o">=</span> <span class="p">(</span><span class="n">x</span> <span class="o">-</span> <span class="n">mean</span><span class="p">)</span> <span class="o">/</span> <span class="n">torch</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="n">var</span> <span class="o">+</span> <span class="bp">self</span><span class="o">.</span><span class="n">eps</span><span class="p">)</span>

        <span class="c1"># Scale and shift</span>
        <span class="n">out</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">gamma</span> <span class="o">*</span> <span class="n">x_norm</span> <span class="o">+</span> <span class="bp">self</span><span class="o">.</span><span class="n">beta</span>

        <span class="k">return</span> <span class="n">out</span>

<span class="c1"># Test</span>
<span class="n">manual_ln</span> <span class="o">=</span> <span class="n">LayerNormManual</span><span class="p">(</span><span class="mi">512</span><span class="p">)</span>
<span class="n">pytorch_ln</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">LayerNorm</span><span class="p">(</span><span class="mi">512</span><span class="p">)</span>

<span class="n">x</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">32</span><span class="p">,</span> <span class="mi">100</span><span class="p">,</span> <span class="mi">512</span><span class="p">)</span>  <span class="c1"># (batch, seq, features)</span>

<span class="n">out_manual</span> <span class="o">=</span> <span class="n">manual_ln</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
<span class="n">out_pytorch</span> <span class="o">=</span> <span class="n">pytorch_ln</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>

<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Output shape: </span><span class="si">{</span><span class="n">out_manual</span><span class="o">.</span><span class="n">shape</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Mean per sample close to 0: </span><span class="si">{</span><span class="n">out_manual</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">mean</span><span class="p">()</span><span class="o">.</span><span class="n">item</span><span class="p">()</span><span class="si">:</span><span class="s2">.6f</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Std per sample close to 1: </span><span class="si">{</span><span class="n">out_manual</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">std</span><span class="p">()</span><span class="o">.</span><span class="n">item</span><span class="p">()</span><span class="si">:</span><span class="s2">.6f</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
</code></pre></div>

<h3 id="35-use-cases">3.5 Use Cases<a class="header-link" href="#35-use-cases" title="Permanent link">&para;</a></h3>
<p><strong>Best for</strong>:
- Transformers (BERT, GPT, ViT)
- RNNs, LSTMs
- Small batch sizes
- Variable sequence lengths</p>
<hr />
<h2 id="4-group-normalization">4. Group Normalization<a class="header-link" href="#4-group-normalization" title="Permanent link">&para;</a></h2>
<h3 id="41-core-concept">4.1 Core Concept<a class="header-link" href="#41-core-concept" title="Permanent link">&para;</a></h3>
<p><strong>Group Normalization</strong> (GroupNorm) divides channels into groups and normalizes within each group.</p>
<div class="highlight"><pre><span></span><code><span class="nx">Input</span><span class="p">:</span><span class="w"> </span><span class="p">(</span><span class="nx">N</span><span class="p">,</span><span class="w"> </span><span class="nx">C</span><span class="p">,</span><span class="w"> </span><span class="nx">H</span><span class="p">,</span><span class="w"> </span><span class="nx">W</span><span class="p">)</span>
<span class="nx">Groups</span><span class="p">:</span><span class="w"> </span><span class="nx">G</span>

<span class="nx">Split</span><span class="w"> </span><span class="nx">C</span><span class="w"> </span><span class="nx">channels</span><span class="w"> </span><span class="nx">into</span><span class="w"> </span><span class="nx">G</span><span class="w"> </span><span class="nx">groups</span><span class="w"> </span><span class="nx">of</span><span class="w"> </span><span class="p">(</span><span class="nx">C</span><span class="o">/</span><span class="nx">G</span><span class="p">)</span><span class="w"> </span><span class="nx">channels</span><span class="w"> </span><span class="nx">each</span>
<span class="nx">Normalize</span><span class="w"> </span><span class="nx">each</span><span class="w"> </span><span class="nx">group</span><span class="w"> </span><span class="nx">separately</span>

<span class="nx">Special</span><span class="w"> </span><span class="nx">cases</span><span class="p">:</span>
<span class="w">  </span><span class="nx">G</span><span class="w"> </span><span class="p">=</span><span class="w"> </span><span class="mi">1</span><span class="w">     </span><span class="err">â†’</span><span class="w"> </span><span class="nx">Layer</span><span class="w"> </span><span class="nx">Normalization</span><span class="w"> </span><span class="p">(</span><span class="nx">one</span><span class="w"> </span><span class="nx">group</span><span class="w"> </span><span class="p">=</span><span class="w"> </span><span class="nx">all</span><span class="w"> </span><span class="nx">channels</span><span class="p">)</span>
<span class="w">  </span><span class="nx">G</span><span class="w"> </span><span class="p">=</span><span class="w"> </span><span class="nx">C</span><span class="w">     </span><span class="err">â†’</span><span class="w"> </span><span class="nx">Instance</span><span class="w"> </span><span class="nx">Normalization</span><span class="w"> </span><span class="p">(</span><span class="nx">each</span><span class="w"> </span><span class="nx">channel</span><span class="w"> </span><span class="k">is</span><span class="w"> </span><span class="nx">a</span><span class="w"> </span><span class="nx">group</span><span class="p">)</span>
<span class="w">  </span><span class="nx">G</span><span class="w"> </span><span class="p">=</span><span class="w"> </span><span class="mi">32</span><span class="w">    </span><span class="err">â†’</span><span class="w"> </span><span class="nx">Common</span><span class="w"> </span><span class="kd">choice</span><span class="w"> </span><span class="p">(</span><span class="nx">Wu</span><span class="w"> </span><span class="o">&amp;</span><span class="w"> </span><span class="nx">He</span><span class="p">,</span><span class="w"> </span><span class="mi">2018</span><span class="p">)</span>
</code></pre></div>

<p><strong>Visualization</strong>:</p>
<div class="highlight"><pre><span></span><code><span class="n">Channels</span><span class="p">:</span><span class="w"> </span><span class="p">[</span><span class="n">c0</span><span class="p">,</span><span class="w"> </span><span class="n">c1</span><span class="p">,</span><span class="w"> </span><span class="n">c2</span><span class="p">,</span><span class="w"> </span><span class="n">c3</span><span class="p">,</span><span class="w"> </span><span class="n">c4</span><span class="p">,</span><span class="w"> </span><span class="n">c5</span><span class="p">,</span><span class="w"> </span><span class="n">c6</span><span class="p">,</span><span class="w"> </span><span class="n">c7</span><span class="p">]</span>
<span class="n">Groups</span><span class="w"> </span><span class="p">(</span><span class="n">G</span><span class="o">=</span><span class="mi">4</span><span class="p">):</span><span class="w"> </span><span class="p">[</span><span class="n">c0</span><span class="p">,</span><span class="n">c1</span><span class="p">]</span><span class="w"> </span><span class="p">[</span><span class="n">c2</span><span class="p">,</span><span class="n">c3</span><span class="p">]</span><span class="w"> </span><span class="p">[</span><span class="n">c4</span><span class="p">,</span><span class="n">c5</span><span class="p">]</span><span class="w"> </span><span class="p">[</span><span class="n">c6</span><span class="p">,</span><span class="n">c7</span><span class="p">]</span>

<span class="n">For</span><span class="w"> </span><span class="n">each</span><span class="w"> </span><span class="n">sample</span><span class="w"> </span><span class="ow">in</span><span class="w"> </span><span class="n">batch</span><span class="p">:</span>
<span class="w">  </span><span class="n">For</span><span class="w"> </span><span class="n">each</span><span class="w"> </span><span class="n">group</span><span class="p">:</span>
<span class="w">    </span><span class="n">Calculate</span><span class="w"> </span><span class="n">mean</span><span class="o">/</span><span class="k">var</span><span class="w"> </span><span class="n">across</span><span class="w"> </span><span class="p">(</span><span class="n">C</span><span class="o">/</span><span class="n">G</span><span class="p">,</span><span class="w"> </span><span class="n">H</span><span class="p">,</span><span class="w"> </span><span class="n">W</span><span class="p">)</span>
<span class="w">    </span><span class="n">Normalize</span>
</code></pre></div>

<h3 id="42-formula">4.2 Formula<a class="header-link" href="#42-formula" title="Permanent link">&para;</a></h3>
<div class="highlight"><pre><span></span><code>For each sample n, group g:
  Î¼â‚™,â‚˜ = (1/(C/G Â· H Â· W)) Î£ x_n,g,h,w
  ÏƒÂ²â‚™,â‚˜ = (1/(C/G Â· H Â· W)) Î£ (x_n,g,h,w - Î¼â‚™,â‚˜)Â²
  xÌ‚_n,g,h,w = (x_n,g,h,w - Î¼â‚™,â‚˜) / âˆš(ÏƒÂ²â‚™,â‚˜ + Îµ)
  y_n,c,h,w = Î³_c Â· xÌ‚_n,c,h,w + Î²_c
</code></pre></div>

<h3 id="43-pytorch-groupnorm">4.3 PyTorch GroupNorm<a class="header-link" href="#43-pytorch-groupnorm" title="Permanent link">&para;</a></h3>
<div class="highlight"><pre><span></span><code><span class="kn">import</span><span class="w"> </span><span class="nn">torch</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">torch.nn</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">nn</span>

<span class="c1"># GroupNorm with 32 groups (common choice)</span>
<span class="n">gn</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">GroupNorm</span><span class="p">(</span><span class="n">num_groups</span><span class="o">=</span><span class="mi">32</span><span class="p">,</span> <span class="n">num_channels</span><span class="o">=</span><span class="mi">64</span><span class="p">)</span>

<span class="c1"># Example: ResNet block with GroupNorm</span>
<span class="k">class</span><span class="w"> </span><span class="nc">ResNetBlock</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="k">def</span><span class="w"> </span><span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">channels</span><span class="p">,</span> <span class="n">groups</span><span class="o">=</span><span class="mi">32</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">conv1</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Conv2d</span><span class="p">(</span><span class="n">channels</span><span class="p">,</span> <span class="n">channels</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="n">padding</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">gn1</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">GroupNorm</span><span class="p">(</span><span class="n">groups</span><span class="p">,</span> <span class="n">channels</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">conv2</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Conv2d</span><span class="p">(</span><span class="n">channels</span><span class="p">,</span> <span class="n">channels</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="n">padding</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">gn2</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">GroupNorm</span><span class="p">(</span><span class="n">groups</span><span class="p">,</span> <span class="n">channels</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">relu</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">ReLU</span><span class="p">(</span><span class="n">inplace</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>

    <span class="k">def</span><span class="w"> </span><span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
        <span class="n">identity</span> <span class="o">=</span> <span class="n">x</span>

        <span class="n">out</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">conv1</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="n">out</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">gn1</span><span class="p">(</span><span class="n">out</span><span class="p">)</span>
        <span class="n">out</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">relu</span><span class="p">(</span><span class="n">out</span><span class="p">)</span>

        <span class="n">out</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">conv2</span><span class="p">(</span><span class="n">out</span><span class="p">)</span>
        <span class="n">out</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">gn2</span><span class="p">(</span><span class="n">out</span><span class="p">)</span>

        <span class="n">out</span> <span class="o">+=</span> <span class="n">identity</span>
        <span class="n">out</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">relu</span><span class="p">(</span><span class="n">out</span><span class="p">)</span>

        <span class="k">return</span> <span class="n">out</span>

<span class="c1"># Usage</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">ResNetBlock</span><span class="p">(</span><span class="n">channels</span><span class="o">=</span><span class="mi">64</span><span class="p">,</span> <span class="n">groups</span><span class="o">=</span><span class="mi">32</span><span class="p">)</span>
<span class="n">x</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">4</span><span class="p">,</span> <span class="mi">64</span><span class="p">,</span> <span class="mi">56</span><span class="p">,</span> <span class="mi">56</span><span class="p">)</span>  <span class="c1"># Small batch size!</span>
<span class="n">out</span> <span class="o">=</span> <span class="n">model</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">out</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>  <span class="c1"># torch.Size([4, 64, 56, 56])</span>
</code></pre></div>

<h3 id="44-choosing-number-of-groups">4.4 Choosing Number of Groups<a class="header-link" href="#44-choosing-number-of-groups" title="Permanent link">&para;</a></h3>
<div class="highlight"><pre><span></span><code><span class="kn">import</span><span class="w"> </span><span class="nn">torch</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">torch.nn</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">nn</span>

<span class="c1"># Rule: num_channels must be divisible by num_groups</span>

<span class="c1"># Common configurations</span>
<span class="n">configs</span> <span class="o">=</span> <span class="p">[</span>
    <span class="p">(</span><span class="mi">64</span><span class="p">,</span> <span class="mi">32</span><span class="p">),</span>   <span class="c1"># 64 channels, 32 groups â†’ 2 channels/group</span>
    <span class="p">(</span><span class="mi">128</span><span class="p">,</span> <span class="mi">32</span><span class="p">),</span>  <span class="c1"># 128 channels, 32 groups â†’ 4 channels/group</span>
    <span class="p">(</span><span class="mi">256</span><span class="p">,</span> <span class="mi">32</span><span class="p">),</span>  <span class="c1"># 256 channels, 32 groups â†’ 8 channels/group</span>
<span class="p">]</span>

<span class="k">for</span> <span class="n">channels</span><span class="p">,</span> <span class="n">groups</span> <span class="ow">in</span> <span class="n">configs</span><span class="p">:</span>
    <span class="n">gn</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">GroupNorm</span><span class="p">(</span><span class="n">groups</span><span class="p">,</span> <span class="n">channels</span><span class="p">)</span>
    <span class="n">x</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="n">channels</span><span class="p">,</span> <span class="mi">16</span><span class="p">,</span> <span class="mi">16</span><span class="p">)</span>  <span class="c1"># Small batch!</span>
    <span class="n">out</span> <span class="o">=</span> <span class="n">gn</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="n">channels</span><span class="si">}</span><span class="s2"> channels, </span><span class="si">{</span><span class="n">groups</span><span class="si">}</span><span class="s2"> groups â†’ &quot;</span>
          <span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="n">channels</span><span class="w"> </span><span class="o">//</span><span class="w"> </span><span class="n">groups</span><span class="si">}</span><span class="s2"> channels/group, shape: </span><span class="si">{</span><span class="n">out</span><span class="o">.</span><span class="n">shape</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>

<span class="c1"># Special cases</span>
<span class="n">gn_layer</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">GroupNorm</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">64</span><span class="p">)</span>      <span class="c1"># G=1 â†’ LayerNorm behavior</span>
<span class="n">gn_instance</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">GroupNorm</span><span class="p">(</span><span class="mi">64</span><span class="p">,</span> <span class="mi">64</span><span class="p">)</span>  <span class="c1"># G=C â†’ InstanceNorm behavior</span>
</code></pre></div>

<h3 id="45-use-cases">4.5 Use Cases<a class="header-link" href="#45-use-cases" title="Permanent link">&para;</a></h3>
<p><strong>Best for</strong>:
- Object detection (Mask R-CNN, Faster R-CNN)
- Image segmentation
- Small batch sizes (batch size = 1, 2, 4)
- Transfer learning with frozen BatchNorm
- Scenarios where BatchNorm statistics are unreliable</p>
<p><strong>Performance</strong>:
- COCO object detection: GroupNorm matches BatchNorm with large batches
- With small batches (1-4): GroupNorm significantly outperforms BatchNorm</p>
<hr />
<h2 id="5-instance-normalization">5. Instance Normalization<a class="header-link" href="#5-instance-normalization" title="Permanent link">&para;</a></h2>
<h3 id="51-core-concept">5.1 Core Concept<a class="header-link" href="#51-core-concept" title="Permanent link">&para;</a></h3>
<p><strong>Instance Normalization</strong> (InstanceNorm) normalizes each channel of each sample independently.</p>
<div class="highlight"><pre><span></span><code><span class="n">For</span><span class="w"> </span><span class="n">each</span><span class="w"> </span><span class="n">sample</span><span class="p">,</span><span class="w"> </span><span class="k">for</span><span class="w"> </span><span class="n">each</span><span class="w"> </span><span class="n">channel</span><span class="p">:</span>
<span class="w">  </span><span class="n">Calculate</span><span class="w"> </span><span class="n">mean</span><span class="w"> </span><span class="ow">and</span><span class="w"> </span><span class="n">variance</span><span class="w"> </span><span class="n">across</span><span class="w"> </span><span class="n">spatial</span><span class="w"> </span><span class="n">dimensions</span><span class="w"> </span><span class="p">(</span><span class="n">H</span><span class="p">,</span><span class="w"> </span><span class="n">W</span><span class="p">)</span>
<span class="w">  </span><span class="n">Normalize</span>
</code></pre></div>

<p><strong>Equivalent to GroupNorm with G = C</strong> (each channel is its own group).</p>
<h3 id="52-formula">5.2 Formula<a class="header-link" href="#52-formula" title="Permanent link">&para;</a></h3>
<div class="highlight"><pre><span></span><code>For each sample n, channel c:
  Î¼â‚™,c = (1/(H Â· W)) Î£ x_n,c,h,w
  ÏƒÂ²â‚™,c = (1/(H Â· W)) Î£ (x_n,c,h,w - Î¼â‚™,c)Â²
  xÌ‚_n,c,h,w = (x_n,c,h,w - Î¼â‚™,c) / âˆš(ÏƒÂ²â‚™,c + Îµ)
  y_n,c,h,w = Î³_c Â· xÌ‚_n,c,h,w + Î²_c
</code></pre></div>

<h3 id="53-pytorch-instancenorm">5.3 PyTorch InstanceNorm<a class="header-link" href="#53-pytorch-instancenorm" title="Permanent link">&para;</a></h3>
<div class="highlight"><pre><span></span><code><span class="kn">import</span><span class="w"> </span><span class="nn">torch</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">torch.nn</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">nn</span>

<span class="c1"># For 2D images</span>
<span class="n">in2d</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">InstanceNorm2d</span><span class="p">(</span><span class="mi">64</span><span class="p">)</span>

<span class="c1"># For 1D sequences</span>
<span class="n">in1d</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">InstanceNorm1d</span><span class="p">(</span><span class="mi">128</span><span class="p">)</span>

<span class="c1"># Example: Style Transfer Network</span>
<span class="k">class</span><span class="w"> </span><span class="nc">StyleTransferBlock</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="k">def</span><span class="w"> </span><span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">channels</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">conv</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Conv2d</span><span class="p">(</span><span class="n">channels</span><span class="p">,</span> <span class="n">channels</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="n">padding</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">norm</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">InstanceNorm2d</span><span class="p">(</span><span class="n">channels</span><span class="p">,</span> <span class="n">affine</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">relu</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">ReLU</span><span class="p">(</span><span class="n">inplace</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>

    <span class="k">def</span><span class="w"> </span><span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
        <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">conv</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">norm</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">relu</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">x</span>

<span class="c1"># Usage</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">StyleTransferBlock</span><span class="p">(</span><span class="mi">64</span><span class="p">)</span>
<span class="n">x</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">64</span><span class="p">,</span> <span class="mi">256</span><span class="p">,</span> <span class="mi">256</span><span class="p">)</span>  <span class="c1"># Batch size = 1 is fine!</span>
<span class="n">out</span> <span class="o">=</span> <span class="n">model</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">out</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>  <span class="c1"># torch.Size([1, 64, 256, 256])</span>
</code></pre></div>

<h3 id="54-why-instance-normalization">5.4 Why Instance Normalization?<a class="header-link" href="#54-why-instance-normalization" title="Permanent link">&para;</a></h3>
<p><strong>Key insight</strong>: For style transfer, we want to normalize out instance-specific contrast information.</p>
<p><strong>Example</strong>:</p>
<div class="highlight"><pre><span></span><code><span class="kn">import</span><span class="w"> </span><span class="nn">torch</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">torch.nn</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">nn</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">torchvision</span><span class="w"> </span><span class="kn">import</span> <span class="n">transforms</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">PIL</span><span class="w"> </span><span class="kn">import</span> <span class="n">Image</span>

<span class="c1"># Load content and style images</span>
<span class="n">content</span> <span class="o">=</span> <span class="n">Image</span><span class="o">.</span><span class="n">open</span><span class="p">(</span><span class="s1">&#39;content.jpg&#39;</span><span class="p">)</span>
<span class="n">style</span> <span class="o">=</span> <span class="n">Image</span><span class="o">.</span><span class="n">open</span><span class="p">(</span><span class="s1">&#39;style.jpg&#39;</span><span class="p">)</span>

<span class="c1"># Style transfer with InstanceNorm</span>
<span class="k">class</span><span class="w"> </span><span class="nc">FastStyleTransfer</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="k">def</span><span class="w"> </span><span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
        <span class="c1"># Encoder</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">encoder</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Sequential</span><span class="p">(</span>
            <span class="n">nn</span><span class="o">.</span><span class="n">Conv2d</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="mi">32</span><span class="p">,</span> <span class="mi">9</span><span class="p">,</span> <span class="n">padding</span><span class="o">=</span><span class="mi">4</span><span class="p">),</span>
            <span class="n">nn</span><span class="o">.</span><span class="n">InstanceNorm2d</span><span class="p">(</span><span class="mi">32</span><span class="p">,</span> <span class="n">affine</span><span class="o">=</span><span class="kc">True</span><span class="p">),</span>
            <span class="n">nn</span><span class="o">.</span><span class="n">ReLU</span><span class="p">(</span><span class="n">inplace</span><span class="o">=</span><span class="kc">True</span><span class="p">),</span>
            <span class="n">nn</span><span class="o">.</span><span class="n">Conv2d</span><span class="p">(</span><span class="mi">32</span><span class="p">,</span> <span class="mi">64</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="n">stride</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">padding</span><span class="o">=</span><span class="mi">1</span><span class="p">),</span>
            <span class="n">nn</span><span class="o">.</span><span class="n">InstanceNorm2d</span><span class="p">(</span><span class="mi">64</span><span class="p">,</span> <span class="n">affine</span><span class="o">=</span><span class="kc">True</span><span class="p">),</span>
            <span class="n">nn</span><span class="o">.</span><span class="n">ReLU</span><span class="p">(</span><span class="n">inplace</span><span class="o">=</span><span class="kc">True</span><span class="p">),</span>
        <span class="p">)</span>

        <span class="c1"># Residual blocks with InstanceNorm</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">residual</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Sequential</span><span class="p">(</span>
            <span class="o">*</span><span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">_residual_block</span><span class="p">(</span><span class="mi">64</span><span class="p">)</span> <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">5</span><span class="p">)]</span>
        <span class="p">)</span>

        <span class="c1"># Decoder</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">decoder</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Sequential</span><span class="p">(</span>
            <span class="n">nn</span><span class="o">.</span><span class="n">ConvTranspose2d</span><span class="p">(</span><span class="mi">64</span><span class="p">,</span> <span class="mi">32</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="n">stride</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">padding</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">output_padding</span><span class="o">=</span><span class="mi">1</span><span class="p">),</span>
            <span class="n">nn</span><span class="o">.</span><span class="n">InstanceNorm2d</span><span class="p">(</span><span class="mi">32</span><span class="p">,</span> <span class="n">affine</span><span class="o">=</span><span class="kc">True</span><span class="p">),</span>
            <span class="n">nn</span><span class="o">.</span><span class="n">ReLU</span><span class="p">(</span><span class="n">inplace</span><span class="o">=</span><span class="kc">True</span><span class="p">),</span>
            <span class="n">nn</span><span class="o">.</span><span class="n">Conv2d</span><span class="p">(</span><span class="mi">32</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">9</span><span class="p">,</span> <span class="n">padding</span><span class="o">=</span><span class="mi">4</span><span class="p">),</span>
            <span class="n">nn</span><span class="o">.</span><span class="n">Tanh</span><span class="p">()</span>
        <span class="p">)</span>

    <span class="k">def</span><span class="w"> </span><span class="nf">_residual_block</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">channels</span><span class="p">):</span>
        <span class="k">return</span> <span class="n">nn</span><span class="o">.</span><span class="n">Sequential</span><span class="p">(</span>
            <span class="n">nn</span><span class="o">.</span><span class="n">Conv2d</span><span class="p">(</span><span class="n">channels</span><span class="p">,</span> <span class="n">channels</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="n">padding</span><span class="o">=</span><span class="mi">1</span><span class="p">),</span>
            <span class="n">nn</span><span class="o">.</span><span class="n">InstanceNorm2d</span><span class="p">(</span><span class="n">channels</span><span class="p">,</span> <span class="n">affine</span><span class="o">=</span><span class="kc">True</span><span class="p">),</span>
            <span class="n">nn</span><span class="o">.</span><span class="n">ReLU</span><span class="p">(</span><span class="n">inplace</span><span class="o">=</span><span class="kc">True</span><span class="p">),</span>
            <span class="n">nn</span><span class="o">.</span><span class="n">Conv2d</span><span class="p">(</span><span class="n">channels</span><span class="p">,</span> <span class="n">channels</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="n">padding</span><span class="o">=</span><span class="mi">1</span><span class="p">),</span>
            <span class="n">nn</span><span class="o">.</span><span class="n">InstanceNorm2d</span><span class="p">(</span><span class="n">channels</span><span class="p">,</span> <span class="n">affine</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
        <span class="p">)</span>

    <span class="k">def</span><span class="w"> </span><span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
        <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">encoder</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">residual</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">decoder</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">x</span>
</code></pre></div>

<h3 id="55-use-cases">5.5 Use Cases<a class="header-link" href="#55-use-cases" title="Permanent link">&para;</a></h3>
<p><strong>Best for</strong>:
- Style transfer (neural style, fast style transfer)
- Image-to-image translation (pix2pix, CycleGAN)
- Generative models (GANs for image synthesis)
- Texture synthesis</p>
<hr />
<h2 id="6-rmsnorm-root-mean-square-normalization">6. RMSNorm (Root Mean Square Normalization)<a class="header-link" href="#6-rmsnorm-root-mean-square-normalization" title="Permanent link">&para;</a></h2>
<h3 id="61-core-concept">6.1 Core Concept<a class="header-link" href="#61-core-concept" title="Permanent link">&para;</a></h3>
<p><strong>RMSNorm</strong> simplifies LayerNorm by removing mean centering, normalizing only by the root mean square.</p>
<p><strong>Key difference</strong>:</p>
<div class="highlight"><pre><span></span><code><span class="n">LayerNorm</span><span class="o">:</span><span class="w">  </span><span class="n">x</span><span class="err">Ì‚</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="o">(</span><span class="n">x</span><span class="w"> </span><span class="o">-</span><span class="w"> </span><span class="err">Î¼</span><span class="o">)</span><span class="w"> </span><span class="o">/</span><span class="w"> </span><span class="err">Ïƒ</span><span class="w">           </span><span class="err">#</span><span class="w"> </span><span class="n">center</span><span class="w"> </span><span class="n">then</span><span class="w"> </span><span class="n">scale</span>
<span class="n">RMSNorm</span><span class="o">:</span><span class="w">    </span><span class="n">x</span><span class="err">Ì‚</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">x</span><span class="w"> </span><span class="o">/</span><span class="w"> </span><span class="n">RMS</span><span class="o">(</span><span class="n">x</span><span class="o">)</span><span class="w">            </span><span class="err">#</span><span class="w"> </span><span class="n">scale</span><span class="w"> </span><span class="n">only</span>
</code></pre></div>

<p><strong>Formula</strong>:</p>
<div class="highlight"><pre><span></span><code>RMS(x) = âˆš((1/n) Î£ xáµ¢Â²)
xÌ‚áµ¢ = xáµ¢ / RMS(x)
yáµ¢ = Î³ Â· xÌ‚áµ¢                            # scale (no bias Î²)
</code></pre></div>

<h3 id="62-why-rmsnorm">6.2 Why RMSNorm?<a class="header-link" href="#62-why-rmsnorm" title="Permanent link">&para;</a></h3>
<p><strong>Advantages</strong>:
1. <strong>Simpler computation</strong> â€” no mean calculation or subtraction
2. <strong>Faster</strong> â€” ~10-15% speedup in large models
3. <strong>Similar performance</strong> â€” empirically matches LayerNorm
4. <strong>Widely adopted</strong> â€” LLaMA, LLaMA 2, LLaMA 3, Gemma, Mistral</p>
<p><strong>Computational savings</strong>:</p>
<div class="highlight"><pre><span></span><code><span class="n">LayerNorm</span><span class="o">:</span><span class="w">  </span><span class="mi">2</span><span class="w"> </span><span class="n">passes</span><span class="w"> </span><span class="o">(</span><span class="n">mean</span><span class="o">,</span><span class="w"> </span><span class="n">then</span><span class="w"> </span><span class="n">variance</span><span class="o">)</span><span class="w"> </span><span class="o">+</span><span class="w"> </span><span class="mi">2</span><span class="w"> </span><span class="n">ops</span><span class="w"> </span><span class="o">(</span><span class="n">subtract</span><span class="o">,</span><span class="w"> </span><span class="n">divide</span><span class="o">)</span>
<span class="n">RMSNorm</span><span class="o">:</span><span class="w">    </span><span class="mi">1</span><span class="w"> </span><span class="n">pass</span><span class="w"> </span><span class="o">(</span><span class="n">RMS</span><span class="o">)</span><span class="w"> </span><span class="o">+</span><span class="w"> </span><span class="mi">1</span><span class="w"> </span><span class="n">op</span><span class="w"> </span><span class="o">(</span><span class="n">divide</span><span class="o">)</span>
</code></pre></div>

<h3 id="63-manual-implementation">6.3 Manual Implementation<a class="header-link" href="#63-manual-implementation" title="Permanent link">&para;</a></h3>
<div class="highlight"><pre><span></span><code><span class="kn">import</span><span class="w"> </span><span class="nn">torch</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">torch.nn</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">nn</span>

<span class="k">class</span><span class="w"> </span><span class="nc">RMSNorm</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="k">def</span><span class="w"> </span><span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">dim</span><span class="p">,</span> <span class="n">eps</span><span class="o">=</span><span class="mf">1e-6</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">eps</span> <span class="o">=</span> <span class="n">eps</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">weight</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Parameter</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">ones</span><span class="p">(</span><span class="n">dim</span><span class="p">))</span>

    <span class="k">def</span><span class="w"> </span><span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
        <span class="c1"># x shape: (..., dim)</span>

        <span class="c1"># Calculate RMS</span>
        <span class="n">rms</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">x</span> <span class="o">**</span> <span class="mi">2</span><span class="p">,</span> <span class="n">dim</span><span class="o">=-</span><span class="mi">1</span><span class="p">,</span> <span class="n">keepdim</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span> <span class="o">+</span> <span class="bp">self</span><span class="o">.</span><span class="n">eps</span><span class="p">)</span>

        <span class="c1"># Normalize and scale</span>
        <span class="n">x_norm</span> <span class="o">=</span> <span class="n">x</span> <span class="o">/</span> <span class="n">rms</span>
        <span class="n">out</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">weight</span> <span class="o">*</span> <span class="n">x_norm</span>

        <span class="k">return</span> <span class="n">out</span>

<span class="c1"># Test</span>
<span class="n">rms_norm</span> <span class="o">=</span> <span class="n">RMSNorm</span><span class="p">(</span><span class="mi">512</span><span class="p">)</span>
<span class="n">x</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">32</span><span class="p">,</span> <span class="mi">100</span><span class="p">,</span> <span class="mi">512</span><span class="p">)</span>  <span class="c1"># (batch, seq, features)</span>
<span class="n">out</span> <span class="o">=</span> <span class="n">rms_norm</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>

<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Output shape: </span><span class="si">{</span><span class="n">out</span><span class="o">.</span><span class="n">shape</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;RMS: </span><span class="si">{</span><span class="n">torch</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">out</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="w"> </span><span class="o">**</span><span class="w"> </span><span class="mi">2</span><span class="p">))</span><span class="o">.</span><span class="n">item</span><span class="p">()</span><span class="si">:</span><span class="s2">.6f</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>  <span class="c1"># Should be ~1.0</span>
</code></pre></div>

<h3 id="64-comparison-with-layernorm">6.4 Comparison with LayerNorm<a class="header-link" href="#64-comparison-with-layernorm" title="Permanent link">&para;</a></h3>
<div class="highlight"><pre><span></span><code><span class="kn">import</span><span class="w"> </span><span class="nn">torch</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">torch.nn</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">nn</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">time</span>

<span class="k">class</span><span class="w"> </span><span class="nc">RMSNorm</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="k">def</span><span class="w"> </span><span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">dim</span><span class="p">,</span> <span class="n">eps</span><span class="o">=</span><span class="mf">1e-6</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">eps</span> <span class="o">=</span> <span class="n">eps</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">weight</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Parameter</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">ones</span><span class="p">(</span><span class="n">dim</span><span class="p">))</span>

    <span class="k">def</span><span class="w"> </span><span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
        <span class="n">rms</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">x</span> <span class="o">**</span> <span class="mi">2</span><span class="p">,</span> <span class="n">dim</span><span class="o">=-</span><span class="mi">1</span><span class="p">,</span> <span class="n">keepdim</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span> <span class="o">+</span> <span class="bp">self</span><span class="o">.</span><span class="n">eps</span><span class="p">)</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">weight</span> <span class="o">*</span> <span class="p">(</span><span class="n">x</span> <span class="o">/</span> <span class="n">rms</span><span class="p">)</span>

<span class="c1"># Benchmark</span>
<span class="n">dim</span> <span class="o">=</span> <span class="mi">4096</span>
<span class="n">seq_len</span> <span class="o">=</span> <span class="mi">2048</span>
<span class="n">batch_size</span> <span class="o">=</span> <span class="mi">8</span>

<span class="n">x</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="n">batch_size</span><span class="p">,</span> <span class="n">seq_len</span><span class="p">,</span> <span class="n">dim</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="s1">&#39;cuda&#39;</span><span class="p">)</span>

<span class="n">layer_norm</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">LayerNorm</span><span class="p">(</span><span class="n">dim</span><span class="p">)</span><span class="o">.</span><span class="n">cuda</span><span class="p">()</span>
<span class="n">rms_norm</span> <span class="o">=</span> <span class="n">RMSNorm</span><span class="p">(</span><span class="n">dim</span><span class="p">)</span><span class="o">.</span><span class="n">cuda</span><span class="p">()</span>

<span class="c1"># Warmup</span>
<span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">10</span><span class="p">):</span>
    <span class="n">_</span> <span class="o">=</span> <span class="n">layer_norm</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
    <span class="n">_</span> <span class="o">=</span> <span class="n">rms_norm</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>

<span class="c1"># LayerNorm timing</span>
<span class="n">torch</span><span class="o">.</span><span class="n">cuda</span><span class="o">.</span><span class="n">synchronize</span><span class="p">()</span>
<span class="n">start</span> <span class="o">=</span> <span class="n">time</span><span class="o">.</span><span class="n">time</span><span class="p">()</span>
<span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">100</span><span class="p">):</span>
    <span class="n">_</span> <span class="o">=</span> <span class="n">layer_norm</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
<span class="n">torch</span><span class="o">.</span><span class="n">cuda</span><span class="o">.</span><span class="n">synchronize</span><span class="p">()</span>
<span class="n">ln_time</span> <span class="o">=</span> <span class="n">time</span><span class="o">.</span><span class="n">time</span><span class="p">()</span> <span class="o">-</span> <span class="n">start</span>

<span class="c1"># RMSNorm timing</span>
<span class="n">torch</span><span class="o">.</span><span class="n">cuda</span><span class="o">.</span><span class="n">synchronize</span><span class="p">()</span>
<span class="n">start</span> <span class="o">=</span> <span class="n">time</span><span class="o">.</span><span class="n">time</span><span class="p">()</span>
<span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">100</span><span class="p">):</span>
    <span class="n">_</span> <span class="o">=</span> <span class="n">rms_norm</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
<span class="n">torch</span><span class="o">.</span><span class="n">cuda</span><span class="o">.</span><span class="n">synchronize</span><span class="p">()</span>
<span class="n">rms_time</span> <span class="o">=</span> <span class="n">time</span><span class="o">.</span><span class="n">time</span><span class="p">()</span> <span class="o">-</span> <span class="n">start</span>

<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;LayerNorm: </span><span class="si">{</span><span class="n">ln_time</span><span class="si">:</span><span class="s2">.4f</span><span class="si">}</span><span class="s2">s&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;RMSNorm:   </span><span class="si">{</span><span class="n">rms_time</span><span class="si">:</span><span class="s2">.4f</span><span class="si">}</span><span class="s2">s&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Speedup:   </span><span class="si">{</span><span class="n">ln_time</span><span class="w"> </span><span class="o">/</span><span class="w"> </span><span class="n">rms_time</span><span class="si">:</span><span class="s2">.2f</span><span class="si">}</span><span class="s2">x&quot;</span><span class="p">)</span>
</code></pre></div>

<h3 id="65-rmsnorm-in-llama">6.5 RMSNorm in LLaMA<a class="header-link" href="#65-rmsnorm-in-llama" title="Permanent link">&para;</a></h3>
<div class="highlight"><pre><span></span><code><span class="kn">import</span><span class="w"> </span><span class="nn">torch</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">torch.nn</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">nn</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">torch.nn.functional</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">F</span>

<span class="k">class</span><span class="w"> </span><span class="nc">LLaMATransformerBlock</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;LLaMA-style transformer block with RMSNorm.&quot;&quot;&quot;</span>

    <span class="k">def</span><span class="w"> </span><span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">dim</span><span class="p">,</span> <span class="n">num_heads</span><span class="p">,</span> <span class="n">mlp_ratio</span><span class="o">=</span><span class="mi">4</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>

        <span class="c1"># Pre-normalization with RMSNorm</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">attn_norm</span> <span class="o">=</span> <span class="n">RMSNorm</span><span class="p">(</span><span class="n">dim</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">attn</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">MultiheadAttention</span><span class="p">(</span><span class="n">dim</span><span class="p">,</span> <span class="n">num_heads</span><span class="p">,</span> <span class="n">batch_first</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">ffn_norm</span> <span class="o">=</span> <span class="n">RMSNorm</span><span class="p">(</span><span class="n">dim</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">ffn</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Sequential</span><span class="p">(</span>
            <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">dim</span><span class="p">,</span> <span class="n">mlp_ratio</span> <span class="o">*</span> <span class="n">dim</span><span class="p">,</span> <span class="n">bias</span><span class="o">=</span><span class="kc">False</span><span class="p">),</span>
            <span class="n">nn</span><span class="o">.</span><span class="n">SiLU</span><span class="p">(),</span>  <span class="c1"># LLaMA uses SiLU (Swish) activation</span>
            <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">mlp_ratio</span> <span class="o">*</span> <span class="n">dim</span><span class="p">,</span> <span class="n">dim</span><span class="p">,</span> <span class="n">bias</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
        <span class="p">)</span>

    <span class="k">def</span><span class="w"> </span><span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
        <span class="c1"># Attention with RMSNorm</span>
        <span class="n">h</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">attn_norm</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="n">h</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">attn</span><span class="p">(</span><span class="n">h</span><span class="p">,</span> <span class="n">h</span><span class="p">,</span> <span class="n">h</span><span class="p">)[</span><span class="mi">0</span><span class="p">]</span>
        <span class="n">x</span> <span class="o">=</span> <span class="n">x</span> <span class="o">+</span> <span class="n">h</span>

        <span class="c1"># FFN with RMSNorm</span>
        <span class="n">h</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">ffn_norm</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="n">h</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">ffn</span><span class="p">(</span><span class="n">h</span><span class="p">)</span>
        <span class="n">x</span> <span class="o">=</span> <span class="n">x</span> <span class="o">+</span> <span class="n">h</span>

        <span class="k">return</span> <span class="n">x</span>

<span class="c1"># Example usage</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">LLaMATransformerBlock</span><span class="p">(</span><span class="n">dim</span><span class="o">=</span><span class="mi">4096</span><span class="p">,</span> <span class="n">num_heads</span><span class="o">=</span><span class="mi">32</span><span class="p">)</span>
<span class="n">x</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2048</span><span class="p">,</span> <span class="mi">4096</span><span class="p">)</span>  <span class="c1"># (batch, seq_len, dim)</span>
<span class="n">out</span> <span class="o">=</span> <span class="n">model</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">out</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>  <span class="c1"># torch.Size([1, 2048, 4096])</span>
</code></pre></div>

<h3 id="66-use-cases">6.6 Use Cases<a class="header-link" href="#66-use-cases" title="Permanent link">&para;</a></h3>
<p><strong>Best for</strong>:
- Large language models (LLaMA, Mistral, Gemma)
- Any Transformer-based model where speed matters
- Models trained from scratch (not fine-tuning LayerNorm models)</p>
<hr />
<h2 id="7-other-normalization-techniques">7. Other Normalization Techniques<a class="header-link" href="#7-other-normalization-techniques" title="Permanent link">&para;</a></h2>
<h3 id="71-weight-normalization">7.1 Weight Normalization<a class="header-link" href="#71-weight-normalization" title="Permanent link">&para;</a></h3>
<p><strong>Weight Normalization</strong> reparameterizes weight vectors to decouple magnitude and direction.</p>
<div class="highlight"><pre><span></span><code><span class="n">Original</span><span class="o">:</span><span class="w">     </span><span class="n">w</span>
<span class="n">Reparameterized</span><span class="o">:</span><span class="w">   </span><span class="n">w</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">g</span><span class="w"> </span><span class="err">Â·</span><span class="w"> </span><span class="o">(</span><span class="n">v</span><span class="w"> </span><span class="o">/</span><span class="w"> </span><span class="o">||</span><span class="n">v</span><span class="o">||)</span>

<span class="n">g</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">scalar</span><span class="w"> </span><span class="n">magnitude</span><span class="w"> </span><span class="o">(</span><span class="n">learnable</span><span class="o">)</span>
<span class="n">v</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">direction</span><span class="w"> </span><span class="n">vector</span><span class="w"> </span><span class="o">(</span><span class="n">learnable</span><span class="o">)</span>
</code></pre></div>

<div class="highlight"><pre><span></span><code><span class="kn">import</span><span class="w"> </span><span class="nn">torch</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">torch.nn</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">nn</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">torch.nn.utils</span><span class="w"> </span><span class="kn">import</span> <span class="n">weight_norm</span>

<span class="c1"># Apply weight normalization to a layer</span>
<span class="n">linear</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="mi">128</span><span class="p">,</span> <span class="mi">64</span><span class="p">)</span>
<span class="n">linear</span> <span class="o">=</span> <span class="n">weight_norm</span><span class="p">(</span><span class="n">linear</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="s1">&#39;weight&#39;</span><span class="p">)</span>

<span class="c1"># The weight is reparameterized as: weight = g * v / ||v||</span>
<span class="nb">print</span><span class="p">(</span><span class="n">linear</span><span class="o">.</span><span class="n">weight_g</span><span class="p">)</span>  <span class="c1"># magnitude parameter</span>
<span class="nb">print</span><span class="p">(</span><span class="n">linear</span><span class="o">.</span><span class="n">weight_v</span><span class="p">)</span>  <span class="c1"># direction parameter</span>

<span class="c1"># Forward pass</span>
<span class="n">x</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">32</span><span class="p">,</span> <span class="mi">128</span><span class="p">)</span>
<span class="n">out</span> <span class="o">=</span> <span class="n">linear</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>

<span class="c1"># Remove weight normalization (merge g and v back into weight)</span>
<span class="n">linear</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">utils</span><span class="o">.</span><span class="n">remove_weight_norm</span><span class="p">(</span><span class="n">linear</span><span class="p">)</span>
</code></pre></div>

<p><strong>Use cases</strong>: RNNs, GANs, reinforcement learning (A3C)</p>
<h3 id="72-spectral-normalization">7.2 Spectral Normalization<a class="header-link" href="#72-spectral-normalization" title="Permanent link">&para;</a></h3>
<p><strong>Spectral Normalization</strong> constrains the spectral norm (largest singular value) of weight matrices to 1, stabilizing GAN training.</p>
<div class="highlight"><pre><span></span><code>Spectral norm: Ïƒ(W) = max singular value of W
Normalized weight: W_SN = W / Ïƒ(W)
</code></pre></div>

<div class="highlight"><pre><span></span><code><span class="kn">import</span><span class="w"> </span><span class="nn">torch</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">torch.nn</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">nn</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">torch.nn.utils</span><span class="w"> </span><span class="kn">import</span> <span class="n">spectral_norm</span>

<span class="c1"># Apply spectral normalization</span>
<span class="n">conv</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Conv2d</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="mi">64</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="n">padding</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
<span class="n">conv</span> <span class="o">=</span> <span class="n">spectral_norm</span><span class="p">(</span><span class="n">conv</span><span class="p">)</span>

<span class="c1"># Discriminator with Spectral Normalization (for GANs)</span>
<span class="k">class</span><span class="w"> </span><span class="nc">SNDiscriminator</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="k">def</span><span class="w"> </span><span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">model</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Sequential</span><span class="p">(</span>
            <span class="n">spectral_norm</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Conv2d</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="mi">64</span><span class="p">,</span> <span class="mi">4</span><span class="p">,</span> <span class="n">stride</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">padding</span><span class="o">=</span><span class="mi">1</span><span class="p">)),</span>
            <span class="n">nn</span><span class="o">.</span><span class="n">LeakyReLU</span><span class="p">(</span><span class="mf">0.2</span><span class="p">,</span> <span class="n">inplace</span><span class="o">=</span><span class="kc">True</span><span class="p">),</span>
            <span class="n">spectral_norm</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Conv2d</span><span class="p">(</span><span class="mi">64</span><span class="p">,</span> <span class="mi">128</span><span class="p">,</span> <span class="mi">4</span><span class="p">,</span> <span class="n">stride</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">padding</span><span class="o">=</span><span class="mi">1</span><span class="p">)),</span>
            <span class="n">nn</span><span class="o">.</span><span class="n">LeakyReLU</span><span class="p">(</span><span class="mf">0.2</span><span class="p">,</span> <span class="n">inplace</span><span class="o">=</span><span class="kc">True</span><span class="p">),</span>
            <span class="n">spectral_norm</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Conv2d</span><span class="p">(</span><span class="mi">128</span><span class="p">,</span> <span class="mi">256</span><span class="p">,</span> <span class="mi">4</span><span class="p">,</span> <span class="n">stride</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">padding</span><span class="o">=</span><span class="mi">1</span><span class="p">)),</span>
            <span class="n">nn</span><span class="o">.</span><span class="n">LeakyReLU</span><span class="p">(</span><span class="mf">0.2</span><span class="p">,</span> <span class="n">inplace</span><span class="o">=</span><span class="kc">True</span><span class="p">),</span>
            <span class="n">spectral_norm</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Conv2d</span><span class="p">(</span><span class="mi">256</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">4</span><span class="p">,</span> <span class="n">stride</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">padding</span><span class="o">=</span><span class="mi">0</span><span class="p">)),</span>
        <span class="p">)</span>

    <span class="k">def</span><span class="w"> </span><span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">model</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>

<span class="c1"># Usage</span>
<span class="n">disc</span> <span class="o">=</span> <span class="n">SNDiscriminator</span><span class="p">()</span>
<span class="n">x</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">16</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">64</span><span class="p">,</span> <span class="mi">64</span><span class="p">)</span>
<span class="n">out</span> <span class="o">=</span> <span class="n">disc</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">out</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>  <span class="c1"># torch.Size([16, 1, 5, 5])</span>
</code></pre></div>

<p><strong>Use cases</strong>: GAN discriminators (SNGAN, BigGAN, StyleGAN2)</p>
<h3 id="73-adaptive-instance-normalization-adain">7.3 Adaptive Instance Normalization (AdaIN)<a class="header-link" href="#73-adaptive-instance-normalization-adain" title="Permanent link">&para;</a></h3>
<p><strong>AdaIN</strong> adaptively adjusts InstanceNorm statistics based on style input, enabling real-time style transfer.</p>
<div class="highlight"><pre><span></span><code>AdaIN(content, style) = Ïƒ(style) Â· ((content - Î¼(content)) / Ïƒ(content)) + Î¼(style)

Transfer style statistics (mean, std) to content features
</code></pre></div>

<div class="highlight"><pre><span></span><code><span class="kn">import</span><span class="w"> </span><span class="nn">torch</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">torch.nn</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">nn</span>

<span class="k">class</span><span class="w"> </span><span class="nc">AdaIN</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="k">def</span><span class="w"> </span><span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>

    <span class="k">def</span><span class="w"> </span><span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">content</span><span class="p">,</span> <span class="n">style</span><span class="p">):</span>
        <span class="c1"># content, style: (N, C, H, W)</span>

        <span class="c1"># Calculate statistics</span>
        <span class="n">content_mean</span> <span class="o">=</span> <span class="n">content</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">dim</span><span class="o">=</span><span class="p">[</span><span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">],</span> <span class="n">keepdim</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
        <span class="n">content_std</span> <span class="o">=</span> <span class="n">content</span><span class="o">.</span><span class="n">std</span><span class="p">(</span><span class="n">dim</span><span class="o">=</span><span class="p">[</span><span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">],</span> <span class="n">keepdim</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>

        <span class="n">style_mean</span> <span class="o">=</span> <span class="n">style</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">dim</span><span class="o">=</span><span class="p">[</span><span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">],</span> <span class="n">keepdim</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
        <span class="n">style_std</span> <span class="o">=</span> <span class="n">style</span><span class="o">.</span><span class="n">std</span><span class="p">(</span><span class="n">dim</span><span class="o">=</span><span class="p">[</span><span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">],</span> <span class="n">keepdim</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>

        <span class="c1"># Normalize content, then apply style statistics</span>
        <span class="n">normalized</span> <span class="o">=</span> <span class="p">(</span><span class="n">content</span> <span class="o">-</span> <span class="n">content_mean</span><span class="p">)</span> <span class="o">/</span> <span class="p">(</span><span class="n">content_std</span> <span class="o">+</span> <span class="mf">1e-5</span><span class="p">)</span>
        <span class="n">stylized</span> <span class="o">=</span> <span class="n">normalized</span> <span class="o">*</span> <span class="n">style_std</span> <span class="o">+</span> <span class="n">style_mean</span>

        <span class="k">return</span> <span class="n">stylized</span>

<span class="c1"># Style transfer with AdaIN</span>
<span class="k">class</span><span class="w"> </span><span class="nc">StyleTransferNet</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="k">def</span><span class="w"> </span><span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">encoder</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Sequential</span><span class="p">(</span>
            <span class="n">nn</span><span class="o">.</span><span class="n">Conv2d</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="mi">64</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="n">padding</span><span class="o">=</span><span class="mi">1</span><span class="p">),</span>
            <span class="n">nn</span><span class="o">.</span><span class="n">ReLU</span><span class="p">(</span><span class="n">inplace</span><span class="o">=</span><span class="kc">True</span><span class="p">),</span>
            <span class="n">nn</span><span class="o">.</span><span class="n">Conv2d</span><span class="p">(</span><span class="mi">64</span><span class="p">,</span> <span class="mi">128</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="n">padding</span><span class="o">=</span><span class="mi">1</span><span class="p">),</span>
            <span class="n">nn</span><span class="o">.</span><span class="n">ReLU</span><span class="p">(</span><span class="n">inplace</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
        <span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">adain</span> <span class="o">=</span> <span class="n">AdaIN</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">decoder</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Sequential</span><span class="p">(</span>
            <span class="n">nn</span><span class="o">.</span><span class="n">Conv2d</span><span class="p">(</span><span class="mi">128</span><span class="p">,</span> <span class="mi">64</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="n">padding</span><span class="o">=</span><span class="mi">1</span><span class="p">),</span>
            <span class="n">nn</span><span class="o">.</span><span class="n">ReLU</span><span class="p">(</span><span class="n">inplace</span><span class="o">=</span><span class="kc">True</span><span class="p">),</span>
            <span class="n">nn</span><span class="o">.</span><span class="n">Conv2d</span><span class="p">(</span><span class="mi">64</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="n">padding</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
        <span class="p">)</span>

    <span class="k">def</span><span class="w"> </span><span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">content</span><span class="p">,</span> <span class="n">style</span><span class="p">):</span>
        <span class="n">content_feat</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">encoder</span><span class="p">(</span><span class="n">content</span><span class="p">)</span>
        <span class="n">style_feat</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">encoder</span><span class="p">(</span><span class="n">style</span><span class="p">)</span>

        <span class="c1"># AdaIN layer</span>
        <span class="n">t</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">adain</span><span class="p">(</span><span class="n">content_feat</span><span class="p">,</span> <span class="n">style_feat</span><span class="p">)</span>

        <span class="n">out</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">decoder</span><span class="p">(</span><span class="n">t</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">out</span>

<span class="c1"># Usage</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">StyleTransferNet</span><span class="p">()</span>
<span class="n">content</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">256</span><span class="p">,</span> <span class="mi">256</span><span class="p">)</span>
<span class="n">style</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">256</span><span class="p">,</span> <span class="mi">256</span><span class="p">)</span>
<span class="n">stylized</span> <span class="o">=</span> <span class="n">model</span><span class="p">(</span><span class="n">content</span><span class="p">,</span> <span class="n">style</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">stylized</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>  <span class="c1"># torch.Size([1, 3, 256, 256])</span>
</code></pre></div>

<p><strong>Use cases</strong>: Real-time style transfer, image-to-image translation</p>
<h3 id="74-comparison-table">7.4 Comparison Table<a class="header-link" href="#74-comparison-table" title="Permanent link">&para;</a></h3>
<table>
<thead>
<tr>
<th>Method</th>
<th>Normalizes Across</th>
<th>Learnable Params</th>
<th>Batch-Dependent</th>
<th>Use Case</th>
</tr>
</thead>
<tbody>
<tr>
<td><strong>Batch Norm</strong></td>
<td>(N) for each (C,H,W)</td>
<td>Î³, Î², running stats</td>
<td>Yes</td>
<td>CNNs, large batches</td>
</tr>
<tr>
<td><strong>Layer Norm</strong></td>
<td>(C,H,W) for each N</td>
<td>Î³, Î²</td>
<td>No</td>
<td>Transformers, RNNs</td>
</tr>
<tr>
<td><strong>Instance Norm</strong></td>
<td>(H,W) for each (N,C)</td>
<td>Î³, Î² (optional)</td>
<td>No</td>
<td>Style transfer, GANs</td>
</tr>
<tr>
<td><strong>Group Norm</strong></td>
<td>(C/G,H,W) for each (N,G)</td>
<td>Î³, Î²</td>
<td>No</td>
<td>Detection, small batches</td>
</tr>
<tr>
<td><strong>RMSNorm</strong></td>
<td>(C,H,W) for each N</td>
<td>Î³</td>
<td>No</td>
<td>LLMs, fast Transformers</td>
</tr>
<tr>
<td><strong>Weight Norm</strong></td>
<td>Weight vectors</td>
<td>g, v</td>
<td>No</td>
<td>RNNs, GANs</td>
</tr>
<tr>
<td><strong>Spectral Norm</strong></td>
<td>Weight matrices</td>
<td>â€”</td>
<td>No</td>
<td>GAN discriminators</td>
</tr>
<tr>
<td><strong>AdaIN</strong></td>
<td>(H,W) conditioned on style</td>
<td>â€”</td>
<td>No</td>
<td>Style transfer</td>
</tr>
</tbody>
</table>
<hr />
<h2 id="8-comprehensive-comparison">8. Comprehensive Comparison<a class="header-link" href="#8-comprehensive-comparison" title="Permanent link">&para;</a></h2>
<h3 id="81-visual-comparison">8.1 Visual Comparison<a class="header-link" href="#81-visual-comparison" title="Permanent link">&para;</a></h3>
<div class="highlight"><pre><span></span><code><span class="n">Input</span><span class="w"> </span><span class="n">tensor</span><span class="p">:</span><span class="w"> </span><span class="p">(</span><span class="n">N</span><span class="p">,</span><span class="w"> </span><span class="n">C</span><span class="p">,</span><span class="w"> </span><span class="n">H</span><span class="p">,</span><span class="w"> </span><span class="n">W</span><span class="p">)</span>
<span class="n">N</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">batch</span><span class="w"> </span><span class="p">(</span><span class="mi">4</span><span class="w"> </span><span class="n">samples</span><span class="p">)</span>
<span class="n">C</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">channels</span><span class="w"> </span><span class="p">(</span><span class="mi">3</span><span class="p">)</span>
<span class="n">H</span><span class="p">,</span><span class="w"> </span><span class="n">W</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">height</span><span class="p">,</span><span class="w"> </span><span class="n">width</span><span class="w"> </span><span class="p">(</span><span class="mi">32</span><span class="w"> </span><span class="err">Ã—</span><span class="w"> </span><span class="mi">32</span><span class="p">)</span>

<span class="err">â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”</span>
<span class="err">â”‚</span><span class="w">  </span><span class="n">Batch</span><span class="w"> </span><span class="n">Normalization</span><span class="w">                                                </span><span class="err">â”‚</span>
<span class="err">â”‚</span><span class="w">  </span><span class="err">â”Œâ”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”</span><span class="w">                                      </span><span class="err">â”‚</span>
<span class="err">â”‚</span><span class="w">  </span><span class="err">â”‚</span><span class="w"> </span><span class="n">N</span><span class="o">=</span><span class="mi">0</span><span class="w">  </span><span class="err">â”‚</span><span class="w"> </span><span class="n">N</span><span class="o">=</span><span class="mi">1</span><span class="w">  </span><span class="err">â”‚</span><span class="w"> </span><span class="n">N</span><span class="o">=</span><span class="mi">2</span><span class="w">  </span><span class="err">â”‚</span><span class="w"> </span><span class="n">N</span><span class="o">=</span><span class="mi">3</span><span class="w">  </span><span class="err">â”‚</span><span class="w">                                      </span><span class="err">â”‚</span>
<span class="err">â”‚</span><span class="w">  </span><span class="err">â”œâ”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”¤</span><span class="w">  </span><span class="n">For</span><span class="w"> </span><span class="n">each</span><span class="w"> </span><span class="p">(</span><span class="n">C</span><span class="p">,</span><span class="w"> </span><span class="n">H</span><span class="p">,</span><span class="w"> </span><span class="n">W</span><span class="p">)</span><span class="w"> </span><span class="n">position</span><span class="p">:</span><span class="w">       </span><span class="err">â”‚</span>
<span class="err">â”‚</span><span class="w">  </span><span class="err">â”‚</span><span class="w"> </span><span class="n">c</span><span class="o">=</span><span class="mi">0</span><span class="w">  </span><span class="err">â”‚</span><span class="w"> </span><span class="n">c</span><span class="o">=</span><span class="mi">0</span><span class="w">  </span><span class="err">â”‚</span><span class="w"> </span><span class="n">c</span><span class="o">=</span><span class="mi">0</span><span class="w">  </span><span class="err">â”‚</span><span class="w"> </span><span class="n">c</span><span class="o">=</span><span class="mi">0</span><span class="w">  </span><span class="err">â”‚</span><span class="w">  </span><span class="n">Calculate</span><span class="w"> </span><span class="n">mean</span><span class="o">/</span><span class="k">var</span><span class="w"> </span><span class="n">across</span><span class="w"> </span><span class="n">N</span><span class="w">        </span><span class="err">â”‚</span>
<span class="err">â”‚</span><span class="w">  </span><span class="err">â”‚</span><span class="w"> </span><span class="n">h</span><span class="p">,</span><span class="n">w</span><span class="w">  </span><span class="err">â”‚</span><span class="w"> </span><span class="n">h</span><span class="p">,</span><span class="n">w</span><span class="w">  </span><span class="err">â”‚</span><span class="w"> </span><span class="n">h</span><span class="p">,</span><span class="n">w</span><span class="w">  </span><span class="err">â”‚</span><span class="w"> </span><span class="n">h</span><span class="p">,</span><span class="n">w</span><span class="w">  </span><span class="err">â”‚</span><span class="w">  </span><span class="p">(</span><span class="mi">4</span><span class="w"> </span><span class="n">values</span><span class="p">)</span><span class="w">                          </span><span class="err">â”‚</span>
<span class="err">â”‚</span><span class="w">  </span><span class="err">â””â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”˜</span><span class="w">                                      </span><span class="err">â”‚</span>
<span class="err">â”‚</span><span class="w">  </span><span class="n">Normalize</span><span class="p">:</span><span class="w"> </span><span class="p">(</span><span class="n">x</span><span class="w"> </span><span class="o">-</span><span class="w"> </span><span class="err">Î¼</span><span class="n">_batch</span><span class="p">)</span><span class="w"> </span><span class="o">/</span><span class="w"> </span><span class="err">Ïƒ</span><span class="n">_batch</span><span class="w">                                </span><span class="err">â”‚</span>
<span class="err">â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜</span>

<span class="err">â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”</span>
<span class="err">â”‚</span><span class="w">  </span><span class="n">Layer</span><span class="w"> </span><span class="n">Normalization</span><span class="w">                                                </span><span class="err">â”‚</span>
<span class="err">â”‚</span><span class="w">  </span><span class="err">â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”</span><span class="w">                                      </span><span class="err">â”‚</span>
<span class="err">â”‚</span><span class="w">  </span><span class="err">â”‚</span><span class="w">       </span><span class="n">N</span><span class="o">=</span><span class="mi">0</span><span class="w">                 </span><span class="err">â”‚</span><span class="w">  </span><span class="n">For</span><span class="w"> </span><span class="n">each</span><span class="w"> </span><span class="n">sample</span><span class="p">:</span><span class="w">                    </span><span class="err">â”‚</span>
<span class="err">â”‚</span><span class="w">  </span><span class="err">â”œâ”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤</span><span class="w">  </span><span class="n">Calculate</span><span class="w"> </span><span class="n">mean</span><span class="o">/</span><span class="k">var</span><span class="w"> </span><span class="n">across</span><span class="w">           </span><span class="err">â”‚</span>
<span class="err">â”‚</span><span class="w">  </span><span class="err">â”‚</span><span class="w"> </span><span class="n">c</span><span class="o">=</span><span class="mi">0</span><span class="w">   </span><span class="err">â”‚</span><span class="w"> </span><span class="n">c</span><span class="o">=</span><span class="mi">1</span><span class="w">   </span><span class="err">â”‚</span><span class="w"> </span><span class="n">c</span><span class="o">=</span><span class="mi">2</span><span class="w">       </span><span class="err">â”‚</span><span class="w">  </span><span class="n">all</span><span class="w"> </span><span class="p">(</span><span class="n">C</span><span class="p">,</span><span class="w"> </span><span class="n">H</span><span class="p">,</span><span class="w"> </span><span class="n">W</span><span class="p">)</span><span class="w">                       </span><span class="err">â”‚</span>
<span class="err">â”‚</span><span class="w">  </span><span class="err">â”‚</span><span class="w"> </span><span class="p">(</span><span class="n">all</span><span class="w">  </span><span class="err">â”‚</span><span class="w"> </span><span class="p">(</span><span class="n">all</span><span class="w">  </span><span class="err">â”‚</span><span class="w"> </span><span class="p">(</span><span class="n">all</span><span class="w">      </span><span class="err">â”‚</span><span class="w">  </span><span class="p">(</span><span class="mi">3</span><span class="w"> </span><span class="err">Ã—</span><span class="w"> </span><span class="mi">32</span><span class="w"> </span><span class="err">Ã—</span><span class="w"> </span><span class="mi">32</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="mi">3072</span><span class="w"> </span><span class="n">values</span><span class="p">)</span><span class="w">         </span><span class="err">â”‚</span>
<span class="err">â”‚</span><span class="w">  </span><span class="err">â”‚</span><span class="w"> </span><span class="n">h</span><span class="p">,</span><span class="n">w</span><span class="p">)</span><span class="w">  </span><span class="err">â”‚</span><span class="w"> </span><span class="n">h</span><span class="p">,</span><span class="n">w</span><span class="p">)</span><span class="w">  </span><span class="err">â”‚</span><span class="w"> </span><span class="n">h</span><span class="p">,</span><span class="n">w</span><span class="p">)</span><span class="w">      </span><span class="err">â”‚</span><span class="w">                                      </span><span class="err">â”‚</span>
<span class="err">â”‚</span><span class="w">  </span><span class="err">â””â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜</span><span class="w">                                      </span><span class="err">â”‚</span>
<span class="err">â”‚</span><span class="w">  </span><span class="n">Normalize</span><span class="p">:</span><span class="w"> </span><span class="p">(</span><span class="n">x</span><span class="w"> </span><span class="o">-</span><span class="w"> </span><span class="err">Î¼</span><span class="n">_layer</span><span class="p">)</span><span class="w"> </span><span class="o">/</span><span class="w"> </span><span class="err">Ïƒ</span><span class="n">_layer</span><span class="w">                                </span><span class="err">â”‚</span>
<span class="err">â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜</span>

<span class="err">â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”</span>
<span class="err">â”‚</span><span class="w">  </span><span class="n">Instance</span><span class="w"> </span><span class="n">Normalization</span><span class="w">                                             </span><span class="err">â”‚</span>
<span class="err">â”‚</span><span class="w">  </span><span class="err">â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”</span><span class="w">                                                 </span><span class="err">â”‚</span>
<span class="err">â”‚</span><span class="w">  </span><span class="err">â”‚</span><span class="w">  </span><span class="n">N</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span><span class="w"> </span><span class="n">C</span><span class="o">=</span><span class="mi">0</span><span class="w">      </span><span class="err">â”‚</span><span class="w">  </span><span class="n">For</span><span class="w"> </span><span class="n">each</span><span class="w"> </span><span class="p">(</span><span class="n">sample</span><span class="p">,</span><span class="w"> </span><span class="n">channel</span><span class="p">):</span><span class="w">                    </span><span class="err">â”‚</span>
<span class="err">â”‚</span><span class="w">  </span><span class="err">â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤</span><span class="w">  </span><span class="n">Calculate</span><span class="w"> </span><span class="n">mean</span><span class="o">/</span><span class="k">var</span><span class="w"> </span><span class="n">across</span><span class="w">                      </span><span class="err">â”‚</span>
<span class="err">â”‚</span><span class="w">  </span><span class="err">â”‚</span><span class="w">   </span><span class="p">(</span><span class="n">all</span><span class="w"> </span><span class="n">h</span><span class="p">,</span><span class="n">w</span><span class="p">)</span><span class="w">    </span><span class="err">â”‚</span><span class="w">  </span><span class="p">(</span><span class="n">H</span><span class="p">,</span><span class="w"> </span><span class="n">W</span><span class="p">)</span><span class="w">                                         </span><span class="err">â”‚</span>
<span class="err">â”‚</span><span class="w">  </span><span class="err">â”‚</span><span class="w">                </span><span class="err">â”‚</span><span class="w">  </span><span class="p">(</span><span class="mi">32</span><span class="w"> </span><span class="err">Ã—</span><span class="w"> </span><span class="mi">32</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="mi">1024</span><span class="w"> </span><span class="n">values</span><span class="p">)</span><span class="w">                        </span><span class="err">â”‚</span>
<span class="err">â”‚</span><span class="w">  </span><span class="err">â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜</span><span class="w">                                                 </span><span class="err">â”‚</span>
<span class="err">â”‚</span><span class="w">  </span><span class="n">Normalize</span><span class="p">:</span><span class="w"> </span><span class="p">(</span><span class="n">x</span><span class="w"> </span><span class="o">-</span><span class="w"> </span><span class="err">Î¼</span><span class="n">_instance</span><span class="p">)</span><span class="w"> </span><span class="o">/</span><span class="w"> </span><span class="err">Ïƒ</span><span class="n">_instance</span><span class="w">                          </span><span class="err">â”‚</span>
<span class="err">â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜</span>

<span class="err">â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”</span>
<span class="err">â”‚</span><span class="w">  </span><span class="n">Group</span><span class="w"> </span><span class="n">Normalization</span><span class="w"> </span><span class="p">(</span><span class="n">G</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span><span class="w"> </span><span class="n">so</span><span class="w"> </span><span class="mi">1</span><span class="w"> </span><span class="n">channel</span><span class="w"> </span><span class="n">per</span><span class="w"> </span><span class="n">group</span><span class="p">)</span><span class="w">                 </span><span class="err">â”‚</span>
<span class="err">â”‚</span><span class="w">  </span><span class="err">â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”</span><span class="w">                                                 </span><span class="err">â”‚</span>
<span class="err">â”‚</span><span class="w">  </span><span class="err">â”‚</span><span class="w"> </span><span class="n">N</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span><span class="w"> </span><span class="n">G</span><span class="o">=</span><span class="mi">0</span><span class="w">       </span><span class="err">â”‚</span><span class="w">  </span><span class="n">For</span><span class="w"> </span><span class="n">each</span><span class="w"> </span><span class="p">(</span><span class="n">sample</span><span class="p">,</span><span class="w"> </span><span class="n">group</span><span class="p">):</span><span class="w">                      </span><span class="err">â”‚</span>
<span class="err">â”‚</span><span class="w">  </span><span class="err">â”‚</span><span class="w"> </span><span class="p">(</span><span class="n">c</span><span class="o">=</span><span class="mi">0</span><span class="w"> </span><span class="n">only</span><span class="p">)</span><span class="w">     </span><span class="err">â”‚</span><span class="w">  </span><span class="n">Calculate</span><span class="w"> </span><span class="n">mean</span><span class="o">/</span><span class="k">var</span><span class="w"> </span><span class="n">across</span><span class="w">                      </span><span class="err">â”‚</span>
<span class="err">â”‚</span><span class="w">  </span><span class="err">â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤</span><span class="w">  </span><span class="p">(</span><span class="n">C</span><span class="o">/</span><span class="n">G</span><span class="p">,</span><span class="w"> </span><span class="n">H</span><span class="p">,</span><span class="w"> </span><span class="n">W</span><span class="p">)</span><span class="w">                                    </span><span class="err">â”‚</span>
<span class="err">â”‚</span><span class="w">  </span><span class="err">â”‚</span><span class="w">   </span><span class="p">(</span><span class="n">all</span><span class="w"> </span><span class="n">h</span><span class="p">,</span><span class="n">w</span><span class="p">)</span><span class="w">    </span><span class="err">â”‚</span><span class="w">  </span><span class="p">(</span><span class="mi">1</span><span class="w"> </span><span class="err">Ã—</span><span class="w"> </span><span class="mi">32</span><span class="w"> </span><span class="err">Ã—</span><span class="w"> </span><span class="mi">32</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="mi">1024</span><span class="w"> </span><span class="n">values</span><span class="p">)</span><span class="w">                    </span><span class="err">â”‚</span>
<span class="err">â”‚</span><span class="w">  </span><span class="err">â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜</span><span class="w">                                                 </span><span class="err">â”‚</span>
<span class="err">â”‚</span><span class="w">  </span><span class="n">Normalize</span><span class="p">:</span><span class="w"> </span><span class="p">(</span><span class="n">x</span><span class="w"> </span><span class="o">-</span><span class="w"> </span><span class="err">Î¼</span><span class="n">_group</span><span class="p">)</span><span class="w"> </span><span class="o">/</span><span class="w"> </span><span class="err">Ïƒ</span><span class="n">_group</span><span class="w">                                </span><span class="err">â”‚</span>
<span class="err">â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜</span>
</code></pre></div>

<h3 id="82-when-to-use-which">8.2 When to Use Which?<a class="header-link" href="#82-when-to-use-which" title="Permanent link">&para;</a></h3>
<h4 id="decision-tree">Decision Tree<a class="header-link" href="#decision-tree" title="Permanent link">&para;</a></h4>
<div class="highlight"><pre><span></span><code>Are you using a Transformer?
  â”œâ”€ Yes â†’ LayerNorm or RMSNorm
  â”‚         â”œâ”€ Speed critical? â†’ RMSNorm (LLaMA, Mistral)
  â”‚         â””â”€ Otherwise â†’ LayerNorm (BERT, ViT)
  â”‚
  â””â”€ No â†’ Are you using a CNN?
           â”œâ”€ Yes â†’ Is batch size large (â‰¥16)?
           â”‚         â”œâ”€ Yes â†’ BatchNorm
           â”‚         â””â”€ No â†’ GroupNorm
           â”‚
           â””â”€ No â†’ Is it a GAN or style transfer?
                     â”œâ”€ Yes â†’ InstanceNorm or AdaIN
                     â””â”€ No â†’ LayerNorm (safe default)
</code></pre></div>

<h4 id="architecture-specific-recommendations">Architecture-Specific Recommendations<a class="header-link" href="#architecture-specific-recommendations" title="Permanent link">&para;</a></h4>
<p><strong>Convolutional Neural Networks (CNNs)</strong>:</p>
<div class="highlight"><pre><span></span><code><span class="c1"># Standard classification (ImageNet)</span>
<span class="c1"># Batch size: 32-256</span>
<span class="k">class</span><span class="w"> </span><span class="nc">ResNet</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="k">def</span><span class="w"> </span><span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">layer</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Sequential</span><span class="p">(</span>
            <span class="n">nn</span><span class="o">.</span><span class="n">Conv2d</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="mi">64</span><span class="p">,</span> <span class="mi">7</span><span class="p">,</span> <span class="n">stride</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">padding</span><span class="o">=</span><span class="mi">3</span><span class="p">),</span>
            <span class="n">nn</span><span class="o">.</span><span class="n">BatchNorm2d</span><span class="p">(</span><span class="mi">64</span><span class="p">),</span>  <span class="c1"># â† BatchNorm for large batches</span>
            <span class="n">nn</span><span class="o">.</span><span class="n">ReLU</span><span class="p">(</span><span class="n">inplace</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
        <span class="p">)</span>
</code></pre></div>

<p><strong>Object Detection / Segmentation</strong>:</p>
<div class="highlight"><pre><span></span><code><span class="c1"># Mask R-CNN, Faster R-CNN</span>
<span class="c1"># Batch size: 1-4 (limited by GPU memory for large images)</span>
<span class="k">class</span><span class="w"> </span><span class="nc">DetectionBackbone</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="k">def</span><span class="w"> </span><span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">layer</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Sequential</span><span class="p">(</span>
            <span class="n">nn</span><span class="o">.</span><span class="n">Conv2d</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="mi">64</span><span class="p">,</span> <span class="mi">7</span><span class="p">,</span> <span class="n">stride</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">padding</span><span class="o">=</span><span class="mi">3</span><span class="p">),</span>
            <span class="n">nn</span><span class="o">.</span><span class="n">GroupNorm</span><span class="p">(</span><span class="mi">32</span><span class="p">,</span> <span class="mi">64</span><span class="p">),</span>  <span class="c1"># â† GroupNorm for small batches</span>
            <span class="n">nn</span><span class="o">.</span><span class="n">ReLU</span><span class="p">(</span><span class="n">inplace</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
        <span class="p">)</span>
</code></pre></div>

<p><strong>Transformers (Vision or Language)</strong>:</p>
<div class="highlight"><pre><span></span><code><span class="c1"># BERT, GPT, ViT</span>
<span class="k">class</span><span class="w"> </span><span class="nc">TransformerEncoder</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="k">def</span><span class="w"> </span><span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">d_model</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">norm1</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">LayerNorm</span><span class="p">(</span><span class="n">d_model</span><span class="p">)</span>  <span class="c1"># â† LayerNorm standard</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">attn</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">MultiheadAttention</span><span class="p">(</span><span class="n">d_model</span><span class="p">,</span> <span class="mi">8</span><span class="p">,</span> <span class="n">batch_first</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">norm2</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">LayerNorm</span><span class="p">(</span><span class="n">d_model</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">ffn</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Sequential</span><span class="p">(</span>
            <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">d_model</span><span class="p">,</span> <span class="mi">4</span> <span class="o">*</span> <span class="n">d_model</span><span class="p">),</span>
            <span class="n">nn</span><span class="o">.</span><span class="n">GELU</span><span class="p">(),</span>
            <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="mi">4</span> <span class="o">*</span> <span class="n">d_model</span><span class="p">,</span> <span class="n">d_model</span><span class="p">)</span>
        <span class="p">)</span>
</code></pre></div>

<p><strong>Large Language Models</strong>:</p>
<div class="highlight"><pre><span></span><code><span class="c1"># LLaMA, Mistral, Gemma</span>
<span class="k">class</span><span class="w"> </span><span class="nc">LLMTransformerBlock</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="k">def</span><span class="w"> </span><span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">dim</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">attn_norm</span> <span class="o">=</span> <span class="n">RMSNorm</span><span class="p">(</span><span class="n">dim</span><span class="p">)</span>  <span class="c1"># â† RMSNorm for speed</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">ffn_norm</span> <span class="o">=</span> <span class="n">RMSNorm</span><span class="p">(</span><span class="n">dim</span><span class="p">)</span>
</code></pre></div>

<p><strong>Generative Adversarial Networks (GANs)</strong>:</p>
<div class="highlight"><pre><span></span><code><span class="c1"># Generator: InstanceNorm for style</span>
<span class="k">class</span><span class="w"> </span><span class="nc">Generator</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="k">def</span><span class="w"> </span><span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">layer</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Sequential</span><span class="p">(</span>
            <span class="n">nn</span><span class="o">.</span><span class="n">Conv2d</span><span class="p">(</span><span class="mi">128</span><span class="p">,</span> <span class="mi">64</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="n">padding</span><span class="o">=</span><span class="mi">1</span><span class="p">),</span>
            <span class="n">nn</span><span class="o">.</span><span class="n">InstanceNorm2d</span><span class="p">(</span><span class="mi">64</span><span class="p">),</span>  <span class="c1"># â† InstanceNorm</span>
            <span class="n">nn</span><span class="o">.</span><span class="n">ReLU</span><span class="p">(</span><span class="n">inplace</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
        <span class="p">)</span>

<span class="c1"># Discriminator: Spectral Normalization</span>
<span class="k">class</span><span class="w"> </span><span class="nc">Discriminator</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="k">def</span><span class="w"> </span><span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">layer</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Sequential</span><span class="p">(</span>
            <span class="n">spectral_norm</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Conv2d</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="mi">64</span><span class="p">,</span> <span class="mi">4</span><span class="p">,</span> <span class="n">stride</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">padding</span><span class="o">=</span><span class="mi">1</span><span class="p">)),</span>  <span class="c1"># â† SpectralNorm</span>
            <span class="n">nn</span><span class="o">.</span><span class="n">LeakyReLU</span><span class="p">(</span><span class="mf">0.2</span><span class="p">,</span> <span class="n">inplace</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
        <span class="p">)</span>
</code></pre></div>

<h3 id="83-performance-benchmarks">8.3 Performance Benchmarks<a class="header-link" href="#83-performance-benchmarks" title="Permanent link">&para;</a></h3>
<div class="highlight"><pre><span></span><code><span class="kn">import</span><span class="w"> </span><span class="nn">torch</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">torch.nn</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">nn</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">time</span>

<span class="k">def</span><span class="w"> </span><span class="nf">benchmark_normalization</span><span class="p">(</span><span class="n">norm_layer</span><span class="p">,</span> <span class="n">input_shape</span><span class="p">,</span> <span class="n">num_iterations</span><span class="o">=</span><span class="mi">1000</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Benchmark a normalization layer.&quot;&quot;&quot;</span>
    <span class="n">x</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="o">*</span><span class="n">input_shape</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="s1">&#39;cuda&#39;</span><span class="p">)</span>

    <span class="c1"># Warmup</span>
    <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">10</span><span class="p">):</span>
        <span class="n">_</span> <span class="o">=</span> <span class="n">norm_layer</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>

    <span class="n">torch</span><span class="o">.</span><span class="n">cuda</span><span class="o">.</span><span class="n">synchronize</span><span class="p">()</span>
    <span class="n">start</span> <span class="o">=</span> <span class="n">time</span><span class="o">.</span><span class="n">time</span><span class="p">()</span>
    <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">num_iterations</span><span class="p">):</span>
        <span class="n">_</span> <span class="o">=</span> <span class="n">norm_layer</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
    <span class="n">torch</span><span class="o">.</span><span class="n">cuda</span><span class="o">.</span><span class="n">synchronize</span><span class="p">()</span>
    <span class="n">elapsed</span> <span class="o">=</span> <span class="n">time</span><span class="o">.</span><span class="n">time</span><span class="p">()</span> <span class="o">-</span> <span class="n">start</span>

    <span class="k">return</span> <span class="n">elapsed</span>

<span class="c1"># Test configuration</span>
<span class="n">batch_size</span> <span class="o">=</span> <span class="mi">32</span>
<span class="n">channels</span> <span class="o">=</span> <span class="mi">256</span>
<span class="n">height</span><span class="p">,</span> <span class="n">width</span> <span class="o">=</span> <span class="mi">56</span><span class="p">,</span> <span class="mi">56</span>
<span class="n">input_shape</span> <span class="o">=</span> <span class="p">(</span><span class="n">batch_size</span><span class="p">,</span> <span class="n">channels</span><span class="p">,</span> <span class="n">height</span><span class="p">,</span> <span class="n">width</span><span class="p">)</span>

<span class="c1"># Normalization layers</span>
<span class="n">norms</span> <span class="o">=</span> <span class="p">{</span>
    <span class="s1">&#39;BatchNorm2d&#39;</span><span class="p">:</span> <span class="n">nn</span><span class="o">.</span><span class="n">BatchNorm2d</span><span class="p">(</span><span class="n">channels</span><span class="p">)</span><span class="o">.</span><span class="n">cuda</span><span class="p">(),</span>
    <span class="s1">&#39;GroupNorm (G=32)&#39;</span><span class="p">:</span> <span class="n">nn</span><span class="o">.</span><span class="n">GroupNorm</span><span class="p">(</span><span class="mi">32</span><span class="p">,</span> <span class="n">channels</span><span class="p">)</span><span class="o">.</span><span class="n">cuda</span><span class="p">(),</span>
    <span class="s1">&#39;InstanceNorm2d&#39;</span><span class="p">:</span> <span class="n">nn</span><span class="o">.</span><span class="n">InstanceNorm2d</span><span class="p">(</span><span class="n">channels</span><span class="p">)</span><span class="o">.</span><span class="n">cuda</span><span class="p">(),</span>
<span class="p">}</span>

<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Input shape: </span><span class="si">{</span><span class="n">input_shape</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Iterations: 1000</span><span class="se">\n</span><span class="s2">&quot;</span><span class="p">)</span>

<span class="n">results</span> <span class="o">=</span> <span class="p">{}</span>
<span class="k">for</span> <span class="n">name</span><span class="p">,</span> <span class="n">norm</span> <span class="ow">in</span> <span class="n">norms</span><span class="o">.</span><span class="n">items</span><span class="p">():</span>
    <span class="n">norm</span><span class="o">.</span><span class="n">eval</span><span class="p">()</span>
    <span class="n">elapsed</span> <span class="o">=</span> <span class="n">benchmark_normalization</span><span class="p">(</span><span class="n">norm</span><span class="p">,</span> <span class="n">input_shape</span><span class="p">)</span>
    <span class="n">results</span><span class="p">[</span><span class="n">name</span><span class="p">]</span> <span class="o">=</span> <span class="n">elapsed</span>
    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="n">name</span><span class="si">:</span><span class="s2">20s</span><span class="si">}</span><span class="s2">: </span><span class="si">{</span><span class="n">elapsed</span><span class="si">:</span><span class="s2">.4f</span><span class="si">}</span><span class="s2">s&quot;</span><span class="p">)</span>

<span class="c1"># Relative speeds</span>
<span class="n">baseline</span> <span class="o">=</span> <span class="n">results</span><span class="p">[</span><span class="s1">&#39;BatchNorm2d&#39;</span><span class="p">]</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;</span><span class="se">\n</span><span class="s2">Relative to BatchNorm2d:&quot;</span><span class="p">)</span>
<span class="k">for</span> <span class="n">name</span><span class="p">,</span> <span class="n">elapsed</span> <span class="ow">in</span> <span class="n">results</span><span class="o">.</span><span class="n">items</span><span class="p">():</span>
    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="n">name</span><span class="si">:</span><span class="s2">20s</span><span class="si">}</span><span class="s2">: </span><span class="si">{</span><span class="n">elapsed</span><span class="w"> </span><span class="o">/</span><span class="w"> </span><span class="n">baseline</span><span class="si">:</span><span class="s2">.2f</span><span class="si">}</span><span class="s2">x&quot;</span><span class="p">)</span>
</code></pre></div>

<p><strong>Typical results</strong> (RTX 3090):</p>
<div class="highlight"><pre><span></span><code><span class="n">BatchNorm2d</span><span class="w">         </span><span class="o">:</span><span class="w"> </span><span class="mf">0.1234</span><span class="n">s</span><span class="w">  </span><span class="o">(</span><span class="mf">1.00</span><span class="n">x</span><span class="o">)</span>
<span class="n">GroupNorm</span><span class="w"> </span><span class="o">(</span><span class="n">G</span><span class="o">=</span><span class="mi">32</span><span class="o">)</span><span class="w">    </span><span class="o">:</span><span class="w"> </span><span class="mf">0.1456</span><span class="n">s</span><span class="w">  </span><span class="o">(</span><span class="mf">1.18</span><span class="n">x</span><span class="o">)</span>
<span class="n">InstanceNorm2d</span><span class="w">      </span><span class="o">:</span><span class="w"> </span><span class="mf">0.1389</span><span class="n">s</span><span class="w">  </span><span class="o">(</span><span class="mf">1.13</span><span class="n">x</span><span class="o">)</span>
</code></pre></div>

<p><strong>For Transformers</strong> (seq_len=2048, d_model=4096):</p>
<div class="highlight"><pre><span></span><code><span class="n">LayerNorm</span><span class="w">           </span><span class="o">:</span><span class="w"> </span><span class="mf">0.2145</span><span class="n">s</span><span class="w">  </span><span class="o">(</span><span class="mf">1.00</span><span class="n">x</span><span class="o">)</span>
<span class="n">RMSNorm</span><span class="w">             </span><span class="o">:</span><span class="w"> </span><span class="mf">0.1876</span><span class="n">s</span><span class="w">  </span><span class="o">(</span><span class="mf">0.87</span><span class="n">x</span><span class="o">)</span><span class="w">  </span><span class="err">â†</span><span class="w"> </span><span class="o">~</span><span class="mi">13</span><span class="o">%</span><span class="w"> </span><span class="n">faster</span>
</code></pre></div>

<hr />
<h2 id="9-practical-tips">9. Practical Tips<a class="header-link" href="#9-practical-tips" title="Permanent link">&para;</a></h2>
<h3 id="91-initialization-of-and">9.1 Initialization of Î³ and Î²<a class="header-link" href="#91-initialization-of-and" title="Permanent link">&para;</a></h3>
<p><strong>Default initialization</strong> (PyTorch):</p>
<div class="highlight"><pre><span></span><code><span class="c1"># Î³ (scale) initialized to 1</span>
<span class="c1"># Î² (shift) initialized to 0</span>
<span class="c1"># This preserves the original distribution initially</span>
</code></pre></div>

<p><strong>Special cases</strong>:</p>
<div class="highlight"><pre><span></span><code><span class="kn">import</span><span class="w"> </span><span class="nn">torch.nn</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">nn</span>

<span class="c1"># Initialize Î³ to 0 for residual blocks (He et al., 2019)</span>
<span class="c1"># &quot;Fixup Initialization&quot; â€” helps very deep networks</span>
<span class="k">class</span><span class="w"> </span><span class="nc">ResidualBlock</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="k">def</span><span class="w"> </span><span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">channels</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">conv1</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Conv2d</span><span class="p">(</span><span class="n">channels</span><span class="p">,</span> <span class="n">channels</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="n">padding</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">bn1</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">BatchNorm2d</span><span class="p">(</span><span class="n">channels</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">conv2</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Conv2d</span><span class="p">(</span><span class="n">channels</span><span class="p">,</span> <span class="n">channels</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="n">padding</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">bn2</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">BatchNorm2d</span><span class="p">(</span><span class="n">channels</span><span class="p">)</span>

        <span class="c1"># Zero-initialize the last BatchNorm in each block</span>
        <span class="n">nn</span><span class="o">.</span><span class="n">init</span><span class="o">.</span><span class="n">constant_</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">bn2</span><span class="o">.</span><span class="n">weight</span><span class="p">,</span> <span class="mi">0</span><span class="p">)</span>  <span class="c1"># Î³ = 0</span>
        <span class="n">nn</span><span class="o">.</span><span class="n">init</span><span class="o">.</span><span class="n">constant_</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">bn2</span><span class="o">.</span><span class="n">bias</span><span class="p">,</span> <span class="mi">0</span><span class="p">)</span>    <span class="c1"># Î² = 0</span>

    <span class="k">def</span><span class="w"> </span><span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
        <span class="n">identity</span> <span class="o">=</span> <span class="n">x</span>
        <span class="n">out</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">conv1</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="n">out</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">bn1</span><span class="p">(</span><span class="n">out</span><span class="p">)</span>
        <span class="n">out</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">relu</span><span class="p">(</span><span class="n">out</span><span class="p">)</span>
        <span class="n">out</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">conv2</span><span class="p">(</span><span class="n">out</span><span class="p">)</span>
        <span class="n">out</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">bn2</span><span class="p">(</span><span class="n">out</span><span class="p">)</span>  <span class="c1"># Initially outputs 0, so out = identity</span>
        <span class="n">out</span> <span class="o">+=</span> <span class="n">identity</span>
        <span class="n">out</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">relu</span><span class="p">(</span><span class="n">out</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">out</span>
</code></pre></div>

<h3 id="92-interaction-with-weight-initialization">9.2 Interaction with Weight Initialization<a class="header-link" href="#92-interaction-with-weight-initialization" title="Permanent link">&para;</a></h3>
<p><strong>BatchNorm makes networks less sensitive to weight initialization</strong>, but you should still use proper initialization:</p>
<div class="highlight"><pre><span></span><code><span class="kn">import</span><span class="w"> </span><span class="nn">torch.nn</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">nn</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">torch.nn.init</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">init</span>

<span class="k">class</span><span class="w"> </span><span class="nc">ConvBNReLU</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="k">def</span><span class="w"> </span><span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">in_channels</span><span class="p">,</span> <span class="n">out_channels</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">conv</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Conv2d</span><span class="p">(</span><span class="n">in_channels</span><span class="p">,</span> <span class="n">out_channels</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="n">padding</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">bn</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">BatchNorm2d</span><span class="p">(</span><span class="n">out_channels</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">relu</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">ReLU</span><span class="p">(</span><span class="n">inplace</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>

        <span class="c1"># He initialization for ReLU</span>
        <span class="n">init</span><span class="o">.</span><span class="n">kaiming_normal_</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">conv</span><span class="o">.</span><span class="n">weight</span><span class="p">,</span> <span class="n">mode</span><span class="o">=</span><span class="s1">&#39;fan_out&#39;</span><span class="p">,</span> <span class="n">nonlinearity</span><span class="o">=</span><span class="s1">&#39;relu&#39;</span><span class="p">)</span>
        <span class="n">init</span><span class="o">.</span><span class="n">constant_</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">conv</span><span class="o">.</span><span class="n">bias</span><span class="p">,</span> <span class="mi">0</span><span class="p">)</span>

    <span class="k">def</span><span class="w"> </span><span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
        <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">conv</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">bn</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">relu</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">x</span>
</code></pre></div>

<h3 id="93-normalization-and-learning-rate">9.3 Normalization and Learning Rate<a class="header-link" href="#93-normalization-and-learning-rate" title="Permanent link">&para;</a></h3>
<p><strong>Key insight</strong>: Normalization allows higher learning rates.</p>
<div class="highlight"><pre><span></span><code><span class="kn">import</span><span class="w"> </span><span class="nn">torch.optim</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">optim</span>

<span class="c1"># Without normalization</span>
<span class="n">model_no_norm</span> <span class="o">=</span> <span class="n">MyModel</span><span class="p">(</span><span class="n">use_norm</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
<span class="n">optimizer</span> <span class="o">=</span> <span class="n">optim</span><span class="o">.</span><span class="n">SGD</span><span class="p">(</span><span class="n">model_no_norm</span><span class="o">.</span><span class="n">parameters</span><span class="p">(),</span> <span class="n">lr</span><span class="o">=</span><span class="mf">0.01</span><span class="p">)</span>  <span class="c1"># Conservative LR</span>

<span class="c1"># With BatchNorm/LayerNorm</span>
<span class="n">model_with_norm</span> <span class="o">=</span> <span class="n">MyModel</span><span class="p">(</span><span class="n">use_norm</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="n">optimizer</span> <span class="o">=</span> <span class="n">optim</span><span class="o">.</span><span class="n">SGD</span><span class="p">(</span><span class="n">model_with_norm</span><span class="o">.</span><span class="n">parameters</span><span class="p">(),</span> <span class="n">lr</span><span class="o">=</span><span class="mf">0.1</span><span class="p">)</span>  <span class="c1"># 10x higher LR!</span>

<span class="c1"># Modern Transformers with LayerNorm</span>
<span class="c1"># Can use even higher learning rates with warmup</span>
<span class="n">optimizer</span> <span class="o">=</span> <span class="n">optim</span><span class="o">.</span><span class="n">AdamW</span><span class="p">(</span><span class="n">model</span><span class="o">.</span><span class="n">parameters</span><span class="p">(),</span> <span class="n">lr</span><span class="o">=</span><span class="mf">1e-3</span><span class="p">)</span>
</code></pre></div>

<p><strong>Learning rate scaling rule</strong> (Goyal et al., 2017):</p>
<div class="highlight"><pre><span></span><code>When increasing batch size by k, increase learning rate by k
(only works with BatchNorm!)

Batch 256, LR 0.1  â†’  Batch 1024, LR 0.4
</code></pre></div>

<h3 id="94-common-bugs-and-pitfalls">9.4 Common Bugs and Pitfalls<a class="header-link" href="#94-common-bugs-and-pitfalls" title="Permanent link">&para;</a></h3>
<h4 id="bug-1-forgetting-modeleval">Bug 1: Forgetting model.eval()<a class="header-link" href="#bug-1-forgetting-modeleval" title="Permanent link">&para;</a></h4>
<div class="highlight"><pre><span></span><code><span class="kn">import</span><span class="w"> </span><span class="nn">torch</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">torch.nn</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">nn</span>

<span class="n">model</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Sequential</span><span class="p">(</span>
    <span class="n">nn</span><span class="o">.</span><span class="n">Conv2d</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="mi">64</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="n">padding</span><span class="o">=</span><span class="mi">1</span><span class="p">),</span>
    <span class="n">nn</span><span class="o">.</span><span class="n">BatchNorm2d</span><span class="p">(</span><span class="mi">64</span><span class="p">),</span>
    <span class="n">nn</span><span class="o">.</span><span class="n">ReLU</span><span class="p">()</span>
<span class="p">)</span>

<span class="c1"># Training mode</span>
<span class="n">model</span><span class="o">.</span><span class="n">train</span><span class="p">()</span>
<span class="n">x</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">32</span><span class="p">,</span> <span class="mi">32</span><span class="p">)</span>
<span class="n">out_train</span> <span class="o">=</span> <span class="n">model</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>

<span class="c1"># Inference â€” WRONG! Still using batch statistics</span>
<span class="c1"># out_test = model(x)  # Bug: still in training mode!</span>

<span class="c1"># Inference â€” CORRECT</span>
<span class="n">model</span><span class="o">.</span><span class="n">eval</span><span class="p">()</span>  <span class="c1"># Switch to eval mode!</span>
<span class="n">out_test</span> <span class="o">=</span> <span class="n">model</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>

<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Outputs same? </span><span class="si">{</span><span class="n">torch</span><span class="o">.</span><span class="n">allclose</span><span class="p">(</span><span class="n">out_train</span><span class="p">,</span><span class="w"> </span><span class="n">out_test</span><span class="p">)</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>  <span class="c1"># False</span>
</code></pre></div>

<h4 id="bug-2-wrong-dimension-ordering">Bug 2: Wrong dimension ordering<a class="header-link" href="#bug-2-wrong-dimension-ordering" title="Permanent link">&para;</a></h4>
<div class="highlight"><pre><span></span><code><span class="c1"># WRONG: (seq_len, batch, features) for LayerNorm</span>
<span class="n">x</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">100</span><span class="p">,</span> <span class="mi">32</span><span class="p">,</span> <span class="mi">512</span><span class="p">)</span>  <span class="c1"># (seq, batch, features)</span>
<span class="n">ln</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">LayerNorm</span><span class="p">(</span><span class="mi">512</span><span class="p">)</span>
<span class="n">out</span> <span class="o">=</span> <span class="n">ln</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>  <span class="c1"># Works, but normalizes last dim only</span>

<span class="c1"># CORRECT: Normalize across features (last dim)</span>
<span class="c1"># Make sure your tensor layout matches the normalized_shape!</span>

<span class="c1"># For (batch, seq, features) â€” standard now</span>
<span class="n">x</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">32</span><span class="p">,</span> <span class="mi">100</span><span class="p">,</span> <span class="mi">512</span><span class="p">)</span>  <span class="c1"># (batch, seq, features)</span>
<span class="n">ln</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">LayerNorm</span><span class="p">(</span><span class="mi">512</span><span class="p">)</span>
<span class="n">out</span> <span class="o">=</span> <span class="n">ln</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>  <span class="c1"># Correct: normalizes across features dim</span>
</code></pre></div>

<h4 id="bug-3-groupnorm-with-incompatible-channels">Bug 3: GroupNorm with incompatible channels<a class="header-link" href="#bug-3-groupnorm-with-incompatible-channels" title="Permanent link">&para;</a></h4>
<div class="highlight"><pre><span></span><code><span class="c1"># WRONG: num_channels not divisible by num_groups</span>
<span class="k">try</span><span class="p">:</span>
    <span class="n">gn</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">GroupNorm</span><span class="p">(</span><span class="n">num_groups</span><span class="o">=</span><span class="mi">32</span><span class="p">,</span> <span class="n">num_channels</span><span class="o">=</span><span class="mi">50</span><span class="p">)</span>  <span class="c1"># 50 % 32 != 0</span>
<span class="k">except</span> <span class="ne">ValueError</span> <span class="k">as</span> <span class="n">e</span><span class="p">:</span>
    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Error: </span><span class="si">{</span><span class="n">e</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>

<span class="c1"># CORRECT: ensure divisibility</span>
<span class="n">gn</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">GroupNorm</span><span class="p">(</span><span class="n">num_groups</span><span class="o">=</span><span class="mi">32</span><span class="p">,</span> <span class="n">num_channels</span><span class="o">=</span><span class="mi">64</span><span class="p">)</span>  <span class="c1"># 64 % 32 == 0 âœ“</span>
</code></pre></div>

<h4 id="bug-4-batchnorm-with-batch_size-1">Bug 4: BatchNorm with batch_size = 1<a class="header-link" href="#bug-4-batchnorm-with-batch_size-1" title="Permanent link">&para;</a></h4>
<div class="highlight"><pre><span></span><code><span class="c1"># PROBLEM: BatchNorm with single sample</span>
<span class="n">bn</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">BatchNorm2d</span><span class="p">(</span><span class="mi">64</span><span class="p">)</span>
<span class="n">x</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">64</span><span class="p">,</span> <span class="mi">32</span><span class="p">,</span> <span class="mi">32</span><span class="p">)</span>  <span class="c1"># Batch size = 1</span>

<span class="n">bn</span><span class="o">.</span><span class="n">train</span><span class="p">()</span>
<span class="n">out</span> <span class="o">=</span> <span class="n">bn</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>  <span class="c1"># Variance = 0! (single sample)</span>
<span class="c1"># Results in NaN or unstable training</span>

<span class="c1"># SOLUTION 1: Use GroupNorm or LayerNorm</span>
<span class="n">gn</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">GroupNorm</span><span class="p">(</span><span class="mi">32</span><span class="p">,</span> <span class="mi">64</span><span class="p">)</span>
<span class="n">out</span> <span class="o">=</span> <span class="n">gn</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>  <span class="c1"># Works fine with batch_size=1</span>

<span class="c1"># SOLUTION 2: Set BatchNorm to eval mode</span>
<span class="n">bn</span><span class="o">.</span><span class="n">eval</span><span class="p">()</span>
<span class="n">out</span> <span class="o">=</span> <span class="n">bn</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>  <span class="c1"># Uses running statistics</span>
</code></pre></div>

<h4 id="bug-5-mixing-frozen-and-trainable-batchnorm">Bug 5: Mixing frozen and trainable BatchNorm<a class="header-link" href="#bug-5-mixing-frozen-and-trainable-batchnorm" title="Permanent link">&para;</a></h4>
<div class="highlight"><pre><span></span><code><span class="c1"># PROBLEM: Fine-tuning with frozen BatchNorm statistics</span>
<span class="n">pretrained_model</span> <span class="o">=</span> <span class="n">torchvision</span><span class="o">.</span><span class="n">models</span><span class="o">.</span><span class="n">resnet50</span><span class="p">(</span><span class="n">pretrained</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>

<span class="c1"># Freeze all parameters</span>
<span class="k">for</span> <span class="n">param</span> <span class="ow">in</span> <span class="n">pretrained_model</span><span class="o">.</span><span class="n">parameters</span><span class="p">():</span>
    <span class="n">param</span><span class="o">.</span><span class="n">requires_grad</span> <span class="o">=</span> <span class="kc">False</span>

<span class="c1"># This is NOT enough! BatchNorm still uses training mode statistics</span>
<span class="n">pretrained_model</span><span class="o">.</span><span class="n">train</span><span class="p">()</span>  <span class="c1"># BUG: BatchNorm in training mode!</span>

<span class="c1"># SOLUTION: Set to eval mode OR set BatchNorm modules to eval</span>
<span class="n">pretrained_model</span><span class="o">.</span><span class="n">eval</span><span class="p">()</span>  <span class="c1"># Safe for inference</span>

<span class="c1"># For fine-tuning:</span>
<span class="k">def</span><span class="w"> </span><span class="nf">set_bn_eval</span><span class="p">(</span><span class="n">module</span><span class="p">):</span>
    <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">module</span><span class="p">,</span> <span class="n">nn</span><span class="o">.</span><span class="n">BatchNorm2d</span><span class="p">):</span>
        <span class="n">module</span><span class="o">.</span><span class="n">eval</span><span class="p">()</span>

<span class="n">pretrained_model</span><span class="o">.</span><span class="n">train</span><span class="p">()</span>
<span class="n">pretrained_model</span><span class="o">.</span><span class="n">apply</span><span class="p">(</span><span class="n">set_bn_eval</span><span class="p">)</span>  <span class="c1"># Keep BatchNorm in eval mode during training</span>
</code></pre></div>

<h3 id="95-best-practices-checklist">9.5 Best Practices Checklist<a class="header-link" href="#95-best-practices-checklist" title="Permanent link">&para;</a></h3>
<p>âœ… <strong>Always call <code>model.eval()</code> before inference</strong></p>
<p>âœ… <strong>Match normalization to your architecture</strong>:
   - CNNs (large batch) â†’ BatchNorm
   - CNNs (small batch) â†’ GroupNorm
   - Transformers â†’ LayerNorm or RMSNorm
   - GANs â†’ InstanceNorm (G), SpectralNorm (D)</p>
<p>âœ… <strong>Use appropriate initialization</strong> (Kaiming for ReLU, Xavier for Tanh)</p>
<p>âœ… <strong>Increase learning rate</strong> when using normalization</p>
<p>âœ… <strong>For transfer learning</strong>, consider freezing BatchNorm statistics or replacing with GroupNorm</p>
<p>âœ… <strong>Monitor running statistics</strong> â€” ensure they stabilize during training</p>
<p>âœ… <strong>For distributed training</strong>, use SyncBatchNorm if needed:</p>
<div class="highlight"><pre><span></span><code><span class="n">model</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">SyncBatchNorm</span><span class="o">.</span><span class="n">convert_sync_batchnorm</span><span class="p">(</span><span class="n">model</span><span class="p">)</span>
</code></pre></div>

<hr />
<h2 id="exercises">Exercises<a class="header-link" href="#exercises" title="Permanent link">&para;</a></h2>
<h3 id="exercise-1-implement-and-compare-normalization-methods">Exercise 1: Implement and Compare Normalization Methods<a class="header-link" href="#exercise-1-implement-and-compare-normalization-methods" title="Permanent link">&para;</a></h3>
<p>Implement a CNN with different normalization methods and compare their performance on CIFAR-10.</p>
<p><strong>Tasks</strong>:
1. Create four identical CNNs, each using a different normalization:
   - BatchNorm2d
   - GroupNorm (32 groups)
   - LayerNorm
   - No normalization (baseline)
2. Train each for 20 epochs on CIFAR-10
3. Plot training curves (loss and accuracy)
4. Report final test accuracy for each
5. Experiment with batch sizes [4, 16, 64] and observe which normalization is most robust</p>
<p><strong>Starter code</strong>:</p>
<div class="highlight"><pre><span></span><code><span class="kn">import</span><span class="w"> </span><span class="nn">torch</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">torch.nn</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">nn</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">torchvision</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">torchvision.transforms</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">transforms</span>

<span class="k">class</span><span class="w"> </span><span class="nc">CIFAR10Net</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="k">def</span><span class="w"> </span><span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">norm_type</span><span class="o">=</span><span class="s1">&#39;batch&#39;</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">norm_type</span> <span class="o">=</span> <span class="n">norm_type</span>

        <span class="k">def</span><span class="w"> </span><span class="nf">conv_block</span><span class="p">(</span><span class="n">in_c</span><span class="p">,</span> <span class="n">out_c</span><span class="p">):</span>
            <span class="n">layers</span> <span class="o">=</span> <span class="p">[</span><span class="n">nn</span><span class="o">.</span><span class="n">Conv2d</span><span class="p">(</span><span class="n">in_c</span><span class="p">,</span> <span class="n">out_c</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="n">padding</span><span class="o">=</span><span class="mi">1</span><span class="p">)]</span>

            <span class="k">if</span> <span class="n">norm_type</span> <span class="o">==</span> <span class="s1">&#39;batch&#39;</span><span class="p">:</span>
                <span class="n">layers</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">BatchNorm2d</span><span class="p">(</span><span class="n">out_c</span><span class="p">))</span>
            <span class="k">elif</span> <span class="n">norm_type</span> <span class="o">==</span> <span class="s1">&#39;group&#39;</span><span class="p">:</span>
                <span class="n">layers</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">GroupNorm</span><span class="p">(</span><span class="mi">32</span><span class="p">,</span> <span class="n">out_c</span><span class="p">))</span>
            <span class="k">elif</span> <span class="n">norm_type</span> <span class="o">==</span> <span class="s1">&#39;layer&#39;</span><span class="p">:</span>
                <span class="c1"># LayerNorm for 2D: normalize over (C, H, W)</span>
                <span class="n">layers</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">GroupNorm</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">out_c</span><span class="p">))</span>  <span class="c1"># G=1 is LayerNorm</span>
            <span class="c1"># &#39;none&#39;: no normalization</span>

            <span class="n">layers</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">ReLU</span><span class="p">(</span><span class="n">inplace</span><span class="o">=</span><span class="kc">True</span><span class="p">))</span>
            <span class="k">return</span> <span class="n">nn</span><span class="o">.</span><span class="n">Sequential</span><span class="p">(</span><span class="o">*</span><span class="n">layers</span><span class="p">)</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">features</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Sequential</span><span class="p">(</span>
            <span class="n">conv_block</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="mi">64</span><span class="p">),</span>
            <span class="n">conv_block</span><span class="p">(</span><span class="mi">64</span><span class="p">,</span> <span class="mi">64</span><span class="p">),</span>
            <span class="n">nn</span><span class="o">.</span><span class="n">MaxPool2d</span><span class="p">(</span><span class="mi">2</span><span class="p">),</span>
            <span class="n">conv_block</span><span class="p">(</span><span class="mi">64</span><span class="p">,</span> <span class="mi">128</span><span class="p">),</span>
            <span class="n">conv_block</span><span class="p">(</span><span class="mi">128</span><span class="p">,</span> <span class="mi">128</span><span class="p">),</span>
            <span class="n">nn</span><span class="o">.</span><span class="n">MaxPool2d</span><span class="p">(</span><span class="mi">2</span><span class="p">),</span>
            <span class="n">conv_block</span><span class="p">(</span><span class="mi">128</span><span class="p">,</span> <span class="mi">256</span><span class="p">),</span>
            <span class="n">conv_block</span><span class="p">(</span><span class="mi">256</span><span class="p">,</span> <span class="mi">256</span><span class="p">),</span>
            <span class="n">nn</span><span class="o">.</span><span class="n">MaxPool2d</span><span class="p">(</span><span class="mi">2</span><span class="p">),</span>
        <span class="p">)</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">classifier</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Sequential</span><span class="p">(</span>
            <span class="n">nn</span><span class="o">.</span><span class="n">AdaptiveAvgPool2d</span><span class="p">(</span><span class="mi">1</span><span class="p">),</span>
            <span class="n">nn</span><span class="o">.</span><span class="n">Flatten</span><span class="p">(),</span>
            <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="mi">256</span><span class="p">,</span> <span class="mi">10</span><span class="p">)</span>
        <span class="p">)</span>

    <span class="k">def</span><span class="w"> </span><span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
        <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">features</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">classifier</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">x</span>

<span class="c1"># TODO: Implement training loop and comparison</span>
</code></pre></div>

<h3 id="exercise-2-rmsnorm-vs-layernorm-in-transformers">Exercise 2: RMSNorm vs LayerNorm in Transformers<a class="header-link" href="#exercise-2-rmsnorm-vs-layernorm-in-transformers" title="Permanent link">&para;</a></h3>
<p>Implement a small Transformer and compare RMSNorm vs LayerNorm in terms of speed and performance.</p>
<p><strong>Tasks</strong>:
1. Implement a character-level language model (predict next character)
2. Train two versions: one with LayerNorm, one with RMSNorm
3. Use a text dataset (e.g., Shakespeare, WikiText-2)
4. Measure:
   - Training time per epoch
   - Final perplexity
   - Inference speed
5. Analyze: Does RMSNorm match LayerNorm performance while being faster?</p>
<p><strong>Starter code</strong>:</p>
<div class="highlight"><pre><span></span><code><span class="kn">import</span><span class="w"> </span><span class="nn">torch</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">torch.nn</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">nn</span>

<span class="k">class</span><span class="w"> </span><span class="nc">TransformerLM</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="k">def</span><span class="w"> </span><span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">vocab_size</span><span class="p">,</span> <span class="n">d_model</span><span class="o">=</span><span class="mi">512</span><span class="p">,</span> <span class="n">num_heads</span><span class="o">=</span><span class="mi">8</span><span class="p">,</span>
                 <span class="n">num_layers</span><span class="o">=</span><span class="mi">6</span><span class="p">,</span> <span class="n">norm_type</span><span class="o">=</span><span class="s1">&#39;layer&#39;</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">embedding</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Embedding</span><span class="p">(</span><span class="n">vocab_size</span><span class="p">,</span> <span class="n">d_model</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">pos_encoding</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Parameter</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1000</span><span class="p">,</span> <span class="n">d_model</span><span class="p">))</span>

        <span class="c1"># Choose normalization</span>
        <span class="k">if</span> <span class="n">norm_type</span> <span class="o">==</span> <span class="s1">&#39;layer&#39;</span><span class="p">:</span>
            <span class="n">norm_cls</span> <span class="o">=</span> <span class="k">lambda</span><span class="p">:</span> <span class="n">nn</span><span class="o">.</span><span class="n">LayerNorm</span><span class="p">(</span><span class="n">d_model</span><span class="p">)</span>
        <span class="k">elif</span> <span class="n">norm_type</span> <span class="o">==</span> <span class="s1">&#39;rms&#39;</span><span class="p">:</span>
            <span class="n">norm_cls</span> <span class="o">=</span> <span class="k">lambda</span><span class="p">:</span> <span class="n">RMSNorm</span><span class="p">(</span><span class="n">d_model</span><span class="p">)</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">layers</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">ModuleList</span><span class="p">([</span>
            <span class="n">TransformerBlock</span><span class="p">(</span><span class="n">d_model</span><span class="p">,</span> <span class="n">num_heads</span><span class="p">,</span> <span class="n">norm_cls</span><span class="p">)</span>
            <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">num_layers</span><span class="p">)</span>
        <span class="p">])</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">output</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">d_model</span><span class="p">,</span> <span class="n">vocab_size</span><span class="p">)</span>

    <span class="k">def</span><span class="w"> </span><span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
        <span class="c1"># x: (batch, seq_len)</span>
        <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">embedding</span><span class="p">(</span><span class="n">x</span><span class="p">)</span> <span class="o">+</span> <span class="bp">self</span><span class="o">.</span><span class="n">pos_encoding</span><span class="p">[:,</span> <span class="p">:</span><span class="n">x</span><span class="o">.</span><span class="n">size</span><span class="p">(</span><span class="mi">1</span><span class="p">),</span> <span class="p">:]</span>

        <span class="k">for</span> <span class="n">layer</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">layers</span><span class="p">:</span>
            <span class="n">x</span> <span class="o">=</span> <span class="n">layer</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>

        <span class="n">logits</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">output</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">logits</span>

<span class="c1"># TODO: Implement training and benchmarking</span>
</code></pre></div>

<h3 id="exercise-3-adaptive-instance-normalization-for-style-transfer">Exercise 3: Adaptive Instance Normalization for Style Transfer<a class="header-link" href="#exercise-3-adaptive-instance-normalization-for-style-transfer" title="Permanent link">&para;</a></h3>
<p>Implement a simple style transfer network using AdaIN.</p>
<p><strong>Tasks</strong>:
1. Implement an encoder-decoder architecture with AdaIN in the middle
2. Use a pre-trained VGG network as the encoder (freeze weights)
3. Train a decoder to reconstruct images
4. Implement the AdaIN layer that transfers style statistics
5. Test on content and style images (use torchvision datasets or your own)
6. Visualize the stylized output
7. <strong>Bonus</strong>: Implement controllable style transfer (Î± parameter to blend content/style)</p>
<p><strong>Starter code</strong>:</p>
<div class="highlight"><pre><span></span><code><span class="kn">import</span><span class="w"> </span><span class="nn">torch</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">torch.nn</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">nn</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">torchvision</span><span class="w"> </span><span class="kn">import</span> <span class="n">models</span>

<span class="k">class</span><span class="w"> </span><span class="nc">AdaIN</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="k">def</span><span class="w"> </span><span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>

    <span class="k">def</span><span class="w"> </span><span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">content</span><span class="p">,</span> <span class="n">style</span><span class="p">):</span>
        <span class="c1"># TODO: Implement AdaIN</span>
        <span class="c1"># 1. Calculate mean and std of content</span>
        <span class="c1"># 2. Calculate mean and std of style</span>
        <span class="c1"># 3. Normalize content, apply style statistics</span>
        <span class="k">pass</span>

<span class="k">class</span><span class="w"> </span><span class="nc">StyleTransferNet</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="k">def</span><span class="w"> </span><span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>

        <span class="c1"># Encoder: VGG19 (frozen)</span>
        <span class="n">vgg</span> <span class="o">=</span> <span class="n">models</span><span class="o">.</span><span class="n">vgg19</span><span class="p">(</span><span class="n">pretrained</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span><span class="o">.</span><span class="n">features</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">encoder</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Sequential</span><span class="p">(</span><span class="o">*</span><span class="nb">list</span><span class="p">(</span><span class="n">vgg</span><span class="o">.</span><span class="n">children</span><span class="p">())[:</span><span class="mi">21</span><span class="p">])</span>  <span class="c1"># Up to relu4_1</span>
        <span class="k">for</span> <span class="n">param</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">encoder</span><span class="o">.</span><span class="n">parameters</span><span class="p">():</span>
            <span class="n">param</span><span class="o">.</span><span class="n">requires_grad</span> <span class="o">=</span> <span class="kc">False</span>

        <span class="c1"># AdaIN layer</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">adain</span> <span class="o">=</span> <span class="n">AdaIN</span><span class="p">()</span>

        <span class="c1"># Decoder: mirror of encoder</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">decoder</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Sequential</span><span class="p">(</span>
            <span class="c1"># TODO: Implement decoder (reverse of encoder)</span>
            <span class="c1"># Use ConvTranspose2d or Upsample + Conv2d</span>
        <span class="p">)</span>

    <span class="k">def</span><span class="w"> </span><span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">content</span><span class="p">,</span> <span class="n">style</span><span class="p">):</span>
        <span class="c1"># TODO:</span>
        <span class="c1"># 1. Encode content and style</span>
        <span class="c1"># 2. Apply AdaIN</span>
        <span class="c1"># 3. Decode</span>
        <span class="k">pass</span>

<span class="c1"># TODO: Implement training loop with perceptual loss</span>
</code></pre></div>

<hr />
<h2 id="references">References<a class="header-link" href="#references" title="Permanent link">&para;</a></h2>
<ol>
<li><strong>Batch Normalization</strong>:</li>
<li>Ioffe &amp; Szegedy (2015). "Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift." ICML.</li>
<li>
<p>Santurkar et al. (2018). "How Does Batch Normalization Help Optimization?" NeurIPS.</p>
</li>
<li>
<p><strong>Layer Normalization</strong>:</p>
</li>
<li>
<p>Ba et al. (2016). "Layer Normalization." arXiv:1607.06450.</p>
</li>
<li>
<p><strong>Group Normalization</strong>:</p>
</li>
<li>
<p>Wu &amp; He (2018). "Group Normalization." ECCV.</p>
</li>
<li>
<p><strong>Instance Normalization</strong>:</p>
</li>
<li>
<p>Ulyanov et al. (2016). "Instance Normalization: The Missing Ingredient for Fast Stylization." arXiv:1607.08022.</p>
</li>
<li>
<p><strong>RMSNorm</strong>:</p>
</li>
<li>Zhang &amp; Sennrich (2019). "Root Mean Square Layer Normalization." NeurIPS.</li>
<li>
<p>Touvron et al. (2023). "LLaMA: Open and Efficient Foundation Language Models." arXiv:2302.13971.</p>
</li>
<li>
<p><strong>Spectral Normalization</strong>:</p>
</li>
<li>
<p>Miyato et al. (2018). "Spectral Normalization for Generative Adversarial Networks." ICLR.</p>
</li>
<li>
<p><strong>AdaIN</strong>:</p>
</li>
<li>
<p>Huang &amp; Belongie (2017). "Arbitrary Style Transfer in Real-time with Adaptive Instance Normalization." ICCV.</p>
</li>
<li>
<p><strong>Comprehensive Analysis</strong>:</p>
</li>
<li>Bjorck et al. (2018). "Understanding Batch Normalization." NeurIPS.</li>
<li>
<p>Goyal et al. (2017). "Accurate, Large Minibatch SGD: Training ImageNet in 1 Hour." arXiv:1706.02677.</p>
</li>
<li>
<p><strong>PyTorch Documentation</strong>:</p>
</li>
<li>https://pytorch.org/docs/stable/nn.html#normalization-layers</li>
<li>https://pytorch.org/docs/stable/generated/torch.nn.BatchNorm2d.html</li>
<li>https://pytorch.org/docs/stable/generated/torch.nn.LayerNorm.html</li>
<li>
<p>https://pytorch.org/docs/stable/generated/torch.nn.GroupNorm.html</p>
</li>
<li>
<p><strong>Practical Guides</strong>:</p>
<ul>
<li>He et al. (2019). "Bag of Tricks for Image Classification with Convolutional Neural Networks." CVPR.</li>
<li>Xiong et al. (2020). "On Layer Normalization in the Transformer Architecture." ICML.</li>
</ul>
</li>
</ol>
    </div>

    
<div class="lesson-toolbar lesson-toolbar--bottom">
    <div class="toolbar-nav toolbar-nav--prev">
        
        <a href="/study/en/Deep_Learning/25_Optimizers.html" class="nav-prev">
            <span class="nav-label">Previous</span>
            <span class="nav-title">25. Optimizers</span>
        </a>
        
    </div>
    <div class="toolbar-actions">
        <button class="btn btn-copy-link" data-action="copy-link" title="Copy link">
            <span class="icon">ğŸ”—</span>
            <span class="text">Copy link</span>
        </button>
        <a href="/study/en/Deep_Learning/" class="btn btn-topic-link" title="Back to topic list">
            <span class="icon">ğŸ“‹</span>
            <span class="text">Topic list</span>
        </a>
    </div>
    <div class="toolbar-nav toolbar-nav--next">
        
        <a href="/study/en/Deep_Learning/27_TensorBoard.html" class="nav-next">
            <span class="nav-label">Next</span>
            <span class="nav-title">27. TensorBoard Visualization</span>
        </a>
        
    </div>
</div>


    <div class="toolbar-keyboard-hint">
        <kbd>&larr;</kbd> <kbd>&rarr;</kbd> to navigate between lessons
    </div>
</article>

<!-- Scroll to top floating button -->
<button class="scroll-to-top" id="scroll-to-top" title="Scroll to top" aria-label="Scroll to top">â†‘</button>

            </div>
        </main>
    </div>

    <script src="/study/static/js/app.js"></script>
    <script>
        function switchLanguage(newLang) {
            const path = window.location.pathname;
            const base = '/study';
            const currentLang = 'en';
            const newPath = path.replace(base + '/' + currentLang + '/', base + '/' + newLang + '/');
            window.location.href = newPath + window.location.search;
        }
    </script>
    
<script>
    document.addEventListener('DOMContentLoaded', function() {
        // Copy link - independent feedback per button
        document.querySelectorAll('[data-action="copy-link"]').forEach(function(btn) {
            btn.addEventListener('click', function() {
                navigator.clipboard.writeText(window.location.href);
                var textEl = this.querySelector('.text');
                var originalText = textEl.textContent;
                textEl.textContent = 'Copied!';
                setTimeout(function() { textEl.textContent = originalText; }, 2000);
            });
        });

        // Add copy buttons to code blocks
        document.querySelectorAll('.lesson-content pre').forEach(function(pre) {
            var copyBtn = document.createElement('button');
            copyBtn.className = 'code-copy-btn';
            copyBtn.textContent = 'Copy';
            copyBtn.addEventListener('click', function() {
                var code = pre.querySelector('code');
                var text = code ? code.textContent : pre.textContent;
                navigator.clipboard.writeText(text);
                copyBtn.textContent = 'Copied!';
                setTimeout(function() { copyBtn.textContent = 'Copy'; }, 2000);
            });
            pre.style.position = 'relative';
            pre.appendChild(copyBtn);
        });

        // Scroll to top button
        var scrollBtn = document.getElementById('scroll-to-top');
        window.addEventListener('scroll', function() {
            scrollBtn.classList.toggle('visible', window.scrollY > 300);
        });
        scrollBtn.addEventListener('click', function() {
            window.scrollTo({ top: 0, behavior: 'smooth' });
        });

        // Keyboard shortcuts: left/right arrows for lesson navigation
        document.addEventListener('keydown', function(e) {
            var tag = document.activeElement.tagName;
            if (tag === 'INPUT' || tag === 'TEXTAREA' || tag === 'SELECT') return;

            if (e.key === 'ArrowLeft') {
                var prevLink = document.querySelector('.nav-prev');
                if (prevLink) prevLink.click();
            } else if (e.key === 'ArrowRight') {
                var nextLink = document.querySelector('.nav-next');
                if (nextLink) nextLink.click();
            }
        });
    });
</script>

</body>
</html>
{% endraw %}