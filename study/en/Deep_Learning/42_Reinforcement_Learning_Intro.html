{% raw %}
<!DOCTYPE html>
<html lang="en" data-theme="light">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>42. Reinforcement Learning Introduction - Study Materials</title>
    <link rel="stylesheet" href="/study/static/css/style.css">
    <link rel="stylesheet" href="/study/static/css/highlight.css">
    <!-- KaTeX for LaTeX math rendering -->
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.11/dist/katex.min.css">
    <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.11/dist/katex.min.js"></script>
    <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.11/dist/contrib/auto-render.min.js"
        onload="renderMathInElement(document.body, {
            delimiters: [
                {left: '$$', right: '$$', display: true},
                {left: '$', right: '$', display: false}
            ],
            throwOnError: false
        });"></script>
    
</head>
<body>
    <div class="app-container">
        <!-- Sidebar -->
        <aside class="sidebar" id="sidebar">
            <div class="sidebar-header">
                <a href="/study/en/" class="logo">Study Materials</a>
                <button class="sidebar-toggle" id="sidebar-toggle" aria-label="Toggle sidebar">
                    <span></span>
                </button>
            </div>

            <nav class="sidebar-nav">
                <a href="/study/en/" class="nav-item ">
                    <span class="nav-icon">ğŸ </span>
                    <span class="nav-text">Home</span>
                </a>
                <a href="/study/examples/" class="nav-item ">
                    <span class="nav-icon">ğŸ’»</span>
                    <span class="nav-text">Examples</span>
                </a>
            </nav>

            <div class="sidebar-search">
                <form action="/study/en/search.html" method="get" id="search-form">
                    <input type="search" name="q" placeholder="Search..." id="search-sidebar-input">
                </form>
            </div>

            <!-- Language Selector -->
            <div class="sidebar-lang">
                <select id="lang-select" class="lang-selector" onchange="switchLanguage(this.value)">
                    
                    <option value="en" selected>
                        English
                    </option>
                    
                    <option value="ko" >
                        í•œêµ­ì–´
                    </option>
                    
                </select>
            </div>

            <div class="sidebar-footer">
                <button class="theme-toggle" id="theme-toggle" aria-label="Toggle theme">
                    <span class="theme-icon light">â˜€ï¸</span>
                    <span class="theme-icon dark">ğŸŒ™</span>
                </button>
            </div>
        </aside>

        <!-- Main Content -->
        <main class="main-content">
            <header class="main-header">
                <button class="menu-toggle" id="menu-toggle" aria-label="Open menu">
                    <span></span>
                </button>
                
<nav class="breadcrumb">
    <a href="/study/en/">Topics</a>
    <span class="separator">/</span>
    <a href="/study/en/Deep_Learning/">Deep Learning</a>
    <span class="separator">/</span>
    <span class="current">42. Reinforcement Learning Introduction</span>
</nav>

            </header>

            <div class="content">
                
<article class="lesson-article">
    <header class="lesson-header">
        <h1>42. Reinforcement Learning Introduction</h1>
    </header>

    
<div class="lesson-toolbar lesson-toolbar--top">
    <div class="toolbar-nav toolbar-nav--prev">
        
        <a href="/study/en/Deep_Learning/41_Model_Saving_Deployment.html" class="nav-prev">
            <span class="nav-label">Previous</span>
            <span class="nav-title">41. Model Saving and Deployment</span>
        </a>
        
    </div>
    <div class="toolbar-actions">
        <button class="btn btn-copy-link" data-action="copy-link" title="Copy link">
            <span class="icon">ğŸ”—</span>
            <span class="text">Copy link</span>
        </button>
        <a href="/study/en/Deep_Learning/" class="btn btn-topic-link" title="Back to topic list">
            <span class="icon">ğŸ“‹</span>
            <span class="text">Topic list</span>
        </a>
    </div>
    <div class="toolbar-nav toolbar-nav--next">
        
    </div>
</div>


    
    <nav class="toc" id="toc">
        <h2>Table of Contents</h2>
        <div class="toc">
<ul>
<li><a href="#learning-objectives">Learning Objectives</a></li>
<li><a href="#1-reinforcement-learning-overview">1. Reinforcement Learning Overview</a><ul>
<li><a href="#definition-and-characteristics">Definition and Characteristics</a></li>
<li><a href="#supervised-learning-vs-reinforcement-learning">Supervised Learning vs Reinforcement Learning</a></li>
<li><a href="#applications-of-reinforcement-learning">Applications of Reinforcement Learning</a></li>
</ul>
</li>
<li><a href="#2-mdp-markov-decision-process">2. MDP (Markov Decision Process)</a><ul>
<li><a href="#components">Components</a></li>
<li><a href="#markov-property">Markov Property</a></li>
<li><a href="#interaction-loop">Interaction Loop</a></li>
</ul>
</li>
<li><a href="#3-value-functions">3. Value Functions</a><ul>
<li><a href="#state-value-function-v">State Value Function (V)</a></li>
<li><a href="#action-value-function-q">Action Value Function (Q)</a></li>
<li><a href="#bellman-equation">Bellman Equation</a></li>
</ul>
</li>
<li><a href="#4-q-learning">4. Q-Learning</a><ul>
<li><a href="#algorithm-overview">Algorithm Overview</a></li>
<li><a href="#q-learning-update">Q-Learning Update</a></li>
<li><a href="#pytorch-implementation">PyTorch Implementation</a></li>
</ul>
</li>
<li><a href="#5-deep-q-network-dqn">5. Deep Q-Network (DQN)</a><ul>
<li><a href="#core-idea">Core Idea</a></li>
<li><a href="#dqn-architecture">DQN Architecture</a></li>
<li><a href="#experience-replay">Experience Replay</a></li>
<li><a href="#dqn-pytorch-implementation">DQN PyTorch Implementation</a></li>
<li><a href="#dqn-training-loop">DQN Training Loop</a></li>
</ul>
</li>
<li><a href="#6-policy-gradient">6. Policy Gradient</a><ul>
<li><a href="#idea">Idea</a></li>
<li><a href="#policy-gradient-theorem">Policy Gradient Theorem</a></li>
<li><a href="#reinforce-algorithm">REINFORCE Algorithm</a></li>
<li><a href="#reinforce-training">REINFORCE Training</a></li>
</ul>
</li>
<li><a href="#7-actor-critic">7. Actor-Critic</a><ul>
<li><a href="#idea_1">Idea</a></li>
<li><a href="#advantage-function">Advantage Function</a></li>
<li><a href="#actor-critic-implementation">Actor-Critic Implementation</a></li>
</ul>
</li>
<li><a href="#8-environments-and-experiments">8. Environments and Experiments</a><ul>
<li><a href="#using-openai-gym">Using OpenAI Gym</a></li>
<li><a href="#experiment-example-cartpole">Experiment Example: CartPole</a></li>
</ul>
</li>
<li><a href="#9-algorithm-comparison">9. Algorithm Comparison</a><ul>
<li><a href="#main-algorithm-characteristics">Main Algorithm Characteristics</a></li>
<li><a href="#selection-guide">Selection Guide</a></li>
</ul>
</li>
<li><a href="#10-advanced-topics-overview">10. Advanced Topics Overview</a><ul>
<li><a href="#double-dqn">Double DQN</a></li>
<li><a href="#dueling-dqn">Dueling DQN</a></li>
<li><a href="#prioritized-experience-replay">Prioritized Experience Replay</a></li>
</ul>
</li>
<li><a href="#summary">Summary</a><ul>
<li><a href="#key-concepts">Key Concepts</a></li>
<li><a href="#practical-tips">Practical Tips</a></li>
</ul>
</li>
<li><a href="#references">References</a></li>
</ul>
</div>

    </nav>
    

    <div class="lesson-content markdown-body">
        <p><a href="./41_Model_Saving_Deployment.md">Previous: Model Saving and Deployment</a></p>
<hr />
<h1 id="42-reinforcement-learning-introduction">42. Reinforcement Learning Introduction<a class="header-link" href="#42-reinforcement-learning-introduction" title="Permanent link">&para;</a></h1>
<h2 id="learning-objectives">Learning Objectives<a class="header-link" href="#learning-objectives" title="Permanent link">&para;</a></h2>
<ul>
<li>Understand basic concepts and terminology of reinforcement learning</li>
<li>MDP (Markov Decision Process) framework</li>
<li>Q-Learning and Value-based methods</li>
<li>Policy Gradient overview</li>
<li>Deep RL basics (DQN)</li>
<li>PyTorch implementation and practice</li>
</ul>
<hr />
<h2 id="1-reinforcement-learning-overview">1. Reinforcement Learning Overview<a class="header-link" href="#1-reinforcement-learning-overview" title="Permanent link">&para;</a></h2>
<h3 id="definition-and-characteristics">Definition and Characteristics<a class="header-link" href="#definition-and-characteristics" title="Permanent link">&para;</a></h3>
<div class="highlight"><pre><span></span><code>Reinforcement Learning: Agent learns to maximize rewards through environment interaction

Characteristics:
1. Trial and Error Learning
2. Delayed Reward
3. Exploration-Exploitation tradeoff
4. Sequential Decision Making
</code></pre></div>

<h3 id="supervised-learning-vs-reinforcement-learning">Supervised Learning vs Reinforcement Learning<a class="header-link" href="#supervised-learning-vs-reinforcement-learning" title="Permanent link">&para;</a></h3>
<div class="highlight"><pre><span></span><code>â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚           Supervised Learning vs Reinforcement Learning      â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                              â”‚
â”‚  Supervised Learning                                         â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”   answer    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”                        â”‚
â”‚  â”‚ Input x â”‚ â”€â”€â”€â”€â”€â”€â”€â”€â”€â†’ â”‚ Label y â”‚                        â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜             â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                        â”‚
â”‚  Immediate feedback, correct answer provided                 â”‚
â”‚                                                              â”‚
â”‚  Reinforcement Learning                                      â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”  action  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”  reward  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”‚
â”‚  â”‚ State s â”‚ â”€â”€â”€â”€â”€â”€â†’ â”‚ Action aâ”‚ â”€â”€â”€â”€â”€â”€â†’ â”‚ Reward râ”‚     â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â”‚
â”‚       â†‘                    â”‚                   â”‚             â”‚
â”‚       â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜             â”‚
â”‚  Delayed feedback, exploration required                      â”‚
â”‚                                                              â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
</code></pre></div>

<h3 id="applications-of-reinforcement-learning">Applications of Reinforcement Learning<a class="header-link" href="#applications-of-reinforcement-learning" title="Permanent link">&para;</a></h3>
<div class="highlight"><pre><span></span><code><span class="n">Games</span><span class="o">:</span><span class="w"> </span><span class="n">AlphaGo</span><span class="o">,</span><span class="w"> </span><span class="n">Atari</span><span class="o">,</span><span class="w"> </span><span class="n">StarCraft</span><span class="w"> </span><span class="n">II</span>
<span class="n">Robotics</span><span class="o">:</span><span class="w"> </span><span class="n">Robot</span><span class="w"> </span><span class="n">control</span><span class="o">,</span><span class="w"> </span><span class="n">autonomous</span><span class="w"> </span><span class="n">driving</span>
<span class="n">Finance</span><span class="o">:</span><span class="w"> </span><span class="n">Portfolio</span><span class="w"> </span><span class="n">optimization</span><span class="o">,</span><span class="w"> </span><span class="n">algorithmic</span><span class="w"> </span><span class="n">trading</span>
<span class="n">Recommendation</span><span class="o">:</span><span class="w"> </span><span class="n">Personalized</span><span class="w"> </span><span class="n">recommendations</span><span class="o">,</span><span class="w"> </span><span class="n">dialogue</span><span class="w"> </span><span class="n">systems</span>
<span class="n">Resource</span><span class="w"> </span><span class="n">management</span><span class="o">:</span><span class="w"> </span><span class="n">Datacenter</span><span class="w"> </span><span class="n">cooling</span><span class="o">,</span><span class="w"> </span><span class="n">network</span><span class="w"> </span><span class="n">optimization</span>
</code></pre></div>

<hr />
<h2 id="2-mdp-markov-decision-process">2. MDP (Markov Decision Process)<a class="header-link" href="#2-mdp-markov-decision-process" title="Permanent link">&para;</a></h2>
<h3 id="components">Components<a class="header-link" href="#components" title="Permanent link">&para;</a></h3>
<div class="highlight"><pre><span></span><code><span class="nx">MDP</span><span class="w"> </span><span class="p">=</span><span class="w"> </span><span class="p">(</span><span class="nx">S</span><span class="p">,</span><span class="w"> </span><span class="nx">A</span><span class="p">,</span><span class="w"> </span><span class="nx">P</span><span class="p">,</span><span class="w"> </span><span class="nx">R</span><span class="p">,</span><span class="w"> </span><span class="nx">Î³</span><span class="p">)</span>

<span class="nx">S</span><span class="p">:</span><span class="w"> </span><span class="nx">State</span><span class="w"> </span><span class="p">(</span><span class="nx">state</span><span class="w"> </span><span class="nx">set</span><span class="p">)</span>
<span class="w">   </span><span class="o">-</span><span class="w"> </span><span class="nx">Environment</span><span class="w"> </span><span class="nx">state</span><span class="w"> </span><span class="nx">observed</span><span class="w"> </span><span class="nx">by</span><span class="w"> </span><span class="nx">agent</span>
<span class="w">   </span><span class="o">-</span><span class="w"> </span><span class="nx">e</span><span class="p">.</span><span class="nx">g</span><span class="p">.,</span><span class="w"> </span><span class="nx">game</span><span class="w"> </span><span class="nx">screen</span><span class="p">,</span><span class="w"> </span><span class="nx">robot</span><span class="w"> </span><span class="nx">position</span><span class="o">/</span><span class="nx">velocity</span>

<span class="nx">A</span><span class="p">:</span><span class="w"> </span><span class="nx">Action</span><span class="w"> </span><span class="p">(</span><span class="nx">action</span><span class="w"> </span><span class="nx">set</span><span class="p">)</span>
<span class="w">   </span><span class="o">-</span><span class="w"> </span><span class="nx">Actions</span><span class="w"> </span><span class="nx">agent</span><span class="w"> </span><span class="nx">can</span><span class="w"> </span><span class="nx">take</span>
<span class="w">   </span><span class="o">-</span><span class="w"> </span><span class="nx">e</span><span class="p">.</span><span class="nx">g</span><span class="p">.,</span><span class="w"> </span><span class="nx">move</span><span class="w"> </span><span class="nx">up</span><span class="o">/</span><span class="nx">down</span><span class="o">/</span><span class="nx">left</span><span class="o">/</span><span class="nx">right</span><span class="p">,</span><span class="w"> </span><span class="nx">motor</span><span class="w"> </span><span class="nx">torque</span>

<span class="nx">P</span><span class="p">:</span><span class="w"> </span><span class="nx">Transition</span><span class="w"> </span><span class="nx">Probability</span>
<span class="w">   </span><span class="o">-</span><span class="w"> </span><span class="nx">P</span><span class="p">(</span><span class="nx">s</span><span class="err">&#39;</span><span class="o">|</span><span class="nx">s</span><span class="p">,</span><span class="w"> </span><span class="nx">a</span><span class="p">):</span><span class="w"> </span><span class="nx">probability</span><span class="w"> </span><span class="nx">of</span><span class="w"> </span><span class="nx">transitioning</span><span class="w"> </span><span class="nx">to</span><span class="w"> </span><span class="nx">s</span><span class="err">&#39;</span><span class="w"> </span><span class="nx">when</span><span class="w"> </span><span class="nx">taking</span><span class="w"> </span><span class="nx">action</span><span class="w"> </span><span class="nx">a</span><span class="w"> </span><span class="k">in</span><span class="w"> </span><span class="nx">state</span><span class="w"> </span><span class="nx">s</span>

<span class="nx">R</span><span class="p">:</span><span class="w"> </span><span class="nx">Reward</span><span class="w"> </span><span class="p">(</span><span class="nx">reward</span><span class="w"> </span><span class="nx">function</span><span class="p">)</span>
<span class="w">   </span><span class="o">-</span><span class="w"> </span><span class="nx">R</span><span class="p">(</span><span class="nx">s</span><span class="p">,</span><span class="w"> </span><span class="nx">a</span><span class="p">,</span><span class="w"> </span><span class="nx">s</span><span class="err">&#39;</span><span class="p">):</span><span class="w"> </span><span class="nx">reward</span><span class="w"> </span><span class="nx">received</span><span class="w"> </span><span class="nx">during</span><span class="w"> </span><span class="nx">state</span><span class="w"> </span><span class="nx">transition</span>

<span class="nx">Î³</span><span class="p">:</span><span class="w"> </span><span class="nx">Discount</span><span class="w"> </span><span class="nx">Factor</span>
<span class="w">   </span><span class="o">-</span><span class="w"> </span><span class="nx">Present</span><span class="w"> </span><span class="nx">value</span><span class="w"> </span><span class="nx">of</span><span class="w"> </span><span class="nx">future</span><span class="w"> </span><span class="nx">rewards</span><span class="w"> </span><span class="p">(</span><span class="mi">0</span><span class="w"> </span><span class="p">&lt;</span><span class="w"> </span><span class="nx">Î³</span><span class="w"> </span><span class="err">â‰¤</span><span class="w"> </span><span class="mi">1</span><span class="p">)</span>
</code></pre></div>

<h3 id="markov-property">Markov Property<a class="header-link" href="#markov-property" title="Permanent link">&para;</a></h3>
<div class="highlight"><pre><span></span><code>Future depends only on current state (independent of past):

P(s_{t+1} | s_t, a_t) = P(s_{t+1} | s_0, a_0, ..., s_t, a_t)

Meaning: Current state contains sufficient information
</code></pre></div>

<h3 id="interaction-loop">Interaction Loop<a class="header-link" href="#interaction-loop" title="Permanent link">&para;</a></h3>
<div class="highlight"><pre><span></span><code><span class="c1"># RL basic loop</span>
<span class="k">def</span><span class="w"> </span><span class="nf">rl_loop</span><span class="p">(</span><span class="n">env</span><span class="p">,</span> <span class="n">agent</span><span class="p">,</span> <span class="n">episodes</span><span class="o">=</span><span class="mi">1000</span><span class="p">):</span>
    <span class="k">for</span> <span class="n">episode</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">episodes</span><span class="p">):</span>
        <span class="n">state</span> <span class="o">=</span> <span class="n">env</span><span class="o">.</span><span class="n">reset</span><span class="p">()</span>
        <span class="n">done</span> <span class="o">=</span> <span class="kc">False</span>
        <span class="n">total_reward</span> <span class="o">=</span> <span class="mi">0</span>

        <span class="k">while</span> <span class="ow">not</span> <span class="n">done</span><span class="p">:</span>
            <span class="c1"># 1. Agent selects action</span>
            <span class="n">action</span> <span class="o">=</span> <span class="n">agent</span><span class="o">.</span><span class="n">select_action</span><span class="p">(</span><span class="n">state</span><span class="p">)</span>

            <span class="c1"># 2. Environment returns next state and reward</span>
            <span class="n">next_state</span><span class="p">,</span> <span class="n">reward</span><span class="p">,</span> <span class="n">done</span><span class="p">,</span> <span class="n">info</span> <span class="o">=</span> <span class="n">env</span><span class="o">.</span><span class="n">step</span><span class="p">(</span><span class="n">action</span><span class="p">)</span>

            <span class="c1"># 3. Agent learns from experience</span>
            <span class="n">agent</span><span class="o">.</span><span class="n">learn</span><span class="p">(</span><span class="n">state</span><span class="p">,</span> <span class="n">action</span><span class="p">,</span> <span class="n">reward</span><span class="p">,</span> <span class="n">next_state</span><span class="p">,</span> <span class="n">done</span><span class="p">)</span>

            <span class="c1"># 4. Update state</span>
            <span class="n">state</span> <span class="o">=</span> <span class="n">next_state</span>
            <span class="n">total_reward</span> <span class="o">+=</span> <span class="n">reward</span>

        <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Episode </span><span class="si">{</span><span class="n">episode</span><span class="si">}</span><span class="s2">: Total Reward = </span><span class="si">{</span><span class="n">total_reward</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
</code></pre></div>

<hr />
<h2 id="3-value-functions">3. Value Functions<a class="header-link" href="#3-value-functions" title="Permanent link">&para;</a></h2>
<h3 id="state-value-function-v">State Value Function (V)<a class="header-link" href="#state-value-function-v" title="Permanent link">&para;</a></h3>
<div class="highlight"><pre><span></span><code>V^Ï€(s) = E[G_t | S_t = s, Ï€]

G_t = R_{t+1} + Î³R_{t+2} + Î³Â²R_{t+3} + ...
    = Î£_{k=0}^âˆ Î³^k R_{t+k+1}

Meaning: Expected cumulative reward when following policy Ï€ from state s
</code></pre></div>

<h3 id="action-value-function-q">Action Value Function (Q)<a class="header-link" href="#action-value-function-q" title="Permanent link">&para;</a></h3>
<div class="highlight"><pre><span></span><code>Q^Ï€(s, a) = E[G_t | S_t = s, A_t = a, Ï€]

Meaning: Expected reward when taking action a in state s, then following Ï€
</code></pre></div>

<h3 id="bellman-equation">Bellman Equation<a class="header-link" href="#bellman-equation" title="Permanent link">&para;</a></h3>
<div class="highlight"><pre><span></span><code><span class="c1"># Bellman equation (core!)</span>

<span class="c1"># Value Function</span>
<span class="n">V</span><span class="p">(</span><span class="n">s</span><span class="p">)</span> <span class="o">=</span> <span class="n">max_a</span> <span class="p">[</span> <span class="n">R</span><span class="p">(</span><span class="n">s</span><span class="p">,</span> <span class="n">a</span><span class="p">)</span> <span class="o">+</span> <span class="n">Î³</span> <span class="o">*</span> <span class="n">Î£</span> <span class="n">P</span><span class="p">(</span><span class="n">s</span><span class="s1">&#39;|s,a) * V(s&#39;</span><span class="p">)</span> <span class="p">]</span>

<span class="c1"># Q Function</span>
<span class="n">Q</span><span class="p">(</span><span class="n">s</span><span class="p">,</span> <span class="n">a</span><span class="p">)</span> <span class="o">=</span> <span class="n">R</span><span class="p">(</span><span class="n">s</span><span class="p">,</span> <span class="n">a</span><span class="p">)</span> <span class="o">+</span> <span class="n">Î³</span> <span class="o">*</span> <span class="n">Î£</span> <span class="n">P</span><span class="p">(</span><span class="n">s</span><span class="s1">&#39;|s,a) * max_a&#39;</span> <span class="n">Q</span><span class="p">(</span><span class="n">s</span><span class="s1">&#39;, a&#39;</span><span class="p">)</span>

<span class="c1"># Meaning: Current value = immediate reward + discounted future value</span>
</code></pre></div>

<hr />
<h2 id="4-q-learning">4. Q-Learning<a class="header-link" href="#4-q-learning" title="Permanent link">&para;</a></h2>
<h3 id="algorithm-overview">Algorithm Overview<a class="header-link" href="#algorithm-overview" title="Permanent link">&para;</a></h3>
<div class="highlight"><pre><span></span><code>Q-Learning: Model-free, Off-policy algorithm

Features:
1. No environment model (P) required
2. Learn optimal policy from data collected with different policy (Îµ-greedy)
3. Store Q values in table form
</code></pre></div>

<h3 id="q-learning-update">Q-Learning Update<a class="header-link" href="#q-learning-update" title="Permanent link">&para;</a></h3>
<div class="highlight"><pre><span></span><code><span class="c1"># Q-Learning update rule</span>

<span class="n">Q</span><span class="p">(</span><span class="n">s</span><span class="p">,</span> <span class="n">a</span><span class="p">)</span> <span class="err">â†</span> <span class="n">Q</span><span class="p">(</span><span class="n">s</span><span class="p">,</span> <span class="n">a</span><span class="p">)</span> <span class="o">+</span> <span class="n">Î±</span> <span class="o">*</span> <span class="p">[</span><span class="n">r</span> <span class="o">+</span> <span class="n">Î³</span> <span class="o">*</span> <span class="n">max_a</span><span class="s1">&#39; Q(s&#39;</span><span class="p">,</span> <span class="n">a</span><span class="s1">&#39;) - Q(s, a)]</span>

<span class="c1"># Breakdown:</span>
<span class="c1"># TD Target: r + Î³ * max_a&#39; Q(s&#39;, a&#39;)  (target)</span>
<span class="c1"># TD Error: TD Target - Q(s, a)        (error)</span>
<span class="c1"># Î±: Learning Rate</span>
</code></pre></div>

<h3 id="pytorch-implementation">PyTorch Implementation<a class="header-link" href="#pytorch-implementation" title="Permanent link">&para;</a></h3>
<div class="highlight"><pre><span></span><code><span class="kn">import</span><span class="w"> </span><span class="nn">numpy</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">np</span>

<span class="k">class</span><span class="w"> </span><span class="nc">QLearningAgent</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Q-Learning Agent (Tabular) (â­â­â­)&quot;&quot;&quot;</span>
    <span class="k">def</span><span class="w"> </span><span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">state_size</span><span class="p">,</span> <span class="n">action_size</span><span class="p">,</span> <span class="n">lr</span><span class="o">=</span><span class="mf">0.1</span><span class="p">,</span> <span class="n">gamma</span><span class="o">=</span><span class="mf">0.99</span><span class="p">,</span> <span class="n">epsilon</span><span class="o">=</span><span class="mf">1.0</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">state_size</span> <span class="o">=</span> <span class="n">state_size</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">action_size</span> <span class="o">=</span> <span class="n">action_size</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">lr</span> <span class="o">=</span> <span class="n">lr</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">gamma</span> <span class="o">=</span> <span class="n">gamma</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">epsilon</span> <span class="o">=</span> <span class="n">epsilon</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">epsilon_min</span> <span class="o">=</span> <span class="mf">0.01</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">epsilon_decay</span> <span class="o">=</span> <span class="mf">0.995</span>

        <span class="c1"># Initialize Q-Table</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">q_table</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">((</span><span class="n">state_size</span><span class="p">,</span> <span class="n">action_size</span><span class="p">))</span>

    <span class="k">def</span><span class="w"> </span><span class="nf">select_action</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">state</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Îµ-greedy action selection&quot;&quot;&quot;</span>
        <span class="k">if</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">random</span><span class="p">()</span> <span class="o">&lt;</span> <span class="bp">self</span><span class="o">.</span><span class="n">epsilon</span><span class="p">:</span>
            <span class="k">return</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">randint</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">action_size</span><span class="p">)</span>  <span class="c1"># explore</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="k">return</span> <span class="n">np</span><span class="o">.</span><span class="n">argmax</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">q_table</span><span class="p">[</span><span class="n">state</span><span class="p">])</span>       <span class="c1"># exploit</span>

    <span class="k">def</span><span class="w"> </span><span class="nf">learn</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">state</span><span class="p">,</span> <span class="n">action</span><span class="p">,</span> <span class="n">reward</span><span class="p">,</span> <span class="n">next_state</span><span class="p">,</span> <span class="n">done</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Update Q-Table&quot;&quot;&quot;</span>
        <span class="c1"># TD Target</span>
        <span class="k">if</span> <span class="n">done</span><span class="p">:</span>
            <span class="n">td_target</span> <span class="o">=</span> <span class="n">reward</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">td_target</span> <span class="o">=</span> <span class="n">reward</span> <span class="o">+</span> <span class="bp">self</span><span class="o">.</span><span class="n">gamma</span> <span class="o">*</span> <span class="n">np</span><span class="o">.</span><span class="n">max</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">q_table</span><span class="p">[</span><span class="n">next_state</span><span class="p">])</span>

        <span class="c1"># TD Error</span>
        <span class="n">td_error</span> <span class="o">=</span> <span class="n">td_target</span> <span class="o">-</span> <span class="bp">self</span><span class="o">.</span><span class="n">q_table</span><span class="p">[</span><span class="n">state</span><span class="p">,</span> <span class="n">action</span><span class="p">]</span>

        <span class="c1"># Update</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">q_table</span><span class="p">[</span><span class="n">state</span><span class="p">,</span> <span class="n">action</span><span class="p">]</span> <span class="o">+=</span> <span class="bp">self</span><span class="o">.</span><span class="n">lr</span> <span class="o">*</span> <span class="n">td_error</span>

        <span class="c1"># Epsilon Decay</span>
        <span class="k">if</span> <span class="n">done</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">epsilon</span> <span class="o">=</span> <span class="nb">max</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">epsilon_min</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">epsilon</span> <span class="o">*</span> <span class="bp">self</span><span class="o">.</span><span class="n">epsilon_decay</span><span class="p">)</span>

    <span class="k">def</span><span class="w"> </span><span class="nf">get_policy</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Return learned policy&quot;&quot;&quot;</span>
        <span class="k">return</span> <span class="n">np</span><span class="o">.</span><span class="n">argmax</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">q_table</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
</code></pre></div>

<hr />
<h2 id="5-deep-q-network-dqn">5. Deep Q-Network (DQN)<a class="header-link" href="#5-deep-q-network-dqn" title="Permanent link">&para;</a></h2>
<h3 id="core-idea">Core Idea<a class="header-link" href="#core-idea" title="Permanent link">&para;</a></h3>
<div class="highlight"><pre><span></span><code><span class="n">Problem</span><span class="o">:</span><span class="w"> </span><span class="n">Q</span><span class="o">-</span><span class="n">Table</span><span class="w"> </span><span class="n">infeasible</span><span class="w"> </span><span class="k">for</span><span class="w"> </span><span class="n">large</span><span class="w"> </span><span class="n">state</span><span class="w"> </span><span class="n">spaces</span>
<span class="n">Solution</span><span class="o">:</span><span class="w"> </span><span class="n">Approximate</span><span class="w"> </span><span class="n">Q</span><span class="o">(</span><span class="n">s</span><span class="o">,</span><span class="w"> </span><span class="n">a</span><span class="o">)</span><span class="w"> </span><span class="k">with</span><span class="w"> </span><span class="n">neural</span><span class="w"> </span><span class="n">network</span>

<span class="n">Q</span><span class="o">(</span><span class="n">s</span><span class="o">,</span><span class="w"> </span><span class="n">a</span><span class="o">;</span><span class="w"> </span><span class="err">Î¸</span><span class="o">)</span><span class="w"> </span><span class="err">â‰ˆ</span><span class="w"> </span><span class="n">Q</span><span class="o">*(</span><span class="n">s</span><span class="o">,</span><span class="w"> </span><span class="n">a</span><span class="o">)</span>

<span class="n">Key</span><span class="w"> </span><span class="n">techniques</span><span class="o">:</span>
<span class="mi">1</span><span class="o">.</span><span class="w"> </span><span class="n">Experience</span><span class="w"> </span><span class="n">Replay</span><span class="o">:</span><span class="w"> </span><span class="n">Improve</span><span class="w"> </span><span class="n">efficiency</span><span class="w"> </span><span class="n">by</span><span class="w"> </span><span class="n">reusing</span><span class="w"> </span><span class="n">experiences</span>
<span class="mi">2</span><span class="o">.</span><span class="w"> </span><span class="n">Target</span><span class="w"> </span><span class="n">Network</span><span class="o">:</span><span class="w"> </span><span class="n">Stabilize</span><span class="w"> </span><span class="n">training</span>
</code></pre></div>

<h3 id="dqn-architecture">DQN Architecture<a class="header-link" href="#dqn-architecture" title="Permanent link">&para;</a></h3>
<div class="highlight"><pre><span></span><code>â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                     DQN Architecture                         â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                              â”‚
â”‚  State s                                                     â”‚
â”‚     â”‚                                                        â”‚
â”‚     â–¼                                                        â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                    â”‚
â”‚  â”‚  Neural Network (CNN/MLP)           â”‚                    â”‚
â”‚  â”‚  Input: State s                     â”‚                    â”‚
â”‚  â”‚  Output: Q(s, a) for all actions    â”‚                    â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                    â”‚
â”‚     â”‚                                                        â”‚
â”‚     â–¼                                                        â”‚
â”‚  [Q(s, a_1), Q(s, a_2), ..., Q(s, a_n)]                     â”‚
â”‚     â”‚                                                        â”‚
â”‚     â–¼                                                        â”‚
â”‚  Action = argmax_a Q(s, a)                                  â”‚
â”‚                                                              â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
</code></pre></div>

<h3 id="experience-replay">Experience Replay<a class="header-link" href="#experience-replay" title="Permanent link">&para;</a></h3>
<div class="highlight"><pre><span></span><code><span class="kn">from</span><span class="w"> </span><span class="nn">collections</span><span class="w"> </span><span class="kn">import</span> <span class="n">deque</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">random</span>

<span class="k">class</span><span class="w"> </span><span class="nc">ReplayBuffer</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Experience Replay Buffer (â­â­â­)&quot;&quot;&quot;</span>
    <span class="k">def</span><span class="w"> </span><span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">capacity</span><span class="o">=</span><span class="mi">10000</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">buffer</span> <span class="o">=</span> <span class="n">deque</span><span class="p">(</span><span class="n">maxlen</span><span class="o">=</span><span class="n">capacity</span><span class="p">)</span>

    <span class="k">def</span><span class="w"> </span><span class="nf">push</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">state</span><span class="p">,</span> <span class="n">action</span><span class="p">,</span> <span class="n">reward</span><span class="p">,</span> <span class="n">next_state</span><span class="p">,</span> <span class="n">done</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Store experience&quot;&quot;&quot;</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">buffer</span><span class="o">.</span><span class="n">append</span><span class="p">((</span><span class="n">state</span><span class="p">,</span> <span class="n">action</span><span class="p">,</span> <span class="n">reward</span><span class="p">,</span> <span class="n">next_state</span><span class="p">,</span> <span class="n">done</span><span class="p">))</span>

    <span class="k">def</span><span class="w"> </span><span class="nf">sample</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">batch_size</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Random sampling&quot;&quot;&quot;</span>
        <span class="n">batch</span> <span class="o">=</span> <span class="n">random</span><span class="o">.</span><span class="n">sample</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">buffer</span><span class="p">,</span> <span class="n">batch_size</span><span class="p">)</span>
        <span class="n">states</span><span class="p">,</span> <span class="n">actions</span><span class="p">,</span> <span class="n">rewards</span><span class="p">,</span> <span class="n">next_states</span><span class="p">,</span> <span class="n">dones</span> <span class="o">=</span> <span class="nb">zip</span><span class="p">(</span><span class="o">*</span><span class="n">batch</span><span class="p">)</span>
        <span class="k">return</span> <span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">states</span><span class="p">),</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">actions</span><span class="p">),</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">rewards</span><span class="p">),</span>
                <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">next_states</span><span class="p">),</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">dones</span><span class="p">))</span>

    <span class="k">def</span><span class="w"> </span><span class="fm">__len__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="k">return</span> <span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">buffer</span><span class="p">)</span>
</code></pre></div>

<h3 id="dqn-pytorch-implementation">DQN PyTorch Implementation<a class="header-link" href="#dqn-pytorch-implementation" title="Permanent link">&para;</a></h3>
<div class="highlight"><pre><span></span><code><span class="kn">import</span><span class="w"> </span><span class="nn">torch</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">torch.nn</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">nn</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">torch.optim</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">optim</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">copy</span>

<span class="k">class</span><span class="w"> </span><span class="nc">QNetwork</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Q-Network (MLP) (â­â­â­)&quot;&quot;&quot;</span>
    <span class="k">def</span><span class="w"> </span><span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">state_dim</span><span class="p">,</span> <span class="n">action_dim</span><span class="p">,</span> <span class="n">hidden_dim</span><span class="o">=</span><span class="mi">128</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">net</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Sequential</span><span class="p">(</span>
            <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">state_dim</span><span class="p">,</span> <span class="n">hidden_dim</span><span class="p">),</span>
            <span class="n">nn</span><span class="o">.</span><span class="n">ReLU</span><span class="p">(),</span>
            <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">hidden_dim</span><span class="p">,</span> <span class="n">hidden_dim</span><span class="p">),</span>
            <span class="n">nn</span><span class="o">.</span><span class="n">ReLU</span><span class="p">(),</span>
            <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">hidden_dim</span><span class="p">,</span> <span class="n">action_dim</span><span class="p">)</span>
        <span class="p">)</span>

    <span class="k">def</span><span class="w"> </span><span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">net</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>


<span class="k">class</span><span class="w"> </span><span class="nc">DQNAgent</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;DQN Agent (â­â­â­â­)&quot;&quot;&quot;</span>
    <span class="k">def</span><span class="w"> </span><span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">state_dim</span><span class="p">,</span> <span class="n">action_dim</span><span class="p">,</span> <span class="n">lr</span><span class="o">=</span><span class="mf">1e-3</span><span class="p">,</span> <span class="n">gamma</span><span class="o">=</span><span class="mf">0.99</span><span class="p">,</span>
                 <span class="n">epsilon</span><span class="o">=</span><span class="mf">1.0</span><span class="p">,</span> <span class="n">epsilon_min</span><span class="o">=</span><span class="mf">0.01</span><span class="p">,</span> <span class="n">epsilon_decay</span><span class="o">=</span><span class="mf">0.995</span><span class="p">,</span>
                 <span class="n">buffer_size</span><span class="o">=</span><span class="mi">10000</span><span class="p">,</span> <span class="n">batch_size</span><span class="o">=</span><span class="mi">64</span><span class="p">,</span> <span class="n">target_update</span><span class="o">=</span><span class="mi">100</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">state_dim</span> <span class="o">=</span> <span class="n">state_dim</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">action_dim</span> <span class="o">=</span> <span class="n">action_dim</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">gamma</span> <span class="o">=</span> <span class="n">gamma</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">epsilon</span> <span class="o">=</span> <span class="n">epsilon</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">epsilon_min</span> <span class="o">=</span> <span class="n">epsilon_min</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">epsilon_decay</span> <span class="o">=</span> <span class="n">epsilon_decay</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">batch_size</span> <span class="o">=</span> <span class="n">batch_size</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">target_update</span> <span class="o">=</span> <span class="n">target_update</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">learn_step</span> <span class="o">=</span> <span class="mi">0</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">device</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">device</span><span class="p">(</span><span class="s1">&#39;cuda&#39;</span> <span class="k">if</span> <span class="n">torch</span><span class="o">.</span><span class="n">cuda</span><span class="o">.</span><span class="n">is_available</span><span class="p">()</span> <span class="k">else</span> <span class="s1">&#39;cpu&#39;</span><span class="p">)</span>

        <span class="c1"># Q-Networks</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">q_network</span> <span class="o">=</span> <span class="n">QNetwork</span><span class="p">(</span><span class="n">state_dim</span><span class="p">,</span> <span class="n">action_dim</span><span class="p">)</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">device</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">target_network</span> <span class="o">=</span> <span class="n">copy</span><span class="o">.</span><span class="n">deepcopy</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">q_network</span><span class="p">)</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">optimizer</span> <span class="o">=</span> <span class="n">optim</span><span class="o">.</span><span class="n">Adam</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">q_network</span><span class="o">.</span><span class="n">parameters</span><span class="p">(),</span> <span class="n">lr</span><span class="o">=</span><span class="n">lr</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">criterion</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">MSELoss</span><span class="p">()</span>

        <span class="c1"># Replay Buffer</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">replay_buffer</span> <span class="o">=</span> <span class="n">ReplayBuffer</span><span class="p">(</span><span class="n">buffer_size</span><span class="p">)</span>

    <span class="k">def</span><span class="w"> </span><span class="nf">select_action</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">state</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Îµ-greedy action selection&quot;&quot;&quot;</span>
        <span class="k">if</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">random</span><span class="p">()</span> <span class="o">&lt;</span> <span class="bp">self</span><span class="o">.</span><span class="n">epsilon</span><span class="p">:</span>
            <span class="k">return</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">randint</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">action_dim</span><span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">state</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">FloatTensor</span><span class="p">(</span><span class="n">state</span><span class="p">)</span><span class="o">.</span><span class="n">unsqueeze</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">device</span><span class="p">)</span>
            <span class="k">with</span> <span class="n">torch</span><span class="o">.</span><span class="n">no_grad</span><span class="p">():</span>
                <span class="n">q_values</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">q_network</span><span class="p">(</span><span class="n">state</span><span class="p">)</span>
            <span class="k">return</span> <span class="n">q_values</span><span class="o">.</span><span class="n">argmax</span><span class="p">()</span><span class="o">.</span><span class="n">item</span><span class="p">()</span>

    <span class="k">def</span><span class="w"> </span><span class="nf">store_transition</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">state</span><span class="p">,</span> <span class="n">action</span><span class="p">,</span> <span class="n">reward</span><span class="p">,</span> <span class="n">next_state</span><span class="p">,</span> <span class="n">done</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Store experience&quot;&quot;&quot;</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">replay_buffer</span><span class="o">.</span><span class="n">push</span><span class="p">(</span><span class="n">state</span><span class="p">,</span> <span class="n">action</span><span class="p">,</span> <span class="n">reward</span><span class="p">,</span> <span class="n">next_state</span><span class="p">,</span> <span class="n">done</span><span class="p">)</span>

    <span class="k">def</span><span class="w"> </span><span class="nf">learn</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;DQN training&quot;&quot;&quot;</span>
        <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">replay_buffer</span><span class="p">)</span> <span class="o">&lt;</span> <span class="bp">self</span><span class="o">.</span><span class="n">batch_size</span><span class="p">:</span>
            <span class="k">return</span>

        <span class="c1"># Sample batch</span>
        <span class="n">states</span><span class="p">,</span> <span class="n">actions</span><span class="p">,</span> <span class="n">rewards</span><span class="p">,</span> <span class="n">next_states</span><span class="p">,</span> <span class="n">dones</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">replay_buffer</span><span class="o">.</span><span class="n">sample</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">batch_size</span><span class="p">)</span>

        <span class="n">states</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">FloatTensor</span><span class="p">(</span><span class="n">states</span><span class="p">)</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">device</span><span class="p">)</span>
        <span class="n">actions</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">LongTensor</span><span class="p">(</span><span class="n">actions</span><span class="p">)</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">device</span><span class="p">)</span>
        <span class="n">rewards</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">FloatTensor</span><span class="p">(</span><span class="n">rewards</span><span class="p">)</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">device</span><span class="p">)</span>
        <span class="n">next_states</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">FloatTensor</span><span class="p">(</span><span class="n">next_states</span><span class="p">)</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">device</span><span class="p">)</span>
        <span class="n">dones</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">FloatTensor</span><span class="p">(</span><span class="n">dones</span><span class="p">)</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">device</span><span class="p">)</span>

        <span class="c1"># Current Q values</span>
        <span class="n">current_q</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">q_network</span><span class="p">(</span><span class="n">states</span><span class="p">)</span><span class="o">.</span><span class="n">gather</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">actions</span><span class="o">.</span><span class="n">unsqueeze</span><span class="p">(</span><span class="mi">1</span><span class="p">))</span><span class="o">.</span><span class="n">squeeze</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span>

        <span class="c1"># Target Q values (with target network)</span>
        <span class="k">with</span> <span class="n">torch</span><span class="o">.</span><span class="n">no_grad</span><span class="p">():</span>
            <span class="n">next_q</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">target_network</span><span class="p">(</span><span class="n">next_states</span><span class="p">)</span><span class="o">.</span><span class="n">max</span><span class="p">(</span><span class="mi">1</span><span class="p">)[</span><span class="mi">0</span><span class="p">]</span>
            <span class="n">target_q</span> <span class="o">=</span> <span class="n">rewards</span> <span class="o">+</span> <span class="bp">self</span><span class="o">.</span><span class="n">gamma</span> <span class="o">*</span> <span class="n">next_q</span> <span class="o">*</span> <span class="p">(</span><span class="mi">1</span> <span class="o">-</span> <span class="n">dones</span><span class="p">)</span>

        <span class="c1"># Loss and update</span>
        <span class="n">loss</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">criterion</span><span class="p">(</span><span class="n">current_q</span><span class="p">,</span> <span class="n">target_q</span><span class="p">)</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">optimizer</span><span class="o">.</span><span class="n">zero_grad</span><span class="p">()</span>
        <span class="n">loss</span><span class="o">.</span><span class="n">backward</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">optimizer</span><span class="o">.</span><span class="n">step</span><span class="p">()</span>

        <span class="c1"># Update target network</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">learn_step</span> <span class="o">+=</span> <span class="mi">1</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">learn_step</span> <span class="o">%</span> <span class="bp">self</span><span class="o">.</span><span class="n">target_update</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">target_network</span><span class="o">.</span><span class="n">load_state_dict</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">q_network</span><span class="o">.</span><span class="n">state_dict</span><span class="p">())</span>

        <span class="c1"># Epsilon decay</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">epsilon</span> <span class="o">=</span> <span class="nb">max</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">epsilon_min</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">epsilon</span> <span class="o">*</span> <span class="bp">self</span><span class="o">.</span><span class="n">epsilon_decay</span><span class="p">)</span>

        <span class="k">return</span> <span class="n">loss</span><span class="o">.</span><span class="n">item</span><span class="p">()</span>
</code></pre></div>

<h3 id="dqn-training-loop">DQN Training Loop<a class="header-link" href="#dqn-training-loop" title="Permanent link">&para;</a></h3>
<div class="highlight"><pre><span></span><code><span class="k">def</span><span class="w"> </span><span class="nf">train_dqn</span><span class="p">(</span><span class="n">env</span><span class="p">,</span> <span class="n">agent</span><span class="p">,</span> <span class="n">episodes</span><span class="o">=</span><span class="mi">500</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;DQN Training Loop (â­â­â­)&quot;&quot;&quot;</span>
    <span class="n">rewards_history</span> <span class="o">=</span> <span class="p">[]</span>

    <span class="k">for</span> <span class="n">episode</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">episodes</span><span class="p">):</span>
        <span class="n">state</span> <span class="o">=</span> <span class="n">env</span><span class="o">.</span><span class="n">reset</span><span class="p">()</span>
        <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">state</span><span class="p">,</span> <span class="nb">tuple</span><span class="p">):</span>  <span class="c1"># gym 0.26+</span>
            <span class="n">state</span> <span class="o">=</span> <span class="n">state</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>

        <span class="n">total_reward</span> <span class="o">=</span> <span class="mi">0</span>
        <span class="n">done</span> <span class="o">=</span> <span class="kc">False</span>

        <span class="k">while</span> <span class="ow">not</span> <span class="n">done</span><span class="p">:</span>
            <span class="n">action</span> <span class="o">=</span> <span class="n">agent</span><span class="o">.</span><span class="n">select_action</span><span class="p">(</span><span class="n">state</span><span class="p">)</span>
            <span class="n">result</span> <span class="o">=</span> <span class="n">env</span><span class="o">.</span><span class="n">step</span><span class="p">(</span><span class="n">action</span><span class="p">)</span>
            <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">result</span><span class="p">)</span> <span class="o">==</span> <span class="mi">5</span><span class="p">:</span>  <span class="c1"># gym 0.26+</span>
                <span class="n">next_state</span><span class="p">,</span> <span class="n">reward</span><span class="p">,</span> <span class="n">terminated</span><span class="p">,</span> <span class="n">truncated</span><span class="p">,</span> <span class="n">info</span> <span class="o">=</span> <span class="n">result</span>
                <span class="n">done</span> <span class="o">=</span> <span class="n">terminated</span> <span class="ow">or</span> <span class="n">truncated</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="n">next_state</span><span class="p">,</span> <span class="n">reward</span><span class="p">,</span> <span class="n">done</span><span class="p">,</span> <span class="n">info</span> <span class="o">=</span> <span class="n">result</span>

            <span class="n">agent</span><span class="o">.</span><span class="n">store_transition</span><span class="p">(</span><span class="n">state</span><span class="p">,</span> <span class="n">action</span><span class="p">,</span> <span class="n">reward</span><span class="p">,</span> <span class="n">next_state</span><span class="p">,</span> <span class="n">done</span><span class="p">)</span>
            <span class="n">loss</span> <span class="o">=</span> <span class="n">agent</span><span class="o">.</span><span class="n">learn</span><span class="p">()</span>

            <span class="n">state</span> <span class="o">=</span> <span class="n">next_state</span>
            <span class="n">total_reward</span> <span class="o">+=</span> <span class="n">reward</span>

        <span class="n">rewards_history</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">total_reward</span><span class="p">)</span>

        <span class="k">if</span> <span class="n">episode</span> <span class="o">%</span> <span class="mi">10</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
            <span class="n">avg_reward</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">rewards_history</span><span class="p">[</span><span class="o">-</span><span class="mi">10</span><span class="p">:])</span>
            <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Episode </span><span class="si">{</span><span class="n">episode</span><span class="si">}</span><span class="s2">: Avg Reward = </span><span class="si">{</span><span class="n">avg_reward</span><span class="si">:</span><span class="s2">.2f</span><span class="si">}</span><span class="s2">, Epsilon = </span><span class="si">{</span><span class="n">agent</span><span class="o">.</span><span class="n">epsilon</span><span class="si">:</span><span class="s2">.3f</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>

    <span class="k">return</span> <span class="n">rewards_history</span>
</code></pre></div>

<hr />
<h2 id="6-policy-gradient">6. Policy Gradient<a class="header-link" href="#6-policy-gradient" title="Permanent link">&para;</a></h2>
<h3 id="idea">Idea<a class="header-link" href="#idea" title="Permanent link">&para;</a></h3>
<div class="highlight"><pre><span></span><code><span class="nt">Value-based</span><span class="w"> </span><span class="o">(</span><span class="nt">DQN</span><span class="o">):</span><span class="w"> </span><span class="nt">Learn</span><span class="w"> </span><span class="nt">Q</span><span class="w"> </span><span class="nt">function</span><span class="w"> </span><span class="err">â†’</span><span class="w"> </span><span class="nt">Derive</span><span class="w"> </span><span class="nt">policy</span><span class="w"> </span><span class="nt">indirectly</span>
<span class="nt">Policy-based</span><span class="o">:</span><span class="w"> </span><span class="nt">Learn</span><span class="w"> </span><span class="nt">policy</span><span class="w"> </span><span class="nt">directly</span>

<span class="nt">Policy</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="nt">Ï€_Î¸</span><span class="o">(</span><span class="nt">a</span><span class="o">|</span><span class="nt">s</span><span class="o">)</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="nt">P</span><span class="o">(</span><span class="nt">a</span><span class="o">|</span><span class="nt">s</span><span class="o">;</span><span class="w"> </span><span class="nt">Î¸</span><span class="o">)</span>

<span class="nt">Advantages</span><span class="o">:</span>
<span class="nt">1</span><span class="o">.</span><span class="w"> </span><span class="nt">Handle</span><span class="w"> </span><span class="nt">continuous</span><span class="w"> </span><span class="nt">action</span><span class="w"> </span><span class="nt">spaces</span>
<span class="nt">2</span><span class="o">.</span><span class="w"> </span><span class="nt">Learn</span><span class="w"> </span><span class="nt">stochastic</span><span class="w"> </span><span class="nt">policies</span>
<span class="nt">3</span><span class="o">.</span><span class="w"> </span><span class="nt">Guaranteed</span><span class="w"> </span><span class="nt">convergence</span><span class="w"> </span><span class="o">(</span><span class="nt">to</span><span class="w"> </span><span class="nt">local</span><span class="w"> </span><span class="nt">optimum</span><span class="o">)</span>
</code></pre></div>

<h3 id="policy-gradient-theorem">Policy Gradient Theorem<a class="header-link" href="#policy-gradient-theorem" title="Permanent link">&para;</a></h3>
<div class="highlight"><pre><span></span><code><span class="c1"># Objective: maximize J(Î¸) = E[Î£ R_t]</span>

<span class="c1"># Gradient:</span>
<span class="err">âˆ‡</span><span class="n">_Î¸</span> <span class="n">J</span><span class="p">(</span><span class="n">Î¸</span><span class="p">)</span> <span class="o">=</span> <span class="n">E</span><span class="p">[</span> <span class="n">Î£_t</span> <span class="err">âˆ‡</span><span class="n">_Î¸</span> <span class="n">log</span> <span class="n">Ï€_Î¸</span><span class="p">(</span><span class="n">a_t</span><span class="o">|</span><span class="n">s_t</span><span class="p">)</span> <span class="o">*</span> <span class="n">G_t</span> <span class="p">]</span>

<span class="c1"># G_t: Cumulative reward (Return) from time t</span>
<span class="c1"># log Ï€_Î¸: Log probability of policy</span>
</code></pre></div>

<h3 id="reinforce-algorithm">REINFORCE Algorithm<a class="header-link" href="#reinforce-algorithm" title="Permanent link">&para;</a></h3>
<div class="highlight"><pre><span></span><code><span class="k">class</span><span class="w"> </span><span class="nc">PolicyNetwork</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Policy Network (â­â­â­)&quot;&quot;&quot;</span>
    <span class="k">def</span><span class="w"> </span><span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">state_dim</span><span class="p">,</span> <span class="n">action_dim</span><span class="p">,</span> <span class="n">hidden_dim</span><span class="o">=</span><span class="mi">128</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">net</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Sequential</span><span class="p">(</span>
            <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">state_dim</span><span class="p">,</span> <span class="n">hidden_dim</span><span class="p">),</span>
            <span class="n">nn</span><span class="o">.</span><span class="n">ReLU</span><span class="p">(),</span>
            <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">hidden_dim</span><span class="p">,</span> <span class="n">hidden_dim</span><span class="p">),</span>
            <span class="n">nn</span><span class="o">.</span><span class="n">ReLU</span><span class="p">(),</span>
            <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">hidden_dim</span><span class="p">,</span> <span class="n">action_dim</span><span class="p">),</span>
            <span class="n">nn</span><span class="o">.</span><span class="n">Softmax</span><span class="p">(</span><span class="n">dim</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span>
        <span class="p">)</span>

    <span class="k">def</span><span class="w"> </span><span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">net</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>


<span class="k">class</span><span class="w"> </span><span class="nc">REINFORCEAgent</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;REINFORCE (Monte Carlo Policy Gradient) (â­â­â­â­)&quot;&quot;&quot;</span>
    <span class="k">def</span><span class="w"> </span><span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">state_dim</span><span class="p">,</span> <span class="n">action_dim</span><span class="p">,</span> <span class="n">lr</span><span class="o">=</span><span class="mf">1e-3</span><span class="p">,</span> <span class="n">gamma</span><span class="o">=</span><span class="mf">0.99</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">gamma</span> <span class="o">=</span> <span class="n">gamma</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">device</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">device</span><span class="p">(</span><span class="s1">&#39;cuda&#39;</span> <span class="k">if</span> <span class="n">torch</span><span class="o">.</span><span class="n">cuda</span><span class="o">.</span><span class="n">is_available</span><span class="p">()</span> <span class="k">else</span> <span class="s1">&#39;cpu&#39;</span><span class="p">)</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">policy</span> <span class="o">=</span> <span class="n">PolicyNetwork</span><span class="p">(</span><span class="n">state_dim</span><span class="p">,</span> <span class="n">action_dim</span><span class="p">)</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">device</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">optimizer</span> <span class="o">=</span> <span class="n">optim</span><span class="o">.</span><span class="n">Adam</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">policy</span><span class="o">.</span><span class="n">parameters</span><span class="p">(),</span> <span class="n">lr</span><span class="o">=</span><span class="n">lr</span><span class="p">)</span>

        <span class="c1"># Episode buffer</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">log_probs</span> <span class="o">=</span> <span class="p">[]</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">rewards</span> <span class="o">=</span> <span class="p">[]</span>

    <span class="k">def</span><span class="w"> </span><span class="nf">select_action</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">state</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Stochastic action selection&quot;&quot;&quot;</span>
        <span class="n">state</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">FloatTensor</span><span class="p">(</span><span class="n">state</span><span class="p">)</span><span class="o">.</span><span class="n">unsqueeze</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">device</span><span class="p">)</span>
        <span class="n">probs</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">policy</span><span class="p">(</span><span class="n">state</span><span class="p">)</span>

        <span class="c1"># Sample from Categorical distribution</span>
        <span class="n">dist</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">distributions</span><span class="o">.</span><span class="n">Categorical</span><span class="p">(</span><span class="n">probs</span><span class="p">)</span>
        <span class="n">action</span> <span class="o">=</span> <span class="n">dist</span><span class="o">.</span><span class="n">sample</span><span class="p">()</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">log_probs</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">dist</span><span class="o">.</span><span class="n">log_prob</span><span class="p">(</span><span class="n">action</span><span class="p">))</span>

        <span class="k">return</span> <span class="n">action</span><span class="o">.</span><span class="n">item</span><span class="p">()</span>

    <span class="k">def</span><span class="w"> </span><span class="nf">store_reward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">reward</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Store reward&quot;&quot;&quot;</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">rewards</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">reward</span><span class="p">)</span>

    <span class="k">def</span><span class="w"> </span><span class="nf">learn</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Learn at end of episode&quot;&quot;&quot;</span>
        <span class="c1"># Compute returns (from end to beginning)</span>
        <span class="n">returns</span> <span class="o">=</span> <span class="p">[]</span>
        <span class="n">G</span> <span class="o">=</span> <span class="mi">0</span>
        <span class="k">for</span> <span class="n">r</span> <span class="ow">in</span> <span class="nb">reversed</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">rewards</span><span class="p">):</span>
            <span class="n">G</span> <span class="o">=</span> <span class="n">r</span> <span class="o">+</span> <span class="bp">self</span><span class="o">.</span><span class="n">gamma</span> <span class="o">*</span> <span class="n">G</span>
            <span class="n">returns</span><span class="o">.</span><span class="n">insert</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">G</span><span class="p">)</span>

        <span class="n">returns</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">FloatTensor</span><span class="p">(</span><span class="n">returns</span><span class="p">)</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">device</span><span class="p">)</span>

        <span class="c1"># Normalize (baseline effect)</span>
        <span class="n">returns</span> <span class="o">=</span> <span class="p">(</span><span class="n">returns</span> <span class="o">-</span> <span class="n">returns</span><span class="o">.</span><span class="n">mean</span><span class="p">())</span> <span class="o">/</span> <span class="p">(</span><span class="n">returns</span><span class="o">.</span><span class="n">std</span><span class="p">()</span> <span class="o">+</span> <span class="mf">1e-8</span><span class="p">)</span>

        <span class="c1"># Policy Gradient loss</span>
        <span class="n">loss</span> <span class="o">=</span> <span class="mi">0</span>
        <span class="k">for</span> <span class="n">log_prob</span><span class="p">,</span> <span class="n">G</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">log_probs</span><span class="p">,</span> <span class="n">returns</span><span class="p">):</span>
            <span class="n">loss</span> <span class="o">-=</span> <span class="n">log_prob</span> <span class="o">*</span> <span class="n">G</span>  <span class="c1"># negative: gradient ascent</span>

        <span class="c1"># Update</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">optimizer</span><span class="o">.</span><span class="n">zero_grad</span><span class="p">()</span>
        <span class="n">loss</span><span class="o">.</span><span class="n">backward</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">optimizer</span><span class="o">.</span><span class="n">step</span><span class="p">()</span>

        <span class="c1"># Clear buffer</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">log_probs</span> <span class="o">=</span> <span class="p">[]</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">rewards</span> <span class="o">=</span> <span class="p">[]</span>

        <span class="k">return</span> <span class="n">loss</span><span class="o">.</span><span class="n">item</span><span class="p">()</span>
</code></pre></div>

<h3 id="reinforce-training">REINFORCE Training<a class="header-link" href="#reinforce-training" title="Permanent link">&para;</a></h3>
<div class="highlight"><pre><span></span><code><span class="k">def</span><span class="w"> </span><span class="nf">train_reinforce</span><span class="p">(</span><span class="n">env</span><span class="p">,</span> <span class="n">agent</span><span class="p">,</span> <span class="n">episodes</span><span class="o">=</span><span class="mi">1000</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;REINFORCE Training (â­â­â­)&quot;&quot;&quot;</span>
    <span class="n">rewards_history</span> <span class="o">=</span> <span class="p">[]</span>

    <span class="k">for</span> <span class="n">episode</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">episodes</span><span class="p">):</span>
        <span class="n">state</span> <span class="o">=</span> <span class="n">env</span><span class="o">.</span><span class="n">reset</span><span class="p">()</span>
        <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">state</span><span class="p">,</span> <span class="nb">tuple</span><span class="p">):</span>
            <span class="n">state</span> <span class="o">=</span> <span class="n">state</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>

        <span class="n">done</span> <span class="o">=</span> <span class="kc">False</span>
        <span class="n">total_reward</span> <span class="o">=</span> <span class="mi">0</span>

        <span class="k">while</span> <span class="ow">not</span> <span class="n">done</span><span class="p">:</span>
            <span class="n">action</span> <span class="o">=</span> <span class="n">agent</span><span class="o">.</span><span class="n">select_action</span><span class="p">(</span><span class="n">state</span><span class="p">)</span>
            <span class="n">result</span> <span class="o">=</span> <span class="n">env</span><span class="o">.</span><span class="n">step</span><span class="p">(</span><span class="n">action</span><span class="p">)</span>
            <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">result</span><span class="p">)</span> <span class="o">==</span> <span class="mi">5</span><span class="p">:</span>
                <span class="n">next_state</span><span class="p">,</span> <span class="n">reward</span><span class="p">,</span> <span class="n">terminated</span><span class="p">,</span> <span class="n">truncated</span><span class="p">,</span> <span class="n">_</span> <span class="o">=</span> <span class="n">result</span>
                <span class="n">done</span> <span class="o">=</span> <span class="n">terminated</span> <span class="ow">or</span> <span class="n">truncated</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="n">next_state</span><span class="p">,</span> <span class="n">reward</span><span class="p">,</span> <span class="n">done</span><span class="p">,</span> <span class="n">_</span> <span class="o">=</span> <span class="n">result</span>

            <span class="n">agent</span><span class="o">.</span><span class="n">store_reward</span><span class="p">(</span><span class="n">reward</span><span class="p">)</span>

            <span class="n">state</span> <span class="o">=</span> <span class="n">next_state</span>
            <span class="n">total_reward</span> <span class="o">+=</span> <span class="n">reward</span>

        <span class="c1"># Learn at episode end</span>
        <span class="n">agent</span><span class="o">.</span><span class="n">learn</span><span class="p">()</span>
        <span class="n">rewards_history</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">total_reward</span><span class="p">)</span>

        <span class="k">if</span> <span class="n">episode</span> <span class="o">%</span> <span class="mi">50</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
            <span class="n">avg_reward</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">rewards_history</span><span class="p">[</span><span class="o">-</span><span class="mi">50</span><span class="p">:])</span>
            <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Episode </span><span class="si">{</span><span class="n">episode</span><span class="si">}</span><span class="s2">: Avg Reward = </span><span class="si">{</span><span class="n">avg_reward</span><span class="si">:</span><span class="s2">.2f</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>

    <span class="k">return</span> <span class="n">rewards_history</span>
</code></pre></div>

<hr />
<h2 id="7-actor-critic">7. Actor-Critic<a class="header-link" href="#7-actor-critic" title="Permanent link">&para;</a></h2>
<h3 id="idea_1">Idea<a class="header-link" href="#idea_1" title="Permanent link">&para;</a></h3>
<div class="highlight"><pre><span></span><code><span class="n">REINFORCE</span><span class="w"> </span><span class="n">problem</span><span class="p">:</span><span class="w"> </span><span class="n">High</span><span class="w"> </span><span class="n">variance</span><span class="w"> </span><span class="p">(</span><span class="n">Monte</span><span class="w"> </span><span class="n">Carlo</span><span class="w"> </span><span class="n">estimation</span><span class="p">)</span>
<span class="n">Solution</span><span class="p">:</span><span class="w"> </span><span class="n">Estimate</span><span class="w"> </span><span class="n">value</span><span class="w"> </span><span class="n">with</span><span class="w"> </span><span class="n">Critic</span><span class="w"> </span><span class="err">â†’</span><span class="w"> </span><span class="n">reduce</span><span class="w"> </span><span class="n">variance</span>

<span class="n">Actor</span><span class="p">:</span><span class="w"> </span><span class="n">Policy</span><span class="w"> </span><span class="err">Ï€</span><span class="n">_Î¸</span><span class="w"> </span><span class="p">(</span><span class="n">decide</span><span class="w"> </span><span class="n">actions</span><span class="p">)</span>
<span class="n">Critic</span><span class="p">:</span><span class="w"> </span><span class="n">Value</span><span class="w"> </span><span class="n">V_Ï†</span><span class="w"> </span><span class="p">(</span><span class="n">evaluate</span><span class="w"> </span><span class="n">states</span><span class="p">)</span>
</code></pre></div>

<h3 id="advantage-function">Advantage Function<a class="header-link" href="#advantage-function" title="Permanent link">&para;</a></h3>
<div class="highlight"><pre><span></span><code><span class="c1"># Advantage = Q(s,a) - V(s)</span>
<span class="c1"># Meaning: How much better is this action compared to average</span>

<span class="c1"># Estimated with TD Error:</span>
<span class="n">A</span><span class="p">(</span><span class="n">s</span><span class="p">,</span> <span class="n">a</span><span class="p">)</span> <span class="err">â‰ˆ</span> <span class="n">r</span> <span class="o">+</span> <span class="n">Î³V</span><span class="p">(</span><span class="n">s</span><span class="s1">&#39;) - V(s)</span>
</code></pre></div>

<h3 id="actor-critic-implementation">Actor-Critic Implementation<a class="header-link" href="#actor-critic-implementation" title="Permanent link">&para;</a></h3>
<div class="highlight"><pre><span></span><code><span class="k">class</span><span class="w"> </span><span class="nc">ActorCritic</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Actor-Critic Network (â­â­â­â­)&quot;&quot;&quot;</span>
    <span class="k">def</span><span class="w"> </span><span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">state_dim</span><span class="p">,</span> <span class="n">action_dim</span><span class="p">,</span> <span class="n">hidden_dim</span><span class="o">=</span><span class="mi">128</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>

        <span class="c1"># Shared feature extractor</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">shared</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Sequential</span><span class="p">(</span>
            <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">state_dim</span><span class="p">,</span> <span class="n">hidden_dim</span><span class="p">),</span>
            <span class="n">nn</span><span class="o">.</span><span class="n">ReLU</span><span class="p">()</span>
        <span class="p">)</span>

        <span class="c1"># Actor (Policy)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">actor</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Sequential</span><span class="p">(</span>
            <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">hidden_dim</span><span class="p">,</span> <span class="n">action_dim</span><span class="p">),</span>
            <span class="n">nn</span><span class="o">.</span><span class="n">Softmax</span><span class="p">(</span><span class="n">dim</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span>
        <span class="p">)</span>

        <span class="c1"># Critic (Value)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">critic</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">hidden_dim</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>

    <span class="k">def</span><span class="w"> </span><span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
        <span class="n">features</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">shared</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="n">policy</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">actor</span><span class="p">(</span><span class="n">features</span><span class="p">)</span>
        <span class="n">value</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">critic</span><span class="p">(</span><span class="n">features</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">policy</span><span class="p">,</span> <span class="n">value</span>


<span class="k">class</span><span class="w"> </span><span class="nc">A2CAgent</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Advantage Actor-Critic (â­â­â­â­)&quot;&quot;&quot;</span>
    <span class="k">def</span><span class="w"> </span><span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">state_dim</span><span class="p">,</span> <span class="n">action_dim</span><span class="p">,</span> <span class="n">lr</span><span class="o">=</span><span class="mf">1e-3</span><span class="p">,</span> <span class="n">gamma</span><span class="o">=</span><span class="mf">0.99</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">gamma</span> <span class="o">=</span> <span class="n">gamma</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">device</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">device</span><span class="p">(</span><span class="s1">&#39;cuda&#39;</span> <span class="k">if</span> <span class="n">torch</span><span class="o">.</span><span class="n">cuda</span><span class="o">.</span><span class="n">is_available</span><span class="p">()</span> <span class="k">else</span> <span class="s1">&#39;cpu&#39;</span><span class="p">)</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">network</span> <span class="o">=</span> <span class="n">ActorCritic</span><span class="p">(</span><span class="n">state_dim</span><span class="p">,</span> <span class="n">action_dim</span><span class="p">)</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">device</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">optimizer</span> <span class="o">=</span> <span class="n">optim</span><span class="o">.</span><span class="n">Adam</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">network</span><span class="o">.</span><span class="n">parameters</span><span class="p">(),</span> <span class="n">lr</span><span class="o">=</span><span class="n">lr</span><span class="p">)</span>

    <span class="k">def</span><span class="w"> </span><span class="nf">select_action</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">state</span><span class="p">):</span>
        <span class="n">state</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">FloatTensor</span><span class="p">(</span><span class="n">state</span><span class="p">)</span><span class="o">.</span><span class="n">unsqueeze</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">device</span><span class="p">)</span>
        <span class="n">policy</span><span class="p">,</span> <span class="n">_</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">network</span><span class="p">(</span><span class="n">state</span><span class="p">)</span>
        <span class="n">dist</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">distributions</span><span class="o">.</span><span class="n">Categorical</span><span class="p">(</span><span class="n">policy</span><span class="p">)</span>
        <span class="n">action</span> <span class="o">=</span> <span class="n">dist</span><span class="o">.</span><span class="n">sample</span><span class="p">()</span>
        <span class="k">return</span> <span class="n">action</span><span class="o">.</span><span class="n">item</span><span class="p">(),</span> <span class="n">dist</span><span class="o">.</span><span class="n">log_prob</span><span class="p">(</span><span class="n">action</span><span class="p">)</span>

    <span class="k">def</span><span class="w"> </span><span class="nf">learn</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">state</span><span class="p">,</span> <span class="n">action</span><span class="p">,</span> <span class="n">reward</span><span class="p">,</span> <span class="n">next_state</span><span class="p">,</span> <span class="n">done</span><span class="p">,</span> <span class="n">log_prob</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;One-step Actor-Critic Update&quot;&quot;&quot;</span>
        <span class="n">state</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">FloatTensor</span><span class="p">(</span><span class="n">state</span><span class="p">)</span><span class="o">.</span><span class="n">unsqueeze</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">device</span><span class="p">)</span>
        <span class="n">next_state</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">FloatTensor</span><span class="p">(</span><span class="n">next_state</span><span class="p">)</span><span class="o">.</span><span class="n">unsqueeze</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">device</span><span class="p">)</span>
        <span class="n">reward</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">FloatTensor</span><span class="p">([</span><span class="n">reward</span><span class="p">])</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">device</span><span class="p">)</span>
        <span class="n">done</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">FloatTensor</span><span class="p">([</span><span class="n">done</span><span class="p">])</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">device</span><span class="p">)</span>

        <span class="n">_</span><span class="p">,</span> <span class="n">value</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">network</span><span class="p">(</span><span class="n">state</span><span class="p">)</span>
        <span class="n">_</span><span class="p">,</span> <span class="n">next_value</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">network</span><span class="p">(</span><span class="n">next_state</span><span class="p">)</span>

        <span class="c1"># TD Target and Advantage</span>
        <span class="n">td_target</span> <span class="o">=</span> <span class="n">reward</span> <span class="o">+</span> <span class="bp">self</span><span class="o">.</span><span class="n">gamma</span> <span class="o">*</span> <span class="n">next_value</span> <span class="o">*</span> <span class="p">(</span><span class="mi">1</span> <span class="o">-</span> <span class="n">done</span><span class="p">)</span>
        <span class="n">advantage</span> <span class="o">=</span> <span class="n">td_target</span> <span class="o">-</span> <span class="n">value</span>

        <span class="c1"># Actor Loss (policy gradient with advantage)</span>
        <span class="n">actor_loss</span> <span class="o">=</span> <span class="o">-</span><span class="n">log_prob</span> <span class="o">*</span> <span class="n">advantage</span><span class="o">.</span><span class="n">detach</span><span class="p">()</span>

        <span class="c1"># Critic Loss (value function)</span>
        <span class="n">critic_loss</span> <span class="o">=</span> <span class="n">advantage</span><span class="o">.</span><span class="n">pow</span><span class="p">(</span><span class="mi">2</span><span class="p">)</span>

        <span class="c1"># Total Loss</span>
        <span class="n">loss</span> <span class="o">=</span> <span class="n">actor_loss</span> <span class="o">+</span> <span class="mf">0.5</span> <span class="o">*</span> <span class="n">critic_loss</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">optimizer</span><span class="o">.</span><span class="n">zero_grad</span><span class="p">()</span>
        <span class="n">loss</span><span class="o">.</span><span class="n">backward</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">optimizer</span><span class="o">.</span><span class="n">step</span><span class="p">()</span>

        <span class="k">return</span> <span class="n">loss</span><span class="o">.</span><span class="n">item</span><span class="p">()</span>
</code></pre></div>

<hr />
<h2 id="8-environments-and-experiments">8. Environments and Experiments<a class="header-link" href="#8-environments-and-experiments" title="Permanent link">&para;</a></h2>
<h3 id="using-openai-gym">Using OpenAI Gym<a class="header-link" href="#using-openai-gym" title="Permanent link">&para;</a></h3>
<div class="highlight"><pre><span></span><code><span class="kn">import</span><span class="w"> </span><span class="nn">gymnasium</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">gym</span>

<span class="c1"># Create environment</span>
<span class="n">env</span> <span class="o">=</span> <span class="n">gym</span><span class="o">.</span><span class="n">make</span><span class="p">(</span><span class="s1">&#39;CartPole-v1&#39;</span><span class="p">)</span>

<span class="c1"># Environment info</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;State space: </span><span class="si">{</span><span class="n">env</span><span class="o">.</span><span class="n">observation_space</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>      <span class="c1"># Box(4,)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Action space: </span><span class="si">{</span><span class="n">env</span><span class="o">.</span><span class="n">action_space</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>          <span class="c1"># Discrete(2)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;State dim: </span><span class="si">{</span><span class="n">env</span><span class="o">.</span><span class="n">observation_space</span><span class="o">.</span><span class="n">shape</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>  <span class="c1"># (4,)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Action dim: </span><span class="si">{</span><span class="n">env</span><span class="o">.</span><span class="n">action_space</span><span class="o">.</span><span class="n">n</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>          <span class="c1"># 2</span>

<span class="c1"># Run episode</span>
<span class="n">state</span><span class="p">,</span> <span class="n">_</span> <span class="o">=</span> <span class="n">env</span><span class="o">.</span><span class="n">reset</span><span class="p">()</span>
<span class="n">done</span> <span class="o">=</span> <span class="kc">False</span>

<span class="k">while</span> <span class="ow">not</span> <span class="n">done</span><span class="p">:</span>
    <span class="n">action</span> <span class="o">=</span> <span class="n">env</span><span class="o">.</span><span class="n">action_space</span><span class="o">.</span><span class="n">sample</span><span class="p">()</span>  <span class="c1"># random action</span>
    <span class="n">next_state</span><span class="p">,</span> <span class="n">reward</span><span class="p">,</span> <span class="n">terminated</span><span class="p">,</span> <span class="n">truncated</span><span class="p">,</span> <span class="n">info</span> <span class="o">=</span> <span class="n">env</span><span class="o">.</span><span class="n">step</span><span class="p">(</span><span class="n">action</span><span class="p">)</span>
    <span class="n">done</span> <span class="o">=</span> <span class="n">terminated</span> <span class="ow">or</span> <span class="n">truncated</span>
    <span class="n">state</span> <span class="o">=</span> <span class="n">next_state</span>

<span class="n">env</span><span class="o">.</span><span class="n">close</span><span class="p">()</span>
</code></pre></div>

<h3 id="experiment-example-cartpole">Experiment Example: CartPole<a class="header-link" href="#experiment-example-cartpole" title="Permanent link">&para;</a></h3>
<div class="highlight"><pre><span></span><code><span class="k">def</span><span class="w"> </span><span class="nf">run_experiment</span><span class="p">():</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;CartPole experiment (â­â­â­)&quot;&quot;&quot;</span>
    <span class="n">env</span> <span class="o">=</span> <span class="n">gym</span><span class="o">.</span><span class="n">make</span><span class="p">(</span><span class="s1">&#39;CartPole-v1&#39;</span><span class="p">)</span>
    <span class="n">state_dim</span> <span class="o">=</span> <span class="n">env</span><span class="o">.</span><span class="n">observation_space</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>  <span class="c1"># 4</span>
    <span class="n">action_dim</span> <span class="o">=</span> <span class="n">env</span><span class="o">.</span><span class="n">action_space</span><span class="o">.</span><span class="n">n</span>              <span class="c1"># 2</span>

    <span class="c1"># DQN agent</span>
    <span class="n">agent</span> <span class="o">=</span> <span class="n">DQNAgent</span><span class="p">(</span><span class="n">state_dim</span><span class="p">,</span> <span class="n">action_dim</span><span class="p">)</span>

    <span class="c1"># Training</span>
    <span class="n">rewards</span> <span class="o">=</span> <span class="n">train_dqn</span><span class="p">(</span><span class="n">env</span><span class="p">,</span> <span class="n">agent</span><span class="p">,</span> <span class="n">episodes</span><span class="o">=</span><span class="mi">500</span><span class="p">)</span>

    <span class="c1"># Visualize results</span>
    <span class="kn">import</span><span class="w"> </span><span class="nn">matplotlib.pyplot</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">plt</span>

    <span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">10</span><span class="p">,</span> <span class="mi">5</span><span class="p">))</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">rewards</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s1">&#39;Episode&#39;</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s1">&#39;Reward&#39;</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s1">&#39;DQN on CartPole-v1&#39;</span><span class="p">)</span>

    <span class="c1"># Moving average</span>
    <span class="n">window</span> <span class="o">=</span> <span class="mi">50</span>
    <span class="n">ma</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">convolve</span><span class="p">(</span><span class="n">rewards</span><span class="p">,</span> <span class="n">np</span><span class="o">.</span><span class="n">ones</span><span class="p">(</span><span class="n">window</span><span class="p">)</span><span class="o">/</span><span class="n">window</span><span class="p">,</span> <span class="n">mode</span><span class="o">=</span><span class="s1">&#39;valid&#39;</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="nb">range</span><span class="p">(</span><span class="n">window</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="nb">len</span><span class="p">(</span><span class="n">rewards</span><span class="p">)),</span> <span class="n">ma</span><span class="p">,</span> <span class="s1">&#39;r-&#39;</span><span class="p">,</span> <span class="n">linewidth</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>

    <span class="n">plt</span><span class="o">.</span><span class="n">savefig</span><span class="p">(</span><span class="s1">&#39;cartpole_dqn.png&#39;</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">close</span><span class="p">()</span>

    <span class="n">env</span><span class="o">.</span><span class="n">close</span><span class="p">()</span>
    <span class="k">return</span> <span class="n">rewards</span>
</code></pre></div>

<hr />
<h2 id="9-algorithm-comparison">9. Algorithm Comparison<a class="header-link" href="#9-algorithm-comparison" title="Permanent link">&para;</a></h2>
<h3 id="main-algorithm-characteristics">Main Algorithm Characteristics<a class="header-link" href="#main-algorithm-characteristics" title="Permanent link">&para;</a></h3>
<table>
<thead>
<tr>
<th>Algorithm</th>
<th>Type</th>
<th>On/Off-Policy</th>
<th>Features</th>
</tr>
</thead>
<tbody>
<tr>
<td>Q-Learning</td>
<td>Value-based</td>
<td>Off-policy</td>
<td>Table, simple</td>
</tr>
<tr>
<td>DQN</td>
<td>Value-based</td>
<td>Off-policy</td>
<td>Neural network, experience replay</td>
</tr>
<tr>
<td>REINFORCE</td>
<td>Policy-based</td>
<td>On-policy</td>
<td>Monte Carlo, high variance</td>
</tr>
<tr>
<td>A2C/A3C</td>
<td>Actor-Critic</td>
<td>On-policy</td>
<td>Advantage, parallelization</td>
</tr>
<tr>
<td>PPO</td>
<td>Actor-Critic</td>
<td>On-policy</td>
<td>Stable, practical</td>
</tr>
<tr>
<td>SAC</td>
<td>Actor-Critic</td>
<td>Off-policy</td>
<td>Continuous actions, entropy</td>
</tr>
</tbody>
</table>
<h3 id="selection-guide">Selection Guide<a class="header-link" href="#selection-guide" title="Permanent link">&para;</a></h3>
<div class="highlight"><pre><span></span><code><span class="nx">Discrete</span><span class="w"> </span><span class="nx">action</span><span class="w"> </span><span class="nx">space</span><span class="p">:</span>
<span class="o">-</span><span class="w"> </span><span class="nx">Simple</span><span class="w"> </span><span class="nx">problems</span><span class="p">:</span><span class="w"> </span><span class="nx">DQN</span>
<span class="o">-</span><span class="w"> </span><span class="nx">Complex</span><span class="w"> </span><span class="nx">problems</span><span class="p">:</span><span class="w"> </span><span class="nx">PPO</span>

<span class="nx">Continuous</span><span class="w"> </span><span class="nx">action</span><span class="w"> </span><span class="nx">space</span><span class="p">:</span>
<span class="o">-</span><span class="w"> </span><span class="nx">Stable</span><span class="p">:</span><span class="w"> </span><span class="nx">SAC</span>
<span class="o">-</span><span class="w"> </span><span class="nx">Fast</span><span class="w"> </span><span class="nx">learning</span><span class="p">:</span><span class="w"> </span><span class="nx">PPO</span>

<span class="nx">Resource</span><span class="w"> </span><span class="nx">constraints</span><span class="p">:</span>
<span class="o">-</span><span class="w"> </span><span class="nx">A2C</span><span class="w"> </span><span class="p">(</span><span class="nx">single</span><span class="w"> </span><span class="nx">machine</span><span class="p">)</span>

<span class="nx">Large</span><span class="o">-</span><span class="nx">scale</span><span class="w"> </span><span class="nx">parallel</span><span class="p">:</span>
<span class="o">-</span><span class="w"> </span><span class="nx">A3C</span><span class="p">,</span><span class="w"> </span><span class="nx">PPO</span>
</code></pre></div>

<hr />
<h2 id="10-advanced-topics-overview">10. Advanced Topics Overview<a class="header-link" href="#10-advanced-topics-overview" title="Permanent link">&para;</a></h2>
<h3 id="double-dqn">Double DQN<a class="header-link" href="#double-dqn" title="Permanent link">&para;</a></h3>
<div class="highlight"><pre><span></span><code><span class="c1"># DQN problem: Q value overestimation</span>
<span class="c1"># Solution: Use different networks for action selection and evaluation</span>

<span class="c1"># Original DQN:</span>
<span class="n">target_q</span> <span class="o">=</span> <span class="n">reward</span> <span class="o">+</span> <span class="n">gamma</span> <span class="o">*</span> <span class="n">target_net</span><span class="p">(</span><span class="n">next_state</span><span class="p">)</span><span class="o">.</span><span class="n">max</span><span class="p">()</span>

<span class="c1"># Double DQN:</span>
<span class="n">best_action</span> <span class="o">=</span> <span class="n">q_net</span><span class="p">(</span><span class="n">next_state</span><span class="p">)</span><span class="o">.</span><span class="n">argmax</span><span class="p">()</span>
<span class="n">target_q</span> <span class="o">=</span> <span class="n">reward</span> <span class="o">+</span> <span class="n">gamma</span> <span class="o">*</span> <span class="n">target_net</span><span class="p">(</span><span class="n">next_state</span><span class="p">)[</span><span class="n">best_action</span><span class="p">]</span>
</code></pre></div>

<h3 id="dueling-dqn">Dueling DQN<a class="header-link" href="#dueling-dqn" title="Permanent link">&para;</a></h3>
<div class="highlight"><pre><span></span><code><span class="c1"># Q = V + A (Value + Advantage)</span>
<span class="c1"># Separate state value and action advantage</span>

<span class="k">class</span><span class="w"> </span><span class="nc">DuelingNetwork</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="k">def</span><span class="w"> </span><span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">state_dim</span><span class="p">,</span> <span class="n">action_dim</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">feature</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Sequential</span><span class="p">(</span>
            <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">state_dim</span><span class="p">,</span> <span class="mi">128</span><span class="p">),</span>
            <span class="n">nn</span><span class="o">.</span><span class="n">ReLU</span><span class="p">()</span>
        <span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">value</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="mi">128</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">advantage</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="mi">128</span><span class="p">,</span> <span class="n">action_dim</span><span class="p">)</span>

    <span class="k">def</span><span class="w"> </span><span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
        <span class="n">f</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">feature</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="n">v</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">value</span><span class="p">(</span><span class="n">f</span><span class="p">)</span>
        <span class="n">a</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">advantage</span><span class="p">(</span><span class="n">f</span><span class="p">)</span>
        <span class="n">q</span> <span class="o">=</span> <span class="n">v</span> <span class="o">+</span> <span class="n">a</span> <span class="o">-</span> <span class="n">a</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">dim</span><span class="o">=-</span><span class="mi">1</span><span class="p">,</span> <span class="n">keepdim</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">q</span>
</code></pre></div>

<h3 id="prioritized-experience-replay">Prioritized Experience Replay<a class="header-link" href="#prioritized-experience-replay" title="Permanent link">&para;</a></h3>
<div class="highlight"><pre><span></span><code><span class="c1"># Sample important experiences (high TD Error) more frequently</span>
<span class="c1"># P(i) âˆ |TD_error_i|^Î±</span>

<span class="c1"># Use Sum Tree data structure for implementation</span>
</code></pre></div>

<hr />
<h2 id="summary">Summary<a class="header-link" href="#summary" title="Permanent link">&para;</a></h2>
<h3 id="key-concepts">Key Concepts<a class="header-link" href="#key-concepts" title="Permanent link">&para;</a></h3>
<ol>
<li><strong>MDP</strong>: Define problem with states, actions, rewards, transitions</li>
<li><strong>Bellman Equation</strong>: Current value = immediate reward + future value</li>
<li><strong>Q-Learning</strong>: Learn Q function with TD</li>
<li><strong>DQN</strong>: Neural network + experience replay + target network</li>
<li><strong>Policy Gradient</strong>: Direct policy optimization</li>
<li><strong>Actor-Critic</strong>: Actor + Critic to reduce variance</li>
</ol>
<h3 id="practical-tips">Practical Tips<a class="header-link" href="#practical-tips" title="Permanent link">&para;</a></h3>
<div class="highlight"><pre><span></span><code><span class="c1"># 1. Reward design is key</span>
<span class="c1"># - Sparse reward â†’ difficult learning</span>
<span class="c1"># - Shaped reward â†’ helps learning (but can bias)</span>

<span class="c1"># 2. Hyperparameter tuning</span>
<span class="c1"># - Learning rate: 1e-4 ~ 1e-3</span>
<span class="c1"># - Gamma: 0.99</span>
<span class="c1"># - Epsilon decay: slowly</span>

<span class="c1"># 3. Debugging</span>
<span class="c1"># - Check reward curve</span>
<span class="c1"># - Monitor Q value distribution</span>
<span class="c1"># - Visualize learned policy</span>
</code></pre></div>

<hr />
<h2 id="references">References<a class="header-link" href="#references" title="Permanent link">&para;</a></h2>
<ul>
<li>Sutton &amp; Barto: http://incompleteideas.net/book/the-book.html</li>
<li>DQN: https://arxiv.org/abs/1312.5602</li>
<li>Policy Gradient: https://proceedings.neurips.cc/paper/1999/file/464d828b85b0bed98e80ade0a5c43b0f-Paper.pdf</li>
<li>OpenAI Spinning Up: https://spinningup.openai.com/</li>
<li>Gymnasium: https://gymnasium.farama.org/</li>
</ul>
    </div>

    
<div class="lesson-toolbar lesson-toolbar--bottom">
    <div class="toolbar-nav toolbar-nav--prev">
        
        <a href="/study/en/Deep_Learning/41_Model_Saving_Deployment.html" class="nav-prev">
            <span class="nav-label">Previous</span>
            <span class="nav-title">41. Model Saving and Deployment</span>
        </a>
        
    </div>
    <div class="toolbar-actions">
        <button class="btn btn-copy-link" data-action="copy-link" title="Copy link">
            <span class="icon">ğŸ”—</span>
            <span class="text">Copy link</span>
        </button>
        <a href="/study/en/Deep_Learning/" class="btn btn-topic-link" title="Back to topic list">
            <span class="icon">ğŸ“‹</span>
            <span class="text">Topic list</span>
        </a>
    </div>
    <div class="toolbar-nav toolbar-nav--next">
        
    </div>
</div>


    <div class="toolbar-keyboard-hint">
        <kbd>&larr;</kbd> <kbd>&rarr;</kbd> to navigate between lessons
    </div>
</article>

<!-- Scroll to top floating button -->
<button class="scroll-to-top" id="scroll-to-top" title="Scroll to top" aria-label="Scroll to top">â†‘</button>

            </div>
        </main>
    </div>

    <script src="/study/static/js/app.js"></script>
    <script>
        function switchLanguage(newLang) {
            const path = window.location.pathname;
            const base = '/study';
            const currentLang = 'en';
            const newPath = path.replace(base + '/' + currentLang + '/', base + '/' + newLang + '/');
            window.location.href = newPath + window.location.search;
        }
    </script>
    
<script>
    document.addEventListener('DOMContentLoaded', function() {
        // Copy link - independent feedback per button
        document.querySelectorAll('[data-action="copy-link"]').forEach(function(btn) {
            btn.addEventListener('click', function() {
                navigator.clipboard.writeText(window.location.href);
                var textEl = this.querySelector('.text');
                var originalText = textEl.textContent;
                textEl.textContent = 'Copied!';
                setTimeout(function() { textEl.textContent = originalText; }, 2000);
            });
        });

        // Add copy buttons to code blocks
        document.querySelectorAll('.lesson-content pre').forEach(function(pre) {
            var copyBtn = document.createElement('button');
            copyBtn.className = 'code-copy-btn';
            copyBtn.textContent = 'Copy';
            copyBtn.addEventListener('click', function() {
                var code = pre.querySelector('code');
                var text = code ? code.textContent : pre.textContent;
                navigator.clipboard.writeText(text);
                copyBtn.textContent = 'Copied!';
                setTimeout(function() { copyBtn.textContent = 'Copy'; }, 2000);
            });
            pre.style.position = 'relative';
            pre.appendChild(copyBtn);
        });

        // Scroll to top button
        var scrollBtn = document.getElementById('scroll-to-top');
        window.addEventListener('scroll', function() {
            scrollBtn.classList.toggle('visible', window.scrollY > 300);
        });
        scrollBtn.addEventListener('click', function() {
            window.scrollTo({ top: 0, behavior: 'smooth' });
        });

        // Keyboard shortcuts: left/right arrows for lesson navigation
        document.addEventListener('keydown', function(e) {
            var tag = document.activeElement.tagName;
            if (tag === 'INPUT' || tag === 'TEXTAREA' || tag === 'SELECT') return;

            if (e.key === 'ArrowLeft') {
                var prevLink = document.querySelector('.nav-prev');
                if (prevLink) prevLink.click();
            } else if (e.key === 'ArrowRight') {
                var nextLink = document.querySelector('.nav-next');
                if (nextLink) nextLink.click();
            }
        });
    });
</script>

</body>
</html>
{% endraw %}