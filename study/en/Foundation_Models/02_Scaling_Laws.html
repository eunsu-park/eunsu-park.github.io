{% raw %}
<!DOCTYPE html>
<html lang="en" data-theme="light">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Scaling Laws - Study Materials</title>
    <link rel="stylesheet" href="/study/static/css/style.css">
    <link rel="stylesheet" href="/study/static/css/highlight.css">
    <!-- KaTeX for LaTeX math rendering -->
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.11/dist/katex.min.css">
    <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.11/dist/katex.min.js"></script>
    <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.11/dist/contrib/auto-render.min.js"
        onload="renderMathInElement(document.body, {
            delimiters: [
                {left: '$$', right: '$$', display: true},
                {left: '$', right: '$', display: false}
            ],
            throwOnError: false
        });"></script>
    
</head>
<body>
    <div class="app-container">
        <!-- Sidebar -->
        <aside class="sidebar" id="sidebar">
            <div class="sidebar-header">
                <a href="/study/en/" class="logo">Study Materials</a>
                <button class="sidebar-toggle" id="sidebar-toggle" aria-label="Toggle sidebar">
                    <span></span>
                </button>
            </div>

            <nav class="sidebar-nav">
                <a href="/study/en/" class="nav-item ">
                    <span class="nav-icon">ğŸ </span>
                    <span class="nav-text">Home</span>
                </a>
                <a href="/study/examples/" class="nav-item ">
                    <span class="nav-icon">ğŸ’»</span>
                    <span class="nav-text">Examples</span>
                </a>
            </nav>

            <div class="sidebar-search">
                <form action="/study/en/search.html" method="get" id="search-form">
                    <input type="search" name="q" placeholder="Search..." id="search-sidebar-input">
                </form>
            </div>

            <!-- Language Selector -->
            <div class="sidebar-lang">
                <select id="lang-select" class="lang-selector" onchange="switchLanguage(this.value)">
                    
                    <option value="en" selected>
                        English
                    </option>
                    
                    <option value="ko" >
                        í•œêµ­ì–´
                    </option>
                    
                </select>
            </div>

            <div class="sidebar-footer">
                <button class="theme-toggle" id="theme-toggle" aria-label="Toggle theme">
                    <span class="theme-icon light">â˜€ï¸</span>
                    <span class="theme-icon dark">ğŸŒ™</span>
                </button>
            </div>
        </aside>

        <!-- Main Content -->
        <main class="main-content">
            <header class="main-header">
                <button class="menu-toggle" id="menu-toggle" aria-label="Open menu">
                    <span></span>
                </button>
                
<nav class="breadcrumb">
    <a href="/study/en/">Topics</a>
    <span class="separator">/</span>
    <a href="/study/en/Foundation_Models/">Foundation Models</a>
    <span class="separator">/</span>
    <span class="current">Scaling Laws</span>
</nav>

            </header>

            <div class="content">
                
<article class="lesson-article">
    <header class="lesson-header">
        <h1>Scaling Laws</h1>
    </header>

    
<div class="lesson-toolbar lesson-toolbar--top">
    <div class="toolbar-nav toolbar-nav--prev">
        
        <a href="/study/en/Foundation_Models/01_Foundation_Model_Paradigm.html" class="nav-prev">
            <span class="nav-label">Previous</span>
            <span class="nav-title">Foundation Model Paradigm</span>
        </a>
        
    </div>
    <div class="toolbar-actions">
        <button class="btn btn-copy-link" data-action="copy-link" title="Copy link">
            <span class="icon">ğŸ”—</span>
            <span class="text">Copy link</span>
        </button>
        <a href="/study/en/Foundation_Models/" class="btn btn-topic-link" title="Back to topic list">
            <span class="icon">ğŸ“‹</span>
            <span class="text">Topic list</span>
        </a>
    </div>
    <div class="toolbar-nav toolbar-nav--next">
        
        <a href="/study/en/Foundation_Models/03_Emergent_Abilities.html" class="nav-next">
            <span class="nav-label">Next</span>
            <span class="nav-title">Emergent Abilities</span>
        </a>
        
    </div>
</div>


    
    <nav class="toc" id="toc">
        <h2>Table of Contents</h2>
        <div class="toc">
<ul>
<li><a href="#learning-objectives">Learning Objectives</a></li>
<li><a href="#1-what-are-scaling-laws">1. What are Scaling Laws?</a><ul>
<li><a href="#11-definition">1.1 Definition</a></li>
<li><a href="#12-why-important">1.2 Why Important?</a></li>
</ul>
</li>
<li><a href="#2-kaplan-scaling-laws-2020">2. Kaplan Scaling Laws (2020)</a><ul>
<li><a href="#21-openais-initial-research">2.1 OpenAI's Initial Research</a></li>
<li><a href="#22-visualization">2.2 Visualization</a></li>
<li><a href="#23-model-design-following-kaplans-law">2.3 Model Design Following Kaplan's Law</a></li>
</ul>
</li>
<li><a href="#3-chinchilla-scaling-laws-2022">3. Chinchilla Scaling Laws (2022)</a><ul>
<li><a href="#31-deepminds-rediscovery">3.1 DeepMind's Rediscovery</a></li>
<li><a href="#32-chinchilla-vs-gopher-comparison">3.2 Chinchilla vs Gopher Comparison</a></li>
<li><a href="#33-status-of-existing-models">3.3 Status of Existing Models</a></li>
</ul>
</li>
<li><a href="#4-mathematical-formulation">4. Mathematical Formulation</a><ul>
<li><a href="#41-loss-function">4.1 Loss Function</a></li>
<li><a href="#42-scaling-law-simulation-with-python">4.2 Scaling Law Simulation with Python</a></li>
</ul>
</li>
<li><a href="#5-application-in-real-models">5. Application in Real Models</a><ul>
<li><a href="#51-scaling-comparison-of-major-models">5.1 Scaling Comparison of Major Models</a></li>
<li><a href="#52-benefits-of-over-training">5.2 Benefits of Over-training</a></li>
<li><a href="#53-practical-guidelines">5.3 Practical Guidelines</a></li>
</ul>
</li>
<li><a href="#6-extensions-of-scaling-laws">6. Extensions of Scaling Laws</a><ul>
<li><a href="#61-scaling-in-other-domains">6.1 Scaling in Other Domains</a></li>
<li><a href="#62-fine-tuning-scaling-laws">6.2 Fine-tuning Scaling Laws</a></li>
<li><a href="#63-inference-scaling-test-time-compute">6.3 Inference Scaling (Test-time Compute)</a></li>
</ul>
</li>
<li><a href="#7-limitations-of-scaling">7. Limitations of Scaling</a><ul>
<li><a href="#71-physical-limits">7.1 Physical Limits</a></li>
<li><a href="#72-improvement-directions-beyond-scaling">7.2 Improvement Directions Beyond Scaling</a></li>
</ul>
</li>
<li><a href="#summary">Summary</a><ul>
<li><a href="#key-concepts">Key Concepts</a></li>
<li><a href="#practical-formulas">Practical Formulas</a></li>
<li><a href="#next-steps">Next Steps</a></li>
</ul>
</li>
<li><a href="#references">References</a><ul>
<li><a href="#key-papers">Key Papers</a></li>
<li><a href="#additional-resources">Additional Resources</a></li>
</ul>
</li>
</ul>
</div>

    </nav>
    

    <div class="lesson-content markdown-body">
        <h1 id="scaling-laws">Scaling Laws<a class="header-link" href="#scaling-laws" title="Permanent link">&para;</a></h1>
<h2 id="learning-objectives">Learning Objectives<a class="header-link" href="#learning-objectives" title="Permanent link">&para;</a></h2>
<ul>
<li>Understand the concept and mathematical form of Scaling Laws</li>
<li>Compare Kaplan et al. vs Chinchilla laws</li>
<li>Learn compute-optimal training strategies</li>
<li>Grasp how to apply Scaling Laws in practice</li>
</ul>
<hr />
<h2 id="1-what-are-scaling-laws">1. What are Scaling Laws?<a class="header-link" href="#1-what-are-scaling-laws" title="Permanent link">&para;</a></h2>
<h3 id="11-definition">1.1 Definition<a class="header-link" href="#11-definition" title="Permanent link">&para;</a></h3>
<p><strong>Scaling Laws</strong> are empirical laws that describe the relationship between <strong>number of parameters (N)</strong>, <strong>amount of data (D)</strong>, <strong>compute (C)</strong>, and <strong>performance (Loss)</strong> of models.</p>
<div class="highlight"><pre><span></span><code>â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    Core Relationships in Scaling Laws            â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                 â”‚
â”‚  Loss â‰ˆ A/N^Î± + B/D^Î² + E                                       â”‚
â”‚                                                                 â”‚
â”‚  N = Number of model parameters                                 â”‚
â”‚  D = Number of training data tokens                             â”‚
â”‚  C = Compute (FLOPs) â‰ˆ 6 Ã— N Ã— D                                â”‚
â”‚  E = Irreducible minimum loss (entropy of data)                 â”‚
â”‚                                                                 â”‚
â”‚  Key findings:                                                  â”‚
â”‚  â€¢ Loss decreases according to Power Law with respect to N, D   â”‚
â”‚  â€¢ When C is fixed, there exists an optimal ratio of N and D    â”‚
â”‚  â€¢ Larger models utilize data more efficiently                  â”‚
â”‚                                                                 â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
</code></pre></div>

<h3 id="12-why-important">1.2 Why Important?<a class="header-link" href="#12-why-important" title="Permanent link">&para;</a></h3>
<div class="highlight"><pre><span></span><code><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">Practical value of Scaling Laws:</span>

<span class="sd">1. Cost Prediction</span>
<span class="sd">   - Estimate required resources before training</span>
<span class="sd">   - &quot;How much is needed to train a 10B model?&quot;</span>

<span class="sd">2. Optimal Allocation</span>
<span class="sd">   - Decide model size vs data amount with fixed budget</span>
<span class="sd">   - &quot;What&#39;s the best setup with $100M budget?&quot;</span>

<span class="sd">3. Performance Prediction</span>
<span class="sd">   - Estimate large model performance from small models</span>
<span class="sd">   - &quot;With current 7B model, how much better will 70B be?&quot;</span>

<span class="sd">4. Research Planning</span>
<span class="sd">   - Determine research directions with high ROI</span>
<span class="sd">   - &quot;Should we increase data or scale up model?&quot;</span>
<span class="sd">&quot;&quot;&quot;</span>
</code></pre></div>

<hr />
<h2 id="2-kaplan-scaling-laws-2020">2. Kaplan Scaling Laws (2020)<a class="header-link" href="#2-kaplan-scaling-laws-2020" title="Permanent link">&para;</a></h2>
<h3 id="21-openais-initial-research">2.1 OpenAI's Initial Research<a class="header-link" href="#21-openais-initial-research" title="Permanent link">&para;</a></h3>
<p>Laws discovered in Kaplan et al.'s 2020 paper "Scaling Laws for Neural Language Models":</p>
<div class="highlight"><pre><span></span><code>â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    Kaplan Scaling Laws                           â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                 â”‚
â”‚  1. Loss vs Parameters                                          â”‚
â”‚     L(N) = (N_c / N)^Î±_N, where Î±_N â‰ˆ 0.076                     â”‚
â”‚                                                                 â”‚
â”‚  2. Loss vs Data                                                â”‚
â”‚     L(D) = (D_c / D)^Î±_D, where Î±_D â‰ˆ 0.095                     â”‚
â”‚                                                                 â”‚
â”‚  3. Loss vs Compute                                             â”‚
â”‚     L(C) = (C_c / C)^Î±_C, where Î±_C â‰ˆ 0.050                     â”‚
â”‚                                                                 â”‚
â”‚  Key claims:                                                    â”‚
â”‚  â€¢ Parameter count is most important (Î±_N &lt; Î±_D)                â”‚
â”‚  â€¢ For same compute, larger model + less data is better         â”‚
â”‚  â€¢ N âˆ C^0.73, D âˆ C^0.27 (Compute-optimal allocation)          â”‚
â”‚                                                                 â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
</code></pre></div>

<h3 id="22-visualization">2.2 Visualization<a class="header-link" href="#22-visualization" title="Permanent link">&para;</a></h3>
<div class="highlight"><pre><span></span><code>   Loss (Log)
       â”‚
   3.5 â”œâ”€â—â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ 100M params
       â”‚   â•²
   3.0 â”œâ”€â”€â”€â”€â”€â—â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€  1B params
       â”‚       â•²
   2.5 â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â—â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ 10B params
       â”‚           â•²
   2.0 â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â—â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€100B params
       â”‚               â•²
   1.5 â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â—â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€  1T params (predicted)
       â”‚
       â””â”€â”€â”€â”¬â”€â”€â”€â”¬â”€â”€â”€â”¬â”€â”€â”€â”¬â”€â”€â”€â”¬â”€â”€â”€â”¬â”€â”€â”€â”¬â”€â”€â–¶
          10^18  19   20   21   22   23   Compute (FLOPs)

   â€¢ Straight line = Power Law (linear in log scale)
   â€¢ Slope = Î±_C â‰ˆ 0.05
</code></pre></div>

<h3 id="23-model-design-following-kaplans-law">2.3 Model Design Following Kaplan's Law<a class="header-link" href="#23-model-design-following-kaplans-law" title="Permanent link">&para;</a></h3>
<div class="highlight"><pre><span></span><code><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">Example application of Kaplan&#39;s law:</span>

<span class="sd">Compute budget: 10^21 FLOPs</span>

<span class="sd">Kaplan optimal allocation:</span>
<span class="sd">- N âˆ C^0.73 â†’ N â‰ˆ 10^15 (about 1 trillion parameters?!)</span>
<span class="sd">- D âˆ C^0.27 â†’ D â‰ˆ 10^9 (about 1 billion tokens)</span>

<span class="sd">Problem:</span>
<span class="sd">- Model becomes too large with insufficient data</span>
<span class="sd">- GPT-3 (175B) followed this law but...</span>
<span class="sd">- Chinchilla refuted this</span>
<span class="sd">&quot;&quot;&quot;</span>
</code></pre></div>

<hr />
<h2 id="3-chinchilla-scaling-laws-2022">3. Chinchilla Scaling Laws (2022)<a class="header-link" href="#3-chinchilla-scaling-laws-2022" title="Permanent link">&para;</a></h2>
<h3 id="31-deepminds-rediscovery">3.1 DeepMind's Rediscovery<a class="header-link" href="#31-deepminds-rediscovery" title="Permanent link">&para;</a></h3>
<p>Hoffmann et al.'s "Training Compute-Optimal Large Language Models" revised Kaplan's law:</p>
<div class="highlight"><pre><span></span><code>â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚<span class="w">                    </span><span class="nv">Chinchilla</span><span class="w"> </span><span class="nv">Scaling</span><span class="w"> </span><span class="nv">Laws</span><span class="w">                       </span>â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚<span class="w">                                                                 </span>â”‚
â”‚<span class="w">  </span><span class="nv">Key</span><span class="w"> </span><span class="nv">finding</span>:<span class="w"> </span><span class="nv">Existing</span><span class="w"> </span><span class="nv">models</span><span class="w"> </span><span class="nv">are</span><span class="w"> </span><span class="nv">Under</span><span class="o">-</span><span class="nv">trained</span><span class="o">!</span><span class="w">                </span>â”‚
â”‚<span class="w">                                                                 </span>â”‚
â”‚<span class="w">  </span><span class="nv">Compute</span><span class="o">-</span><span class="nv">optimal</span><span class="w"> </span><span class="nv">scaling</span>:<span class="w">                                        </span>â”‚
â”‚<span class="w">  </span>â€¢<span class="w"> </span><span class="nv">N</span><span class="w"> </span>âˆ<span class="w"> </span><span class="nv">C</span><span class="o">^</span><span class="mi">0</span>.<span class="mi">5</span><span class="w">  </span><span class="ss">(</span><span class="nv">number</span><span class="w"> </span><span class="nv">of</span><span class="w"> </span><span class="nv">parameters</span><span class="ss">)</span><span class="w">                            </span>â”‚
â”‚<span class="w">  </span>â€¢<span class="w"> </span><span class="nv">D</span><span class="w"> </span>âˆ<span class="w"> </span><span class="nv">C</span><span class="o">^</span><span class="mi">0</span>.<span class="mi">5</span><span class="w">  </span><span class="ss">(</span><span class="nv">number</span><span class="w"> </span><span class="nv">of</span><span class="w"> </span><span class="nv">data</span><span class="w"> </span><span class="nv">tokens</span><span class="ss">)</span><span class="w">                           </span>â”‚
â”‚<span class="w">  </span>â€¢<span class="w"> </span><span class="nv">i</span>.<span class="nv">e</span>.,<span class="w"> </span><span class="nv">N</span><span class="w"> </span><span class="nv">and</span><span class="w"> </span><span class="nv">D</span><span class="w"> </span><span class="nv">should</span><span class="w"> </span><span class="nv">increase</span><span class="w"> </span><span class="nv">at</span><span class="w"> </span><span class="nv">the</span><span class="w"> </span><span class="nv">same</span><span class="w"> </span><span class="nv">rate</span><span class="w"> </span><span class="k">for</span><span class="w"> </span><span class="nv">optimality</span>â”‚
â”‚<span class="w">                                                                 </span>â”‚
â”‚<span class="w">  </span><span class="nv">Practical</span><span class="w"> </span><span class="nv">rule</span>:<span class="w">                                                </span>â”‚
â”‚<span class="w">  </span><span class="nv">D</span><span class="w"> </span>â‰ˆ<span class="w"> </span><span class="mi">20</span><span class="w"> </span>Ã—<span class="w"> </span><span class="nv">N</span><span class="w">  </span><span class="ss">(</span><span class="nv">tokens</span><span class="w"> </span>â‰ˆ<span class="w"> </span><span class="mi">20</span><span class="w"> </span>Ã—<span class="w"> </span><span class="nv">parameters</span><span class="ss">)</span><span class="w">                         </span>â”‚
â”‚<span class="w">                                                                 </span>â”‚
â”‚<span class="w">  </span><span class="nv">Examples</span>:<span class="w">                                                      </span>â”‚
â”‚<span class="w">  </span>â€¢<span class="w"> </span><span class="mi">1</span><span class="nv">B</span><span class="w"> </span><span class="nv">model</span><span class="w"> </span>â†’<span class="w"> </span><span class="mi">20</span><span class="nv">B</span><span class="w"> </span><span class="nv">tokens</span><span class="w"> </span><span class="nv">needed</span><span class="w">                                 </span>â”‚
â”‚<span class="w">  </span>â€¢<span class="w"> </span><span class="mi">7</span><span class="nv">B</span><span class="w"> </span><span class="nv">model</span><span class="w"> </span>â†’<span class="w"> </span><span class="mi">140</span><span class="nv">B</span><span class="w"> </span><span class="nv">tokens</span><span class="w"> </span><span class="nv">needed</span><span class="w">                                </span>â”‚
â”‚<span class="w">  </span>â€¢<span class="w"> </span><span class="mi">70</span><span class="nv">B</span><span class="w"> </span><span class="nv">model</span><span class="w"> </span>â†’<span class="w"> </span><span class="mi">1</span>.<span class="mi">4</span><span class="nv">T</span><span class="w"> </span><span class="nv">tokens</span><span class="w"> </span><span class="nv">needed</span><span class="w">                               </span>â”‚
â”‚<span class="w">                                                                 </span>â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
</code></pre></div>

<h3 id="32-chinchilla-vs-gopher-comparison">3.2 Chinchilla vs Gopher Comparison<a class="header-link" href="#32-chinchilla-vs-gopher-comparison" title="Permanent link">&para;</a></h3>
<div class="highlight"><pre><span></span><code><span class="err">â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”</span>
<span class="err">â”‚</span><span class="w">               </span><span class="nx">Chinchilla</span><span class="w"> </span><span class="p">(</span><span class="mi">70</span><span class="nx">B</span><span class="p">)</span><span class="w"> </span><span class="nx">vs</span><span class="w"> </span><span class="nx">Gopher</span><span class="w"> </span><span class="p">(</span><span class="mi">280</span><span class="nx">B</span><span class="p">)</span><span class="w">                  </span><span class="err">â”‚</span>
<span class="err">â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤</span>
<span class="err">â”‚</span><span class="w">                                                                 </span><span class="err">â”‚</span>
<span class="err">â”‚</span><span class="w">  </span><span class="nx">Model</span><span class="w">      </span><span class="err">â”‚</span><span class="w"> </span><span class="nx">Parameters</span><span class="err">â”‚</span><span class="w"> </span><span class="nx">Train</span><span class="w"> </span><span class="nx">Tokens</span><span class="err">â”‚</span><span class="w"> </span><span class="nx">Compute</span><span class="w">   </span><span class="err">â”‚</span><span class="w"> </span><span class="nx">Performance</span><span class="err">â”‚</span>
<span class="err">â”‚</span><span class="w">  </span><span class="err">â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”‚â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”‚â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”‚â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”‚â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”‚</span>
<span class="err">â”‚</span><span class="w">  </span><span class="nx">Gopher</span><span class="w">     </span><span class="err">â”‚</span><span class="w"> </span><span class="mi">280</span><span class="nx">B</span><span class="w">     </span><span class="err">â”‚</span><span class="w"> </span><span class="mi">300</span><span class="nx">B</span><span class="w">        </span><span class="err">â”‚</span><span class="w"> </span><span class="m m-Double">5.0</span><span class="err">Ã—</span><span class="mi">10</span><span class="o">^</span><span class="mi">23</span><span class="w"> </span><span class="err">â”‚</span><span class="w"> </span><span class="nx">Baseline</span><span class="w">   </span><span class="err">â”‚</span>
<span class="err">â”‚</span><span class="w">  </span><span class="nx">Chinchilla</span><span class="w"> </span><span class="err">â”‚</span><span class="w"> </span><span class="mi">70</span><span class="nx">B</span><span class="w">      </span><span class="err">â”‚</span><span class="w"> </span><span class="m m-Double">1.4</span><span class="nx">T</span><span class="w">        </span><span class="err">â”‚</span><span class="w"> </span><span class="m m-Double">5.0</span><span class="err">Ã—</span><span class="mi">10</span><span class="o">^</span><span class="mi">23</span><span class="w"> </span><span class="err">â”‚</span><span class="w"> </span><span class="o">+</span><span class="mi">10</span><span class="o">%</span><span class="w"> </span><span class="nx">better</span><span class="err">â”‚</span>
<span class="err">â”‚</span><span class="w">                                                                 </span><span class="err">â”‚</span>
<span class="err">â”‚</span><span class="w">  </span><span class="nx">Conclusion</span><span class="p">:</span><span class="w">                                                    </span><span class="err">â”‚</span>
<span class="err">â”‚</span><span class="w">  </span><span class="err">â€¢</span><span class="w"> </span><span class="mi">4</span><span class="nx">x</span><span class="w"> </span><span class="nx">smaller</span><span class="w"> </span><span class="nx">model</span><span class="w"> </span><span class="nx">performs</span><span class="w"> </span><span class="nx">better</span><span class="w"> </span><span class="nx">with</span><span class="w"> </span><span class="nx">same</span><span class="w"> </span><span class="nx">compute</span><span class="p">!</span><span class="w">          </span><span class="err">â”‚</span>
<span class="err">â”‚</span><span class="w">  </span><span class="err">â€¢</span><span class="w"> </span><span class="nx">Gopher</span><span class="w"> </span><span class="k">is</span><span class="w"> </span><span class="nx">Under</span><span class="o">-</span><span class="nx">trained</span><span class="w"> </span><span class="p">(</span><span class="nx">insufficient</span><span class="w"> </span><span class="nx">data</span><span class="p">)</span><span class="w">                  </span><span class="err">â”‚</span>
<span class="err">â”‚</span><span class="w">  </span><span class="err">â€¢</span><span class="w"> </span><span class="nx">Simply</span><span class="w"> </span><span class="nx">increasing</span><span class="w"> </span><span class="nx">model</span><span class="w"> </span><span class="nx">size</span><span class="w"> </span><span class="k">is</span><span class="w"> </span><span class="nx">inefficient</span><span class="w">                  </span><span class="err">â”‚</span>
<span class="err">â”‚</span><span class="w">                                                                 </span><span class="err">â”‚</span>
<span class="err">â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜</span>
</code></pre></div>

<h3 id="33-status-of-existing-models">3.3 Status of Existing Models<a class="header-link" href="#33-status-of-existing-models" title="Permanent link">&para;</a></h3>
<div class="highlight"><pre><span></span><code>             Tokens (D)
                â”‚
          10T   â”œ                               â— LLaMA 2 (2023)
                â”‚                           â—
           1T   â”œ                       â— Chinchilla (Optimal)
                â”‚                   â•±
         100B   â”œ               â•±       â— GPT-3 (Under-trained)
                â”‚           â•±
          10B   â”œ       â•±
                â”‚   â•±                   â— Gopher (Very Under-trained)
           1B   â”œâ”€
                â””â”€â”€â”€â”¬â”€â”€â”€â”¬â”€â”€â”€â”¬â”€â”€â”€â”¬â”€â”€â”€â”¬â”€â”€â”€â”¬â”€â”€â”€â”¬â”€â”€â”€â–¶
                   1B  10B 100B  1T  10T      Parameters (N)

             â•± = Compute-optimal frontier (D â‰ˆ 20N)

             Points below the line are Under-trained
</code></pre></div>

<hr />
<h2 id="4-mathematical-formulation">4. Mathematical Formulation<a class="header-link" href="#4-mathematical-formulation" title="Permanent link">&para;</a></h2>
<h3 id="41-loss-function">4.1 Loss Function<a class="header-link" href="#41-loss-function" title="Permanent link">&para;</a></h3>
<div class="highlight"><pre><span></span><code><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">Mathematical form of Scaling Law:</span>

<span class="sd">1. Single Variable Scaling</span>
<span class="sd">   L(N) = (N_c / N)^Î± + L_âˆ     # Consider parameters only</span>
<span class="sd">   L(D) = (D_c / D)^Î² + L_âˆ     # Consider data only</span>

<span class="sd">2. Combined Scaling (Chinchilla)</span>
<span class="sd">   L(N, D) = E + A/N^Î± + B/D^Î²</span>

<span class="sd">   where:</span>
<span class="sd">   - E â‰ˆ 1.69 (irreducible loss, data entropy)</span>
<span class="sd">   - A â‰ˆ 406.4</span>
<span class="sd">   - B â‰ˆ 410.7</span>
<span class="sd">   - Î± â‰ˆ 0.34</span>
<span class="sd">   - Î² â‰ˆ 0.28</span>

<span class="sd">3. Compute Perspective</span>
<span class="sd">   C â‰ˆ 6 Ã— N Ã— D  (FLOPs for training)</span>

<span class="sd">   Optimization: min L(N, D) subject to C = 6ND</span>

<span class="sd">   Result: N* âˆ C^0.5, D* âˆ C^0.5</span>
<span class="sd">&quot;&quot;&quot;</span>
</code></pre></div>

<h3 id="42-scaling-law-simulation-with-python">4.2 Scaling Law Simulation with Python<a class="header-link" href="#42-scaling-law-simulation-with-python" title="Permanent link">&para;</a></h3>
<div class="highlight"><pre><span></span><code><span class="kn">import</span><span class="w"> </span><span class="nn">numpy</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">np</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">matplotlib.pyplot</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">plt</span>

<span class="k">def</span><span class="w"> </span><span class="nf">chinchilla_loss</span><span class="p">(</span><span class="n">N</span><span class="p">,</span> <span class="n">D</span><span class="p">,</span> <span class="n">A</span><span class="o">=</span><span class="mf">406.4</span><span class="p">,</span> <span class="n">B</span><span class="o">=</span><span class="mf">410.7</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.34</span><span class="p">,</span> <span class="n">beta</span><span class="o">=</span><span class="mf">0.28</span><span class="p">,</span> <span class="n">E</span><span class="o">=</span><span class="mf">1.69</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Calculate Loss according to Chinchilla Scaling Law</span>

<span class="sd">    Args:</span>
<span class="sd">        N: Number of parameters (billions)</span>
<span class="sd">        D: Number of tokens (billions)</span>

<span class="sd">    Returns:</span>
<span class="sd">        Expected Loss (log of perplexity)</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="n">E</span> <span class="o">+</span> <span class="n">A</span> <span class="o">/</span> <span class="p">(</span><span class="n">N</span> <span class="o">**</span> <span class="n">alpha</span><span class="p">)</span> <span class="o">+</span> <span class="n">B</span> <span class="o">/</span> <span class="p">(</span><span class="n">D</span> <span class="o">**</span> <span class="n">beta</span><span class="p">)</span>

<span class="k">def</span><span class="w"> </span><span class="nf">optimal_allocation</span><span class="p">(</span><span class="n">compute_budget</span><span class="p">,</span> <span class="n">flops_per_token</span><span class="o">=</span><span class="mi">6</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Calculate optimal N, D for given compute budget</span>

<span class="sd">    Args:</span>
<span class="sd">        compute_budget: Total FLOPs (e.g., 10^23)</span>
<span class="sd">        flops_per_token: FLOPs per token (approximately 6N)</span>

<span class="sd">    Returns:</span>
<span class="sd">        optimal_N, optimal_D (in billions)</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="c1"># Chinchilla optimal ratio: D â‰ˆ 20N</span>
    <span class="c1"># C = 6 * N * D = 6 * N * 20N = 120 * N^2</span>
    <span class="c1"># N = sqrt(C / 120)</span>

    <span class="n">optimal_N</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="n">compute_budget</span> <span class="o">/</span> <span class="mi">120</span><span class="p">)</span> <span class="o">/</span> <span class="mf">1e9</span>  <span class="c1"># billions</span>
    <span class="n">optimal_D</span> <span class="o">=</span> <span class="mi">20</span> <span class="o">*</span> <span class="n">optimal_N</span>                        <span class="c1"># billions</span>

    <span class="k">return</span> <span class="n">optimal_N</span><span class="p">,</span> <span class="n">optimal_D</span>

<span class="c1"># Example: 10^23 FLOPs budget</span>
<span class="n">compute</span> <span class="o">=</span> <span class="mf">1e23</span>
<span class="n">N_opt</span><span class="p">,</span> <span class="n">D_opt</span> <span class="o">=</span> <span class="n">optimal_allocation</span><span class="p">(</span><span class="n">compute</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Compute budget: 10^23 FLOPs&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Optimal parameters: </span><span class="si">{</span><span class="n">N_opt</span><span class="si">:</span><span class="s2">.1f</span><span class="si">}</span><span class="s2">B&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Optimal tokens: </span><span class="si">{</span><span class="n">D_opt</span><span class="si">:</span><span class="s2">.1f</span><span class="si">}</span><span class="s2">B&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Expected loss: </span><span class="si">{</span><span class="n">chinchilla_loss</span><span class="p">(</span><span class="n">N_opt</span><span class="p">,</span><span class="w"> </span><span class="n">D_opt</span><span class="p">)</span><span class="si">:</span><span class="s2">.3f</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>

<span class="c1"># Visualization: Loss according to N vs D</span>
<span class="n">N_range</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">logspace</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">50</span><span class="p">)</span>  <span class="c1"># 1B to 1000B</span>
<span class="n">D_range</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">logspace</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">4</span><span class="p">,</span> <span class="mi">50</span><span class="p">)</span>  <span class="c1"># 1B to 10000B</span>

<span class="n">N_grid</span><span class="p">,</span> <span class="n">D_grid</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">meshgrid</span><span class="p">(</span><span class="n">N_range</span><span class="p">,</span> <span class="n">D_range</span><span class="p">)</span>
<span class="n">Loss_grid</span> <span class="o">=</span> <span class="n">chinchilla_loss</span><span class="p">(</span><span class="n">N_grid</span><span class="p">,</span> <span class="n">D_grid</span><span class="p">)</span>

<span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">10</span><span class="p">,</span> <span class="mi">8</span><span class="p">))</span>
<span class="n">plt</span><span class="o">.</span><span class="n">contour</span><span class="p">(</span><span class="n">N_grid</span><span class="p">,</span> <span class="n">D_grid</span><span class="p">,</span> <span class="n">Loss_grid</span><span class="p">,</span> <span class="n">levels</span><span class="o">=</span><span class="mi">20</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xscale</span><span class="p">(</span><span class="s1">&#39;log&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">yscale</span><span class="p">(</span><span class="s1">&#39;log&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s1">&#39;Parameters N (Billions)&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s1">&#39;Tokens D (Billions)&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s1">&#39;Chinchilla Scaling Law: Loss Contours&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">colorbar</span><span class="p">(</span><span class="n">label</span><span class="o">=</span><span class="s1">&#39;Loss&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">N_range</span><span class="p">,</span> <span class="mi">20</span><span class="o">*</span><span class="n">N_range</span><span class="p">,</span> <span class="s1">&#39;r--&#39;</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s1">&#39;Optimal ratio (D=20N)&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">legend</span><span class="p">()</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</code></pre></div>

<hr />
<h2 id="5-application-in-real-models">5. Application in Real Models<a class="header-link" href="#5-application-in-real-models" title="Permanent link">&para;</a></h2>
<h3 id="51-scaling-comparison-of-major-models">5.1 Scaling Comparison of Major Models<a class="header-link" href="#51-scaling-comparison-of-major-models" title="Permanent link">&para;</a></h3>
<table>
<thead>
<tr>
<th>Model</th>
<th>Parameters (N)</th>
<th>Tokens (D)</th>
<th>D/N Ratio</th>
<th>Status</th>
</tr>
</thead>
<tbody>
<tr>
<td>GPT-3</td>
<td>175B</td>
<td>300B</td>
<td>1.7</td>
<td>Under-trained</td>
</tr>
<tr>
<td>Gopher</td>
<td>280B</td>
<td>300B</td>
<td>1.1</td>
<td>Very Under-trained</td>
</tr>
<tr>
<td>Chinchilla</td>
<td>70B</td>
<td>1.4T</td>
<td>20</td>
<td>Optimal</td>
</tr>
<tr>
<td>LLaMA 1</td>
<td>65B</td>
<td>1.4T</td>
<td>21.5</td>
<td>Near-optimal</td>
</tr>
<tr>
<td>LLaMA 2</td>
<td>70B</td>
<td>2T</td>
<td>28.6</td>
<td>Slight Over-trained</td>
</tr>
<tr>
<td>Mistral</td>
<td>7B</td>
<td>8T (est.)</td>
<td>~1000</td>
<td>Over-trained</td>
</tr>
</tbody>
</table>
<h3 id="52-benefits-of-over-training">5.2 Benefits of Over-training<a class="header-link" href="#52-benefits-of-over-training" title="Permanent link">&para;</a></h3>
<div class="highlight"><pre><span></span><code>â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                Over-training Strategy (LLaMA 2, Mistral)         â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                 â”‚
â”‚  Chinchilla is &quot;training&quot; optimal but not &quot;deployment&quot; optimal! â”‚
â”‚                                                                 â”‚
â”‚  From deployment perspective:                                   â”‚
â”‚  â€¢ Inference cost âˆ N (model size)                              â”‚
â”‚  â€¢ Training once, inference trillions of times                  â”‚
â”‚                                                                 â”‚
â”‚  Therefore:                                                     â”‚
â”‚  â€¢ Smaller model + more data = inference efficient              â”‚
â”‚  â€¢ &quot;Inference-optimal&quot; â‰  &quot;Compute-optimal&quot;                      â”‚
â”‚                                                                 â”‚
â”‚  LLaMA 2 strategy:                                              â”‚
â”‚  â€¢ 70B model with 2T tokens (D/N â‰ˆ 29)                          â”‚
â”‚  â€¢ Train longer than Chinchilla                                 â”‚
â”‚  â€¢ Result: Better performance with smaller model                â”‚
â”‚                                                                 â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
</code></pre></div>

<h3 id="53-practical-guidelines">5.3 Practical Guidelines<a class="header-link" href="#53-practical-guidelines" title="Permanent link">&para;</a></h3>
<div class="highlight"><pre><span></span><code><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">Scaling strategies in practice:</span>

<span class="sd">1. Research/Experimentation Phase (Compute-limited)</span>
<span class="sd">   - Follow Chinchilla rule: D â‰ˆ 20N</span>
<span class="sd">   - Iterate quickly with smaller models</span>

<span class="sd">2. Production Deployment (Inference-limited)</span>
<span class="sd">   - Consider over-training: D &gt; 20N</span>
<span class="sd">   - Smaller model + more data</span>
<span class="sd">   - Example: Mistral 7B &gt; LLaMA 2 13B (on some tasks)</span>

<span class="sd">3. Budget Planning</span>
<span class="sd">   - C = 6 * N * D (FLOPs)</span>
<span class="sd">   - GPU hours â‰ˆ C / (GPU_FLOPS * utilization)</span>
<span class="sd">   - Example: A100 80GB = ~300 TFLOPS (effective)</span>

<span class="sd">4. Scale-up Strategy</span>
<span class="sd">   - Tune hyperparameters with small models</span>
<span class="sd">   - Predict large model performance with Scaling Law</span>
<span class="sd">   - Execute large-scale training after validation</span>
<span class="sd">&quot;&quot;&quot;</span>

<span class="k">def</span><span class="w"> </span><span class="nf">estimate_training_cost</span><span class="p">(</span><span class="n">N_billions</span><span class="p">,</span> <span class="n">D_billions</span><span class="p">,</span> <span class="n">gpu_price_per_hour</span><span class="o">=</span><span class="mf">2.0</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Estimate training cost</span>

<span class="sd">    Args:</span>
<span class="sd">        N_billions: Number of parameters (B)</span>
<span class="sd">        D_billions: Number of tokens (B)</span>
<span class="sd">        gpu_price_per_hour: GPU cost per hour (USD)</span>

<span class="sd">    Returns:</span>
<span class="sd">        dict: Expected cost information</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">N</span> <span class="o">=</span> <span class="n">N_billions</span> <span class="o">*</span> <span class="mf">1e9</span>
    <span class="n">D</span> <span class="o">=</span> <span class="n">D_billions</span> <span class="o">*</span> <span class="mf">1e9</span>

    <span class="c1"># 6ND FLOPs for training</span>
    <span class="n">total_flops</span> <span class="o">=</span> <span class="mi">6</span> <span class="o">*</span> <span class="n">N</span> <span class="o">*</span> <span class="n">D</span>

    <span class="c1"># A100 80GB: ~300 TFLOPS effective</span>
    <span class="n">gpu_tflops</span> <span class="o">=</span> <span class="mi">300</span>
    <span class="n">gpu_flops</span> <span class="o">=</span> <span class="n">gpu_tflops</span> <span class="o">*</span> <span class="mf">1e12</span>

    <span class="c1"># Total GPU time</span>
    <span class="n">total_gpu_seconds</span> <span class="o">=</span> <span class="n">total_flops</span> <span class="o">/</span> <span class="n">gpu_flops</span>
    <span class="n">total_gpu_hours</span> <span class="o">=</span> <span class="n">total_gpu_seconds</span> <span class="o">/</span> <span class="mi">3600</span>

    <span class="c1"># Cost</span>
    <span class="n">total_cost</span> <span class="o">=</span> <span class="n">total_gpu_hours</span> <span class="o">*</span> <span class="n">gpu_price_per_hour</span>

    <span class="k">return</span> <span class="p">{</span>
        <span class="s2">&quot;total_flops&quot;</span><span class="p">:</span> <span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="n">total_flops</span><span class="si">:</span><span class="s2">.2e</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">,</span>
        <span class="s2">&quot;gpu_hours&quot;</span><span class="p">:</span> <span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="n">total_gpu_hours</span><span class="si">:</span><span class="s2">,.0f</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">,</span>
        <span class="s2">&quot;cost_usd&quot;</span><span class="p">:</span> <span class="sa">f</span><span class="s2">&quot;$</span><span class="si">{</span><span class="n">total_cost</span><span class="si">:</span><span class="s2">,.0f</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">,</span>
        <span class="s2">&quot;cost_with_8gpus&quot;</span><span class="p">:</span> <span class="sa">f</span><span class="s2">&quot;$</span><span class="si">{</span><span class="n">total_cost</span><span class="o">/</span><span class="mi">8</span><span class="si">:</span><span class="s2">,.0f</span><span class="si">}</span><span class="s2"> (</span><span class="si">{</span><span class="n">total_gpu_hours</span><span class="o">/</span><span class="mi">8</span><span class="si">:</span><span class="s2">,.0f</span><span class="si">}</span><span class="s2"> hours)&quot;</span>
    <span class="p">}</span>

<span class="c1"># Example: LLaMA 2 7B training cost</span>
<span class="n">cost_7b</span> <span class="o">=</span> <span class="n">estimate_training_cost</span><span class="p">(</span><span class="mi">7</span><span class="p">,</span> <span class="mi">2000</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;LLaMA 2 7B (2T tokens):&quot;</span><span class="p">)</span>
<span class="k">for</span> <span class="n">k</span><span class="p">,</span> <span class="n">v</span> <span class="ow">in</span> <span class="n">cost_7b</span><span class="o">.</span><span class="n">items</span><span class="p">():</span>
    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;  </span><span class="si">{</span><span class="n">k</span><span class="si">}</span><span class="s2">: </span><span class="si">{</span><span class="n">v</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
</code></pre></div>

<hr />
<h2 id="6-extensions-of-scaling-laws">6. Extensions of Scaling Laws<a class="header-link" href="#6-extensions-of-scaling-laws" title="Permanent link">&para;</a></h2>
<h3 id="61-scaling-in-other-domains">6.1 Scaling in Other Domains<a class="header-link" href="#61-scaling-in-other-domains" title="Permanent link">&para;</a></h3>
<div class="highlight"><pre><span></span><code><span class="err">â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”</span>
<span class="err">â”‚</span><span class="w">                    </span><span class="nx">Scaling</span><span class="w"> </span><span class="nx">Laws</span><span class="w"> </span><span class="nx">by</span><span class="w"> </span><span class="nx">Domain</span><span class="w">                        </span><span class="err">â”‚</span>
<span class="err">â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤</span>
<span class="err">â”‚</span><span class="w">                                                                 </span><span class="err">â”‚</span>
<span class="err">â”‚</span><span class="w">  </span><span class="nx">Vision</span><span class="w"> </span><span class="p">(</span><span class="nx">ViT</span><span class="p">):</span><span class="w">                                                  </span><span class="err">â”‚</span>
<span class="err">â”‚</span><span class="w">  </span><span class="err">â€¢</span><span class="w"> </span><span class="nx">Similar</span><span class="w"> </span><span class="nx">power</span><span class="w"> </span><span class="nx">law</span><span class="w"> </span><span class="nx">observed</span><span class="w">                                   </span><span class="err">â”‚</span>
<span class="err">â”‚</span><span class="w">  </span><span class="err">â€¢</span><span class="w"> </span><span class="nx">Î±</span><span class="w"> </span><span class="err">â‰ˆ</span><span class="w"> </span><span class="m m-Double">0.05</span><span class="w"> </span><span class="p">(</span><span class="nx">smaller</span><span class="w"> </span><span class="nx">than</span><span class="w"> </span><span class="nx">Language</span><span class="p">)</span><span class="w">                            </span><span class="err">â”‚</span>
<span class="err">â”‚</span><span class="w">  </span><span class="err">â€¢</span><span class="w"> </span><span class="nx">Data</span><span class="w"> </span><span class="nx">quality</span><span class="w"> </span><span class="k">is</span><span class="w"> </span><span class="nx">more</span><span class="w"> </span><span class="nx">important</span><span class="w">                               </span><span class="err">â”‚</span>
<span class="err">â”‚</span><span class="w">                                                                 </span><span class="err">â”‚</span>
<span class="err">â”‚</span><span class="w">  </span><span class="nx">Multimodal</span><span class="w"> </span><span class="p">(</span><span class="nx">CLIP</span><span class="p">):</span><span class="w">                                             </span><span class="err">â”‚</span>
<span class="err">â”‚</span><span class="w">  </span><span class="err">â€¢</span><span class="w"> </span><span class="nx">Separate</span><span class="w"> </span><span class="nx">optimization</span><span class="w"> </span><span class="nx">needed</span><span class="w"> </span><span class="k">for</span><span class="w"> </span><span class="nx">image</span><span class="w"> </span><span class="k">and</span><span class="w"> </span><span class="nx">text</span><span class="w"> </span><span class="nx">scaling</span><span class="w">      </span><span class="err">â”‚</span>
<span class="err">â”‚</span><span class="w">  </span><span class="err">â€¢</span><span class="w"> </span><span class="nx">Quality</span><span class="w"> </span><span class="nx">of</span><span class="w"> </span><span class="nx">data</span><span class="w"> </span><span class="nx">pairs</span><span class="w"> </span><span class="k">is</span><span class="w"> </span><span class="nx">critical</span><span class="w">                            </span><span class="err">â”‚</span>
<span class="err">â”‚</span><span class="w">                                                                 </span><span class="err">â”‚</span>
<span class="err">â”‚</span><span class="w">  </span><span class="nx">Code</span><span class="p">:</span><span class="w">                                                          </span><span class="err">â”‚</span>
<span class="err">â”‚</span><span class="w">  </span><span class="err">â€¢</span><span class="w"> </span><span class="nx">Steeper</span><span class="w"> </span><span class="nx">scaling</span><span class="w"> </span><span class="p">(</span><span class="nx">larger</span><span class="w"> </span><span class="nx">Î±</span><span class="p">)</span><span class="w">                                   </span><span class="err">â”‚</span>
<span class="err">â”‚</span><span class="w">  </span><span class="err">â€¢</span><span class="w"> </span><span class="nx">High</span><span class="o">-</span><span class="nx">quality</span><span class="w"> </span><span class="nx">code</span><span class="w"> </span><span class="nx">data</span><span class="w"> </span><span class="k">is</span><span class="w"> </span><span class="nx">scarce</span><span class="w">                             </span><span class="err">â”‚</span>
<span class="err">â”‚</span><span class="w">                                                                 </span><span class="err">â”‚</span>
<span class="err">â”‚</span><span class="w">  </span><span class="nx">Reasoning</span><span class="p">:</span><span class="w">                                                     </span><span class="err">â”‚</span>
<span class="err">â”‚</span><span class="w">  </span><span class="err">â€¢</span><span class="w"> </span><span class="nx">Not</span><span class="w"> </span><span class="nx">smooth</span><span class="w"> </span><span class="nx">due</span><span class="w"> </span><span class="nx">to</span><span class="w"> </span><span class="nx">emergent</span><span class="w"> </span><span class="nx">behavior</span><span class="w">                          </span><span class="err">â”‚</span>
<span class="err">â”‚</span><span class="w">  </span><span class="err">â€¢</span><span class="w"> </span><span class="nx">Sudden</span><span class="w"> </span><span class="nx">performance</span><span class="w"> </span><span class="nx">improvements</span><span class="w"> </span><span class="nx">at</span><span class="w"> </span><span class="nx">specific</span><span class="w"> </span><span class="nx">thresholds</span><span class="w">       </span><span class="err">â”‚</span>
<span class="err">â”‚</span><span class="w">                                                                 </span><span class="err">â”‚</span>
<span class="err">â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜</span>
</code></pre></div>

<h3 id="62-fine-tuning-scaling-laws">6.2 Fine-tuning Scaling Laws<a class="header-link" href="#62-fine-tuning-scaling-laws" title="Permanent link">&para;</a></h3>
<div class="highlight"><pre><span></span><code><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">Scaling Law also applies to fine-tuning:</span>

<span class="sd">Research findings:</span>
<span class="sd">- Larger base model = less fine-tuning data needed</span>
<span class="sd">- Fine-tuning data also scales according to power law</span>
<span class="sd">- PEFT like LoRA follows similar patterns</span>

<span class="sd">Practical rules:</span>
<span class="sd">- Base model size Ã— 10 = Fine-tuning data amount (approximately)</span>
<span class="sd">- 7B model: ~1K-10K examples</span>
<span class="sd">- 70B model: ~100-1K examples (to achieve same performance)</span>

<span class="sd">However, quality &gt; quantity:</span>
<span class="sd">- 100 high-quality examples &gt; 10,000 low-quality examples</span>
<span class="sd">&quot;&quot;&quot;</span>
</code></pre></div>

<h3 id="63-inference-scaling-test-time-compute">6.3 Inference Scaling (Test-time Compute)<a class="header-link" href="#63-inference-scaling-test-time-compute" title="Permanent link">&para;</a></h3>
<div class="highlight"><pre><span></span><code><span class="err">â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”</span>
<span class="err">â”‚</span><span class="w">                    </span><span class="nx">Inference</span><span class="w"> </span><span class="nx">Scaling</span><span class="w"> </span><span class="p">(</span><span class="nx">o1</span><span class="o">-</span><span class="nx">style</span><span class="p">)</span><span class="w">                  </span><span class="err">â”‚</span>
<span class="err">â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤</span>
<span class="err">â”‚</span><span class="w">                                                                 </span><span class="err">â”‚</span>
<span class="err">â”‚</span><span class="w">  </span><span class="nx">Traditional</span><span class="w"> </span><span class="nx">Scaling</span><span class="p">:</span><span class="w"> </span><span class="nx">Increase</span><span class="w"> </span><span class="nx">compute</span><span class="w"> </span><span class="nx">during</span><span class="w"> </span><span class="nx">training</span><span class="w">          </span><span class="err">â”‚</span>
<span class="err">â”‚</span><span class="w">  </span><span class="nx">Inference</span><span class="w"> </span><span class="nx">Scaling</span><span class="p">:</span><span class="w"> </span><span class="nx">Increase</span><span class="w"> </span><span class="nx">compute</span><span class="w"> </span><span class="nx">during</span><span class="w"> </span><span class="nx">inference</span><span class="w">           </span><span class="err">â”‚</span>
<span class="err">â”‚</span><span class="w">                                                                 </span><span class="err">â”‚</span>
<span class="err">â”‚</span><span class="w">  </span><span class="nx">Methods</span><span class="p">:</span><span class="w">                                                       </span><span class="err">â”‚</span>
<span class="err">â”‚</span><span class="w">  </span><span class="err">â€¢</span><span class="w"> </span><span class="nx">Generate</span><span class="w"> </span><span class="nx">longer</span><span class="w"> </span><span class="nx">Chain</span><span class="o">-</span><span class="nx">of</span><span class="o">-</span><span class="nx">Thought</span><span class="w">                             </span><span class="err">â”‚</span>
<span class="err">â”‚</span><span class="w">  </span><span class="err">â€¢</span><span class="w"> </span><span class="nx">Generate</span><span class="w"> </span><span class="nx">multiple</span><span class="w"> </span><span class="nx">answers</span><span class="w"> </span><span class="k">and</span><span class="w"> </span><span class="nx">vote</span><span class="w"> </span><span class="p">(</span><span class="k">Self</span><span class="o">-</span><span class="nx">consistency</span><span class="p">)</span><span class="w">        </span><span class="err">â”‚</span>
<span class="err">â”‚</span><span class="w">  </span><span class="err">â€¢</span><span class="w"> </span><span class="nx">Tree</span><span class="w"> </span><span class="nx">of</span><span class="w"> </span><span class="nx">Thoughts</span><span class="w"> </span><span class="o">/</span><span class="w"> </span><span class="nx">Beam</span><span class="w"> </span><span class="nx">Search</span><span class="w">                               </span><span class="err">â”‚</span>
<span class="err">â”‚</span><span class="w">  </span><span class="err">â€¢</span><span class="w"> </span><span class="nx">Iterative</span><span class="w"> </span><span class="nx">Verification</span><span class="o">/</span><span class="nx">Refinement</span><span class="w">                            </span><span class="err">â”‚</span>
<span class="err">â”‚</span><span class="w">                                                                 </span><span class="err">â”‚</span>
<span class="err">â”‚</span><span class="w">  </span><span class="nx">Effects</span><span class="p">:</span><span class="w">                                                       </span><span class="err">â”‚</span>
<span class="err">â”‚</span><span class="w">  </span><span class="err">â€¢</span><span class="w"> </span><span class="nx">Significantly</span><span class="w"> </span><span class="nx">improved</span><span class="w"> </span><span class="nx">accuracy</span><span class="w"> </span><span class="nx">on</span><span class="w"> </span><span class="nx">difficult</span><span class="w"> </span><span class="nx">problems</span><span class="w">        </span><span class="err">â”‚</span>
<span class="err">â”‚</span><span class="w">  </span><span class="err">â€¢</span><span class="w"> </span><span class="nx">Performance</span><span class="w"> </span><span class="nx">improvement</span><span class="w"> </span><span class="nx">possible</span><span class="w"> </span><span class="nx">without</span><span class="w"> </span><span class="nx">training</span><span class="w">            </span><span class="err">â”‚</span>
<span class="err">â”‚</span><span class="w">  </span><span class="err">â€¢</span><span class="w"> </span><span class="nx">Paradigm</span><span class="w"> </span><span class="nx">shift</span><span class="w"> </span><span class="nx">from</span><span class="w"> </span><span class="nx">GPT</span><span class="o">-</span><span class="mi">4</span><span class="w"> </span><span class="err">â†’</span><span class="w"> </span><span class="nx">o1</span><span class="w"> </span><span class="p">(</span><span class="nx">inference</span><span class="o">-</span><span class="nx">time</span><span class="w"> </span><span class="nx">scaling</span><span class="p">)</span><span class="w">      </span><span class="err">â”‚</span>
<span class="err">â”‚</span><span class="w">                                                                 </span><span class="err">â”‚</span>
<span class="err">â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜</span>
</code></pre></div>

<hr />
<h2 id="7-limitations-of-scaling">7. Limitations of Scaling<a class="header-link" href="#7-limitations-of-scaling" title="Permanent link">&para;</a></h2>
<h3 id="71-physical-limits">7.1 Physical Limits<a class="header-link" href="#71-physical-limits" title="Permanent link">&para;</a></h3>
<div class="highlight"><pre><span></span><code><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">Real limitations of Scaling:</span>

<span class="sd">1. Data Limits</span>
<span class="sd">   - Total internet text: ~10-50T tokens</span>
<span class="sd">   - High-quality data is much less</span>
<span class="sd">   - As of 2024, data exhaustion discussion beginning</span>

<span class="sd">2. Compute Limits</span>
<span class="sd">   - Power consumption (MW scale)</span>
<span class="sd">   - Semiconductor supply</span>
<span class="sd">   - Cost (billions of dollars)</span>

<span class="sd">3. Architecture Limits</span>
<span class="sd">   - Attention&#39;s O(nÂ²) complexity</span>
<span class="sd">   - Memory bandwidth bottleneck</span>
<span class="sd">   - Communication overhead in distributed training</span>

<span class="sd">4. Diminishing Returns</span>
<span class="sd">   - Î± â‰ˆ 0.05 means 10x compute â†’ ~12% loss reduction</span>
<span class="sd">   - Increasingly larger investments needed</span>
<span class="sd">&quot;&quot;&quot;</span>
</code></pre></div>

<h3 id="72-improvement-directions-beyond-scaling">7.2 Improvement Directions Beyond Scaling<a class="header-link" href="#72-improvement-directions-beyond-scaling" title="Permanent link">&para;</a></h3>
<table>
<thead>
<tr>
<th>Direction</th>
<th>Description</th>
<th>Examples</th>
</tr>
</thead>
<tbody>
<tr>
<td><strong>Architecture</strong></td>
<td>More efficient structures</td>
<td>Mamba, RWKV, Hyena</td>
</tr>
<tr>
<td><strong>Data Quality</strong></td>
<td>High-quality data curation</td>
<td>Phi, LIMA</td>
</tr>
<tr>
<td><strong>Synthetic Data</strong></td>
<td>Generate training data with AI</td>
<td>Self-Instruct</td>
</tr>
<tr>
<td><strong>Efficient Training</strong></td>
<td>Improve training efficiency</td>
<td>Flash Attention, ZeRO</td>
</tr>
<tr>
<td><strong>Test-time Compute</strong></td>
<td>Increase compute during inference</td>
<td>CoT, Self-consistency, o1</td>
</tr>
</tbody>
</table>
<hr />
<h2 id="summary">Summary<a class="header-link" href="#summary" title="Permanent link">&para;</a></h2>
<h3 id="key-concepts">Key Concepts<a class="header-link" href="#key-concepts" title="Permanent link">&para;</a></h3>
<ul>
<li><strong>Scaling Laws</strong>: Power law relationship between parameters, data, compute, and performance</li>
<li><strong>Kaplan</strong>: Prioritize N (large model + less data)</li>
<li><strong>Chinchilla</strong>: Balance N and D (D â‰ˆ 20N)</li>
<li><strong>Over-training</strong>: Train smaller models longer for inference efficiency</li>
</ul>
<h3 id="practical-formulas">Practical Formulas<a class="header-link" href="#practical-formulas" title="Permanent link">&para;</a></h3>
<div class="highlight"><pre><span></span><code>Compute-optimal: D â‰ˆ 20 Ã— N (tokens)
Training FLOPs: C â‰ˆ 6 Ã— N Ã— D
Inference-optimal: Smaller N, larger D
</code></pre></div>

<h3 id="next-steps">Next Steps<a class="header-link" href="#next-steps" title="Permanent link">&para;</a></h3>
<ul>
<li><a href="03_Emergent_Abilities.md">03_Emergent_Abilities.md</a>: Emergent abilities at scale</li>
<li><a href="08_LLaMA_Family.md">08_LLaMA_Family.md</a>: Scaling application case (LLaMA)</li>
</ul>
<hr />
<h2 id="references">References<a class="header-link" href="#references" title="Permanent link">&para;</a></h2>
<h3 id="key-papers">Key Papers<a class="header-link" href="#key-papers" title="Permanent link">&para;</a></h3>
<ul>
<li>Kaplan et al. (2020). "Scaling Laws for Neural Language Models"</li>
<li>Hoffmann et al. (2022). "Training Compute-Optimal Large Language Models" (Chinchilla)</li>
<li>Touvron et al. (2023). "LLaMA 2: Open Foundation and Fine-Tuned Chat Models"</li>
</ul>
<h3 id="additional-resources">Additional Resources<a class="header-link" href="#additional-resources" title="Permanent link">&para;</a></h3>
<ul>
<li><a href="https://epochai.org/trends">Epoch AI Compute Trends</a></li>
<li><a href="https://www.lesswrong.com/posts/midXmMb2Xg37F2Kgn/ai-scaling-calculator">AI Scaling Calculator</a></li>
</ul>
    </div>

    
<div class="lesson-toolbar lesson-toolbar--bottom">
    <div class="toolbar-nav toolbar-nav--prev">
        
        <a href="/study/en/Foundation_Models/01_Foundation_Model_Paradigm.html" class="nav-prev">
            <span class="nav-label">Previous</span>
            <span class="nav-title">Foundation Model Paradigm</span>
        </a>
        
    </div>
    <div class="toolbar-actions">
        <button class="btn btn-copy-link" data-action="copy-link" title="Copy link">
            <span class="icon">ğŸ”—</span>
            <span class="text">Copy link</span>
        </button>
        <a href="/study/en/Foundation_Models/" class="btn btn-topic-link" title="Back to topic list">
            <span class="icon">ğŸ“‹</span>
            <span class="text">Topic list</span>
        </a>
    </div>
    <div class="toolbar-nav toolbar-nav--next">
        
        <a href="/study/en/Foundation_Models/03_Emergent_Abilities.html" class="nav-next">
            <span class="nav-label">Next</span>
            <span class="nav-title">Emergent Abilities</span>
        </a>
        
    </div>
</div>


    <div class="toolbar-keyboard-hint">
        <kbd>&larr;</kbd> <kbd>&rarr;</kbd> to navigate between lessons
    </div>
</article>

<!-- Scroll to top floating button -->
<button class="scroll-to-top" id="scroll-to-top" title="Scroll to top" aria-label="Scroll to top">â†‘</button>

            </div>
        </main>
    </div>

    <script src="/study/static/js/app.js"></script>
    <script>
        function switchLanguage(newLang) {
            const path = window.location.pathname;
            const base = '/study';
            const currentLang = 'en';
            const newPath = path.replace(base + '/' + currentLang + '/', base + '/' + newLang + '/');
            window.location.href = newPath + window.location.search;
        }
    </script>
    
<script>
    document.addEventListener('DOMContentLoaded', function() {
        // Copy link - independent feedback per button
        document.querySelectorAll('[data-action="copy-link"]').forEach(function(btn) {
            btn.addEventListener('click', function() {
                navigator.clipboard.writeText(window.location.href);
                var textEl = this.querySelector('.text');
                var originalText = textEl.textContent;
                textEl.textContent = 'Copied!';
                setTimeout(function() { textEl.textContent = originalText; }, 2000);
            });
        });

        // Add copy buttons to code blocks
        document.querySelectorAll('.lesson-content pre').forEach(function(pre) {
            var copyBtn = document.createElement('button');
            copyBtn.className = 'code-copy-btn';
            copyBtn.textContent = 'Copy';
            copyBtn.addEventListener('click', function() {
                var code = pre.querySelector('code');
                var text = code ? code.textContent : pre.textContent;
                navigator.clipboard.writeText(text);
                copyBtn.textContent = 'Copied!';
                setTimeout(function() { copyBtn.textContent = 'Copy'; }, 2000);
            });
            pre.style.position = 'relative';
            pre.appendChild(copyBtn);
        });

        // Scroll to top button
        var scrollBtn = document.getElementById('scroll-to-top');
        window.addEventListener('scroll', function() {
            scrollBtn.classList.toggle('visible', window.scrollY > 300);
        });
        scrollBtn.addEventListener('click', function() {
            window.scrollTo({ top: 0, behavior: 'smooth' });
        });

        // Keyboard shortcuts: left/right arrows for lesson navigation
        document.addEventListener('keydown', function(e) {
            var tag = document.activeElement.tagName;
            if (tag === 'INPUT' || tag === 'TEXTAREA' || tag === 'SELECT') return;

            if (e.key === 'ArrowLeft') {
                var prevLink = document.querySelector('.nav-prev');
                if (prevLink) prevLink.click();
            } else if (e.key === 'ArrowRight') {
                var nextLink = document.querySelector('.nav-next');
                if (nextLink) nextLink.click();
            }
        });
    });
</script>

</body>
</html>
{% endraw %}