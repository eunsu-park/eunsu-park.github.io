{% raw %}
<!DOCTYPE html>
<html lang="en" data-theme="light">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>LLaMA Family - Study Materials</title>
    <link rel="stylesheet" href="/study/static/css/style.css">
    <link rel="stylesheet" href="/study/static/css/highlight.css">
    <!-- KaTeX for LaTeX math rendering -->
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.11/dist/katex.min.css">
    <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.11/dist/katex.min.js"></script>
    <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.11/dist/contrib/auto-render.min.js"
        onload="renderMathInElement(document.body, {
            delimiters: [
                {left: '$$', right: '$$', display: true},
                {left: '$', right: '$', display: false}
            ],
            throwOnError: false
        });"></script>
    
</head>
<body>
    <div class="app-container">
        <!-- Sidebar -->
        <aside class="sidebar" id="sidebar">
            <div class="sidebar-header">
                <a href="/study/en/" class="logo">Study Materials</a>
                <button class="sidebar-toggle" id="sidebar-toggle" aria-label="Toggle sidebar">
                    <span></span>
                </button>
            </div>

            <nav class="sidebar-nav">
                <a href="/study/en/" class="nav-item ">
                    <span class="nav-icon">ğŸ </span>
                    <span class="nav-text">Home</span>
                </a>
                <a href="/study/examples/" class="nav-item ">
                    <span class="nav-icon">ğŸ’»</span>
                    <span class="nav-text">Examples</span>
                </a>
            </nav>

            <div class="sidebar-search">
                <form action="/study/en/search.html" method="get" id="search-form">
                    <input type="search" name="q" placeholder="Search..." id="search-sidebar-input">
                </form>
            </div>

            <!-- Language Selector -->
            <div class="sidebar-lang">
                <select id="lang-select" class="lang-selector" onchange="switchLanguage(this.value)">
                    
                    <option value="en" selected>
                        English
                    </option>
                    
                    <option value="ko" >
                        í•œêµ­ì–´
                    </option>
                    
                </select>
            </div>

            <div class="sidebar-footer">
                <button class="theme-toggle" id="theme-toggle" aria-label="Toggle theme">
                    <span class="theme-icon light">â˜€ï¸</span>
                    <span class="theme-icon dark">ğŸŒ™</span>
                </button>
            </div>
        </aside>

        <!-- Main Content -->
        <main class="main-content">
            <header class="main-header">
                <button class="menu-toggle" id="menu-toggle" aria-label="Open menu">
                    <span></span>
                </button>
                
<nav class="breadcrumb">
    <a href="/study/en/">Topics</a>
    <span class="separator">/</span>
    <a href="/study/en/Foundation_Models/">Foundation Models</a>
    <span class="separator">/</span>
    <span class="current">LLaMA Family</span>
</nav>

            </header>

            <div class="content">
                
<article class="lesson-article">
    <header class="lesson-header">
        <h1>LLaMA Family</h1>
    </header>

    
<div class="lesson-toolbar lesson-toolbar--top">
    <div class="toolbar-nav toolbar-nav--prev">
        
        <a href="/study/en/Foundation_Models/07_Tokenization_Deep_Dive.html" class="nav-prev">
            <span class="nav-label">Previous</span>
            <span class="nav-title">07. Tokenization Deep Dive</span>
        </a>
        
    </div>
    <div class="toolbar-actions">
        <button class="btn btn-copy-link" data-action="copy-link" title="Copy link">
            <span class="icon">ğŸ”—</span>
            <span class="text">Copy link</span>
        </button>
        <a href="/study/en/Foundation_Models/" class="btn btn-topic-link" title="Back to topic list">
            <span class="icon">ğŸ“‹</span>
            <span class="text">Topic list</span>
        </a>
    </div>
    <div class="toolbar-nav toolbar-nav--next">
        
        <a href="/study/en/Foundation_Models/09_Mistral_MoE.html" class="nav-next">
            <span class="nav-label">Next</span>
            <span class="nav-title">Mistral & Mixture of Experts</span>
        </a>
        
    </div>
</div>


    
    <nav class="toc" id="toc">
        <h2>Table of Contents</h2>
        <div class="toc">
<ul>
<li><a href="#learning-objectives">Learning Objectives</a></li>
<li><a href="#1-llama-overview">1. LLaMA Overview</a><ul>
<li><a href="#11-significance-of-llama">1.1 Significance of LLaMA</a></li>
<li><a href="#12-version-comparison">1.2 Version Comparison</a></li>
</ul>
</li>
<li><a href="#2-llama-architecture">2. LLaMA Architecture</a><ul>
<li><a href="#21-core-components">2.1 Core Components</a></li>
<li><a href="#22-hyperparameters">2.2 Hyperparameters</a></li>
</ul>
</li>
<li><a href="#3-rope-rotary-position-embedding">3. RoPE (Rotary Position Embedding)</a><ul>
<li><a href="#31-concept">3.1 Concept</a></li>
<li><a href="#32-mathematical-understanding">3.2 Mathematical Understanding</a></li>
<li><a href="#33-advantages-of-rope">3.3 Advantages of RoPE</a></li>
</ul>
</li>
<li><a href="#4-rmsnorm">4. RMSNorm</a><ul>
<li><a href="#41-concept">4.1 Concept</a></li>
<li><a href="#42-implementation">4.2 Implementation</a></li>
</ul>
</li>
<li><a href="#5-swiglu">5. SwiGLU</a><ul>
<li><a href="#51-concept">5.1 Concept</a></li>
<li><a href="#52-implementation">5.2 Implementation</a></li>
</ul>
</li>
<li><a href="#6-grouped-query-attention-gqa">6. Grouped Query Attention (GQA)</a><ul>
<li><a href="#61-concept">6.1 Concept</a></li>
<li><a href="#62-implementation">6.2 Implementation</a></li>
</ul>
</li>
<li><a href="#7-llama-practice">7. LLaMA Practice</a><ul>
<li><a href="#71-using-with-huggingface">7.1 Using with HuggingFace</a></li>
<li><a href="#72-efficient-usage-with-quantization">7.2 Efficient Usage with Quantization</a></li>
<li><a href="#73-using-llama-3">7.3 Using LLaMA 3</a></li>
</ul>
</li>
<li><a href="#8-llama-3132-details">8. LLaMA 3.1/3.2 Details</a><ul>
<li><a href="#81-llama-31-july-2024">8.1 LLaMA 3.1 (July 2024)</a></li>
<li><a href="#82-llama-32-september-2024">8.2 LLaMA 3.2 (September 2024)</a></li>
</ul>
</li>
<li><a href="#summary">Summary</a><ul>
<li><a href="#llama-core-technologies">LLaMA Core Technologies</a></li>
<li><a href="#practical-recommendations">Practical Recommendations</a></li>
<li><a href="#next-steps">Next Steps</a></li>
</ul>
</li>
<li><a href="#references">References</a><ul>
<li><a href="#core-papers">Core Papers</a></li>
<li><a href="#code-resources">Code &amp; Resources</a></li>
</ul>
</li>
</ul>
</div>

    </nav>
    

    <div class="lesson-content markdown-body">
        <h1 id="llama-family">LLaMA Family<a class="header-link" href="#llama-family" title="Permanent link">&para;</a></h1>
<h2 id="learning-objectives">Learning Objectives<a class="header-link" href="#learning-objectives" title="Permanent link">&para;</a></h2>
<ul>
<li>Understand the architectural evolution of LLaMA 1/2/3</li>
<li>Master core technologies: RoPE, RMSNorm, SwiGLU</li>
<li>Grasp the Grouped Query Attention (GQA) mechanism</li>
<li>Learn practical LLaMA usage methods</li>
</ul>
<hr />
<h2 id="1-llama-overview">1. LLaMA Overview<a class="header-link" href="#1-llama-overview" title="Permanent link">&para;</a></h2>
<h3 id="11-significance-of-llama">1.1 Significance of LLaMA<a class="header-link" href="#11-significance-of-llama" title="Permanent link">&para;</a></h3>
<p><strong>LLaMA</strong> (Large Language Model Meta AI) is an open-source LLM released by Meta in 2023, leading the democratization of Foundation Model research.</p>
<div class="highlight"><pre><span></span><code><span class="err">â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”</span>
<span class="err">â”‚</span><span class="w">                    </span><span class="nx">Historical</span><span class="w"> </span><span class="nx">Significance</span><span class="w"> </span><span class="nx">of</span><span class="w"> </span><span class="nx">LLaMA</span><span class="w">              </span><span class="err">â”‚</span>
<span class="err">â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤</span>
<span class="err">â”‚</span><span class="w">                                                                 </span><span class="err">â”‚</span>
<span class="err">â”‚</span><span class="w">  </span><span class="nx">Before</span><span class="w"> </span><span class="nx">LLaMA</span><span class="w"> </span><span class="p">(</span><span class="mi">2022</span><span class="p">):</span><span class="w">                                           </span><span class="err">â”‚</span>
<span class="err">â”‚</span><span class="w">  </span><span class="err">â€¢</span><span class="w"> </span><span class="nx">Best</span><span class="o">-</span><span class="nx">performing</span><span class="w"> </span><span class="nx">models</span><span class="w"> </span><span class="nx">only</span><span class="w"> </span><span class="nx">available</span><span class="w"> </span><span class="nx">via</span><span class="w"> </span><span class="nx">API</span><span class="w"> </span><span class="p">(</span><span class="nx">GPT</span><span class="o">-</span><span class="m m-Double">3.5</span><span class="p">,</span><span class="w"> </span><span class="nx">PaLM</span><span class="p">)</span><span class="err">â”‚</span>
<span class="err">â”‚</span><span class="w">  </span><span class="err">â€¢</span><span class="w"> </span><span class="nx">Academic</span><span class="w"> </span><span class="nx">research</span><span class="w"> </span><span class="nx">models</span><span class="w"> </span><span class="nx">lacked</span><span class="w"> </span><span class="nx">performance</span><span class="w">                  </span><span class="err">â”‚</span>
<span class="err">â”‚</span><span class="w">  </span><span class="err">â€¢</span><span class="w"> </span><span class="nx">Limited</span><span class="w"> </span><span class="nx">LLM</span><span class="w"> </span><span class="nx">access</span><span class="w"> </span><span class="k">for</span><span class="w"> </span><span class="nx">open</span><span class="o">-</span><span class="nx">source</span><span class="w"> </span><span class="nx">community</span><span class="w">                 </span><span class="err">â”‚</span>
<span class="err">â”‚</span><span class="w">                                                                 </span><span class="err">â”‚</span>
<span class="err">â”‚</span><span class="w">  </span><span class="nx">After</span><span class="w"> </span><span class="nx">LLaMA</span><span class="w"> </span><span class="p">(</span><span class="mi">2023</span><span class="p">):</span><span class="w">                                            </span><span class="err">â”‚</span>
<span class="err">â”‚</span><span class="w">  </span><span class="err">â€¢</span><span class="w"> </span><span class="nx">Researchers</span><span class="w"> </span><span class="nx">can</span><span class="w"> </span><span class="nx">directly</span><span class="w"> </span><span class="nx">experiment</span><span class="w"> </span><span class="nx">with</span><span class="w"> </span><span class="nx">state</span><span class="o">-</span><span class="nx">of</span><span class="o">-</span><span class="nx">the</span><span class="o">-</span><span class="nx">art</span><span class="w">    </span><span class="err">â”‚</span>
<span class="err">â”‚</span><span class="w">  </span><span class="err">â€¢</span><span class="w"> </span><span class="nx">Explosive</span><span class="w"> </span><span class="nx">growth</span><span class="w"> </span><span class="nx">of</span><span class="w"> </span><span class="nx">derivative</span><span class="w"> </span><span class="nx">models</span><span class="w"> </span><span class="p">(</span><span class="nx">Alpaca</span><span class="p">,</span><span class="w"> </span><span class="nx">Vicuna</span><span class="p">,</span><span class="w"> </span><span class="nx">etc</span><span class="p">.)</span><span class="w"> </span><span class="err">â”‚</span>
<span class="err">â”‚</span><span class="w">  </span><span class="err">â€¢</span><span class="w"> </span><span class="nx">Rapid</span><span class="w"> </span><span class="nx">acceleration</span><span class="w"> </span><span class="nx">of</span><span class="w"> </span><span class="nx">LLM</span><span class="w"> </span><span class="nx">research</span><span class="w">                           </span><span class="err">â”‚</span>
<span class="err">â”‚</span><span class="w">                                                                 </span><span class="err">â”‚</span>
<span class="err">â”‚</span><span class="w">  </span><span class="nx">Key</span><span class="w"> </span><span class="nx">Contributions</span><span class="p">:</span><span class="w">                                              </span><span class="err">â”‚</span>
<span class="err">â”‚</span><span class="w">  </span><span class="err">â€¢</span><span class="w"> </span><span class="nx">Applied</span><span class="w"> </span><span class="nx">Chinchilla</span><span class="w"> </span><span class="nx">rules</span><span class="w"> </span><span class="p">(</span><span class="nx">D</span><span class="p">=</span><span class="mi">20</span><span class="nx">N</span><span class="w"> </span><span class="k">or</span><span class="w"> </span><span class="nx">more</span><span class="p">)</span><span class="w">                     </span><span class="err">â”‚</span>
<span class="err">â”‚</span><span class="w">  </span><span class="err">â€¢</span><span class="w"> </span><span class="nx">Validated</span><span class="w"> </span><span class="nx">efficient</span><span class="w"> </span><span class="nx">architecture</span><span class="w"> </span><span class="nx">choices</span><span class="w">                      </span><span class="err">â”‚</span>
<span class="err">â”‚</span><span class="w">  </span><span class="err">â€¢</span><span class="w"> </span><span class="nx">Published</span><span class="w"> </span><span class="nx">training</span><span class="w"> </span><span class="nx">data</span><span class="w"> </span><span class="nx">composition</span><span class="w">                           </span><span class="err">â”‚</span>
<span class="err">â”‚</span><span class="w">                                                                 </span><span class="err">â”‚</span>
<span class="err">â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜</span>
</code></pre></div>

<h3 id="12-version-comparison">1.2 Version Comparison<a class="header-link" href="#12-version-comparison" title="Permanent link">&para;</a></h3>
<table>
<thead>
<tr>
<th>Feature</th>
<th>LLaMA 1</th>
<th>LLaMA 2</th>
<th>LLaMA 3</th>
<th>LLaMA 3.1</th>
<th>LLaMA 3.2</th>
</tr>
</thead>
<tbody>
<tr>
<td>Release</td>
<td>2023.02</td>
<td>2023.07</td>
<td>2024.04</td>
<td>2024.07</td>
<td>2024.09</td>
</tr>
<tr>
<td>Sizes</td>
<td>7/13/33/65B</td>
<td>7/13/70B</td>
<td>8/70B</td>
<td>8/70/405B</td>
<td>1/3/11/90B</td>
</tr>
<tr>
<td>Tokens</td>
<td>1.4T</td>
<td>2T</td>
<td>15T+</td>
<td>15T+</td>
<td>15T+</td>
</tr>
<tr>
<td>Context</td>
<td>2K</td>
<td>4K</td>
<td>8K</td>
<td>128K</td>
<td>128K</td>
</tr>
<tr>
<td>License</td>
<td>Research</td>
<td>Commercial (conditional)</td>
<td>Commercial (relaxed)</td>
<td>Commercial (relaxed)</td>
<td>Commercial (relaxed)</td>
</tr>
<tr>
<td>GQA</td>
<td>âŒ</td>
<td>âœ… (70B)</td>
<td>âœ… (all)</td>
<td>âœ… (all)</td>
<td>âœ… (all)</td>
</tr>
<tr>
<td>Features</td>
<td>Base architecture</td>
<td>RLHF, Safety</td>
<td>Improved reasoning</td>
<td>128K native, Tool Use</td>
<td>Vision models added</td>
</tr>
</tbody>
</table>
<blockquote>
<p><strong>LLaMA 3.1/3.2 Major Updates</strong> (2024):
- <strong>LLaMA 3.1</strong>: 128K native context, 405B flagship model, Tool Use capability
- <strong>LLaMA 3.2</strong>: Lightweight models (1B/3B) and Vision models (11B/90B) added</p>
</blockquote>
<hr />
<h2 id="2-llama-architecture">2. LLaMA Architecture<a class="header-link" href="#2-llama-architecture" title="Permanent link">&para;</a></h2>
<h3 id="21-core-components">2.1 Core Components<a class="header-link" href="#21-core-components" title="Permanent link">&para;</a></h3>
<div class="highlight"><pre><span></span><code>â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    LLaMA Architecture                            â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                 â”‚
â”‚  Input Tokens                                                   â”‚
â”‚       â”‚                                                         â”‚
â”‚       â–¼                                                         â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                    â”‚
â”‚  â”‚         Token Embedding                 â”‚                    â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                    â”‚
â”‚       â”‚                                                         â”‚
â”‚       â–¼                                                         â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                    â”‚
â”‚  â”‚     RoPE (Rotary Position Embedding)    â”‚  â† Position info   â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                    â”‚
â”‚       â”‚                                                         â”‚
â”‚  â”Œâ”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                    â”‚
â”‚  â”‚         Transformer Block Ã— N           â”‚                    â”‚
â”‚  â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚                    â”‚
â”‚  â”‚  â”‚  RMSNorm (Pre-normalization)      â”‚  â”‚  â† Replaces LN     â”‚
â”‚  â”‚  â”‚            â†“                      â”‚  â”‚                    â”‚
â”‚  â”‚  â”‚  Grouped Query Attention (GQA)    â”‚  â”‚  â† KV Cache eff.   â”‚
â”‚  â”‚  â”‚            â†“                      â”‚  â”‚                    â”‚
â”‚  â”‚  â”‚  Residual Connection              â”‚  â”‚                    â”‚
â”‚  â”‚  â”‚            â†“                      â”‚  â”‚                    â”‚
â”‚  â”‚  â”‚  RMSNorm                          â”‚  â”‚                    â”‚
â”‚  â”‚  â”‚            â†“                      â”‚  â”‚                    â”‚
â”‚  â”‚  â”‚  SwiGLU FFN                       â”‚  â”‚  â† Replaces GELU   â”‚
â”‚  â”‚  â”‚            â†“                      â”‚  â”‚                    â”‚
â”‚  â”‚  â”‚  Residual Connection              â”‚  â”‚                    â”‚
â”‚  â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚                    â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                    â”‚
â”‚       â”‚                                                         â”‚
â”‚       â–¼                                                         â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                    â”‚
â”‚  â”‚         RMSNorm â†’ Linear â†’ Vocab        â”‚                    â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                    â”‚
â”‚       â”‚                                                         â”‚
â”‚       â–¼                                                         â”‚
â”‚  Output Logits                                                  â”‚
â”‚                                                                 â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
</code></pre></div>

<h3 id="22-hyperparameters">2.2 Hyperparameters<a class="header-link" href="#22-hyperparameters" title="Permanent link">&para;</a></h3>
<div class="highlight"><pre><span></span><code><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">LLaMA Model Specification Comparison</span>
<span class="sd">&quot;&quot;&quot;</span>

<span class="n">LLAMA_CONFIGS</span> <span class="o">=</span> <span class="p">{</span>
    <span class="s2">&quot;llama-7b&quot;</span><span class="p">:</span> <span class="p">{</span>
        <span class="s2">&quot;dim&quot;</span><span class="p">:</span> <span class="mi">4096</span><span class="p">,</span>
        <span class="s2">&quot;n_layers&quot;</span><span class="p">:</span> <span class="mi">32</span><span class="p">,</span>
        <span class="s2">&quot;n_heads&quot;</span><span class="p">:</span> <span class="mi">32</span><span class="p">,</span>
        <span class="s2">&quot;n_kv_heads&quot;</span><span class="p">:</span> <span class="mi">32</span><span class="p">,</span>  <span class="c1"># MHA (no GQA)</span>
        <span class="s2">&quot;vocab_size&quot;</span><span class="p">:</span> <span class="mi">32000</span><span class="p">,</span>
        <span class="s2">&quot;ffn_dim&quot;</span><span class="p">:</span> <span class="mi">11008</span><span class="p">,</span>  <span class="c1"># approx 2.67 Ã— dim</span>
        <span class="s2">&quot;context_length&quot;</span><span class="p">:</span> <span class="mi">2048</span><span class="p">,</span>
    <span class="p">},</span>
    <span class="s2">&quot;llama-13b&quot;</span><span class="p">:</span> <span class="p">{</span>
        <span class="s2">&quot;dim&quot;</span><span class="p">:</span> <span class="mi">5120</span><span class="p">,</span>
        <span class="s2">&quot;n_layers&quot;</span><span class="p">:</span> <span class="mi">40</span><span class="p">,</span>
        <span class="s2">&quot;n_heads&quot;</span><span class="p">:</span> <span class="mi">40</span><span class="p">,</span>
        <span class="s2">&quot;n_kv_heads&quot;</span><span class="p">:</span> <span class="mi">40</span><span class="p">,</span>
        <span class="s2">&quot;vocab_size&quot;</span><span class="p">:</span> <span class="mi">32000</span><span class="p">,</span>
        <span class="s2">&quot;ffn_dim&quot;</span><span class="p">:</span> <span class="mi">13824</span><span class="p">,</span>
        <span class="s2">&quot;context_length&quot;</span><span class="p">:</span> <span class="mi">2048</span><span class="p">,</span>
    <span class="p">},</span>
    <span class="s2">&quot;llama-70b&quot;</span><span class="p">:</span> <span class="p">{</span>
        <span class="s2">&quot;dim&quot;</span><span class="p">:</span> <span class="mi">8192</span><span class="p">,</span>
        <span class="s2">&quot;n_layers&quot;</span><span class="p">:</span> <span class="mi">80</span><span class="p">,</span>
        <span class="s2">&quot;n_heads&quot;</span><span class="p">:</span> <span class="mi">64</span><span class="p">,</span>
        <span class="s2">&quot;n_kv_heads&quot;</span><span class="p">:</span> <span class="mi">8</span><span class="p">,</span>  <span class="c1"># GQA! 8 KV heads</span>
        <span class="s2">&quot;vocab_size&quot;</span><span class="p">:</span> <span class="mi">32000</span><span class="p">,</span>
        <span class="s2">&quot;ffn_dim&quot;</span><span class="p">:</span> <span class="mi">28672</span><span class="p">,</span>
        <span class="s2">&quot;context_length&quot;</span><span class="p">:</span> <span class="mi">4096</span><span class="p">,</span>
    <span class="p">},</span>
    <span class="s2">&quot;llama3-8b&quot;</span><span class="p">:</span> <span class="p">{</span>
        <span class="s2">&quot;dim&quot;</span><span class="p">:</span> <span class="mi">4096</span><span class="p">,</span>
        <span class="s2">&quot;n_layers&quot;</span><span class="p">:</span> <span class="mi">32</span><span class="p">,</span>
        <span class="s2">&quot;n_heads&quot;</span><span class="p">:</span> <span class="mi">32</span><span class="p">,</span>
        <span class="s2">&quot;n_kv_heads&quot;</span><span class="p">:</span> <span class="mi">8</span><span class="p">,</span>  <span class="c1"># GQA</span>
        <span class="s2">&quot;vocab_size&quot;</span><span class="p">:</span> <span class="mi">128256</span><span class="p">,</span>  <span class="c1"># Extended vocab</span>
        <span class="s2">&quot;ffn_dim&quot;</span><span class="p">:</span> <span class="mi">14336</span><span class="p">,</span>
        <span class="s2">&quot;context_length&quot;</span><span class="p">:</span> <span class="mi">8192</span><span class="p">,</span>
    <span class="p">},</span>
    <span class="s2">&quot;llama3-70b&quot;</span><span class="p">:</span> <span class="p">{</span>
        <span class="s2">&quot;dim&quot;</span><span class="p">:</span> <span class="mi">8192</span><span class="p">,</span>
        <span class="s2">&quot;n_layers&quot;</span><span class="p">:</span> <span class="mi">80</span><span class="p">,</span>
        <span class="s2">&quot;n_heads&quot;</span><span class="p">:</span> <span class="mi">64</span><span class="p">,</span>
        <span class="s2">&quot;n_kv_heads&quot;</span><span class="p">:</span> <span class="mi">8</span><span class="p">,</span>  <span class="c1"># GQA</span>
        <span class="s2">&quot;vocab_size&quot;</span><span class="p">:</span> <span class="mi">128256</span><span class="p">,</span>
        <span class="s2">&quot;ffn_dim&quot;</span><span class="p">:</span> <span class="mi">28672</span><span class="p">,</span>
        <span class="s2">&quot;context_length&quot;</span><span class="p">:</span> <span class="mi">8192</span><span class="p">,</span>
    <span class="p">},</span>
    <span class="c1"># LLaMA 3.1 (2024.07)</span>
    <span class="s2">&quot;llama3.1-8b&quot;</span><span class="p">:</span> <span class="p">{</span>
        <span class="s2">&quot;dim&quot;</span><span class="p">:</span> <span class="mi">4096</span><span class="p">,</span>
        <span class="s2">&quot;n_layers&quot;</span><span class="p">:</span> <span class="mi">32</span><span class="p">,</span>
        <span class="s2">&quot;n_heads&quot;</span><span class="p">:</span> <span class="mi">32</span><span class="p">,</span>
        <span class="s2">&quot;n_kv_heads&quot;</span><span class="p">:</span> <span class="mi">8</span><span class="p">,</span>
        <span class="s2">&quot;vocab_size&quot;</span><span class="p">:</span> <span class="mi">128256</span><span class="p">,</span>
        <span class="s2">&quot;ffn_dim&quot;</span><span class="p">:</span> <span class="mi">14336</span><span class="p">,</span>
        <span class="s2">&quot;context_length&quot;</span><span class="p">:</span> <span class="mi">131072</span><span class="p">,</span>  <span class="c1"># 128K native</span>
    <span class="p">},</span>
    <span class="s2">&quot;llama3.1-405b&quot;</span><span class="p">:</span> <span class="p">{</span>
        <span class="s2">&quot;dim&quot;</span><span class="p">:</span> <span class="mi">16384</span><span class="p">,</span>
        <span class="s2">&quot;n_layers&quot;</span><span class="p">:</span> <span class="mi">126</span><span class="p">,</span>
        <span class="s2">&quot;n_heads&quot;</span><span class="p">:</span> <span class="mi">128</span><span class="p">,</span>
        <span class="s2">&quot;n_kv_heads&quot;</span><span class="p">:</span> <span class="mi">8</span><span class="p">,</span>
        <span class="s2">&quot;vocab_size&quot;</span><span class="p">:</span> <span class="mi">128256</span><span class="p">,</span>
        <span class="s2">&quot;ffn_dim&quot;</span><span class="p">:</span> <span class="mi">53248</span><span class="p">,</span>
        <span class="s2">&quot;context_length&quot;</span><span class="p">:</span> <span class="mi">131072</span><span class="p">,</span>  <span class="c1"># 128K native</span>
    <span class="p">},</span>
    <span class="c1"># LLaMA 3.2 (2024.09) - Lightweight text models</span>
    <span class="s2">&quot;llama3.2-1b&quot;</span><span class="p">:</span> <span class="p">{</span>
        <span class="s2">&quot;dim&quot;</span><span class="p">:</span> <span class="mi">2048</span><span class="p">,</span>
        <span class="s2">&quot;n_layers&quot;</span><span class="p">:</span> <span class="mi">16</span><span class="p">,</span>
        <span class="s2">&quot;n_heads&quot;</span><span class="p">:</span> <span class="mi">32</span><span class="p">,</span>
        <span class="s2">&quot;n_kv_heads&quot;</span><span class="p">:</span> <span class="mi">8</span><span class="p">,</span>
        <span class="s2">&quot;vocab_size&quot;</span><span class="p">:</span> <span class="mi">128256</span><span class="p">,</span>
        <span class="s2">&quot;ffn_dim&quot;</span><span class="p">:</span> <span class="mi">8192</span><span class="p">,</span>
        <span class="s2">&quot;context_length&quot;</span><span class="p">:</span> <span class="mi">131072</span><span class="p">,</span>
    <span class="p">},</span>
    <span class="s2">&quot;llama3.2-3b&quot;</span><span class="p">:</span> <span class="p">{</span>
        <span class="s2">&quot;dim&quot;</span><span class="p">:</span> <span class="mi">3072</span><span class="p">,</span>
        <span class="s2">&quot;n_layers&quot;</span><span class="p">:</span> <span class="mi">28</span><span class="p">,</span>
        <span class="s2">&quot;n_heads&quot;</span><span class="p">:</span> <span class="mi">24</span><span class="p">,</span>
        <span class="s2">&quot;n_kv_heads&quot;</span><span class="p">:</span> <span class="mi">8</span><span class="p">,</span>
        <span class="s2">&quot;vocab_size&quot;</span><span class="p">:</span> <span class="mi">128256</span><span class="p">,</span>
        <span class="s2">&quot;ffn_dim&quot;</span><span class="p">:</span> <span class="mi">8192</span><span class="p">,</span>
        <span class="s2">&quot;context_length&quot;</span><span class="p">:</span> <span class="mi">131072</span><span class="p">,</span>
    <span class="p">},</span>
<span class="p">}</span>
</code></pre></div>

<hr />
<h2 id="3-rope-rotary-position-embedding">3. RoPE (Rotary Position Embedding)<a class="header-link" href="#3-rope-rotary-position-embedding" title="Permanent link">&para;</a></h2>
<h3 id="31-concept">3.1 Concept<a class="header-link" href="#31-concept" title="Permanent link">&para;</a></h3>
<p><strong>RoPE</strong> is a method that encodes positional information using rotation matrices.</p>
<div class="highlight"><pre><span></span><code>â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    Position Encoding Comparison                  â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                 â”‚
â”‚  1. Sinusoidal (Original Transformer)                           â”‚
â”‚     PE(pos, 2i) = sin(pos / 10000^(2i/d))                       â”‚
â”‚     PE(pos, 2i+1) = cos(pos / 10000^(2i/d))                     â”‚
â”‚     â†’ Added to input (additive)                                 â”‚
â”‚     â†’ Weak relative position information                        â”‚
â”‚                                                                 â”‚
â”‚  2. Learned (BERT, GPT)                                         â”‚
â”‚     PE = Embedding(position)                                    â”‚
â”‚     â†’ Learned vectors                                           â”‚
â”‚     â†’ Difficult to generalize beyond trained lengths            â”‚
â”‚                                                                 â”‚
â”‚  3. RoPE (LLaMA)                                                â”‚
â”‚     R(Î¸) = rotation matrix, Î¸ = f(position)                     â”‚
â”‚     q&#39; = R(Î¸_m) Ã— q, k&#39; = R(Î¸_n) Ã— k                           â”‚
â”‚     q&#39; Â· k&#39; = q Â· k Ã— cos(Î¸_m - Î¸_n)                           â”‚
â”‚     â†’ Naturally encodes relative positions                      â”‚
â”‚     â†’ Length extrapolation possible (with modifications)        â”‚
â”‚                                                                 â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
</code></pre></div>

<h3 id="32-mathematical-understanding">3.2 Mathematical Understanding<a class="header-link" href="#32-mathematical-understanding" title="Permanent link">&para;</a></h3>
<div class="highlight"><pre><span></span><code><span class="kn">import</span><span class="w"> </span><span class="nn">torch</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">math</span>

<span class="k">def</span><span class="w"> </span><span class="nf">precompute_freqs_cis</span><span class="p">(</span><span class="n">dim</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span> <span class="n">seq_len</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span> <span class="n">theta</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="mf">10000.0</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Precompute complex frequencies for RoPE</span>

<span class="sd">    Args:</span>
<span class="sd">        dim: Embedding dimension (head_dim)</span>
<span class="sd">        seq_len: Maximum sequence length</span>
<span class="sd">        theta: Base frequency (10000)</span>

<span class="sd">    Returns:</span>
<span class="sd">        freqs_cis: (seq_len, dim//2) complex tensor</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="c1"># Frequency calculation: Î¸_i = 1 / (theta^(2i/d))</span>
    <span class="n">freqs</span> <span class="o">=</span> <span class="mf">1.0</span> <span class="o">/</span> <span class="p">(</span><span class="n">theta</span> <span class="o">**</span> <span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">dim</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span><span class="o">.</span><span class="n">float</span><span class="p">()</span> <span class="o">/</span> <span class="n">dim</span><span class="p">))</span>
    <span class="c1"># Angle per position: m * Î¸_i</span>
    <span class="n">t</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="n">seq_len</span><span class="p">)</span>
    <span class="n">freqs</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">outer</span><span class="p">(</span><span class="n">t</span><span class="p">,</span> <span class="n">freqs</span><span class="p">)</span>  <span class="c1"># (seq_len, dim//2)</span>
    <span class="c1"># Complex form: e^(i*Î¸) = cos(Î¸) + i*sin(Î¸)</span>
    <span class="n">freqs_cis</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">polar</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">ones_like</span><span class="p">(</span><span class="n">freqs</span><span class="p">),</span> <span class="n">freqs</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">freqs_cis</span>

<span class="k">def</span><span class="w"> </span><span class="nf">apply_rotary_emb</span><span class="p">(</span><span class="n">xq</span><span class="p">,</span> <span class="n">xk</span><span class="p">,</span> <span class="n">freqs_cis</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Apply RoPE to Query and Key</span>

<span class="sd">    Args:</span>
<span class="sd">        xq: Query (batch, seq_len, n_heads, head_dim)</span>
<span class="sd">        xk: Key (batch, seq_len, n_kv_heads, head_dim)</span>
<span class="sd">        freqs_cis: Precomputed complex frequencies</span>

<span class="sd">    Returns:</span>
<span class="sd">        Rotated Query and Key</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="c1"># Convert real to complex (pair adjacent elements)</span>
    <span class="n">xq_</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">view_as_complex</span><span class="p">(</span><span class="n">xq</span><span class="o">.</span><span class="n">float</span><span class="p">()</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="o">*</span><span class="n">xq</span><span class="o">.</span><span class="n">shape</span><span class="p">[:</span><span class="o">-</span><span class="mi">1</span><span class="p">],</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">))</span>
    <span class="n">xk_</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">view_as_complex</span><span class="p">(</span><span class="n">xk</span><span class="o">.</span><span class="n">float</span><span class="p">()</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="o">*</span><span class="n">xk</span><span class="o">.</span><span class="n">shape</span><span class="p">[:</span><span class="o">-</span><span class="mi">1</span><span class="p">],</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">))</span>

    <span class="c1"># Apply rotation (complex multiplication)</span>
    <span class="n">freqs_cis</span> <span class="o">=</span> <span class="n">freqs_cis</span><span class="o">.</span><span class="n">unsqueeze</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span><span class="o">.</span><span class="n">unsqueeze</span><span class="p">(</span><span class="mi">2</span><span class="p">)</span>  <span class="c1"># (1, seq, 1, dim//2)</span>
    <span class="n">xq_out</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">view_as_real</span><span class="p">(</span><span class="n">xq_</span> <span class="o">*</span> <span class="n">freqs_cis</span><span class="p">)</span><span class="o">.</span><span class="n">flatten</span><span class="p">(</span><span class="mi">3</span><span class="p">)</span>
    <span class="n">xk_out</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">view_as_real</span><span class="p">(</span><span class="n">xk_</span> <span class="o">*</span> <span class="n">freqs_cis</span><span class="p">)</span><span class="o">.</span><span class="n">flatten</span><span class="p">(</span><span class="mi">3</span><span class="p">)</span>

    <span class="k">return</span> <span class="n">xq_out</span><span class="o">.</span><span class="n">type_as</span><span class="p">(</span><span class="n">xq</span><span class="p">),</span> <span class="n">xk_out</span><span class="o">.</span><span class="n">type_as</span><span class="p">(</span><span class="n">xk</span><span class="p">)</span>

<span class="c1"># Example</span>
<span class="n">batch</span><span class="p">,</span> <span class="n">seq_len</span><span class="p">,</span> <span class="n">n_heads</span><span class="p">,</span> <span class="n">head_dim</span> <span class="o">=</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">128</span><span class="p">,</span> <span class="mi">32</span><span class="p">,</span> <span class="mi">128</span>
<span class="n">xq</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="n">batch</span><span class="p">,</span> <span class="n">seq_len</span><span class="p">,</span> <span class="n">n_heads</span><span class="p">,</span> <span class="n">head_dim</span><span class="p">)</span>
<span class="n">xk</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="n">batch</span><span class="p">,</span> <span class="n">seq_len</span><span class="p">,</span> <span class="n">n_heads</span><span class="p">,</span> <span class="n">head_dim</span><span class="p">)</span>
<span class="n">freqs_cis</span> <span class="o">=</span> <span class="n">precompute_freqs_cis</span><span class="p">(</span><span class="n">head_dim</span><span class="p">,</span> <span class="n">seq_len</span><span class="p">)</span>

<span class="n">xq_rope</span><span class="p">,</span> <span class="n">xk_rope</span> <span class="o">=</span> <span class="n">apply_rotary_emb</span><span class="p">(</span><span class="n">xq</span><span class="p">,</span> <span class="n">xk</span><span class="p">,</span> <span class="n">freqs_cis</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Output shape: </span><span class="si">{</span><span class="n">xq_rope</span><span class="o">.</span><span class="n">shape</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>  <span class="c1"># (2, 128, 32, 128)</span>
</code></pre></div>

<h3 id="33-advantages-of-rope">3.3 Advantages of RoPE<a class="header-link" href="#33-advantages-of-rope" title="Permanent link">&para;</a></h3>
<div class="highlight"><pre><span></span><code><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">Advantages of RoPE:</span>

<span class="sd">1. Natural relative position encoding</span>
<span class="sd">   - q_m Â· k_n âˆ cos(Î¸_m - Î¸_n)</span>
<span class="sd">   - Depends on relative distance, not absolute position</span>

<span class="sd">2. Extrapolation capability</span>
<span class="sd">   - Can extend beyond trained lengths</span>
<span class="sd">   - (Performance degrades â†’ improved with NTK, YaRN)</span>

<span class="sd">3. Efficiency</span>
<span class="sd">   - No additional parameters</span>
<span class="sd">   - Fast element-wise operations</span>

<span class="sd">4. Compatible with linear Self-attention</span>
<span class="sd">   - Can combine with some efficient attention methods</span>
<span class="sd">&quot;&quot;&quot;</span>
</code></pre></div>

<hr />
<h2 id="4-rmsnorm">4. RMSNorm<a class="header-link" href="#4-rmsnorm" title="Permanent link">&para;</a></h2>
<h3 id="41-concept">4.1 Concept<a class="header-link" href="#41-concept" title="Permanent link">&para;</a></h3>
<p><strong>RMSNorm</strong> is a simplified version of LayerNorm that removes mean computation.</p>
<div class="highlight"><pre><span></span><code><span class="err">â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”</span>
<span class="err">â”‚</span><span class="w">                    </span><span class="n">LayerNorm</span><span class="w"> </span><span class="n">vs</span><span class="w"> </span><span class="n">RMSNorm</span><span class="w">                          </span><span class="err">â”‚</span>
<span class="err">â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤</span>
<span class="err">â”‚</span><span class="w">                                                                 </span><span class="err">â”‚</span>
<span class="err">â”‚</span><span class="w">  </span><span class="n">LayerNorm</span><span class="p">:</span><span class="w">                                                     </span><span class="err">â”‚</span>
<span class="err">â”‚</span><span class="w">  </span><span class="err">â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€</span><span class="w">                             </span><span class="err">â”‚</span>
<span class="err">â”‚</span><span class="w">  </span><span class="err">Î¼</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">mean</span><span class="p">(</span><span class="n">x</span><span class="p">)</span><span class="w">                                                    </span><span class="err">â”‚</span>
<span class="err">â”‚</span><span class="w">  </span><span class="err">Ïƒ</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">std</span><span class="p">(</span><span class="n">x</span><span class="p">)</span><span class="w">                                                     </span><span class="err">â”‚</span>
<span class="err">â”‚</span><span class="w">  </span><span class="n">y</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="err">Î³</span><span class="w"> </span><span class="err">Ã—</span><span class="w"> </span><span class="p">(</span><span class="n">x</span><span class="w"> </span><span class="o">-</span><span class="w"> </span><span class="err">Î¼</span><span class="p">)</span><span class="w"> </span><span class="o">/</span><span class="w"> </span><span class="err">Ïƒ</span><span class="w"> </span><span class="o">+</span><span class="w"> </span><span class="err">Î²</span><span class="w">                                        </span><span class="err">â”‚</span>
<span class="err">â”‚</span><span class="w">                                                                 </span><span class="err">â”‚</span>
<span class="err">â”‚</span><span class="w">  </span><span class="err">â€¢</span><span class="w"> </span><span class="n">Subtract</span><span class="w"> </span><span class="n">mean</span><span class="w"> </span><span class="o">+</span><span class="w"> </span><span class="n">divide</span><span class="w"> </span><span class="n">by</span><span class="w"> </span><span class="n">variance</span><span class="w">                           </span><span class="err">â”‚</span>
<span class="err">â”‚</span><span class="w">  </span><span class="err">â€¢</span><span class="w"> </span><span class="n">Learnable</span><span class="w"> </span><span class="n">scale</span><span class="p">(</span><span class="err">Î³</span><span class="p">)</span><span class="w"> </span><span class="ow">and</span><span class="w"> </span><span class="n">shift</span><span class="p">(</span><span class="err">Î²</span><span class="p">)</span><span class="w">                              </span><span class="err">â”‚</span>
<span class="err">â”‚</span><span class="w">                                                                 </span><span class="err">â”‚</span>
<span class="err">â”‚</span><span class="w">  </span><span class="n">RMSNorm</span><span class="p">:</span><span class="w">                                                       </span><span class="err">â”‚</span>
<span class="err">â”‚</span><span class="w">  </span><span class="err">â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€</span><span class="w">                             </span><span class="err">â”‚</span>
<span class="err">â”‚</span><span class="w">  </span><span class="n">RMS</span><span class="p">(</span><span class="n">x</span><span class="p">)</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="nb">sqrt</span><span class="p">(</span><span class="n">mean</span><span class="p">(</span><span class="n">x</span><span class="o">^</span><span class="mi">2</span><span class="p">))</span><span class="w">                                       </span><span class="err">â”‚</span>
<span class="err">â”‚</span><span class="w">  </span><span class="n">y</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="err">Î³</span><span class="w"> </span><span class="err">Ã—</span><span class="w"> </span><span class="n">x</span><span class="w"> </span><span class="o">/</span><span class="w"> </span><span class="n">RMS</span><span class="p">(</span><span class="n">x</span><span class="p">)</span><span class="w">                                             </span><span class="err">â”‚</span>
<span class="err">â”‚</span><span class="w">                                                                 </span><span class="err">â”‚</span>
<span class="err">â”‚</span><span class="w">  </span><span class="err">â€¢</span><span class="w"> </span><span class="n">No</span><span class="w"> </span><span class="n">mean</span><span class="w"> </span><span class="n">subtraction</span><span class="w"> </span><span class="err">â†’</span><span class="w"> </span><span class="n">Re</span><span class="o">-</span><span class="n">centering</span><span class="w"> </span><span class="n">removed</span><span class="w">                   </span><span class="err">â”‚</span>
<span class="err">â”‚</span><span class="w">  </span><span class="err">â€¢</span><span class="w"> </span><span class="n">Scale</span><span class="w"> </span><span class="n">by</span><span class="w"> </span><span class="n">RMS</span><span class="w"> </span><span class="n">only</span><span class="w">                                            </span><span class="err">â”‚</span>
<span class="err">â”‚</span><span class="w">  </span><span class="err">â€¢</span><span class="w"> </span><span class="n">No</span><span class="w"> </span><span class="n">shift</span><span class="p">(</span><span class="err">Î²</span><span class="p">)</span><span class="w">                                                  </span><span class="err">â”‚</span>
<span class="err">â”‚</span><span class="w">  </span><span class="err">â€¢</span><span class="w"> </span><span class="n">Reduced</span><span class="w"> </span><span class="n">computation</span><span class="p">,</span><span class="w"> </span><span class="n">similar</span><span class="w"> </span><span class="n">performance</span><span class="w">                     </span><span class="err">â”‚</span>
<span class="err">â”‚</span><span class="w">                                                                 </span><span class="err">â”‚</span>
<span class="err">â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜</span>
</code></pre></div>

<h3 id="42-implementation">4.2 Implementation<a class="header-link" href="#42-implementation" title="Permanent link">&para;</a></h3>
<div class="highlight"><pre><span></span><code><span class="kn">import</span><span class="w"> </span><span class="nn">torch</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">torch.nn</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">nn</span>

<span class="k">class</span><span class="w"> </span><span class="nc">RMSNorm</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Root Mean Square Layer Normalization</span>

<span class="sd">    Paper: https://arxiv.org/abs/1910.07467</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">def</span><span class="w"> </span><span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">dim</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span> <span class="n">eps</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="mf">1e-6</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">eps</span> <span class="o">=</span> <span class="n">eps</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">weight</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Parameter</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">ones</span><span class="p">(</span><span class="n">dim</span><span class="p">))</span>  <span class="c1"># scale parameter Î³</span>

    <span class="k">def</span><span class="w"> </span><span class="nf">_norm</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
        <span class="c1"># RMS = sqrt(mean(x^2))</span>
        <span class="k">return</span> <span class="n">x</span> <span class="o">*</span> <span class="n">torch</span><span class="o">.</span><span class="n">rsqrt</span><span class="p">(</span><span class="n">x</span><span class="o">.</span><span class="n">pow</span><span class="p">(</span><span class="mi">2</span><span class="p">)</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="n">keepdim</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span> <span class="o">+</span> <span class="bp">self</span><span class="o">.</span><span class="n">eps</span><span class="p">)</span>

    <span class="k">def</span><span class="w"> </span><span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
        <span class="c1"># output = Î³ Ã— (x / RMS(x))</span>
        <span class="n">output</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_norm</span><span class="p">(</span><span class="n">x</span><span class="o">.</span><span class="n">float</span><span class="p">())</span><span class="o">.</span><span class="n">type_as</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">output</span> <span class="o">*</span> <span class="bp">self</span><span class="o">.</span><span class="n">weight</span>

<span class="c1"># Compare with LayerNorm</span>
<span class="n">x</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">10</span><span class="p">,</span> <span class="mi">512</span><span class="p">)</span>

<span class="n">layer_norm</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">LayerNorm</span><span class="p">(</span><span class="mi">512</span><span class="p">)</span>
<span class="n">rms_norm</span> <span class="o">=</span> <span class="n">RMSNorm</span><span class="p">(</span><span class="mi">512</span><span class="p">)</span>

<span class="c1"># Computation time comparison (RMSNorm is slightly faster)</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">time</span>

<span class="n">start</span> <span class="o">=</span> <span class="n">time</span><span class="o">.</span><span class="n">time</span><span class="p">()</span>
<span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">1000</span><span class="p">):</span>
    <span class="n">_</span> <span class="o">=</span> <span class="n">layer_norm</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;LayerNorm: </span><span class="si">{</span><span class="n">time</span><span class="o">.</span><span class="n">time</span><span class="p">()</span><span class="w"> </span><span class="o">-</span><span class="w"> </span><span class="n">start</span><span class="si">:</span><span class="s2">.4f</span><span class="si">}</span><span class="s2">s&quot;</span><span class="p">)</span>

<span class="n">start</span> <span class="o">=</span> <span class="n">time</span><span class="o">.</span><span class="n">time</span><span class="p">()</span>
<span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">1000</span><span class="p">):</span>
    <span class="n">_</span> <span class="o">=</span> <span class="n">rms_norm</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;RMSNorm: </span><span class="si">{</span><span class="n">time</span><span class="o">.</span><span class="n">time</span><span class="p">()</span><span class="w"> </span><span class="o">-</span><span class="w"> </span><span class="n">start</span><span class="si">:</span><span class="s2">.4f</span><span class="si">}</span><span class="s2">s&quot;</span><span class="p">)</span>
</code></pre></div>

<hr />
<h2 id="5-swiglu">5. SwiGLU<a class="header-link" href="#5-swiglu" title="Permanent link">&para;</a></h2>
<h3 id="51-concept">5.1 Concept<a class="header-link" href="#51-concept" title="Permanent link">&para;</a></h3>
<p><strong>SwiGLU</strong> is a variant of GLU (Gated Linear Unit) that uses the Swish activation function.</p>
<div class="highlight"><pre><span></span><code><span class="err">â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”</span>
<span class="err">â”‚</span><span class="w">                    </span><span class="nx">FFN</span><span class="w"> </span><span class="nx">Activation</span><span class="w"> </span><span class="nx">Function</span><span class="w"> </span><span class="nx">Comparison</span><span class="w">            </span><span class="err">â”‚</span>
<span class="err">â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤</span>
<span class="err">â”‚</span><span class="w">                                                                 </span><span class="err">â”‚</span>
<span class="err">â”‚</span><span class="w">  </span><span class="mi">1</span><span class="p">.</span><span class="w"> </span><span class="nx">ReLU</span><span class="w"> </span><span class="nx">FFN</span><span class="w"> </span><span class="p">(</span><span class="nx">Original</span><span class="w"> </span><span class="nx">Transformer</span><span class="p">):</span><span class="w">                            </span><span class="err">â”‚</span>
<span class="err">â”‚</span><span class="w">     </span><span class="nx">FFN</span><span class="p">(</span><span class="nx">x</span><span class="p">)</span><span class="w"> </span><span class="p">=</span><span class="w"> </span><span class="nx">max</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span><span class="w"> </span><span class="nx">xWâ‚</span><span class="w"> </span><span class="o">+</span><span class="w"> </span><span class="nx">bâ‚</span><span class="p">)</span><span class="nx">Wâ‚‚</span><span class="w"> </span><span class="o">+</span><span class="w"> </span><span class="nx">bâ‚‚</span><span class="w">                            </span><span class="err">â”‚</span>
<span class="err">â”‚</span><span class="w">     </span><span class="err">â€¢</span><span class="w"> </span><span class="nx">Simple</span><span class="w"> </span><span class="nx">but</span><span class="w"> </span><span class="nx">loses</span><span class="w"> </span><span class="nx">information</span><span class="w"> </span><span class="k">in</span><span class="w"> </span><span class="nx">negative</span><span class="w"> </span><span class="nx">region</span><span class="w">           </span><span class="err">â”‚</span>
<span class="err">â”‚</span><span class="w">                                                                 </span><span class="err">â”‚</span>
<span class="err">â”‚</span><span class="w">  </span><span class="mi">2</span><span class="p">.</span><span class="w"> </span><span class="nx">GELU</span><span class="w"> </span><span class="nx">FFN</span><span class="w"> </span><span class="p">(</span><span class="nx">BERT</span><span class="p">,</span><span class="w"> </span><span class="nx">GPT</span><span class="p">):</span><span class="w">                                       </span><span class="err">â”‚</span>
<span class="err">â”‚</span><span class="w">     </span><span class="nx">FFN</span><span class="p">(</span><span class="nx">x</span><span class="p">)</span><span class="w"> </span><span class="p">=</span><span class="w"> </span><span class="nx">GELU</span><span class="p">(</span><span class="nx">xWâ‚</span><span class="p">)</span><span class="nx">Wâ‚‚</span><span class="w">                                        </span><span class="err">â”‚</span>
<span class="err">â”‚</span><span class="w">     </span><span class="nx">GELU</span><span class="p">(</span><span class="nx">x</span><span class="p">)</span><span class="w"> </span><span class="p">=</span><span class="w"> </span><span class="nx">x</span><span class="w"> </span><span class="err">Ã—</span><span class="w"> </span><span class="nx">Î¦</span><span class="p">(</span><span class="nx">x</span><span class="p">)</span><span class="w">  </span><span class="p">(</span><span class="nx">Î¦</span><span class="w"> </span><span class="p">=</span><span class="w"> </span><span class="nx">CDF</span><span class="w"> </span><span class="nx">of</span><span class="w"> </span><span class="nx">normal</span><span class="p">)</span><span class="w">                     </span><span class="err">â”‚</span>
<span class="err">â”‚</span><span class="w">     </span><span class="err">â€¢</span><span class="w"> </span><span class="nx">Smooth</span><span class="w"> </span><span class="nx">activation</span><span class="p">,</span><span class="w"> </span><span class="nx">improved</span><span class="w"> </span><span class="nx">performance</span><span class="w">                   </span><span class="err">â”‚</span>
<span class="err">â”‚</span><span class="w">                                                                 </span><span class="err">â”‚</span>
<span class="err">â”‚</span><span class="w">  </span><span class="mi">3</span><span class="p">.</span><span class="w"> </span><span class="nx">SwiGLU</span><span class="w"> </span><span class="nx">FFN</span><span class="w"> </span><span class="p">(</span><span class="nx">LLaMA</span><span class="p">):</span><span class="w">                                         </span><span class="err">â”‚</span>
<span class="err">â”‚</span><span class="w">     </span><span class="nx">FFN</span><span class="p">(</span><span class="nx">x</span><span class="p">)</span><span class="w"> </span><span class="p">=</span><span class="w"> </span><span class="p">(</span><span class="nx">Swish</span><span class="p">(</span><span class="nx">xWâ‚</span><span class="p">)</span><span class="w"> </span><span class="err">âŠ™</span><span class="w"> </span><span class="nx">xV</span><span class="p">)</span><span class="nx">Wâ‚‚</span><span class="w">                                </span><span class="err">â”‚</span>
<span class="err">â”‚</span><span class="w">     </span><span class="nx">Swish</span><span class="p">(</span><span class="nx">x</span><span class="p">)</span><span class="w"> </span><span class="p">=</span><span class="w"> </span><span class="nx">x</span><span class="w"> </span><span class="err">Ã—</span><span class="w"> </span><span class="nx">Ïƒ</span><span class="p">(</span><span class="nx">x</span><span class="p">)</span><span class="w">  </span><span class="p">(</span><span class="nx">Ïƒ</span><span class="w"> </span><span class="p">=</span><span class="w"> </span><span class="nx">sigmoid</span><span class="p">)</span><span class="w">                          </span><span class="err">â”‚</span>
<span class="err">â”‚</span><span class="w">     </span><span class="err">âŠ™</span><span class="w"> </span><span class="p">=</span><span class="w"> </span><span class="nx">element</span><span class="o">-</span><span class="nx">wise</span><span class="w"> </span><span class="nx">multiplication</span><span class="w">                             </span><span class="err">â”‚</span>
<span class="err">â”‚</span><span class="w">                                                                 </span><span class="err">â”‚</span>
<span class="err">â”‚</span><span class="w">     </span><span class="err">â€¢</span><span class="w"> </span><span class="nx">Gating</span><span class="w"> </span><span class="nx">mechanism</span><span class="w"> </span><span class="nx">controls</span><span class="w"> </span><span class="nx">information</span><span class="w"> </span><span class="nx">flow</span><span class="w">                </span><span class="err">â”‚</span>
<span class="err">â”‚</span><span class="w">     </span><span class="err">â€¢</span><span class="w"> </span><span class="nx">More</span><span class="w"> </span><span class="nx">parameters</span><span class="p">,</span><span class="w"> </span><span class="nx">better</span><span class="w"> </span><span class="nx">performance</span><span class="w">                       </span><span class="err">â”‚</span>
<span class="err">â”‚</span><span class="w">     </span><span class="err">â€¢</span><span class="w"> </span><span class="mi">2</span><span class="o">/</span><span class="mi">3</span><span class="w"> </span><span class="err">Ã—</span><span class="w"> </span><span class="mi">4</span><span class="nx">d</span><span class="w"> </span><span class="nx">hidden</span><span class="w"> </span><span class="nx">dim</span><span class="w"> </span><span class="p">(</span><span class="nx">maintains</span><span class="w"> </span><span class="nx">parameter</span><span class="w"> </span><span class="nx">count</span><span class="p">)</span><span class="w">           </span><span class="err">â”‚</span>
<span class="err">â”‚</span><span class="w">                                                                 </span><span class="err">â”‚</span>
<span class="err">â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜</span>
</code></pre></div>

<h3 id="52-implementation">5.2 Implementation<a class="header-link" href="#52-implementation" title="Permanent link">&para;</a></h3>
<div class="highlight"><pre><span></span><code><span class="kn">import</span><span class="w"> </span><span class="nn">torch</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">torch.nn</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">nn</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">torch.nn.functional</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">F</span>

<span class="k">class</span><span class="w"> </span><span class="nc">SwiGLU</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    SwiGLU: Swish-Gated Linear Unit</span>

<span class="sd">    FFN(x) = (Swish(xWâ‚) âŠ™ xV) Wâ‚‚</span>

<span class="sd">    Paper: https://arxiv.org/abs/2002.05202</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">def</span><span class="w"> </span><span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">dim</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span> <span class="n">hidden_dim</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span> <span class="n">multiple_of</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">256</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>

        <span class="c1"># hidden_dim calculation: 2/3 Ã— 4d, rounded to multiple of 256</span>
        <span class="k">if</span> <span class="n">hidden_dim</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">hidden_dim</span> <span class="o">=</span> <span class="nb">int</span><span class="p">(</span><span class="mi">2</span> <span class="o">*</span> <span class="p">(</span><span class="mi">4</span> <span class="o">*</span> <span class="n">dim</span><span class="p">)</span> <span class="o">/</span> <span class="mi">3</span><span class="p">)</span>
            <span class="n">hidden_dim</span> <span class="o">=</span> <span class="n">multiple_of</span> <span class="o">*</span> <span class="p">((</span><span class="n">hidden_dim</span> <span class="o">+</span> <span class="n">multiple_of</span> <span class="o">-</span> <span class="mi">1</span><span class="p">)</span> <span class="o">//</span> <span class="n">multiple_of</span><span class="p">)</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">w1</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">dim</span><span class="p">,</span> <span class="n">hidden_dim</span><span class="p">,</span> <span class="n">bias</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>  <span class="c1"># gate</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">w2</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">hidden_dim</span><span class="p">,</span> <span class="n">dim</span><span class="p">,</span> <span class="n">bias</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>  <span class="c1"># down projection</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">w3</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">dim</span><span class="p">,</span> <span class="n">hidden_dim</span><span class="p">,</span> <span class="n">bias</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>  <span class="c1"># up projection</span>

    <span class="k">def</span><span class="w"> </span><span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
        <span class="c1"># SwiGLU: Swish(xWâ‚) âŠ™ (xWâ‚ƒ) â†’ Wâ‚‚</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">w2</span><span class="p">(</span><span class="n">F</span><span class="o">.</span><span class="n">silu</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">w1</span><span class="p">(</span><span class="n">x</span><span class="p">))</span> <span class="o">*</span> <span class="bp">self</span><span class="o">.</span><span class="n">w3</span><span class="p">(</span><span class="n">x</span><span class="p">))</span>

<span class="c1"># Compare with standard FFN</span>
<span class="k">class</span><span class="w"> </span><span class="nc">StandardFFN</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="k">def</span><span class="w"> </span><span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">dim</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span> <span class="n">hidden_dim</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="kc">None</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
        <span class="k">if</span> <span class="n">hidden_dim</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">hidden_dim</span> <span class="o">=</span> <span class="mi">4</span> <span class="o">*</span> <span class="n">dim</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">w1</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">dim</span><span class="p">,</span> <span class="n">hidden_dim</span><span class="p">,</span> <span class="n">bias</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">w2</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">hidden_dim</span><span class="p">,</span> <span class="n">dim</span><span class="p">,</span> <span class="n">bias</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>

    <span class="k">def</span><span class="w"> </span><span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">w2</span><span class="p">(</span><span class="n">F</span><span class="o">.</span><span class="n">gelu</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">w1</span><span class="p">(</span><span class="n">x</span><span class="p">)))</span>

<span class="c1"># Parameter count comparison</span>
<span class="n">dim</span> <span class="o">=</span> <span class="mi">4096</span>
<span class="n">swiglu</span> <span class="o">=</span> <span class="n">SwiGLU</span><span class="p">(</span><span class="n">dim</span><span class="p">)</span>  <span class="c1"># 3 linear layers: dimâ†’hidden, dimâ†’hidden, hiddenâ†’dim</span>
<span class="n">standard</span> <span class="o">=</span> <span class="n">StandardFFN</span><span class="p">(</span><span class="n">dim</span><span class="p">)</span>  <span class="c1"># 2 linear layers: dimâ†’4*dim, 4*dimâ†’dim</span>

<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;SwiGLU params: </span><span class="si">{</span><span class="nb">sum</span><span class="p">(</span><span class="n">p</span><span class="o">.</span><span class="n">numel</span><span class="p">()</span><span class="w"> </span><span class="k">for</span><span class="w"> </span><span class="n">p</span><span class="w"> </span><span class="ow">in</span><span class="w"> </span><span class="n">swiglu</span><span class="o">.</span><span class="n">parameters</span><span class="p">())</span><span class="si">:</span><span class="s2">,</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Standard FFN params: </span><span class="si">{</span><span class="nb">sum</span><span class="p">(</span><span class="n">p</span><span class="o">.</span><span class="n">numel</span><span class="p">()</span><span class="w"> </span><span class="k">for</span><span class="w"> </span><span class="n">p</span><span class="w"> </span><span class="ow">in</span><span class="w"> </span><span class="n">standard</span><span class="o">.</span><span class="n">parameters</span><span class="p">())</span><span class="si">:</span><span class="s2">,</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
<span class="c1"># SwiGLU has slightly more but adjusted with hidden_dim to be similar</span>
</code></pre></div>

<hr />
<h2 id="6-grouped-query-attention-gqa">6. Grouped Query Attention (GQA)<a class="header-link" href="#6-grouped-query-attention-gqa" title="Permanent link">&para;</a></h2>
<h3 id="61-concept">6.1 Concept<a class="header-link" href="#61-concept" title="Permanent link">&para;</a></h3>
<p><strong>GQA</strong> is an intermediate form between Multi-Head Attention and Multi-Query Attention.</p>
<div class="highlight"><pre><span></span><code>â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    Attention Method Comparison                   â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                 â”‚
â”‚  1. Multi-Head Attention (MHA):                                 â”‚
â”‚     Q heads: 32  â”‚  K heads: 32  â”‚  V heads: 32                 â”‚
â”‚     â€¢ Each Q head uses independent KV head                      â”‚
â”‚     â€¢ Memory: High (32 Ã— KV cache)                              â”‚
â”‚                                                                 â”‚
â”‚  2. Multi-Query Attention (MQA):                                â”‚
â”‚     Q heads: 32  â”‚  K heads: 1   â”‚  V heads: 1                  â”‚
â”‚     â€¢ All Q heads share same KV                                 â”‚
â”‚     â€¢ Memory: Minimum (1 Ã— KV cache)                            â”‚
â”‚     â€¢ Quality: Slightly lower than MHA                          â”‚
â”‚                                                                 â”‚
â”‚  3. Grouped Query Attention (GQA):                              â”‚
â”‚     Q heads: 32  â”‚  K heads: 8   â”‚  V heads: 8                  â”‚
â”‚     â€¢ Q heads divided into groups sharing KV                    â”‚
â”‚     â€¢ Example: 4 Q heads share 1 KV head                        â”‚
â”‚     â€¢ Memory: Medium (8 Ã— KV cache)                             â”‚
â”‚     â€¢ Quality: Nearly identical to MHA                          â”‚
â”‚                                                                 â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”               â”‚
â”‚  â”‚ MHA          â”‚ MQA           â”‚ GQA           â”‚               â”‚
â”‚  â”‚ Q Q Q Q Q Q  â”‚ Q Q Q Q Q Q   â”‚ Q Qâ”‚Q Qâ”‚Q Q   â”‚               â”‚
â”‚  â”‚ â†“ â†“ â†“ â†“ â†“ â†“  â”‚ â†“ â†“ â†“ â†“ â†“ â†“   â”‚ â†“ â†“â”‚â†“ â†“â”‚â†“ â†“   â”‚               â”‚
â”‚  â”‚ K K K K K K  â”‚     K         â”‚  K â”‚ K â”‚ K    â”‚               â”‚
â”‚  â”‚ V V V V V V  â”‚     V         â”‚  V â”‚ V â”‚ V    â”‚               â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜               â”‚
â”‚                                                                 â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
</code></pre></div>

<h3 id="62-implementation">6.2 Implementation<a class="header-link" href="#62-implementation" title="Permanent link">&para;</a></h3>
<div class="highlight"><pre><span></span><code><span class="kn">import</span><span class="w"> </span><span class="nn">torch</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">torch.nn</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">nn</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">torch.nn.functional</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">F</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">math</span>

<span class="k">class</span><span class="w"> </span><span class="nc">GroupedQueryAttention</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Grouped Query Attention (GQA)</span>

<span class="sd">    Paper: https://arxiv.org/abs/2305.13245</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">def</span><span class="w"> </span><span class="fm">__init__</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">dim</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
        <span class="n">n_heads</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">32</span><span class="p">,</span>
        <span class="n">n_kv_heads</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">8</span><span class="p">,</span>  <span class="c1"># Number of KV heads (&lt; n_heads)</span>
        <span class="n">head_dim</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="p">):</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">n_heads</span> <span class="o">=</span> <span class="n">n_heads</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">n_kv_heads</span> <span class="o">=</span> <span class="n">n_kv_heads</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">head_dim</span> <span class="o">=</span> <span class="n">head_dim</span> <span class="ow">or</span> <span class="n">dim</span> <span class="o">//</span> <span class="n">n_heads</span>

        <span class="c1"># Verify Q heads &gt; KV heads</span>
        <span class="k">assert</span> <span class="n">n_heads</span> <span class="o">%</span> <span class="n">n_kv_heads</span> <span class="o">==</span> <span class="mi">0</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">n_rep</span> <span class="o">=</span> <span class="n">n_heads</span> <span class="o">//</span> <span class="n">n_kv_heads</span>  <span class="c1"># Number of Q heads per KV head</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">wq</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">dim</span><span class="p">,</span> <span class="n">n_heads</span> <span class="o">*</span> <span class="bp">self</span><span class="o">.</span><span class="n">head_dim</span><span class="p">,</span> <span class="n">bias</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">wk</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">dim</span><span class="p">,</span> <span class="n">n_kv_heads</span> <span class="o">*</span> <span class="bp">self</span><span class="o">.</span><span class="n">head_dim</span><span class="p">,</span> <span class="n">bias</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">wv</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">dim</span><span class="p">,</span> <span class="n">n_kv_heads</span> <span class="o">*</span> <span class="bp">self</span><span class="o">.</span><span class="n">head_dim</span><span class="p">,</span> <span class="n">bias</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">wo</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">n_heads</span> <span class="o">*</span> <span class="bp">self</span><span class="o">.</span><span class="n">head_dim</span><span class="p">,</span> <span class="n">dim</span><span class="p">,</span> <span class="n">bias</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>

    <span class="k">def</span><span class="w"> </span><span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">freqs_cis</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">mask</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">kv_cache</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
        <span class="n">batch</span><span class="p">,</span> <span class="n">seq_len</span><span class="p">,</span> <span class="n">_</span> <span class="o">=</span> <span class="n">x</span><span class="o">.</span><span class="n">shape</span>

        <span class="c1"># Compute Q, K, V</span>
        <span class="n">xq</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">wq</span><span class="p">(</span><span class="n">x</span><span class="p">)</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="n">batch</span><span class="p">,</span> <span class="n">seq_len</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">n_heads</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">head_dim</span><span class="p">)</span>
        <span class="n">xk</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">wk</span><span class="p">(</span><span class="n">x</span><span class="p">)</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="n">batch</span><span class="p">,</span> <span class="n">seq_len</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">n_kv_heads</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">head_dim</span><span class="p">)</span>
        <span class="n">xv</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">wv</span><span class="p">(</span><span class="n">x</span><span class="p">)</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="n">batch</span><span class="p">,</span> <span class="n">seq_len</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">n_kv_heads</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">head_dim</span><span class="p">)</span>

        <span class="c1"># Apply RoPE (if available)</span>
        <span class="k">if</span> <span class="n">freqs_cis</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">xq</span><span class="p">,</span> <span class="n">xk</span> <span class="o">=</span> <span class="n">apply_rotary_emb</span><span class="p">(</span><span class="n">xq</span><span class="p">,</span> <span class="n">xk</span><span class="p">,</span> <span class="n">freqs_cis</span><span class="p">)</span>

        <span class="c1"># KV Cache handling (during inference)</span>
        <span class="k">if</span> <span class="n">kv_cache</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">cache_k</span><span class="p">,</span> <span class="n">cache_v</span> <span class="o">=</span> <span class="n">kv_cache</span>
            <span class="n">xk</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">cat</span><span class="p">([</span><span class="n">cache_k</span><span class="p">,</span> <span class="n">xk</span><span class="p">],</span> <span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
            <span class="n">xv</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">cat</span><span class="p">([</span><span class="n">cache_v</span><span class="p">,</span> <span class="n">xv</span><span class="p">],</span> <span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>

        <span class="c1"># Expand KV heads: n_kv_heads â†’ n_heads</span>
        <span class="c1"># (batch, seq, n_kv_heads, head_dim) â†’ (batch, seq, n_heads, head_dim)</span>
        <span class="n">xk</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_repeat_kv</span><span class="p">(</span><span class="n">xk</span><span class="p">)</span>
        <span class="n">xv</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_repeat_kv</span><span class="p">(</span><span class="n">xv</span><span class="p">)</span>

        <span class="c1"># Compute Attention</span>
        <span class="n">xq</span> <span class="o">=</span> <span class="n">xq</span><span class="o">.</span><span class="n">transpose</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>  <span class="c1"># (batch, n_heads, seq, head_dim)</span>
        <span class="n">xk</span> <span class="o">=</span> <span class="n">xk</span><span class="o">.</span><span class="n">transpose</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>
        <span class="n">xv</span> <span class="o">=</span> <span class="n">xv</span><span class="o">.</span><span class="n">transpose</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>

        <span class="n">scores</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">matmul</span><span class="p">(</span><span class="n">xq</span><span class="p">,</span> <span class="n">xk</span><span class="o">.</span><span class="n">transpose</span><span class="p">(</span><span class="o">-</span><span class="mi">2</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">))</span> <span class="o">/</span> <span class="n">math</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">head_dim</span><span class="p">)</span>

        <span class="k">if</span> <span class="n">mask</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">scores</span> <span class="o">=</span> <span class="n">scores</span> <span class="o">+</span> <span class="n">mask</span>

        <span class="n">attn</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">softmax</span><span class="p">(</span><span class="n">scores</span><span class="p">,</span> <span class="n">dim</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span>
        <span class="n">output</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">matmul</span><span class="p">(</span><span class="n">attn</span><span class="p">,</span> <span class="n">xv</span><span class="p">)</span>

        <span class="c1"># Combine results</span>
        <span class="n">output</span> <span class="o">=</span> <span class="n">output</span><span class="o">.</span><span class="n">transpose</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span><span class="o">.</span><span class="n">contiguous</span><span class="p">()</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="n">batch</span><span class="p">,</span> <span class="n">seq_len</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">)</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">wo</span><span class="p">(</span><span class="n">output</span><span class="p">),</span> <span class="p">(</span><span class="n">xk</span><span class="p">,</span> <span class="n">xv</span><span class="p">)</span>

    <span class="k">def</span><span class="w"> </span><span class="nf">_repeat_kv</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Repeat KV heads to match Q heads count&quot;&quot;&quot;</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">n_rep</span> <span class="o">==</span> <span class="mi">1</span><span class="p">:</span>
            <span class="k">return</span> <span class="n">x</span>
        <span class="n">batch</span><span class="p">,</span> <span class="n">seq_len</span><span class="p">,</span> <span class="n">n_kv_heads</span><span class="p">,</span> <span class="n">head_dim</span> <span class="o">=</span> <span class="n">x</span><span class="o">.</span><span class="n">shape</span>
        <span class="n">x</span> <span class="o">=</span> <span class="n">x</span><span class="p">[:,</span> <span class="p">:,</span> <span class="p">:,</span> <span class="kc">None</span><span class="p">,</span> <span class="p">:]</span><span class="o">.</span><span class="n">expand</span><span class="p">(</span><span class="n">batch</span><span class="p">,</span> <span class="n">seq_len</span><span class="p">,</span> <span class="n">n_kv_heads</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">n_rep</span><span class="p">,</span> <span class="n">head_dim</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">x</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="n">batch</span><span class="p">,</span> <span class="n">seq_len</span><span class="p">,</span> <span class="n">n_kv_heads</span> <span class="o">*</span> <span class="bp">self</span><span class="o">.</span><span class="n">n_rep</span><span class="p">,</span> <span class="n">head_dim</span><span class="p">)</span>

<span class="c1"># Memory usage comparison</span>
<span class="k">def</span><span class="w"> </span><span class="nf">compare_kv_cache_memory</span><span class="p">(</span><span class="n">seq_len</span><span class="p">,</span> <span class="n">batch_size</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">dtype_bytes</span><span class="o">=</span><span class="mi">2</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Compare KV cache memory (FP16 basis)&quot;&quot;&quot;</span>
    <span class="n">configs</span> <span class="o">=</span> <span class="p">{</span>
        <span class="s2">&quot;LLaMA-70B (MHA)&quot;</span><span class="p">:</span> <span class="p">{</span><span class="s2">&quot;n_layers&quot;</span><span class="p">:</span> <span class="mi">80</span><span class="p">,</span> <span class="s2">&quot;n_kv_heads&quot;</span><span class="p">:</span> <span class="mi">64</span><span class="p">,</span> <span class="s2">&quot;head_dim&quot;</span><span class="p">:</span> <span class="mi">128</span><span class="p">},</span>
        <span class="s2">&quot;LLaMA-70B (GQA)&quot;</span><span class="p">:</span> <span class="p">{</span><span class="s2">&quot;n_layers&quot;</span><span class="p">:</span> <span class="mi">80</span><span class="p">,</span> <span class="s2">&quot;n_kv_heads&quot;</span><span class="p">:</span> <span class="mi">8</span><span class="p">,</span> <span class="s2">&quot;head_dim&quot;</span><span class="p">:</span> <span class="mi">128</span><span class="p">},</span>
    <span class="p">}</span>

    <span class="k">for</span> <span class="n">name</span><span class="p">,</span> <span class="n">cfg</span> <span class="ow">in</span> <span class="n">configs</span><span class="o">.</span><span class="n">items</span><span class="p">():</span>
        <span class="n">kv_mem</span> <span class="o">=</span> <span class="p">(</span><span class="mi">2</span> <span class="o">*</span>  <span class="c1"># K and V</span>
                  <span class="n">batch_size</span> <span class="o">*</span>
                  <span class="n">cfg</span><span class="p">[</span><span class="s2">&quot;n_layers&quot;</span><span class="p">]</span> <span class="o">*</span>
                  <span class="n">seq_len</span> <span class="o">*</span>
                  <span class="n">cfg</span><span class="p">[</span><span class="s2">&quot;n_kv_heads&quot;</span><span class="p">]</span> <span class="o">*</span>
                  <span class="n">cfg</span><span class="p">[</span><span class="s2">&quot;head_dim&quot;</span><span class="p">]</span> <span class="o">*</span>
                  <span class="n">dtype_bytes</span><span class="p">)</span>
        <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="n">name</span><span class="si">}</span><span class="s2">: </span><span class="si">{</span><span class="n">kv_mem</span><span class="w"> </span><span class="o">/</span><span class="w"> </span><span class="mf">1e9</span><span class="si">:</span><span class="s2">.2f</span><span class="si">}</span><span class="s2"> GB for </span><span class="si">{</span><span class="n">seq_len</span><span class="si">}</span><span class="s2"> tokens&quot;</span><span class="p">)</span>

<span class="n">compare_kv_cache_memory</span><span class="p">(</span><span class="mi">4096</span><span class="p">)</span>
<span class="c1"># LLaMA-70B (MHA): 5.24 GB for 4096 tokens</span>
<span class="c1"># LLaMA-70B (GQA): 0.66 GB for 4096 tokens  â† 8x savings!</span>
</code></pre></div>

<hr />
<h2 id="7-llama-practice">7. LLaMA Practice<a class="header-link" href="#7-llama-practice" title="Permanent link">&para;</a></h2>
<h3 id="71-using-with-huggingface">7.1 Using with HuggingFace<a class="header-link" href="#71-using-with-huggingface" title="Permanent link">&para;</a></h3>
<div class="highlight"><pre><span></span><code><span class="kn">from</span><span class="w"> </span><span class="nn">transformers</span><span class="w"> </span><span class="kn">import</span> <span class="n">AutoModelForCausalLM</span><span class="p">,</span> <span class="n">AutoTokenizer</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">torch</span>

<span class="c1"># Load LLaMA 2 7B</span>
<span class="n">model_name</span> <span class="o">=</span> <span class="s2">&quot;meta-llama/Llama-2-7b-hf&quot;</span>

<span class="n">tokenizer</span> <span class="o">=</span> <span class="n">AutoTokenizer</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="n">model_name</span><span class="p">)</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">AutoModelForCausalLM</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span>
    <span class="n">model_name</span><span class="p">,</span>
    <span class="n">torch_dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">float16</span><span class="p">,</span>
    <span class="n">device_map</span><span class="o">=</span><span class="s2">&quot;auto&quot;</span>
<span class="p">)</span>

<span class="c1"># Text generation</span>
<span class="k">def</span><span class="w"> </span><span class="nf">generate_text</span><span class="p">(</span><span class="n">prompt</span><span class="p">,</span> <span class="n">max_new_tokens</span><span class="o">=</span><span class="mi">100</span><span class="p">,</span> <span class="n">temperature</span><span class="o">=</span><span class="mf">0.7</span><span class="p">):</span>
    <span class="n">inputs</span> <span class="o">=</span> <span class="n">tokenizer</span><span class="p">(</span><span class="n">prompt</span><span class="p">,</span> <span class="n">return_tensors</span><span class="o">=</span><span class="s2">&quot;pt&quot;</span><span class="p">)</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">model</span><span class="o">.</span><span class="n">device</span><span class="p">)</span>

    <span class="n">outputs</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">generate</span><span class="p">(</span>
        <span class="o">**</span><span class="n">inputs</span><span class="p">,</span>
        <span class="n">max_new_tokens</span><span class="o">=</span><span class="n">max_new_tokens</span><span class="p">,</span>
        <span class="n">temperature</span><span class="o">=</span><span class="n">temperature</span><span class="p">,</span>
        <span class="n">do_sample</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
        <span class="n">top_p</span><span class="o">=</span><span class="mf">0.9</span><span class="p">,</span>
        <span class="n">pad_token_id</span><span class="o">=</span><span class="n">tokenizer</span><span class="o">.</span><span class="n">eos_token_id</span>
    <span class="p">)</span>

    <span class="k">return</span> <span class="n">tokenizer</span><span class="o">.</span><span class="n">decode</span><span class="p">(</span><span class="n">outputs</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">skip_special_tokens</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>

<span class="c1"># Usage example</span>
<span class="n">prompt</span> <span class="o">=</span> <span class="s2">&quot;Explain the concept of machine learning in simple terms:&quot;</span>
<span class="n">response</span> <span class="o">=</span> <span class="n">generate_text</span><span class="p">(</span><span class="n">prompt</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">response</span><span class="p">)</span>
</code></pre></div>

<h3 id="72-efficient-usage-with-quantization">7.2 Efficient Usage with Quantization<a class="header-link" href="#72-efficient-usage-with-quantization" title="Permanent link">&para;</a></h3>
<div class="highlight"><pre><span></span><code><span class="kn">from</span><span class="w"> </span><span class="nn">transformers</span><span class="w"> </span><span class="kn">import</span> <span class="n">AutoModelForCausalLM</span><span class="p">,</span> <span class="n">AutoTokenizer</span><span class="p">,</span> <span class="n">BitsAndBytesConfig</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">torch</span>

<span class="c1"># 4-bit quantization configuration</span>
<span class="n">bnb_config</span> <span class="o">=</span> <span class="n">BitsAndBytesConfig</span><span class="p">(</span>
    <span class="n">load_in_4bit</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
    <span class="n">bnb_4bit_quant_type</span><span class="o">=</span><span class="s2">&quot;nf4&quot;</span><span class="p">,</span>          <span class="c1"># NormalFloat4</span>
    <span class="n">bnb_4bit_compute_dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">float16</span><span class="p">,</span>
    <span class="n">bnb_4bit_use_double_quant</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>     <span class="c1"># Double quantization</span>
<span class="p">)</span>

<span class="c1"># Load quantized model</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">AutoModelForCausalLM</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span>
    <span class="s2">&quot;meta-llama/Llama-2-7b-hf&quot;</span><span class="p">,</span>
    <span class="n">quantization_config</span><span class="o">=</span><span class="n">bnb_config</span><span class="p">,</span>
    <span class="n">device_map</span><span class="o">=</span><span class="s2">&quot;auto&quot;</span>
<span class="p">)</span>

<span class="c1"># Check memory usage</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Model memory: </span><span class="si">{</span><span class="n">model</span><span class="o">.</span><span class="n">get_memory_footprint</span><span class="p">()</span><span class="w"> </span><span class="o">/</span><span class="w"> </span><span class="mf">1e9</span><span class="si">:</span><span class="s2">.2f</span><span class="si">}</span><span class="s2"> GB&quot;</span><span class="p">)</span>
<span class="c1"># Approximately 4GB (75% savings compared to FP16)</span>
</code></pre></div>

<h3 id="73-using-llama-3">7.3 Using LLaMA 3<a class="header-link" href="#73-using-llama-3" title="Permanent link">&para;</a></h3>
<div class="highlight"><pre><span></span><code><span class="c1"># LLaMA 3 8B (128K tokenizer)</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">transformers</span><span class="w"> </span><span class="kn">import</span> <span class="n">AutoModelForCausalLM</span><span class="p">,</span> <span class="n">AutoTokenizer</span>

<span class="n">model_name</span> <span class="o">=</span> <span class="s2">&quot;meta-llama/Meta-Llama-3-8B&quot;</span>

<span class="n">tokenizer</span> <span class="o">=</span> <span class="n">AutoTokenizer</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="n">model_name</span><span class="p">)</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">AutoModelForCausalLM</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span>
    <span class="n">model_name</span><span class="p">,</span>
    <span class="n">torch_dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">bfloat16</span><span class="p">,</span>  <span class="c1"># bfloat16 recommended for LLaMA 3</span>
    <span class="n">device_map</span><span class="o">=</span><span class="s2">&quot;auto&quot;</span>
<span class="p">)</span>

<span class="c1"># LLaMA 3 features:</span>
<span class="c1"># - 128K tokenizer (more efficient)</span>
<span class="c1"># - 8K base context (extendable to 128K)</span>
<span class="c1"># - Improved reasoning capability</span>

<span class="n">prompt</span> <span class="o">=</span> <span class="s2">&quot;&quot;&quot;&lt;|begin_of_text|&gt;&lt;|start_header_id|&gt;user&lt;|end_header_id|&gt;</span>

<span class="s2">What is the capital of France?&lt;|eot_id|&gt;&lt;|start_header_id|&gt;assistant&lt;|end_header_id|&gt;</span>

<span class="s2">&quot;&quot;&quot;</span>

<span class="n">inputs</span> <span class="o">=</span> <span class="n">tokenizer</span><span class="p">(</span><span class="n">prompt</span><span class="p">,</span> <span class="n">return_tensors</span><span class="o">=</span><span class="s2">&quot;pt&quot;</span><span class="p">)</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">model</span><span class="o">.</span><span class="n">device</span><span class="p">)</span>
<span class="n">outputs</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">generate</span><span class="p">(</span><span class="o">**</span><span class="n">inputs</span><span class="p">,</span> <span class="n">max_new_tokens</span><span class="o">=</span><span class="mi">50</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">tokenizer</span><span class="o">.</span><span class="n">decode</span><span class="p">(</span><span class="n">outputs</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">skip_special_tokens</span><span class="o">=</span><span class="kc">True</span><span class="p">))</span>
</code></pre></div>

<hr />
<h2 id="8-llama-3132-details">8. LLaMA 3.1/3.2 Details<a class="header-link" href="#8-llama-3132-details" title="Permanent link">&para;</a></h2>
<h3 id="81-llama-31-july-2024">8.1 LLaMA 3.1 (July 2024)<a class="header-link" href="#81-llama-31-july-2024" title="Permanent link">&para;</a></h3>
<div class="highlight"><pre><span></span><code><span class="err">â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”</span>
<span class="err">â”‚</span><span class="w">                    </span><span class="n">LLaMA</span><span class="w"> </span><span class="mf">3.1</span><span class="w"> </span><span class="n">Key</span><span class="w"> </span><span class="n">Features</span><span class="w">                        </span><span class="err">â”‚</span>
<span class="err">â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤</span>
<span class="err">â”‚</span><span class="w">                                                                 </span><span class="err">â”‚</span>
<span class="err">â”‚</span><span class="w">  </span><span class="mf">1.</span><span class="w"> </span><span class="mi">128</span><span class="n">K</span><span class="w"> </span><span class="n">Native</span><span class="w"> </span><span class="n">Context</span><span class="w">                                         </span><span class="err">â”‚</span>
<span class="err">â”‚</span><span class="w">     </span><span class="err">â€¢</span><span class="w"> </span><span class="n">Supports</span><span class="w"> </span><span class="mi">128</span><span class="n">K</span><span class="w"> </span><span class="n">tokens</span><span class="w"> </span><span class="n">from</span><span class="w"> </span><span class="n">training</span><span class="w">                        </span><span class="err">â”‚</span>
<span class="err">â”‚</span><span class="w">     </span><span class="err">â€¢</span><span class="w"> </span><span class="n">Handles</span><span class="w"> </span><span class="n">long</span><span class="w"> </span><span class="n">context</span><span class="w"> </span><span class="n">without</span><span class="w"> </span><span class="n">RoPE</span><span class="w"> </span><span class="n">scaling</span><span class="w">                 </span><span class="err">â”‚</span>
<span class="err">â”‚</span><span class="w">                                                                 </span><span class="err">â”‚</span>
<span class="err">â”‚</span><span class="w">  </span><span class="mf">2.</span><span class="w"> </span><span class="mi">405</span><span class="n">B</span><span class="w"> </span><span class="n">Flagship</span><span class="w"> </span><span class="n">Model</span><span class="w">                                         </span><span class="err">â”‚</span>
<span class="err">â”‚</span><span class="w">     </span><span class="err">â€¢</span><span class="w"> </span><span class="n">GPT</span><span class="o">-</span><span class="mi">4</span><span class="w"> </span><span class="n">level</span><span class="w"> </span><span class="n">performance</span><span class="w">                                   </span><span class="err">â”‚</span>
<span class="err">â”‚</span><span class="w">     </span><span class="err">â€¢</span><span class="w"> </span><span class="mi">126</span><span class="w"> </span><span class="n">layers</span><span class="p">,</span><span class="w"> </span><span class="mi">16</span><span class="n">K</span><span class="w"> </span><span class="n">embedding</span><span class="w"> </span><span class="n">dimension</span><span class="w">                       </span><span class="err">â”‚</span>
<span class="err">â”‚</span><span class="w">                                                                 </span><span class="err">â”‚</span>
<span class="err">â”‚</span><span class="w">  </span><span class="mf">3.</span><span class="w"> </span><span class="n">Tool</span><span class="w"> </span><span class="n">Use</span><span class="w"> </span><span class="n">Capability</span><span class="w">                                         </span><span class="err">â”‚</span>
<span class="err">â”‚</span><span class="w">     </span><span class="err">â€¢</span><span class="w"> </span><span class="n">Function</span><span class="w"> </span><span class="n">Calling</span><span class="w">                                          </span><span class="err">â”‚</span>
<span class="err">â”‚</span><span class="w">     </span><span class="err">â€¢</span><span class="w"> </span><span class="n">Code</span><span class="w"> </span><span class="n">Interpreter</span><span class="w">                                          </span><span class="err">â”‚</span>
<span class="err">â”‚</span><span class="w">     </span><span class="err">â€¢</span><span class="w"> </span><span class="n">Search</span><span class="w"> </span><span class="k">tool</span><span class="w"> </span><span class="n">integration</span><span class="w">                                   </span><span class="err">â”‚</span>
<span class="err">â”‚</span><span class="w">                                                                 </span><span class="err">â”‚</span>
<span class="err">â”‚</span><span class="w">  </span><span class="mf">4.</span><span class="w"> </span><span class="n">Enhanced</span><span class="w"> </span><span class="n">Multilingual</span><span class="w"> </span><span class="n">Support</span><span class="w">                               </span><span class="err">â”‚</span>
<span class="err">â”‚</span><span class="w">     </span><span class="err">â€¢</span><span class="w"> </span><span class="n">English</span><span class="p">,</span><span class="w"> </span><span class="n">German</span><span class="p">,</span><span class="w"> </span><span class="n">French</span><span class="p">,</span><span class="w"> </span><span class="n">Italian</span><span class="w">                          </span><span class="err">â”‚</span>
<span class="err">â”‚</span><span class="w">     </span><span class="err">â€¢</span><span class="w"> </span><span class="n">Portuguese</span><span class="p">,</span><span class="w"> </span><span class="n">Hindi</span><span class="p">,</span><span class="w"> </span><span class="n">Spanish</span><span class="p">,</span><span class="w"> </span><span class="n">Thai</span><span class="w">                          </span><span class="err">â”‚</span>
<span class="err">â”‚</span><span class="w">                                                                 </span><span class="err">â”‚</span>
<span class="err">â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜</span>
</code></pre></div>

<div class="highlight"><pre><span></span><code><span class="c1"># LLaMA 3.1 Tool Use Example</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">transformers</span><span class="w"> </span><span class="kn">import</span> <span class="n">AutoModelForCausalLM</span><span class="p">,</span> <span class="n">AutoTokenizer</span>

<span class="n">model_name</span> <span class="o">=</span> <span class="s2">&quot;meta-llama/Llama-3.1-8B-Instruct&quot;</span>

<span class="n">tokenizer</span> <span class="o">=</span> <span class="n">AutoTokenizer</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="n">model_name</span><span class="p">)</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">AutoModelForCausalLM</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span>
    <span class="n">model_name</span><span class="p">,</span>
    <span class="n">torch_dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">bfloat16</span><span class="p">,</span>
    <span class="n">device_map</span><span class="o">=</span><span class="s2">&quot;auto&quot;</span>
<span class="p">)</span>

<span class="c1"># Tool Use format (LLaMA 3.1 specialized)</span>
<span class="n">tools</span> <span class="o">=</span> <span class="p">[</span>
    <span class="p">{</span>
        <span class="s2">&quot;type&quot;</span><span class="p">:</span> <span class="s2">&quot;function&quot;</span><span class="p">,</span>
        <span class="s2">&quot;function&quot;</span><span class="p">:</span> <span class="p">{</span>
            <span class="s2">&quot;name&quot;</span><span class="p">:</span> <span class="s2">&quot;get_weather&quot;</span><span class="p">,</span>
            <span class="s2">&quot;description&quot;</span><span class="p">:</span> <span class="s2">&quot;Get current weather for a location&quot;</span><span class="p">,</span>
            <span class="s2">&quot;parameters&quot;</span><span class="p">:</span> <span class="p">{</span>
                <span class="s2">&quot;type&quot;</span><span class="p">:</span> <span class="s2">&quot;object&quot;</span><span class="p">,</span>
                <span class="s2">&quot;properties&quot;</span><span class="p">:</span> <span class="p">{</span>
                    <span class="s2">&quot;location&quot;</span><span class="p">:</span> <span class="p">{</span><span class="s2">&quot;type&quot;</span><span class="p">:</span> <span class="s2">&quot;string&quot;</span><span class="p">,</span> <span class="s2">&quot;description&quot;</span><span class="p">:</span> <span class="s2">&quot;City name&quot;</span><span class="p">}</span>
                <span class="p">},</span>
                <span class="s2">&quot;required&quot;</span><span class="p">:</span> <span class="p">[</span><span class="s2">&quot;location&quot;</span><span class="p">]</span>
            <span class="p">}</span>
        <span class="p">}</span>
    <span class="p">}</span>
<span class="p">]</span>

<span class="n">messages</span> <span class="o">=</span> <span class="p">[</span>
    <span class="p">{</span><span class="s2">&quot;role&quot;</span><span class="p">:</span> <span class="s2">&quot;system&quot;</span><span class="p">,</span> <span class="s2">&quot;content&quot;</span><span class="p">:</span> <span class="s2">&quot;You are a helpful assistant with access to tools.&quot;</span><span class="p">},</span>
    <span class="p">{</span><span class="s2">&quot;role&quot;</span><span class="p">:</span> <span class="s2">&quot;user&quot;</span><span class="p">,</span> <span class="s2">&quot;content&quot;</span><span class="p">:</span> <span class="s2">&quot;What&#39;s the weather in Seoul?&quot;</span><span class="p">}</span>
<span class="p">]</span>

<span class="c1"># Generate tool call</span>
<span class="n">inputs</span> <span class="o">=</span> <span class="n">tokenizer</span><span class="o">.</span><span class="n">apply_chat_template</span><span class="p">(</span>
    <span class="n">messages</span><span class="p">,</span>
    <span class="n">tools</span><span class="o">=</span><span class="n">tools</span><span class="p">,</span>
    <span class="n">return_tensors</span><span class="o">=</span><span class="s2">&quot;pt&quot;</span>
<span class="p">)</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">model</span><span class="o">.</span><span class="n">device</span><span class="p">)</span>

<span class="n">outputs</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">generate</span><span class="p">(</span><span class="n">inputs</span><span class="p">,</span> <span class="n">max_new_tokens</span><span class="o">=</span><span class="mi">256</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">tokenizer</span><span class="o">.</span><span class="n">decode</span><span class="p">(</span><span class="n">outputs</span><span class="p">[</span><span class="mi">0</span><span class="p">]))</span>
</code></pre></div>

<h3 id="82-llama-32-september-2024">8.2 LLaMA 3.2 (September 2024)<a class="header-link" href="#82-llama-32-september-2024" title="Permanent link">&para;</a></h3>
<div class="highlight"><pre><span></span><code>â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚<span class="w">                    </span><span class="nv">LLaMA</span><span class="w"> </span><span class="mi">3</span>.<span class="mi">2</span><span class="w"> </span><span class="nv">Model</span><span class="w"> </span><span class="nv">Lineup</span><span class="w">                        </span>â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚<span class="w">                                                                 </span>â”‚
â”‚<span class="w">  </span><span class="nv">Lightweight</span><span class="w"> </span><span class="nv">Text</span><span class="w"> </span><span class="nv">Models</span><span class="w"> </span><span class="ss">(</span><span class="nv">on</span><span class="o">-</span><span class="nv">device</span><span class="w"> </span><span class="nv">optimized</span><span class="ss">)</span>:<span class="w">                 </span>â”‚
â”‚<span class="w">  </span>â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”<span class="w">                </span>â”‚
â”‚<span class="w">  </span>â”‚<span class="w">  </span><span class="nv">LLaMA</span><span class="w"> </span><span class="mi">3</span>.<span class="mi">2</span><span class="w"> </span><span class="mi">1</span><span class="nv">B</span>:<span class="w"> </span><span class="k">For</span><span class="w"> </span><span class="nv">mobile</span><span class="o">/</span><span class="nv">edge</span><span class="w"> </span><span class="nv">devices</span><span class="w">      </span>â”‚<span class="w">                </span>â”‚
â”‚<span class="w">  </span>â”‚<span class="w">  </span><span class="nv">LLaMA</span><span class="w"> </span><span class="mi">3</span>.<span class="mi">2</span><span class="w"> </span><span class="mi">3</span><span class="nv">B</span>:<span class="w"> </span><span class="k">For</span><span class="w"> </span><span class="nv">lightweight</span><span class="w"> </span><span class="nv">applications</span><span class="w"> </span>â”‚<span class="w">                </span>â”‚
â”‚<span class="w">  </span>â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜<span class="w">                </span>â”‚
â”‚<span class="w">                                                                 </span>â”‚
â”‚<span class="w">  </span><span class="nv">Vision</span><span class="w"> </span><span class="nv">Models</span><span class="w"> </span><span class="ss">(</span><span class="nv">multimodal</span><span class="ss">)</span>:<span class="w">                                    </span>â”‚
â”‚<span class="w">  </span>â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”<span class="w">                </span>â”‚
â”‚<span class="w">  </span>â”‚<span class="w">  </span><span class="nv">LLaMA</span><span class="w"> </span><span class="mi">3</span>.<span class="mi">2</span><span class="w"> </span><span class="mi">11</span><span class="nv">B</span><span class="o">-</span><span class="nv">Vision</span>:<span class="w"> </span><span class="nv">Image</span><span class="w"> </span><span class="nv">understanding</span><span class="w">  </span>â”‚<span class="w">                </span>â”‚
â”‚<span class="w">  </span>â”‚<span class="w">  </span><span class="nv">LLaMA</span><span class="w"> </span><span class="mi">3</span>.<span class="mi">2</span><span class="w"> </span><span class="mi">90</span><span class="nv">B</span><span class="o">-</span><span class="nv">Vision</span>:<span class="w"> </span><span class="nv">High</span><span class="o">-</span><span class="nv">performance</span><span class="w">     </span>â”‚<span class="w">                </span>â”‚
â”‚<span class="w">  </span>â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜<span class="w">                </span>â”‚
â”‚<span class="w">                                                                 </span>â”‚
â”‚<span class="w">  </span><span class="nv">Features</span>:<span class="w">                                                       </span>â”‚
â”‚<span class="w">  </span>â€¢<span class="w"> </span><span class="mi">1</span><span class="nv">B</span><span class="o">/</span><span class="mi">3</span><span class="nv">B</span>:<span class="w"> </span><span class="mi">128</span><span class="nv">K</span><span class="w"> </span><span class="nv">context</span>,<span class="w"> </span><span class="nv">on</span><span class="o">-</span><span class="nv">device</span><span class="w"> </span><span class="nv">inference</span><span class="w"> </span><span class="nv">capable</span><span class="w">             </span>â”‚
â”‚<span class="w">  </span>â€¢<span class="w"> </span><span class="mi">11</span><span class="nv">B</span><span class="o">/</span><span class="mi">90</span><span class="nv">B</span>:<span class="w"> </span><span class="nv">Vision</span><span class="w"> </span><span class="nv">encoder</span><span class="w"> </span><span class="nv">integrated</span>,<span class="w"> </span><span class="nv">image</span><span class="o">+</span><span class="nv">text</span><span class="w"> </span><span class="nv">processing</span><span class="w">    </span>â”‚
â”‚<span class="w">  </span>â€¢<span class="w"> </span><span class="nv">Optimized</span><span class="w"> </span><span class="k">for</span><span class="w"> </span><span class="nv">Qualcomm</span>,<span class="w"> </span><span class="nv">MediaTek</span><span class="w"> </span><span class="nv">hardware</span><span class="w">                    </span>â”‚
â”‚<span class="w">                                                                 </span>â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
</code></pre></div>

<div class="highlight"><pre><span></span><code><span class="c1"># LLaMA 3.2 Vision Example</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">transformers</span><span class="w"> </span><span class="kn">import</span> <span class="n">MllamaForConditionalGeneration</span><span class="p">,</span> <span class="n">AutoProcessor</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">PIL</span><span class="w"> </span><span class="kn">import</span> <span class="n">Image</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">requests</span>

<span class="n">model_id</span> <span class="o">=</span> <span class="s2">&quot;meta-llama/Llama-3.2-11B-Vision-Instruct&quot;</span>

<span class="n">model</span> <span class="o">=</span> <span class="n">MllamaForConditionalGeneration</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span>
    <span class="n">model_id</span><span class="p">,</span>
    <span class="n">torch_dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">bfloat16</span><span class="p">,</span>
    <span class="n">device_map</span><span class="o">=</span><span class="s2">&quot;auto&quot;</span><span class="p">,</span>
<span class="p">)</span>
<span class="n">processor</span> <span class="o">=</span> <span class="n">AutoProcessor</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="n">model_id</span><span class="p">)</span>

<span class="c1"># Load image</span>
<span class="n">url</span> <span class="o">=</span> <span class="s2">&quot;https://example.com/image.jpg&quot;</span>
<span class="n">image</span> <span class="o">=</span> <span class="n">Image</span><span class="o">.</span><span class="n">open</span><span class="p">(</span><span class="n">requests</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="n">url</span><span class="p">,</span> <span class="n">stream</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span><span class="o">.</span><span class="n">raw</span><span class="p">)</span>

<span class="c1"># Vision conversation</span>
<span class="n">messages</span> <span class="o">=</span> <span class="p">[</span>
    <span class="p">{</span>
        <span class="s2">&quot;role&quot;</span><span class="p">:</span> <span class="s2">&quot;user&quot;</span><span class="p">,</span>
        <span class="s2">&quot;content&quot;</span><span class="p">:</span> <span class="p">[</span>
            <span class="p">{</span><span class="s2">&quot;type&quot;</span><span class="p">:</span> <span class="s2">&quot;image&quot;</span><span class="p">},</span>
            <span class="p">{</span><span class="s2">&quot;type&quot;</span><span class="p">:</span> <span class="s2">&quot;text&quot;</span><span class="p">,</span> <span class="s2">&quot;text&quot;</span><span class="p">:</span> <span class="s2">&quot;What do you see in this image?&quot;</span><span class="p">}</span>
        <span class="p">]</span>
    <span class="p">}</span>
<span class="p">]</span>

<span class="n">input_text</span> <span class="o">=</span> <span class="n">processor</span><span class="o">.</span><span class="n">apply_chat_template</span><span class="p">(</span><span class="n">messages</span><span class="p">,</span> <span class="n">add_generation_prompt</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="n">inputs</span> <span class="o">=</span> <span class="n">processor</span><span class="p">(</span><span class="n">image</span><span class="p">,</span> <span class="n">input_text</span><span class="p">,</span> <span class="n">return_tensors</span><span class="o">=</span><span class="s2">&quot;pt&quot;</span><span class="p">)</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">model</span><span class="o">.</span><span class="n">device</span><span class="p">)</span>

<span class="n">output</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">generate</span><span class="p">(</span><span class="o">**</span><span class="n">inputs</span><span class="p">,</span> <span class="n">max_new_tokens</span><span class="o">=</span><span class="mi">256</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">processor</span><span class="o">.</span><span class="n">decode</span><span class="p">(</span><span class="n">output</span><span class="p">[</span><span class="mi">0</span><span class="p">]))</span>
</code></pre></div>

<hr />
<h2 id="summary">Summary<a class="header-link" href="#summary" title="Permanent link">&para;</a></h2>
<h3 id="llama-core-technologies">LLaMA Core Technologies<a class="header-link" href="#llama-core-technologies" title="Permanent link">&para;</a></h3>
<table>
<thead>
<tr>
<th>Technology</th>
<th>Effect</th>
</tr>
</thead>
<tbody>
<tr>
<td><strong>RoPE</strong></td>
<td>Relative position encoding, length extension possible</td>
</tr>
<tr>
<td><strong>RMSNorm</strong></td>
<td>Faster and more effective than LayerNorm</td>
</tr>
<tr>
<td><strong>SwiGLU</strong></td>
<td>Better performance than GELU</td>
</tr>
<tr>
<td><strong>GQA</strong></td>
<td>8x KV cache memory savings</td>
</tr>
</tbody>
</table>
<h3 id="practical-recommendations">Practical Recommendations<a class="header-link" href="#practical-recommendations" title="Permanent link">&para;</a></h3>
<ol>
<li><strong>7B/8B</strong>: Single GPU (16GB+), for quick experiments</li>
<li><strong>13B</strong>: 24GB GPU, balanced choice</li>
<li><strong>70B</strong>: Multiple GPUs, when top performance needed</li>
<li><strong>Quantization</strong>: 75% memory savings with 4-bit</li>
</ol>
<h3 id="next-steps">Next Steps<a class="header-link" href="#next-steps" title="Permanent link">&para;</a></h3>
<ul>
<li><a href="09_Mistral_MoE.md">09_Mistral_MoE.md</a>: Mixture of Experts architecture</li>
<li><a href="19_PEFT_Unified.md">19_PEFT_Unified.md</a>: LLaMA Fine-tuning (LoRA)</li>
</ul>
<hr />
<h2 id="references">References<a class="header-link" href="#references" title="Permanent link">&para;</a></h2>
<h3 id="core-papers">Core Papers<a class="header-link" href="#core-papers" title="Permanent link">&para;</a></h3>
<ul>
<li>Touvron et al. (2023). "LLaMA: Open and Efficient Foundation Language Models"</li>
<li>Touvron et al. (2023). "LLaMA 2: Open Foundation and Fine-Tuned Chat Models"</li>
<li>Su et al. (2021). "RoFormer: Enhanced Transformer with Rotary Position Embedding"</li>
<li>Ainslie et al. (2023). "GQA: Training Generalized Multi-Query Transformer Models"</li>
</ul>
<h3 id="code-resources">Code &amp; Resources<a class="header-link" href="#code-resources" title="Permanent link">&para;</a></h3>
<ul>
<li><a href="https://github.com/facebookresearch/llama">LLaMA GitHub</a></li>
<li><a href="https://huggingface.co/meta-llama">HuggingFace LLaMA</a></li>
<li><a href="https://github.com/meta-llama/llama-recipes">LLaMA 3 Recipes</a></li>
</ul>
    </div>

    
<div class="lesson-toolbar lesson-toolbar--bottom">
    <div class="toolbar-nav toolbar-nav--prev">
        
        <a href="/study/en/Foundation_Models/07_Tokenization_Deep_Dive.html" class="nav-prev">
            <span class="nav-label">Previous</span>
            <span class="nav-title">07. Tokenization Deep Dive</span>
        </a>
        
    </div>
    <div class="toolbar-actions">
        <button class="btn btn-copy-link" data-action="copy-link" title="Copy link">
            <span class="icon">ğŸ”—</span>
            <span class="text">Copy link</span>
        </button>
        <a href="/study/en/Foundation_Models/" class="btn btn-topic-link" title="Back to topic list">
            <span class="icon">ğŸ“‹</span>
            <span class="text">Topic list</span>
        </a>
    </div>
    <div class="toolbar-nav toolbar-nav--next">
        
        <a href="/study/en/Foundation_Models/09_Mistral_MoE.html" class="nav-next">
            <span class="nav-label">Next</span>
            <span class="nav-title">Mistral & Mixture of Experts</span>
        </a>
        
    </div>
</div>


    <div class="toolbar-keyboard-hint">
        <kbd>&larr;</kbd> <kbd>&rarr;</kbd> to navigate between lessons
    </div>
</article>

<!-- Scroll to top floating button -->
<button class="scroll-to-top" id="scroll-to-top" title="Scroll to top" aria-label="Scroll to top">â†‘</button>

            </div>
        </main>
    </div>

    <script src="/study/static/js/app.js"></script>
    <script>
        function switchLanguage(newLang) {
            const path = window.location.pathname;
            const base = '/study';
            const currentLang = 'en';
            const newPath = path.replace(base + '/' + currentLang + '/', base + '/' + newLang + '/');
            window.location.href = newPath + window.location.search;
        }
    </script>
    
<script>
    document.addEventListener('DOMContentLoaded', function() {
        // Copy link - independent feedback per button
        document.querySelectorAll('[data-action="copy-link"]').forEach(function(btn) {
            btn.addEventListener('click', function() {
                navigator.clipboard.writeText(window.location.href);
                var textEl = this.querySelector('.text');
                var originalText = textEl.textContent;
                textEl.textContent = 'Copied!';
                setTimeout(function() { textEl.textContent = originalText; }, 2000);
            });
        });

        // Add copy buttons to code blocks
        document.querySelectorAll('.lesson-content pre').forEach(function(pre) {
            var copyBtn = document.createElement('button');
            copyBtn.className = 'code-copy-btn';
            copyBtn.textContent = 'Copy';
            copyBtn.addEventListener('click', function() {
                var code = pre.querySelector('code');
                var text = code ? code.textContent : pre.textContent;
                navigator.clipboard.writeText(text);
                copyBtn.textContent = 'Copied!';
                setTimeout(function() { copyBtn.textContent = 'Copy'; }, 2000);
            });
            pre.style.position = 'relative';
            pre.appendChild(copyBtn);
        });

        // Scroll to top button
        var scrollBtn = document.getElementById('scroll-to-top');
        window.addEventListener('scroll', function() {
            scrollBtn.classList.toggle('visible', window.scrollY > 300);
        });
        scrollBtn.addEventListener('click', function() {
            window.scrollTo({ top: 0, behavior: 'smooth' });
        });

        // Keyboard shortcuts: left/right arrows for lesson navigation
        document.addEventListener('keydown', function(e) {
            var tag = document.activeElement.tagName;
            if (tag === 'INPUT' || tag === 'TEXTAREA' || tag === 'SELECT') return;

            if (e.key === 'ArrowLeft') {
                var prevLink = document.querySelector('.nav-prev');
                if (prevLink) prevLink.click();
            } else if (e.key === 'ArrowRight') {
                var nextLink = document.querySelector('.nav-next');
                if (nextLink) nextLink.click();
            }
        });
    });
</script>

</body>
</html>
{% endraw %}