{% raw %}
<!DOCTYPE html>
<html lang="en" data-theme="light">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>13. Model Quantization - Study Materials</title>
    <link rel="stylesheet" href="/study/static/css/style.css">
    <link rel="stylesheet" href="/study/static/css/highlight.css">
    <!-- KaTeX for LaTeX math rendering -->
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.11/dist/katex.min.css">
    <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.11/dist/katex.min.js"></script>
    <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.11/dist/contrib/auto-render.min.js"
        onload="renderMathInElement(document.body, {
            delimiters: [
                {left: '$$', right: '$$', display: true},
                {left: '$', right: '$', display: false}
            ],
            throwOnError: false
        });"></script>
    
</head>
<body>
    <div class="app-container">
        <!-- Sidebar -->
        <aside class="sidebar" id="sidebar">
            <div class="sidebar-header">
                <a href="/study/en/" class="logo">Study Materials</a>
                <button class="sidebar-toggle" id="sidebar-toggle" aria-label="Toggle sidebar">
                    <span></span>
                </button>
            </div>

            <nav class="sidebar-nav">
                <a href="/study/en/" class="nav-item ">
                    <span class="nav-icon">ğŸ </span>
                    <span class="nav-text">Home</span>
                </a>
                <a href="/study/examples/" class="nav-item ">
                    <span class="nav-icon">ğŸ’»</span>
                    <span class="nav-text">Examples</span>
                </a>
            </nav>

            <div class="sidebar-search">
                <form action="/study/en/search.html" method="get" id="search-form">
                    <input type="search" name="q" placeholder="Search..." id="search-sidebar-input">
                </form>
            </div>

            <!-- Language Selector -->
            <div class="sidebar-lang">
                <select id="lang-select" class="lang-selector" onchange="switchLanguage(this.value)">
                    
                    <option value="en" selected>
                        English
                    </option>
                    
                    <option value="ko" >
                        í•œêµ­ì–´
                    </option>
                    
                </select>
            </div>

            <div class="sidebar-footer">
                <button class="theme-toggle" id="theme-toggle" aria-label="Toggle theme">
                    <span class="theme-icon light">â˜€ï¸</span>
                    <span class="theme-icon dark">ğŸŒ™</span>
                </button>
            </div>
        </aside>

        <!-- Main Content -->
        <main class="main-content">
            <header class="main-header">
                <button class="menu-toggle" id="menu-toggle" aria-label="Open menu">
                    <span></span>
                </button>
                
<nav class="breadcrumb">
    <a href="/study/en/">Topics</a>
    <span class="separator">/</span>
    <a href="/study/en/LLM_and_NLP/">LLM and NLP</a>
    <span class="separator">/</span>
    <span class="current">13. Model Quantization</span>
</nav>

            </header>

            <div class="content">
                
<article class="lesson-article">
    <header class="lesson-header">
        <h1>13. Model Quantization</h1>
    </header>

    
<div class="lesson-toolbar lesson-toolbar--top">
    <div class="toolbar-nav toolbar-nav--prev">
        
        <a href="/study/en/LLM_and_NLP/12_Practical_Chatbot.html" class="nav-prev">
            <span class="nav-label">Previous</span>
            <span class="nav-title">12. Practical Chatbot Project</span>
        </a>
        
    </div>
    <div class="toolbar-actions">
        <button class="btn btn-copy-link" data-action="copy-link" title="Copy link">
            <span class="icon">ğŸ”—</span>
            <span class="text">Copy link</span>
        </button>
        <a href="/study/en/LLM_and_NLP/" class="btn btn-topic-link" title="Back to topic list">
            <span class="icon">ğŸ“‹</span>
            <span class="text">Topic list</span>
        </a>
    </div>
    <div class="toolbar-nav toolbar-nav--next">
        
        <a href="/study/en/LLM_and_NLP/14_RLHF_Alignment.html" class="nav-next">
            <span class="nav-label">Next</span>
            <span class="nav-title">14. RLHF and LLM Alignment</span>
        </a>
        
    </div>
</div>


    
    <nav class="toc" id="toc">
        <h2>Table of Contents</h2>
        <div class="toc">
<ul>
<li><a href="#learning-objectives">Learning Objectives</a></li>
<li><a href="#1-quantization-overview">1. Quantization Overview</a><ul>
<li><a href="#why-is-quantization-needed">Why is Quantization Needed?</a></li>
<li><a href="#quantization-types">Quantization Types</a></li>
<li><a href="#bit-precision-comparison">Bit Precision Comparison</a></li>
</ul>
</li>
<li><a href="#2-quantization-mathematics">2. Quantization Mathematics</a><ul>
<li><a href="#uniform-quantization">Uniform Quantization</a></li>
<li><a href="#asymmetric-quantization">Asymmetric Quantization</a></li>
<li><a href="#group-quantization">Group Quantization</a></li>
</ul>
</li>
<li><a href="#3-bitsandbytes-library">3. bitsandbytes Library</a><ul>
<li><a href="#installation">Installation</a></li>
<li><a href="#8-bit-quantization">8-bit Quantization</a></li>
<li><a href="#4-bit-quantization-nf4">4-bit Quantization (NF4)</a></li>
<li><a href="#nf4-vs-fp4">NF4 vs FP4</a></li>
</ul>
</li>
<li><a href="#4-gptq-gpu-optimized-post-training-quantization">4. GPTQ (GPU-optimized Post-Training Quantization)</a><ul>
<li><a href="#concept">Concept</a></li>
<li><a href="#performing-gptq-quantization">Performing GPTQ Quantization</a></li>
<li><a href="#using-autogptq">Using AutoGPTQ</a></li>
<li><a href="#using-pre-quantized-models">Using Pre-quantized Models</a></li>
</ul>
</li>
<li><a href="#5-awq-activation-aware-weight-quantization">5. AWQ (Activation-aware Weight Quantization)</a><ul>
<li><a href="#concept_1">Concept</a></li>
<li><a href="#awq-quantization">AWQ Quantization</a></li>
<li><a href="#awq-model-inference">AWQ Model Inference</a></li>
</ul>
</li>
<li><a href="#6-qlora-quantized-lora">6. QLoRA (Quantized LoRA)</a><ul>
<li><a href="#concept_2">Concept</a></li>
<li><a href="#qlora-fine-tuning">QLoRA Fine-tuning</a></li>
<li><a href="#merging-qlora-model">Merging QLoRA Model</a></li>
</ul>
</li>
<li><a href="#7-quantization-performance-comparison">7. Quantization Performance Comparison</a><ul>
<li><a href="#benchmark">Benchmark</a></li>
<li><a href="#accuracy-evaluation">Accuracy Evaluation</a></li>
</ul>
</li>
<li><a href="#8-practical-guide">8. Practical Guide</a><ul>
<li><a href="#choosing-quantization-method">Choosing Quantization Method</a></li>
<li><a href="#troubleshooting">Troubleshooting</a></li>
</ul>
</li>
<li><a href="#summary">Summary</a><ul>
<li><a href="#quantization-comparison-table">Quantization Comparison Table</a></li>
<li><a href="#core-code">Core Code</a></li>
</ul>
</li>
<li><a href="#next-steps">Next Steps</a></li>
</ul>
</div>

    </nav>
    

    <div class="lesson-content markdown-body">
        <h1 id="13-model-quantization">13. Model Quantization<a class="header-link" href="#13-model-quantization" title="Permanent link">&para;</a></h1>
<h2 id="learning-objectives">Learning Objectives<a class="header-link" href="#learning-objectives" title="Permanent link">&para;</a></h2>
<ul>
<li>Understand the concept and necessity of quantization</li>
<li>INT8/INT4 quantization techniques</li>
<li>Practice with GPTQ, AWQ, bitsandbytes</li>
<li>Efficient fine-tuning with QLoRA</li>
</ul>
<hr />
<h2 id="1-quantization-overview">1. Quantization Overview<a class="header-link" href="#1-quantization-overview" title="Permanent link">&para;</a></h2>
<h3 id="why-is-quantization-needed">Why is Quantization Needed?<a class="header-link" href="#why-is-quantization-needed" title="Permanent link">&para;</a></h3>
<div class="highlight"><pre><span></span><code>â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    LLM Memory Requirements                   â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                              â”‚
â”‚  Model Size   â”‚  FP32      â”‚  FP16      â”‚  INT8    â”‚  INT4   â”‚
â”‚  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”‚
â”‚  7B params    â”‚  28GB      â”‚  14GB      â”‚  7GB     â”‚  3.5GB  â”‚
â”‚  13B params   â”‚  52GB      â”‚  26GB      â”‚  13GB    â”‚  6.5GB  â”‚
â”‚  70B params   â”‚  280GB     â”‚  140GB     â”‚  70GB    â”‚  35GB   â”‚
â”‚                                                              â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
</code></pre></div>

<h3 id="quantization-types">Quantization Types<a class="header-link" href="#quantization-types" title="Permanent link">&para;</a></h3>
<table>
<thead>
<tr>
<th>Type</th>
<th>Description</th>
<th>Advantages</th>
<th>Disadvantages</th>
</tr>
</thead>
<tbody>
<tr>
<td>Post-Training Quantization (PTQ)</td>
<td>Quantize after training</td>
<td>Fast, simple</td>
<td>Possible accuracy loss</td>
</tr>
<tr>
<td>Quantization-Aware Training (QAT)</td>
<td>Simulate quantization during training</td>
<td>High accuracy</td>
<td>Increased training time</td>
</tr>
<tr>
<td>Dynamic Quantization</td>
<td>Runtime quantization</td>
<td>Flexible</td>
<td>Inference overhead</td>
</tr>
<tr>
<td>Static Quantization</td>
<td>Calibration-based</td>
<td>Fast inference</td>
<td>Calibration required</td>
</tr>
</tbody>
</table>
<h3 id="bit-precision-comparison">Bit Precision Comparison<a class="header-link" href="#bit-precision-comparison" title="Permanent link">&para;</a></h3>
<div class="highlight"><pre><span></span><code><span class="c1"># FP32 (32-bit floating point)</span>
<span class="c1"># Sign 1bit + Exponent 8bit + Mantissa 23bit</span>
<span class="c1"># Range: Â±3.4 Ã— 10^38, Precision: ~7 digits</span>

<span class="c1"># FP16 (16-bit floating point)</span>
<span class="c1"># Sign 1bit + Exponent 5bit + Mantissa 10bit</span>
<span class="c1"># Range: Â±65,504, Precision: ~3 digits</span>

<span class="c1"># BF16 (Brain Float 16)</span>
<span class="c1"># Sign 1bit + Exponent 8bit + Mantissa 7bit</span>
<span class="c1"># Same range as FP32, lower precision</span>

<span class="c1"># INT8 (8-bit integer)</span>
<span class="c1"># Range: -128 ~ 127 or 0 ~ 255</span>

<span class="c1"># INT4 (4-bit integer)</span>
<span class="c1"># Range: -8 ~ 7 or 0 ~ 15</span>
</code></pre></div>

<hr />
<h2 id="2-quantization-mathematics">2. Quantization Mathematics<a class="header-link" href="#2-quantization-mathematics" title="Permanent link">&para;</a></h2>
<h3 id="uniform-quantization">Uniform Quantization<a class="header-link" href="#uniform-quantization" title="Permanent link">&para;</a></h3>
<div class="highlight"><pre><span></span><code><span class="kn">import</span><span class="w"> </span><span class="nn">numpy</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">np</span>

<span class="k">def</span><span class="w"> </span><span class="nf">quantize_symmetric</span><span class="p">(</span><span class="n">tensor</span><span class="p">,</span> <span class="n">bits</span><span class="o">=</span><span class="mi">8</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Symmetric quantization&quot;&quot;&quot;</span>
    <span class="n">qmin</span> <span class="o">=</span> <span class="o">-</span><span class="p">(</span><span class="mi">2</span> <span class="o">**</span> <span class="p">(</span><span class="n">bits</span> <span class="o">-</span> <span class="mi">1</span><span class="p">))</span>
    <span class="n">qmax</span> <span class="o">=</span> <span class="mi">2</span> <span class="o">**</span> <span class="p">(</span><span class="n">bits</span> <span class="o">-</span> <span class="mi">1</span><span class="p">)</span> <span class="o">-</span> <span class="mi">1</span>

    <span class="c1"># Calculate scale</span>
    <span class="n">abs_max</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">abs</span><span class="p">(</span><span class="n">tensor</span><span class="p">)</span><span class="o">.</span><span class="n">max</span><span class="p">()</span>
    <span class="n">scale</span> <span class="o">=</span> <span class="n">abs_max</span> <span class="o">/</span> <span class="n">qmax</span>

    <span class="c1"># Quantize</span>
    <span class="n">quantized</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">round</span><span class="p">(</span><span class="n">tensor</span> <span class="o">/</span> <span class="n">scale</span><span class="p">)</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">int8</span><span class="p">)</span>
    <span class="n">quantized</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">clip</span><span class="p">(</span><span class="n">quantized</span><span class="p">,</span> <span class="n">qmin</span><span class="p">,</span> <span class="n">qmax</span><span class="p">)</span>

    <span class="k">return</span> <span class="n">quantized</span><span class="p">,</span> <span class="n">scale</span>

<span class="k">def</span><span class="w"> </span><span class="nf">dequantize</span><span class="p">(</span><span class="n">quantized</span><span class="p">,</span> <span class="n">scale</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Dequantization&quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="n">quantized</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span> <span class="o">*</span> <span class="n">scale</span>


<span class="c1"># Test</span>
<span class="n">original</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="mf">0.5</span><span class="p">,</span> <span class="o">-</span><span class="mf">1.2</span><span class="p">,</span> <span class="mf">0.3</span><span class="p">,</span> <span class="mf">2.1</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.8</span><span class="p">],</span> <span class="n">dtype</span><span class="o">=</span><span class="n">np</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>
<span class="n">quantized</span><span class="p">,</span> <span class="n">scale</span> <span class="o">=</span> <span class="n">quantize_symmetric</span><span class="p">(</span><span class="n">original</span><span class="p">,</span> <span class="n">bits</span><span class="o">=</span><span class="mi">8</span><span class="p">)</span>
<span class="n">recovered</span> <span class="o">=</span> <span class="n">dequantize</span><span class="p">(</span><span class="n">quantized</span><span class="p">,</span> <span class="n">scale</span><span class="p">)</span>

<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Original: </span><span class="si">{</span><span class="n">original</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Quantized: </span><span class="si">{</span><span class="n">quantized</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Recovered: </span><span class="si">{</span><span class="n">recovered</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Error: </span><span class="si">{</span><span class="n">np</span><span class="o">.</span><span class="n">abs</span><span class="p">(</span><span class="n">original</span><span class="w"> </span><span class="o">-</span><span class="w"> </span><span class="n">recovered</span><span class="p">)</span><span class="o">.</span><span class="n">mean</span><span class="p">()</span><span class="si">:</span><span class="s2">.6f</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
</code></pre></div>

<h3 id="asymmetric-quantization">Asymmetric Quantization<a class="header-link" href="#asymmetric-quantization" title="Permanent link">&para;</a></h3>
<div class="highlight"><pre><span></span><code><span class="k">def</span><span class="w"> </span><span class="nf">quantize_asymmetric</span><span class="p">(</span><span class="n">tensor</span><span class="p">,</span> <span class="n">bits</span><span class="o">=</span><span class="mi">8</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Asymmetric quantization (zero is exactly represented)&quot;&quot;&quot;</span>
    <span class="n">qmin</span> <span class="o">=</span> <span class="mi">0</span>
    <span class="n">qmax</span> <span class="o">=</span> <span class="mi">2</span> <span class="o">**</span> <span class="n">bits</span> <span class="o">-</span> <span class="mi">1</span>

    <span class="c1"># Scale and zero point</span>
    <span class="n">min_val</span> <span class="o">=</span> <span class="n">tensor</span><span class="o">.</span><span class="n">min</span><span class="p">()</span>
    <span class="n">max_val</span> <span class="o">=</span> <span class="n">tensor</span><span class="o">.</span><span class="n">max</span><span class="p">()</span>
    <span class="n">scale</span> <span class="o">=</span> <span class="p">(</span><span class="n">max_val</span> <span class="o">-</span> <span class="n">min_val</span><span class="p">)</span> <span class="o">/</span> <span class="p">(</span><span class="n">qmax</span> <span class="o">-</span> <span class="n">qmin</span><span class="p">)</span>
    <span class="n">zero_point</span> <span class="o">=</span> <span class="nb">round</span><span class="p">(</span><span class="o">-</span><span class="n">min_val</span> <span class="o">/</span> <span class="n">scale</span><span class="p">)</span>

    <span class="c1"># Quantize</span>
    <span class="n">quantized</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">round</span><span class="p">(</span><span class="n">tensor</span> <span class="o">/</span> <span class="n">scale</span> <span class="o">+</span> <span class="n">zero_point</span><span class="p">)</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">uint8</span><span class="p">)</span>
    <span class="n">quantized</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">clip</span><span class="p">(</span><span class="n">quantized</span><span class="p">,</span> <span class="n">qmin</span><span class="p">,</span> <span class="n">qmax</span><span class="p">)</span>

    <span class="k">return</span> <span class="n">quantized</span><span class="p">,</span> <span class="n">scale</span><span class="p">,</span> <span class="n">zero_point</span>

<span class="k">def</span><span class="w"> </span><span class="nf">dequantize_asymmetric</span><span class="p">(</span><span class="n">quantized</span><span class="p">,</span> <span class="n">scale</span><span class="p">,</span> <span class="n">zero_point</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Asymmetric dequantization&quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="p">(</span><span class="n">quantized</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span> <span class="o">-</span> <span class="n">zero_point</span><span class="p">)</span> <span class="o">*</span> <span class="n">scale</span>
</code></pre></div>

<h3 id="group-quantization">Group Quantization<a class="header-link" href="#group-quantization" title="Permanent link">&para;</a></h3>
<div class="highlight"><pre><span></span><code><span class="k">def</span><span class="w"> </span><span class="nf">group_quantize</span><span class="p">(</span><span class="n">tensor</span><span class="p">,</span> <span class="n">group_size</span><span class="o">=</span><span class="mi">128</span><span class="p">,</span> <span class="n">bits</span><span class="o">=</span><span class="mi">4</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Group-wise quantization - improved accuracy&quot;&quot;&quot;</span>
    <span class="c1"># Split tensor into groups</span>
    <span class="n">flat</span> <span class="o">=</span> <span class="n">tensor</span><span class="o">.</span><span class="n">flatten</span><span class="p">()</span>
    <span class="n">pad_size</span> <span class="o">=</span> <span class="p">(</span><span class="n">group_size</span> <span class="o">-</span> <span class="nb">len</span><span class="p">(</span><span class="n">flat</span><span class="p">)</span> <span class="o">%</span> <span class="n">group_size</span><span class="p">)</span> <span class="o">%</span> <span class="n">group_size</span>
    <span class="n">flat</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">pad</span><span class="p">(</span><span class="n">flat</span><span class="p">,</span> <span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">pad_size</span><span class="p">))</span>

    <span class="n">groups</span> <span class="o">=</span> <span class="n">flat</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="n">group_size</span><span class="p">)</span>

    <span class="n">quantized_groups</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="n">scales</span> <span class="o">=</span> <span class="p">[]</span>

    <span class="k">for</span> <span class="n">group</span> <span class="ow">in</span> <span class="n">groups</span><span class="p">:</span>
        <span class="n">q</span><span class="p">,</span> <span class="n">s</span> <span class="o">=</span> <span class="n">quantize_symmetric</span><span class="p">(</span><span class="n">group</span><span class="p">,</span> <span class="n">bits</span><span class="p">)</span>
        <span class="n">quantized_groups</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">q</span><span class="p">)</span>
        <span class="n">scales</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">s</span><span class="p">)</span>

    <span class="k">return</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">quantized_groups</span><span class="p">),</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">scales</span><span class="p">)</span>
</code></pre></div>

<hr />
<h2 id="3-bitsandbytes-library">3. bitsandbytes Library<a class="header-link" href="#3-bitsandbytes-library" title="Permanent link">&para;</a></h2>
<h3 id="installation">Installation<a class="header-link" href="#installation" title="Permanent link">&para;</a></h3>
<div class="highlight"><pre><span></span><code>pip<span class="w"> </span>install<span class="w"> </span>bitsandbytes
pip<span class="w"> </span>install<span class="w"> </span>accelerate
</code></pre></div>

<h3 id="8-bit-quantization">8-bit Quantization<a class="header-link" href="#8-bit-quantization" title="Permanent link">&para;</a></h3>
<div class="highlight"><pre><span></span><code><span class="kn">from</span><span class="w"> </span><span class="nn">transformers</span><span class="w"> </span><span class="kn">import</span> <span class="n">AutoModelForCausalLM</span><span class="p">,</span> <span class="n">AutoTokenizer</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">torch</span>

<span class="c1"># Load in 8-bit</span>
<span class="n">model_8bit</span> <span class="o">=</span> <span class="n">AutoModelForCausalLM</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span>
    <span class="s2">&quot;meta-llama/Llama-2-7b-hf&quot;</span><span class="p">,</span>
    <span class="n">load_in_8bit</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
    <span class="n">device_map</span><span class="o">=</span><span class="s2">&quot;auto&quot;</span>
<span class="p">)</span>

<span class="n">tokenizer</span> <span class="o">=</span> <span class="n">AutoTokenizer</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="s2">&quot;meta-llama/Llama-2-7b-hf&quot;</span><span class="p">)</span>

<span class="c1"># Check memory</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;8bit model memory: </span><span class="si">{</span><span class="n">model_8bit</span><span class="o">.</span><span class="n">get_memory_footprint</span><span class="p">()</span><span class="w"> </span><span class="o">/</span><span class="w"> </span><span class="mf">1e9</span><span class="si">:</span><span class="s2">.2f</span><span class="si">}</span><span class="s2"> GB&quot;</span><span class="p">)</span>

<span class="c1"># Inference</span>
<span class="n">inputs</span> <span class="o">=</span> <span class="n">tokenizer</span><span class="p">(</span><span class="s2">&quot;Hello, my name is&quot;</span><span class="p">,</span> <span class="n">return_tensors</span><span class="o">=</span><span class="s2">&quot;pt&quot;</span><span class="p">)</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="s2">&quot;cuda&quot;</span><span class="p">)</span>
<span class="n">outputs</span> <span class="o">=</span> <span class="n">model_8bit</span><span class="o">.</span><span class="n">generate</span><span class="p">(</span><span class="o">**</span><span class="n">inputs</span><span class="p">,</span> <span class="n">max_new_tokens</span><span class="o">=</span><span class="mi">50</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">tokenizer</span><span class="o">.</span><span class="n">decode</span><span class="p">(</span><span class="n">outputs</span><span class="p">[</span><span class="mi">0</span><span class="p">]))</span>
</code></pre></div>

<h3 id="4-bit-quantization-nf4">4-bit Quantization (NF4)<a class="header-link" href="#4-bit-quantization-nf4" title="Permanent link">&para;</a></h3>
<div class="highlight"><pre><span></span><code><span class="kn">from</span><span class="w"> </span><span class="nn">transformers</span><span class="w"> </span><span class="kn">import</span> <span class="n">BitsAndBytesConfig</span>

<span class="c1"># 4-bit configuration</span>
<span class="n">bnb_config</span> <span class="o">=</span> <span class="n">BitsAndBytesConfig</span><span class="p">(</span>
    <span class="n">load_in_4bit</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
    <span class="n">bnb_4bit_quant_type</span><span class="o">=</span><span class="s2">&quot;nf4&quot;</span><span class="p">,</span>          <span class="c1"># Normal Float 4 (optimized data type)</span>
    <span class="n">bnb_4bit_compute_dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">bfloat16</span><span class="p">,</span>  <span class="c1"># Computation data type</span>
    <span class="n">bnb_4bit_use_double_quant</span><span class="o">=</span><span class="kc">True</span>      <span class="c1"># Double quantization (quantize scales too)</span>
<span class="p">)</span>

<span class="n">model_4bit</span> <span class="o">=</span> <span class="n">AutoModelForCausalLM</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span>
    <span class="s2">&quot;meta-llama/Llama-2-7b-hf&quot;</span><span class="p">,</span>
    <span class="n">quantization_config</span><span class="o">=</span><span class="n">bnb_config</span><span class="p">,</span>
    <span class="n">device_map</span><span class="o">=</span><span class="s2">&quot;auto&quot;</span>
<span class="p">)</span>

<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;4bit model memory: </span><span class="si">{</span><span class="n">model_4bit</span><span class="o">.</span><span class="n">get_memory_footprint</span><span class="p">()</span><span class="w"> </span><span class="o">/</span><span class="w"> </span><span class="mf">1e9</span><span class="si">:</span><span class="s2">.2f</span><span class="si">}</span><span class="s2"> GB&quot;</span><span class="p">)</span>
</code></pre></div>

<h3 id="nf4-vs-fp4">NF4 vs FP4<a class="header-link" href="#nf4-vs-fp4" title="Permanent link">&para;</a></h3>
<div class="highlight"><pre><span></span><code><span class="c1"># NF4 (Normal Float 4)</span>
<span class="c1"># - Optimal quantization assuming normal distribution</span>
<span class="c1"># - Optimized for LLM weights</span>

<span class="c1"># FP4 (Floating Point 4)</span>
<span class="c1"># - General 4-bit floating point</span>
<span class="c1"># - General purpose</span>

<span class="n">bnb_config_fp4</span> <span class="o">=</span> <span class="n">BitsAndBytesConfig</span><span class="p">(</span>
    <span class="n">load_in_4bit</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
    <span class="n">bnb_4bit_quant_type</span><span class="o">=</span><span class="s2">&quot;fp4&quot;</span><span class="p">,</span>  <span class="c1"># Use FP4</span>
<span class="p">)</span>
</code></pre></div>

<hr />
<h2 id="4-gptq-gpu-optimized-post-training-quantization">4. GPTQ (GPU-optimized Post-Training Quantization)<a class="header-link" href="#4-gptq-gpu-optimized-post-training-quantization" title="Permanent link">&para;</a></h2>
<h3 id="concept">Concept<a class="header-link" href="#concept" title="Permanent link">&para;</a></h3>
<div class="highlight"><pre><span></span><code><span class="n">GPTQ</span><span class="w"> </span><span class="n">quantization</span><span class="w"> </span><span class="n">process</span><span class="p">:</span>
<span class="w">    </span><span class="mf">1.</span><span class="w"> </span><span class="n">Prepare</span><span class="w"> </span><span class="n">small</span><span class="w"> </span><span class="n">calibration</span><span class="w"> </span><span class="n">dataset</span>
<span class="w">    </span><span class="mf">2.</span><span class="w"> </span><span class="n">Layer</span><span class="o">-</span><span class="n">wise</span><span class="w"> </span><span class="n">sequential</span><span class="w"> </span><span class="n">quantization</span>
<span class="w">    </span><span class="mf">3.</span><span class="w"> </span><span class="n">Identify</span><span class="w"> </span><span class="n">important</span><span class="w"> </span><span class="n">weights</span><span class="w"> </span><span class="n">using</span><span class="w"> </span><span class="n">Hessian</span><span class="w"> </span><span class="n">matrix</span>
<span class="w">    </span><span class="mf">4.</span><span class="w"> </span><span class="n">Minimize</span><span class="w"> </span><span class="n">reconstruction</span><span class="w"> </span><span class="n">error</span>

<span class="n">Advantages</span><span class="p">:</span>
<span class="w">    </span><span class="o">-</span><span class="w"> </span><span class="n">High</span><span class="w"> </span><span class="n">compression</span><span class="w"> </span><span class="n">ratio</span><span class="w"> </span><span class="p">(</span><span class="mi">3</span><span class="o">-</span><span class="mi">4</span><span class="n">bit</span><span class="p">)</span>
<span class="w">    </span><span class="o">-</span><span class="w"> </span><span class="n">Fast</span><span class="w"> </span><span class="n">inference</span><span class="w"> </span><span class="n">speed</span>
<span class="w">    </span><span class="o">-</span><span class="w"> </span><span class="n">GPU</span><span class="w"> </span><span class="n">optimized</span>
</code></pre></div>

<h3 id="performing-gptq-quantization">Performing GPTQ Quantization<a class="header-link" href="#performing-gptq-quantization" title="Permanent link">&para;</a></h3>
<div class="highlight"><pre><span></span><code><span class="kn">from</span><span class="w"> </span><span class="nn">transformers</span><span class="w"> </span><span class="kn">import</span> <span class="n">AutoModelForCausalLM</span><span class="p">,</span> <span class="n">AutoTokenizer</span><span class="p">,</span> <span class="n">GPTQConfig</span>

<span class="c1"># Calibration data</span>
<span class="n">calibration_data</span> <span class="o">=</span> <span class="p">[</span>
    <span class="s2">&quot;The quick brown fox jumps over the lazy dog.&quot;</span><span class="p">,</span>
    <span class="s2">&quot;Machine learning is a subset of artificial intelligence.&quot;</span><span class="p">,</span>
    <span class="c1"># ... more data</span>
<span class="p">]</span>

<span class="c1"># GPTQ configuration</span>
<span class="n">gptq_config</span> <span class="o">=</span> <span class="n">GPTQConfig</span><span class="p">(</span>
    <span class="n">bits</span><span class="o">=</span><span class="mi">4</span><span class="p">,</span>
    <span class="n">group_size</span><span class="o">=</span><span class="mi">128</span><span class="p">,</span>                    <span class="c1"># Group size</span>
    <span class="n">desc_act</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>                     <span class="c1"># Activation order descending</span>
    <span class="n">dataset</span><span class="o">=</span><span class="n">calibration_data</span><span class="p">,</span>
    <span class="n">tokenizer</span><span class="o">=</span><span class="n">tokenizer</span>
<span class="p">)</span>

<span class="c1"># Quantize and save</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">AutoModelForCausalLM</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span>
    <span class="s2">&quot;meta-llama/Llama-2-7b-hf&quot;</span><span class="p">,</span>
    <span class="n">quantization_config</span><span class="o">=</span><span class="n">gptq_config</span><span class="p">,</span>
    <span class="n">device_map</span><span class="o">=</span><span class="s2">&quot;auto&quot;</span>
<span class="p">)</span>

<span class="n">model</span><span class="o">.</span><span class="n">save_pretrained</span><span class="p">(</span><span class="s2">&quot;./llama-2-7b-gptq-4bit&quot;</span><span class="p">)</span>
<span class="n">tokenizer</span><span class="o">.</span><span class="n">save_pretrained</span><span class="p">(</span><span class="s2">&quot;./llama-2-7b-gptq-4bit&quot;</span><span class="p">)</span>
</code></pre></div>

<h3 id="using-autogptq">Using AutoGPTQ<a class="header-link" href="#using-autogptq" title="Permanent link">&para;</a></h3>
<div class="highlight"><pre><span></span><code><span class="c1"># pip install auto-gptq</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">auto_gptq</span><span class="w"> </span><span class="kn">import</span> <span class="n">AutoGPTQForCausalLM</span><span class="p">,</span> <span class="n">BaseQuantizeConfig</span>

<span class="c1"># Quantization configuration</span>
<span class="n">quantize_config</span> <span class="o">=</span> <span class="n">BaseQuantizeConfig</span><span class="p">(</span>
    <span class="n">bits</span><span class="o">=</span><span class="mi">4</span><span class="p">,</span>
    <span class="n">group_size</span><span class="o">=</span><span class="mi">128</span><span class="p">,</span>
    <span class="n">desc_act</span><span class="o">=</span><span class="kc">False</span>
<span class="p">)</span>

<span class="c1"># Load model</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">AutoGPTQForCausalLM</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span>
    <span class="s2">&quot;meta-llama/Llama-2-7b-hf&quot;</span><span class="p">,</span>
    <span class="n">quantize_config</span>
<span class="p">)</span>

<span class="c1"># Calibration data</span>
<span class="n">examples</span> <span class="o">=</span> <span class="p">[</span><span class="n">tokenizer</span><span class="p">(</span><span class="n">text</span><span class="p">,</span> <span class="n">return_tensors</span><span class="o">=</span><span class="s2">&quot;pt&quot;</span><span class="p">)</span> <span class="k">for</span> <span class="n">text</span> <span class="ow">in</span> <span class="n">calibration_data</span><span class="p">]</span>

<span class="c1"># Quantize</span>
<span class="n">model</span><span class="o">.</span><span class="n">quantize</span><span class="p">(</span><span class="n">examples</span><span class="p">,</span> <span class="n">batch_size</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>

<span class="c1"># Save</span>
<span class="n">model</span><span class="o">.</span><span class="n">save_quantized</span><span class="p">(</span><span class="s2">&quot;./llama-2-7b-gptq&quot;</span><span class="p">)</span>
</code></pre></div>

<h3 id="using-pre-quantized-models">Using Pre-quantized Models<a class="header-link" href="#using-pre-quantized-models" title="Permanent link">&para;</a></h3>
<div class="highlight"><pre><span></span><code><span class="c1"># Download GPTQ models from TheBloke, etc.</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">transformers</span><span class="w"> </span><span class="kn">import</span> <span class="n">AutoModelForCausalLM</span><span class="p">,</span> <span class="n">AutoTokenizer</span>

<span class="n">model</span> <span class="o">=</span> <span class="n">AutoModelForCausalLM</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span>
    <span class="s2">&quot;TheBloke/Llama-2-7B-GPTQ&quot;</span><span class="p">,</span>
    <span class="n">device_map</span><span class="o">=</span><span class="s2">&quot;auto&quot;</span><span class="p">,</span>
    <span class="n">trust_remote_code</span><span class="o">=</span><span class="kc">True</span>
<span class="p">)</span>

<span class="n">tokenizer</span> <span class="o">=</span> <span class="n">AutoTokenizer</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="s2">&quot;TheBloke/Llama-2-7B-GPTQ&quot;</span><span class="p">)</span>

<span class="c1"># Inference</span>
<span class="n">inputs</span> <span class="o">=</span> <span class="n">tokenizer</span><span class="p">(</span><span class="s2">&quot;What is AI?&quot;</span><span class="p">,</span> <span class="n">return_tensors</span><span class="o">=</span><span class="s2">&quot;pt&quot;</span><span class="p">)</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">model</span><span class="o">.</span><span class="n">device</span><span class="p">)</span>
<span class="n">outputs</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">generate</span><span class="p">(</span><span class="o">**</span><span class="n">inputs</span><span class="p">,</span> <span class="n">max_new_tokens</span><span class="o">=</span><span class="mi">100</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">tokenizer</span><span class="o">.</span><span class="n">decode</span><span class="p">(</span><span class="n">outputs</span><span class="p">[</span><span class="mi">0</span><span class="p">]))</span>
</code></pre></div>

<hr />
<h2 id="5-awq-activation-aware-weight-quantization">5. AWQ (Activation-aware Weight Quantization)<a class="header-link" href="#5-awq-activation-aware-weight-quantization" title="Permanent link">&para;</a></h2>
<h3 id="concept_1">Concept<a class="header-link" href="#concept_1" title="Permanent link">&para;</a></h3>
<div class="highlight"><pre><span></span><code><span class="nv">AWQ</span><span class="w"> </span><span class="nv">features</span>:
<span class="w">    </span><span class="o">-</span><span class="w"> </span><span class="nv">Calculate</span><span class="w"> </span><span class="nv">weight</span><span class="w"> </span><span class="nv">importance</span><span class="w"> </span><span class="nv">based</span><span class="w"> </span><span class="nv">on</span><span class="w"> </span><span class="nv">activations</span>
<span class="w">    </span><span class="o">-</span><span class="w"> </span><span class="nv">Maintain</span><span class="w"> </span><span class="nv">high</span><span class="w"> </span><span class="nv">precision</span><span class="w"> </span><span class="k">for</span><span class="w"> </span><span class="nv">important</span><span class="w"> </span><span class="nv">weights</span>
<span class="w">    </span><span class="o">-</span><span class="w"> </span><span class="nv">Faster</span><span class="w"> </span><span class="nv">quantization</span><span class="w"> </span><span class="nv">than</span><span class="w"> </span><span class="nv">GPTQ</span>
<span class="w">    </span><span class="o">-</span><span class="w"> </span><span class="nv">Similar</span><span class="w"> </span><span class="nv">or</span><span class="w"> </span><span class="nv">better</span><span class="w"> </span><span class="nv">quality</span>
</code></pre></div>

<h3 id="awq-quantization">AWQ Quantization<a class="header-link" href="#awq-quantization" title="Permanent link">&para;</a></h3>
<div class="highlight"><pre><span></span><code><span class="c1"># pip install autoawq</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">awq</span><span class="w"> </span><span class="kn">import</span> <span class="n">AutoAWQForCausalLM</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">transformers</span><span class="w"> </span><span class="kn">import</span> <span class="n">AutoTokenizer</span>

<span class="c1"># Load model</span>
<span class="n">model_path</span> <span class="o">=</span> <span class="s2">&quot;meta-llama/Llama-2-7b-hf&quot;</span>
<span class="n">quant_path</span> <span class="o">=</span> <span class="s2">&quot;./llama-2-7b-awq&quot;</span>

<span class="n">model</span> <span class="o">=</span> <span class="n">AutoAWQForCausalLM</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="n">model_path</span><span class="p">)</span>
<span class="n">tokenizer</span> <span class="o">=</span> <span class="n">AutoTokenizer</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="n">model_path</span><span class="p">)</span>

<span class="c1"># AWQ quantization configuration</span>
<span class="n">quant_config</span> <span class="o">=</span> <span class="p">{</span>
    <span class="s2">&quot;zero_point&quot;</span><span class="p">:</span> <span class="kc">True</span><span class="p">,</span>
    <span class="s2">&quot;q_group_size&quot;</span><span class="p">:</span> <span class="mi">128</span><span class="p">,</span>
    <span class="s2">&quot;w_bit&quot;</span><span class="p">:</span> <span class="mi">4</span><span class="p">,</span>
    <span class="s2">&quot;version&quot;</span><span class="p">:</span> <span class="s2">&quot;GEMM&quot;</span>  <span class="c1"># GEMM or GEMV</span>
<span class="p">}</span>

<span class="c1"># Quantize</span>
<span class="n">model</span><span class="o">.</span><span class="n">quantize</span><span class="p">(</span><span class="n">tokenizer</span><span class="p">,</span> <span class="n">quant_config</span><span class="o">=</span><span class="n">quant_config</span><span class="p">)</span>

<span class="c1"># Save</span>
<span class="n">model</span><span class="o">.</span><span class="n">save_quantized</span><span class="p">(</span><span class="n">quant_path</span><span class="p">)</span>
<span class="n">tokenizer</span><span class="o">.</span><span class="n">save_pretrained</span><span class="p">(</span><span class="n">quant_path</span><span class="p">)</span>
</code></pre></div>

<h3 id="awq-model-inference">AWQ Model Inference<a class="header-link" href="#awq-model-inference" title="Permanent link">&para;</a></h3>
<div class="highlight"><pre><span></span><code><span class="kn">from</span><span class="w"> </span><span class="nn">awq</span><span class="w"> </span><span class="kn">import</span> <span class="n">AutoAWQForCausalLM</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">transformers</span><span class="w"> </span><span class="kn">import</span> <span class="n">AutoTokenizer</span>

<span class="c1"># Load AWQ model</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">AutoAWQForCausalLM</span><span class="o">.</span><span class="n">from_quantized</span><span class="p">(</span>
    <span class="s2">&quot;./llama-2-7b-awq&quot;</span><span class="p">,</span>
    <span class="n">fuse_layers</span><span class="o">=</span><span class="kc">True</span>  <span class="c1"># Speed up with layer fusion</span>
<span class="p">)</span>
<span class="n">tokenizer</span> <span class="o">=</span> <span class="n">AutoTokenizer</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="s2">&quot;./llama-2-7b-awq&quot;</span><span class="p">)</span>

<span class="c1"># Inference</span>
<span class="n">prompt</span> <span class="o">=</span> <span class="s2">&quot;Explain quantum computing in simple terms:&quot;</span>
<span class="n">inputs</span> <span class="o">=</span> <span class="n">tokenizer</span><span class="p">(</span><span class="n">prompt</span><span class="p">,</span> <span class="n">return_tensors</span><span class="o">=</span><span class="s2">&quot;pt&quot;</span><span class="p">)</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">model</span><span class="o">.</span><span class="n">device</span><span class="p">)</span>
<span class="n">outputs</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">generate</span><span class="p">(</span><span class="o">**</span><span class="n">inputs</span><span class="p">,</span> <span class="n">max_new_tokens</span><span class="o">=</span><span class="mi">200</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">tokenizer</span><span class="o">.</span><span class="n">decode</span><span class="p">(</span><span class="n">outputs</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">skip_special_tokens</span><span class="o">=</span><span class="kc">True</span><span class="p">))</span>
</code></pre></div>

<hr />
<h2 id="6-qlora-quantized-lora">6. QLoRA (Quantized LoRA)<a class="header-link" href="#6-qlora-quantized-lora" title="Permanent link">&para;</a></h2>
<h3 id="concept_2">Concept<a class="header-link" href="#concept_2" title="Permanent link">&para;</a></h3>
<div class="highlight"><pre><span></span><code>QLoRA = 4bit quantization + LoRA

    Base model (4bit quantized, frozen)
         â”‚
         â–¼
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚  LoRA A     â”‚  (FP16, trainable)
    â”‚  (r Ã— d)    â”‚
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚
         â–¼
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚  LoRA B     â”‚  (FP16, trainable)
    â”‚  (d Ã— r)    â”‚
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚
         â–¼
    Final output = quantized weights + LoRA correction
</code></pre></div>

<h3 id="qlora-fine-tuning">QLoRA Fine-tuning<a class="header-link" href="#qlora-fine-tuning" title="Permanent link">&para;</a></h3>
<div class="highlight"><pre><span></span><code><span class="kn">import</span><span class="w"> </span><span class="nn">torch</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">transformers</span><span class="w"> </span><span class="kn">import</span> <span class="p">(</span>
    <span class="n">AutoModelForCausalLM</span><span class="p">,</span>
    <span class="n">AutoTokenizer</span><span class="p">,</span>
    <span class="n">BitsAndBytesConfig</span><span class="p">,</span>
    <span class="n">TrainingArguments</span>
<span class="p">)</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">peft</span><span class="w"> </span><span class="kn">import</span> <span class="n">LoraConfig</span><span class="p">,</span> <span class="n">get_peft_model</span><span class="p">,</span> <span class="n">prepare_model_for_kbit_training</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">trl</span><span class="w"> </span><span class="kn">import</span> <span class="n">SFTTrainer</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">datasets</span><span class="w"> </span><span class="kn">import</span> <span class="n">load_dataset</span>

<span class="c1"># 4-bit quantization configuration</span>
<span class="n">bnb_config</span> <span class="o">=</span> <span class="n">BitsAndBytesConfig</span><span class="p">(</span>
    <span class="n">load_in_4bit</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
    <span class="n">bnb_4bit_quant_type</span><span class="o">=</span><span class="s2">&quot;nf4&quot;</span><span class="p">,</span>
    <span class="n">bnb_4bit_compute_dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">bfloat16</span><span class="p">,</span>
    <span class="n">bnb_4bit_use_double_quant</span><span class="o">=</span><span class="kc">True</span>
<span class="p">)</span>

<span class="c1"># Load model</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">AutoModelForCausalLM</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span>
    <span class="s2">&quot;meta-llama/Llama-2-7b-hf&quot;</span><span class="p">,</span>
    <span class="n">quantization_config</span><span class="o">=</span><span class="n">bnb_config</span><span class="p">,</span>
    <span class="n">device_map</span><span class="o">=</span><span class="s2">&quot;auto&quot;</span>
<span class="p">)</span>

<span class="n">tokenizer</span> <span class="o">=</span> <span class="n">AutoTokenizer</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="s2">&quot;meta-llama/Llama-2-7b-hf&quot;</span><span class="p">)</span>
<span class="n">tokenizer</span><span class="o">.</span><span class="n">pad_token</span> <span class="o">=</span> <span class="n">tokenizer</span><span class="o">.</span><span class="n">eos_token</span>

<span class="c1"># Prepare for k-bit training</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">prepare_model_for_kbit_training</span><span class="p">(</span><span class="n">model</span><span class="p">)</span>

<span class="c1"># LoRA configuration</span>
<span class="n">lora_config</span> <span class="o">=</span> <span class="n">LoraConfig</span><span class="p">(</span>
    <span class="n">r</span><span class="o">=</span><span class="mi">16</span><span class="p">,</span>                          <span class="c1"># LoRA rank</span>
    <span class="n">lora_alpha</span><span class="o">=</span><span class="mi">32</span><span class="p">,</span>                 <span class="c1"># Scaling factor</span>
    <span class="n">target_modules</span><span class="o">=</span><span class="p">[</span>               <span class="c1"># Modules to apply</span>
        <span class="s2">&quot;q_proj&quot;</span><span class="p">,</span> <span class="s2">&quot;k_proj&quot;</span><span class="p">,</span> <span class="s2">&quot;v_proj&quot;</span><span class="p">,</span> <span class="s2">&quot;o_proj&quot;</span><span class="p">,</span>
        <span class="s2">&quot;gate_proj&quot;</span><span class="p">,</span> <span class="s2">&quot;up_proj&quot;</span><span class="p">,</span> <span class="s2">&quot;down_proj&quot;</span>
    <span class="p">],</span>
    <span class="n">lora_dropout</span><span class="o">=</span><span class="mf">0.05</span><span class="p">,</span>
    <span class="n">bias</span><span class="o">=</span><span class="s2">&quot;none&quot;</span><span class="p">,</span>
    <span class="n">task_type</span><span class="o">=</span><span class="s2">&quot;CAUSAL_LM&quot;</span>
<span class="p">)</span>

<span class="c1"># Apply LoRA</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">get_peft_model</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">lora_config</span><span class="p">)</span>
<span class="n">model</span><span class="o">.</span><span class="n">print_trainable_parameters</span><span class="p">()</span>
<span class="c1"># Trainable: ~0.1%, total ~400MB</span>

<span class="c1"># Dataset</span>
<span class="n">dataset</span> <span class="o">=</span> <span class="n">load_dataset</span><span class="p">(</span><span class="s2">&quot;databricks/databricks-dolly-15k&quot;</span><span class="p">,</span> <span class="n">split</span><span class="o">=</span><span class="s2">&quot;train&quot;</span><span class="p">)</span>

<span class="k">def</span><span class="w"> </span><span class="nf">format_prompt</span><span class="p">(</span><span class="n">example</span><span class="p">):</span>
    <span class="k">return</span> <span class="sa">f</span><span class="s2">&quot;&quot;&quot;### Instruction:</span>
<span class="si">{</span><span class="n">example</span><span class="p">[</span><span class="s1">&#39;instruction&#39;</span><span class="p">]</span><span class="si">}</span>

<span class="s2">### Input:</span>
<span class="si">{</span><span class="n">example</span><span class="p">[</span><span class="s1">&#39;context&#39;</span><span class="p">]</span><span class="si">}</span>

<span class="s2">### Response:</span>
<span class="si">{</span><span class="n">example</span><span class="p">[</span><span class="s1">&#39;response&#39;</span><span class="p">]</span><span class="si">}</span><span class="s2">&quot;&quot;&quot;</span>

<span class="c1"># Training configuration</span>
<span class="n">training_args</span> <span class="o">=</span> <span class="n">TrainingArguments</span><span class="p">(</span>
    <span class="n">output_dir</span><span class="o">=</span><span class="s2">&quot;./qlora_output&quot;</span><span class="p">,</span>
    <span class="n">num_train_epochs</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span>
    <span class="n">per_device_train_batch_size</span><span class="o">=</span><span class="mi">4</span><span class="p">,</span>
    <span class="n">gradient_accumulation_steps</span><span class="o">=</span><span class="mi">4</span><span class="p">,</span>
    <span class="n">learning_rate</span><span class="o">=</span><span class="mf">2e-4</span><span class="p">,</span>
    <span class="n">warmup_ratio</span><span class="o">=</span><span class="mf">0.03</span><span class="p">,</span>
    <span class="n">logging_steps</span><span class="o">=</span><span class="mi">10</span><span class="p">,</span>
    <span class="n">save_strategy</span><span class="o">=</span><span class="s2">&quot;epoch&quot;</span><span class="p">,</span>
    <span class="n">fp16</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
    <span class="n">optim</span><span class="o">=</span><span class="s2">&quot;paged_adamw_8bit&quot;</span>  <span class="c1"># Memory-efficient optimizer</span>
<span class="p">)</span>

<span class="c1"># Trainer</span>
<span class="n">trainer</span> <span class="o">=</span> <span class="n">SFTTrainer</span><span class="p">(</span>
    <span class="n">model</span><span class="o">=</span><span class="n">model</span><span class="p">,</span>
    <span class="n">train_dataset</span><span class="o">=</span><span class="n">dataset</span><span class="p">,</span>
    <span class="n">formatting_func</span><span class="o">=</span><span class="n">format_prompt</span><span class="p">,</span>
    <span class="n">max_seq_length</span><span class="o">=</span><span class="mi">512</span><span class="p">,</span>
    <span class="n">args</span><span class="o">=</span><span class="n">training_args</span><span class="p">,</span>
<span class="p">)</span>

<span class="c1"># Train</span>
<span class="n">trainer</span><span class="o">.</span><span class="n">train</span><span class="p">()</span>

<span class="c1"># Save</span>
<span class="n">model</span><span class="o">.</span><span class="n">save_pretrained</span><span class="p">(</span><span class="s2">&quot;./qlora_adapter&quot;</span><span class="p">)</span>
</code></pre></div>

<h3 id="merging-qlora-model">Merging QLoRA Model<a class="header-link" href="#merging-qlora-model" title="Permanent link">&para;</a></h3>
<div class="highlight"><pre><span></span><code><span class="kn">from</span><span class="w"> </span><span class="nn">peft</span><span class="w"> </span><span class="kn">import</span> <span class="n">PeftModel</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">transformers</span><span class="w"> </span><span class="kn">import</span> <span class="n">AutoModelForCausalLM</span>

<span class="c1"># Base model (load in FP16)</span>
<span class="n">base_model</span> <span class="o">=</span> <span class="n">AutoModelForCausalLM</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span>
    <span class="s2">&quot;meta-llama/Llama-2-7b-hf&quot;</span><span class="p">,</span>
    <span class="n">torch_dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">float16</span><span class="p">,</span>
    <span class="n">device_map</span><span class="o">=</span><span class="s2">&quot;auto&quot;</span>
<span class="p">)</span>

<span class="c1"># Merge LoRA adapter</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">PeftModel</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="n">base_model</span><span class="p">,</span> <span class="s2">&quot;./qlora_adapter&quot;</span><span class="p">)</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">merge_and_unload</span><span class="p">()</span>  <span class="c1"># Merge adapter to base model</span>

<span class="c1"># Save merged model</span>
<span class="n">model</span><span class="o">.</span><span class="n">save_pretrained</span><span class="p">(</span><span class="s2">&quot;./llama-2-7b-finetuned&quot;</span><span class="p">)</span>
</code></pre></div>

<hr />
<h2 id="7-quantization-performance-comparison">7. Quantization Performance Comparison<a class="header-link" href="#7-quantization-performance-comparison" title="Permanent link">&para;</a></h2>
<h3 id="benchmark">Benchmark<a class="header-link" href="#benchmark" title="Permanent link">&para;</a></h3>
<div class="highlight"><pre><span></span><code><span class="kn">import</span><span class="w"> </span><span class="nn">time</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">torch</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">transformers</span><span class="w"> </span><span class="kn">import</span> <span class="n">AutoModelForCausalLM</span><span class="p">,</span> <span class="n">AutoTokenizer</span>

<span class="k">def</span><span class="w"> </span><span class="nf">benchmark_model</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">tokenizer</span><span class="p">,</span> <span class="n">prompt</span><span class="p">,</span> <span class="n">num_runs</span><span class="o">=</span><span class="mi">5</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Model inference benchmark&quot;&quot;&quot;</span>
    <span class="n">inputs</span> <span class="o">=</span> <span class="n">tokenizer</span><span class="p">(</span><span class="n">prompt</span><span class="p">,</span> <span class="n">return_tensors</span><span class="o">=</span><span class="s2">&quot;pt&quot;</span><span class="p">)</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">model</span><span class="o">.</span><span class="n">device</span><span class="p">)</span>

    <span class="c1"># Warmup</span>
    <span class="k">with</span> <span class="n">torch</span><span class="o">.</span><span class="n">no_grad</span><span class="p">():</span>
        <span class="n">_</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">generate</span><span class="p">(</span><span class="o">**</span><span class="n">inputs</span><span class="p">,</span> <span class="n">max_new_tokens</span><span class="o">=</span><span class="mi">50</span><span class="p">)</span>

    <span class="c1"># Benchmark</span>
    <span class="n">times</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">num_runs</span><span class="p">):</span>
        <span class="n">torch</span><span class="o">.</span><span class="n">cuda</span><span class="o">.</span><span class="n">synchronize</span><span class="p">()</span>
        <span class="n">start</span> <span class="o">=</span> <span class="n">time</span><span class="o">.</span><span class="n">time</span><span class="p">()</span>

        <span class="k">with</span> <span class="n">torch</span><span class="o">.</span><span class="n">no_grad</span><span class="p">():</span>
            <span class="n">outputs</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">generate</span><span class="p">(</span><span class="o">**</span><span class="n">inputs</span><span class="p">,</span> <span class="n">max_new_tokens</span><span class="o">=</span><span class="mi">100</span><span class="p">)</span>

        <span class="n">torch</span><span class="o">.</span><span class="n">cuda</span><span class="o">.</span><span class="n">synchronize</span><span class="p">()</span>
        <span class="n">times</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">time</span><span class="o">.</span><span class="n">time</span><span class="p">()</span> <span class="o">-</span> <span class="n">start</span><span class="p">)</span>

    <span class="k">return</span> <span class="p">{</span>
        <span class="s2">&quot;avg_time&quot;</span><span class="p">:</span> <span class="nb">sum</span><span class="p">(</span><span class="n">times</span><span class="p">)</span> <span class="o">/</span> <span class="nb">len</span><span class="p">(</span><span class="n">times</span><span class="p">),</span>
        <span class="s2">&quot;memory_gb&quot;</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">cuda</span><span class="o">.</span><span class="n">max_memory_allocated</span><span class="p">()</span> <span class="o">/</span> <span class="mf">1e9</span><span class="p">,</span>
        <span class="s2">&quot;output&quot;</span><span class="p">:</span> <span class="n">tokenizer</span><span class="o">.</span><span class="n">decode</span><span class="p">(</span><span class="n">outputs</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span>
    <span class="p">}</span>

<span class="c1"># Compare results</span>
<span class="n">models</span> <span class="o">=</span> <span class="p">{</span>
    <span class="s2">&quot;FP16&quot;</span><span class="p">:</span> <span class="n">model_fp16</span><span class="p">,</span>
    <span class="s2">&quot;INT8&quot;</span><span class="p">:</span> <span class="n">model_8bit</span><span class="p">,</span>
    <span class="s2">&quot;INT4 (NF4)&quot;</span><span class="p">:</span> <span class="n">model_4bit</span><span class="p">,</span>
    <span class="s2">&quot;GPTQ-4bit&quot;</span><span class="p">:</span> <span class="n">model_gptq</span><span class="p">,</span>
    <span class="s2">&quot;AWQ-4bit&quot;</span><span class="p">:</span> <span class="n">model_awq</span><span class="p">,</span>
<span class="p">}</span>

<span class="n">prompt</span> <span class="o">=</span> <span class="s2">&quot;Explain the theory of relativity:&quot;</span>

<span class="k">for</span> <span class="n">name</span><span class="p">,</span> <span class="n">model</span> <span class="ow">in</span> <span class="n">models</span><span class="o">.</span><span class="n">items</span><span class="p">():</span>
    <span class="n">result</span> <span class="o">=</span> <span class="n">benchmark_model</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">tokenizer</span><span class="p">,</span> <span class="n">prompt</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="n">name</span><span class="si">}</span><span class="s2">:&quot;</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;  Time: </span><span class="si">{</span><span class="n">result</span><span class="p">[</span><span class="s1">&#39;avg_time&#39;</span><span class="p">]</span><span class="si">:</span><span class="s2">.2f</span><span class="si">}</span><span class="s2">s&quot;</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;  Memory: </span><span class="si">{</span><span class="n">result</span><span class="p">[</span><span class="s1">&#39;memory_gb&#39;</span><span class="p">]</span><span class="si">:</span><span class="s2">.2f</span><span class="si">}</span><span class="s2"> GB&quot;</span><span class="p">)</span>
</code></pre></div>

<h3 id="accuracy-evaluation">Accuracy Evaluation<a class="header-link" href="#accuracy-evaluation" title="Permanent link">&para;</a></h3>
<div class="highlight"><pre><span></span><code><span class="kn">from</span><span class="w"> </span><span class="nn">datasets</span><span class="w"> </span><span class="kn">import</span> <span class="n">load_dataset</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">evaluate</span>

<span class="c1"># Evaluation dataset</span>
<span class="n">dataset</span> <span class="o">=</span> <span class="n">load_dataset</span><span class="p">(</span><span class="s2">&quot;wikitext&quot;</span><span class="p">,</span> <span class="s2">&quot;wikitext-2-raw-v1&quot;</span><span class="p">,</span> <span class="n">split</span><span class="o">=</span><span class="s2">&quot;test&quot;</span><span class="p">)</span>

<span class="k">def</span><span class="w"> </span><span class="nf">compute_perplexity</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">tokenizer</span><span class="p">,</span> <span class="n">texts</span><span class="p">,</span> <span class="n">max_length</span><span class="o">=</span><span class="mi">1024</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Calculate perplexity&quot;&quot;&quot;</span>
    <span class="n">total_loss</span> <span class="o">=</span> <span class="mi">0</span>
    <span class="n">total_tokens</span> <span class="o">=</span> <span class="mi">0</span>

    <span class="k">for</span> <span class="n">text</span> <span class="ow">in</span> <span class="n">texts</span><span class="p">:</span>
        <span class="n">inputs</span> <span class="o">=</span> <span class="n">tokenizer</span><span class="p">(</span><span class="n">text</span><span class="p">,</span> <span class="n">return_tensors</span><span class="o">=</span><span class="s2">&quot;pt&quot;</span><span class="p">,</span> <span class="n">truncation</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">max_length</span><span class="o">=</span><span class="n">max_length</span><span class="p">)</span>
        <span class="n">inputs</span> <span class="o">=</span> <span class="p">{</span><span class="n">k</span><span class="p">:</span> <span class="n">v</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">model</span><span class="o">.</span><span class="n">device</span><span class="p">)</span> <span class="k">for</span> <span class="n">k</span><span class="p">,</span> <span class="n">v</span> <span class="ow">in</span> <span class="n">inputs</span><span class="o">.</span><span class="n">items</span><span class="p">()}</span>

        <span class="k">with</span> <span class="n">torch</span><span class="o">.</span><span class="n">no_grad</span><span class="p">():</span>
            <span class="n">outputs</span> <span class="o">=</span> <span class="n">model</span><span class="p">(</span><span class="o">**</span><span class="n">inputs</span><span class="p">,</span> <span class="n">labels</span><span class="o">=</span><span class="n">inputs</span><span class="p">[</span><span class="s2">&quot;input_ids&quot;</span><span class="p">])</span>
            <span class="n">total_loss</span> <span class="o">+=</span> <span class="n">outputs</span><span class="o">.</span><span class="n">loss</span><span class="o">.</span><span class="n">item</span><span class="p">()</span> <span class="o">*</span> <span class="n">inputs</span><span class="p">[</span><span class="s2">&quot;input_ids&quot;</span><span class="p">]</span><span class="o">.</span><span class="n">size</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span>
            <span class="n">total_tokens</span> <span class="o">+=</span> <span class="n">inputs</span><span class="p">[</span><span class="s2">&quot;input_ids&quot;</span><span class="p">]</span><span class="o">.</span><span class="n">size</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span>

    <span class="n">perplexity</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="n">total_loss</span> <span class="o">/</span> <span class="n">total_tokens</span><span class="p">))</span>
    <span class="k">return</span> <span class="n">perplexity</span><span class="o">.</span><span class="n">item</span><span class="p">()</span>

<span class="c1"># Compare</span>
<span class="k">for</span> <span class="n">name</span><span class="p">,</span> <span class="n">model</span> <span class="ow">in</span> <span class="n">models</span><span class="o">.</span><span class="n">items</span><span class="p">():</span>
    <span class="n">ppl</span> <span class="o">=</span> <span class="n">compute_perplexity</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">tokenizer</span><span class="p">,</span> <span class="n">dataset</span><span class="p">[</span><span class="s2">&quot;text&quot;</span><span class="p">][:</span><span class="mi">100</span><span class="p">])</span>
    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="n">name</span><span class="si">}</span><span class="s2"> Perplexity: </span><span class="si">{</span><span class="n">ppl</span><span class="si">:</span><span class="s2">.2f</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
</code></pre></div>

<hr />
<h2 id="8-practical-guide">8. Practical Guide<a class="header-link" href="#8-practical-guide" title="Permanent link">&para;</a></h2>
<h3 id="choosing-quantization-method">Choosing Quantization Method<a class="header-link" href="#choosing-quantization-method" title="Permanent link">&para;</a></h3>
<div class="highlight"><pre><span></span><code><span class="err">â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”</span>
<span class="err">â”‚</span><span class="w">                </span><span class="n">Quantization</span><span class="w"> </span><span class="n">Method</span><span class="w"> </span><span class="n">Selection</span><span class="w"> </span><span class="n">Guide</span><span class="w">           </span><span class="err">â”‚</span>
<span class="err">â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤</span>
<span class="err">â”‚</span><span class="w">                                                              </span><span class="err">â”‚</span>
<span class="err">â”‚</span><span class="w">  </span><span class="n">Purpose</span><span class="w">                 </span><span class="err">â”‚</span><span class="w">  </span><span class="n">Recommended</span><span class="w"> </span><span class="n">Method</span><span class="w">               </span><span class="err">â”‚</span>
<span class="err">â”‚</span><span class="w">  </span><span class="err">â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”‚</span>
<span class="err">â”‚</span><span class="w">  </span><span class="n">Fast</span><span class="w"> </span><span class="n">prototyping</span><span class="w">        </span><span class="err">â”‚</span><span class="w">  </span><span class="n">bitsandbytes</span><span class="w"> </span><span class="p">(</span><span class="n">load_in_8bit</span><span class="p">)</span><span class="w">      </span><span class="err">â”‚</span>
<span class="err">â”‚</span><span class="w">  </span><span class="n">Memory</span><span class="o">-</span><span class="n">constrained</span><span class="w"> </span><span class="n">env</span><span class="w">  </span><span class="err">â”‚</span><span class="w">  </span><span class="n">bitsandbytes</span><span class="w"> </span><span class="p">(</span><span class="n">load_in_4bit</span><span class="p">)</span><span class="w">      </span><span class="err">â”‚</span>
<span class="err">â”‚</span><span class="w">  </span><span class="n">Production</span><span class="w"> </span><span class="n">deployment</span><span class="w">   </span><span class="err">â”‚</span><span class="w">  </span><span class="n">GPTQ</span><span class="w"> </span><span class="ow">or</span><span class="w"> </span><span class="n">AWQ</span><span class="w">                      </span><span class="err">â”‚</span>
<span class="err">â”‚</span><span class="w">  </span><span class="n">Fine</span><span class="o">-</span><span class="n">tuning</span><span class="w"> </span><span class="n">needed</span><span class="w">      </span><span class="err">â”‚</span><span class="w">  </span><span class="n">QLoRA</span><span class="w">                            </span><span class="err">â”‚</span>
<span class="err">â”‚</span><span class="w">  </span><span class="n">Maximum</span><span class="w"> </span><span class="n">speed</span><span class="w">           </span><span class="err">â”‚</span><span class="w">  </span><span class="n">AWQ</span><span class="w"> </span><span class="o">+</span><span class="w"> </span><span class="n">fuse_layers</span><span class="w">                </span><span class="err">â”‚</span>
<span class="err">â”‚</span><span class="w">  </span><span class="n">Maximum</span><span class="w"> </span><span class="n">quality</span><span class="w">         </span><span class="err">â”‚</span><span class="w">  </span><span class="n">GPTQ</span><span class="w"> </span><span class="p">(</span><span class="n">desc_act</span><span class="o">=</span><span class="n">True</span><span class="p">)</span><span class="w">             </span><span class="err">â”‚</span>
<span class="err">â”‚</span><span class="w">                                                              </span><span class="err">â”‚</span>
<span class="err">â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜</span>
</code></pre></div>

<h3 id="troubleshooting">Troubleshooting<a class="header-link" href="#troubleshooting" title="Permanent link">&para;</a></h3>
<div class="highlight"><pre><span></span><code><span class="c1"># 1. CUDA Out of Memory</span>
<span class="c1"># - Reduce batch size</span>
<span class="c1"># - Enable gradient_checkpointing</span>
<span class="c1"># - Use lower bit quantization</span>

<span class="n">model</span><span class="o">.</span><span class="n">gradient_checkpointing_enable</span><span class="p">()</span>

<span class="c1"># 2. Quality degradation after quantization</span>
<span class="c1"># - Reduce group_size (64 or 32)</span>
<span class="c1"># - Increase calibration data</span>
<span class="c1"># - Try GPTQ instead of AWQ</span>

<span class="c1"># 3. Slow inference</span>
<span class="c1"># - Enable fuse_layers=True</span>
<span class="c1"># - Use exllama backend (GPTQ)</span>
<span class="c1"># - Utilize batch processing</span>

<span class="kn">from</span><span class="w"> </span><span class="nn">auto_gptq</span><span class="w"> </span><span class="kn">import</span> <span class="n">exllama_set_max_input_length</span>
<span class="n">exllama_set_max_input_length</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="mi">4096</span><span class="p">)</span>
</code></pre></div>

<hr />
<h2 id="summary">Summary<a class="header-link" href="#summary" title="Permanent link">&para;</a></h2>
<h3 id="quantization-comparison-table">Quantization Comparison Table<a class="header-link" href="#quantization-comparison-table" title="Permanent link">&para;</a></h3>
<table>
<thead>
<tr>
<th>Method</th>
<th>Bits</th>
<th>Speed</th>
<th>Quality</th>
<th>Ease of Use</th>
</tr>
</thead>
<tbody>
<tr>
<td>FP16</td>
<td>16</td>
<td>Baseline</td>
<td>Baseline</td>
<td>Easy</td>
</tr>
<tr>
<td>INT8 (bitsandbytes)</td>
<td>8</td>
<td>Fast</td>
<td>High</td>
<td>Easy</td>
</tr>
<tr>
<td>INT4 (NF4)</td>
<td>4</td>
<td>Fast</td>
<td>Good</td>
<td>Easy</td>
</tr>
<tr>
<td>GPTQ</td>
<td>4/3/2</td>
<td>Very Fast</td>
<td>Good</td>
<td>Medium</td>
</tr>
<tr>
<td>AWQ</td>
<td>4</td>
<td>Very Fast</td>
<td>Good</td>
<td>Medium</td>
</tr>
<tr>
<td>QLoRA</td>
<td>4</td>
<td>-</td>
<td>Training</td>
<td>Medium</td>
</tr>
</tbody>
</table>
<h3 id="core-code">Core Code<a class="header-link" href="#core-code" title="Permanent link">&para;</a></h3>
<div class="highlight"><pre><span></span><code><span class="c1"># bitsandbytes 4-bit</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">transformers</span><span class="w"> </span><span class="kn">import</span> <span class="n">BitsAndBytesConfig</span>
<span class="n">config</span> <span class="o">=</span> <span class="n">BitsAndBytesConfig</span><span class="p">(</span><span class="n">load_in_4bit</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">bnb_4bit_quant_type</span><span class="o">=</span><span class="s2">&quot;nf4&quot;</span><span class="p">)</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">AutoModelForCausalLM</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="n">model_id</span><span class="p">,</span> <span class="n">quantization_config</span><span class="o">=</span><span class="n">config</span><span class="p">)</span>

<span class="c1"># QLoRA</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">peft</span><span class="w"> </span><span class="kn">import</span> <span class="n">prepare_model_for_kbit_training</span><span class="p">,</span> <span class="n">get_peft_model</span><span class="p">,</span> <span class="n">LoraConfig</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">prepare_model_for_kbit_training</span><span class="p">(</span><span class="n">model</span><span class="p">)</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">get_peft_model</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">lora_config</span><span class="p">)</span>

<span class="c1"># GPTQ</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">AutoGPTQForCausalLM</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="n">model_id</span><span class="p">,</span> <span class="n">quantize_config</span><span class="p">)</span>
<span class="n">model</span><span class="o">.</span><span class="n">quantize</span><span class="p">(</span><span class="n">examples</span><span class="p">)</span>

<span class="c1"># AWQ</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">AutoAWQForCausalLM</span><span class="o">.</span><span class="n">from_quantized</span><span class="p">(</span><span class="n">quant_path</span><span class="p">,</span> <span class="n">fuse_layers</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
</code></pre></div>

<hr />
<h2 id="next-steps">Next Steps<a class="header-link" href="#next-steps" title="Permanent link">&para;</a></h2>
<p>In <a href="./14_RLHF_Alignment.md">14_RLHF_Alignment.md</a>, we'll learn about LLM alignment techniques (RLHF, DPO).</p>
    </div>

    
<div class="lesson-toolbar lesson-toolbar--bottom">
    <div class="toolbar-nav toolbar-nav--prev">
        
        <a href="/study/en/LLM_and_NLP/12_Practical_Chatbot.html" class="nav-prev">
            <span class="nav-label">Previous</span>
            <span class="nav-title">12. Practical Chatbot Project</span>
        </a>
        
    </div>
    <div class="toolbar-actions">
        <button class="btn btn-copy-link" data-action="copy-link" title="Copy link">
            <span class="icon">ğŸ”—</span>
            <span class="text">Copy link</span>
        </button>
        <a href="/study/en/LLM_and_NLP/" class="btn btn-topic-link" title="Back to topic list">
            <span class="icon">ğŸ“‹</span>
            <span class="text">Topic list</span>
        </a>
    </div>
    <div class="toolbar-nav toolbar-nav--next">
        
        <a href="/study/en/LLM_and_NLP/14_RLHF_Alignment.html" class="nav-next">
            <span class="nav-label">Next</span>
            <span class="nav-title">14. RLHF and LLM Alignment</span>
        </a>
        
    </div>
</div>


    <div class="toolbar-keyboard-hint">
        <kbd>&larr;</kbd> <kbd>&rarr;</kbd> to navigate between lessons
    </div>
</article>

<!-- Scroll to top floating button -->
<button class="scroll-to-top" id="scroll-to-top" title="Scroll to top" aria-label="Scroll to top">â†‘</button>

            </div>
        </main>
    </div>

    <script src="/study/static/js/app.js"></script>
    <script>
        function switchLanguage(newLang) {
            const path = window.location.pathname;
            const base = '/study';
            const currentLang = 'en';
            const newPath = path.replace(base + '/' + currentLang + '/', base + '/' + newLang + '/');
            window.location.href = newPath + window.location.search;
        }
    </script>
    
<script>
    document.addEventListener('DOMContentLoaded', function() {
        // Copy link - independent feedback per button
        document.querySelectorAll('[data-action="copy-link"]').forEach(function(btn) {
            btn.addEventListener('click', function() {
                navigator.clipboard.writeText(window.location.href);
                var textEl = this.querySelector('.text');
                var originalText = textEl.textContent;
                textEl.textContent = 'Copied!';
                setTimeout(function() { textEl.textContent = originalText; }, 2000);
            });
        });

        // Add copy buttons to code blocks
        document.querySelectorAll('.lesson-content pre').forEach(function(pre) {
            var copyBtn = document.createElement('button');
            copyBtn.className = 'code-copy-btn';
            copyBtn.textContent = 'Copy';
            copyBtn.addEventListener('click', function() {
                var code = pre.querySelector('code');
                var text = code ? code.textContent : pre.textContent;
                navigator.clipboard.writeText(text);
                copyBtn.textContent = 'Copied!';
                setTimeout(function() { copyBtn.textContent = 'Copy'; }, 2000);
            });
            pre.style.position = 'relative';
            pre.appendChild(copyBtn);
        });

        // Scroll to top button
        var scrollBtn = document.getElementById('scroll-to-top');
        window.addEventListener('scroll', function() {
            scrollBtn.classList.toggle('visible', window.scrollY > 300);
        });
        scrollBtn.addEventListener('click', function() {
            window.scrollTo({ top: 0, behavior: 'smooth' });
        });

        // Keyboard shortcuts: left/right arrows for lesson navigation
        document.addEventListener('keydown', function(e) {
            var tag = document.activeElement.tagName;
            if (tag === 'INPUT' || tag === 'TEXTAREA' || tag === 'SELECT') return;

            if (e.key === 'ArrowLeft') {
                var prevLink = document.querySelector('.nav-prev');
                if (prevLink) prevLink.click();
            } else if (e.key === 'ArrowRight') {
                var nextLink = document.querySelector('.nav-next');
                if (nextLink) nextLink.click();
            }
        });
    });
</script>

</body>
</html>
{% endraw %}