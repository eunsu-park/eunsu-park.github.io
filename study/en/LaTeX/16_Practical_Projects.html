{% raw %}
<!DOCTYPE html>
<html lang="en" data-theme="light">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Practical Projects - Study Materials</title>
    <link rel="stylesheet" href="/study/static/css/style.css">
    <link rel="stylesheet" href="/study/static/css/highlight.css">
    <!-- KaTeX for LaTeX math rendering -->
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.11/dist/katex.min.css">
    <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.11/dist/katex.min.js"></script>
    <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.11/dist/contrib/auto-render.min.js"
        onload="renderMathInElement(document.body, {
            delimiters: [
                {left: '$$', right: '$$', display: true},
                {left: '$', right: '$', display: false}
            ],
            throwOnError: false
        });"></script>
    
</head>
<body>
    <div class="app-container">
        <!-- Sidebar -->
        <aside class="sidebar" id="sidebar">
            <div class="sidebar-header">
                <a href="/study/en/" class="logo">Study Materials</a>
                <button class="sidebar-toggle" id="sidebar-toggle" aria-label="Toggle sidebar">
                    <span></span>
                </button>
            </div>

            <nav class="sidebar-nav">
                <a href="/study/en/" class="nav-item ">
                    <span class="nav-icon">üè†</span>
                    <span class="nav-text">Home</span>
                </a>
                <a href="/study/examples/" class="nav-item ">
                    <span class="nav-icon">üíª</span>
                    <span class="nav-text">Examples</span>
                </a>
            </nav>

            <div class="sidebar-search">
                <form action="/study/en/search.html" method="get" id="search-form">
                    <input type="search" name="q" placeholder="Search..." id="search-sidebar-input">
                </form>
            </div>

            <!-- Language Selector -->
            <div class="sidebar-lang">
                <select id="lang-select" class="lang-selector" onchange="switchLanguage(this.value)">
                    
                    <option value="en" selected>
                        English
                    </option>
                    
                    <option value="ko" >
                        ÌïúÍµ≠Ïñ¥
                    </option>
                    
                </select>
            </div>

            <div class="sidebar-footer">
                <button class="theme-toggle" id="theme-toggle" aria-label="Toggle theme">
                    <span class="theme-icon light">‚òÄÔ∏è</span>
                    <span class="theme-icon dark">üåô</span>
                </button>
            </div>
        </aside>

        <!-- Main Content -->
        <main class="main-content">
            <header class="main-header">
                <button class="menu-toggle" id="menu-toggle" aria-label="Open menu">
                    <span></span>
                </button>
                
<nav class="breadcrumb">
    <a href="/study/en/">Topics</a>
    <span class="separator">/</span>
    <a href="/study/en/LaTeX/">LaTeX</a>
    <span class="separator">/</span>
    <span class="current">Practical Projects</span>
</nav>

            </header>

            <div class="content">
                
<article class="lesson-article">
    <header class="lesson-header">
        <h1>Practical Projects</h1>
    </header>

    
<div class="lesson-toolbar lesson-toolbar--top">
    <div class="toolbar-nav toolbar-nav--prev">
        
        <a href="/study/en/LaTeX/15_Automation_and_Build.html" class="nav-prev">
            <span class="nav-label">Previous</span>
            <span class="nav-title">Build Systems & Automation</span>
        </a>
        
    </div>
    <div class="toolbar-actions">
        <button class="btn btn-copy-link" data-action="copy-link" title="Copy link">
            <span class="icon">üîó</span>
            <span class="text">Copy link</span>
        </button>
        <a href="/study/en/LaTeX/" class="btn btn-topic-link" title="Back to topic list">
            <span class="icon">üìã</span>
            <span class="text">Topic list</span>
        </a>
    </div>
    <div class="toolbar-nav toolbar-nav--next">
        
    </div>
</div>


    
    <nav class="toc" id="toc">
        <h2>Table of Contents</h2>
        <div class="toc">
<ul>
<li><a href="#introduction">Introduction</a></li>
<li><a href="#project-1-academic-paper">Project 1: Academic Paper</a><ul>
<li><a href="#overview">Overview</a></li>
<li><a href="#complete-source-code">Complete Source Code</a></li>
<li><a href="#compilation">Compilation</a></li>
<li><a href="#common-pitfalls">Common Pitfalls</a></li>
<li><a href="#customization-tips">Customization Tips</a></li>
</ul>
</li>
<li><a href="#project-2-beamer-presentation">Project 2: Beamer Presentation</a><ul>
<li><a href="#overview_1">Overview</a></li>
<li><a href="#complete-source-code_1">Complete Source Code</a></li>
<li><a href="#compilation_1">Compilation</a></li>
<li><a href="#creating-handouts">Creating Handouts</a></li>
<li><a href="#speaker-notes">Speaker Notes</a></li>
<li><a href="#common-pitfalls_1">Common Pitfalls</a></li>
</ul>
</li>
<li><a href="#project-3-tikz-scientific-poster">Project 3: TikZ Scientific Poster</a><ul>
<li><a href="#overview_2">Overview</a></li>
<li><a href="#complete-source-code_2">Complete Source Code</a></li>
<li><a href="#compilation_2">Compilation</a></li>
<li><a href="#printing">Printing</a></li>
<li><a href="#common-pitfalls_2">Common Pitfalls</a></li>
</ul>
</li>
<li><a href="#combining-lessons-integration-map">Combining Lessons: Integration Map</a></li>
<li><a href="#next-steps">Next Steps</a><ul>
<li><a href="#explore-advanced-topics">Explore Advanced Topics</a></li>
<li><a href="#join-the-community">Join the Community</a></li>
<li><a href="#practice-projects">Practice Projects</a></li>
</ul>
</li>
<li><a href="#summary">Summary</a></li>
</ul>
</div>

    </nav>
    

    <div class="lesson-content markdown-body">
        <h1 id="practical-projects">Practical Projects<a class="header-link" href="#practical-projects" title="Permanent link">&para;</a></h1>
<blockquote>
<p><strong>Topic</strong>: LaTeX
<strong>Lesson</strong>: 16 of 16
<strong>Prerequisites</strong>: All previous lessons (01-15)
<strong>Objective</strong>: Apply all learned concepts to three complete, real-world projects: an academic paper, a Beamer presentation, and a scientific poster</p>
</blockquote>
<h2 id="introduction">Introduction<a class="header-link" href="#introduction" title="Permanent link">&para;</a></h2>
<p>This final lesson brings together everything from the previous 15 lessons into <strong>three complete, compilable projects</strong>:</p>
<ol>
<li><strong>Academic Paper</strong>: Full research paper with abstract, sections, figures, tables, equations, and bibliography</li>
<li><strong>Beamer Presentation</strong>: 15-slide conference presentation with custom theme, overlays, and TikZ diagrams</li>
<li><strong>TikZ Scientific Poster</strong>: A0 poster with multi-column layout, plots, and QR code</li>
</ol>
<p>Each project includes:
- Complete source code
- Compilation instructions
- Common pitfalls and solutions
- Customization tips
- Real-world best practices</p>
<hr />
<h2 id="project-1-academic-paper">Project 1: Academic Paper<a class="header-link" href="#project-1-academic-paper" title="Permanent link">&para;</a></h2>
<h3 id="overview">Overview<a class="header-link" href="#overview" title="Permanent link">&para;</a></h3>
<p>A complete research paper template suitable for:
- Conference submissions
- Journal articles
- Technical reports
- Course term papers</p>
<p><strong>Features</strong>:
- Title page with multiple authors and affiliations
- Abstract and keywords
- Two-column format
- Sections with subsections
- Figures with subfigures
- Tables with captions
- Mathematical equations (numbered and unnumbered)
- Algorithm pseudocode
- Cross-references
- Bibliography with BibLaTeX
- Hyperlinks</p>
<h3 id="complete-source-code">Complete Source Code<a class="header-link" href="#complete-source-code" title="Permanent link">&para;</a></h3>
<p><strong>File: <code>paper.tex</code></strong></p>
<div class="highlight"><pre><span></span><code><span class="k">\documentclass</span><span class="na">[conference]</span><span class="nb">{</span>IEEEtran<span class="nb">}</span>

<span class="c">% Packages</span>
<span class="k">\usepackage</span><span class="na">[utf8]</span><span class="nb">{</span>inputenc<span class="nb">}</span>
<span class="k">\usepackage</span><span class="na">[T1]</span><span class="nb">{</span>fontenc<span class="nb">}</span>
<span class="k">\usepackage</span><span class="nb">{</span>amsmath,amssymb,amsthm<span class="nb">}</span>
<span class="k">\usepackage</span><span class="nb">{</span>graphicx<span class="nb">}</span>
<span class="k">\usepackage</span><span class="nb">{</span>subcaption<span class="nb">}</span>
<span class="k">\usepackage</span><span class="nb">{</span>booktabs<span class="nb">}</span>
<span class="k">\usepackage</span><span class="nb">{</span>algorithm<span class="nb">}</span>
<span class="k">\usepackage</span><span class="nb">{</span>algpseudocode<span class="nb">}</span>
<span class="k">\usepackage</span><span class="na">[backend=biber,style=ieee,sorting=none]</span><span class="nb">{</span>biblatex<span class="nb">}</span>
<span class="k">\usepackage</span><span class="na">[hidelinks]</span><span class="nb">{</span>hyperref<span class="nb">}</span>
<span class="k">\usepackage</span><span class="nb">{</span>cleveref<span class="nb">}</span>

<span class="c">% Bibliography</span>
<span class="k">\addbibresource</span><span class="nb">{</span>references.bib<span class="nb">}</span>

<span class="c">% Custom commands (from Lesson 13)</span>
<span class="k">\newcommand</span><span class="nb">{</span><span class="k">\R</span><span class="nb">}{</span><span class="k">\mathbb</span><span class="nb">{</span>R<span class="nb">}}</span>
<span class="k">\newcommand</span><span class="nb">{</span><span class="k">\norm</span><span class="nb">}</span>[1]<span class="nb">{</span><span class="k">\left\|</span> #1 <span class="k">\right\|</span><span class="nb">}</span>
<span class="k">\DeclareMathOperator*</span><span class="nb">{</span><span class="k">\argmin</span><span class="nb">}{</span>arg<span class="k">\,</span>min<span class="nb">}</span>

<span class="c">% Title and authors</span>
<span class="k">\title</span><span class="nb">{</span>Deep Learning for Time Series Forecasting:<span class="k">\\</span>
A Comparative Study of LSTM and Transformer Models<span class="nb">}</span>

<span class="k">\author</span><span class="nb">{</span>
  <span class="k">\IEEEauthorblockN</span><span class="nb">{</span>Alice Johnson<span class="nb">}</span>
  <span class="k">\IEEEauthorblockA</span><span class="nb">{</span>Department of Computer Science<span class="k">\\</span>
    University of Example<span class="k">\\</span>
    alice.johnson@example.edu<span class="nb">}</span>
  <span class="k">\and</span>
  <span class="k">\IEEEauthorblockN</span><span class="nb">{</span>Bob Smith<span class="nb">}</span>
  <span class="k">\IEEEauthorblockA</span><span class="nb">{</span>Research Lab XYZ<span class="k">\\</span>
    Institute of Technology<span class="k">\\</span>
    bob.smith@xyz.org<span class="nb">}</span>
<span class="nb">}</span>

<span class="k">\begin</span><span class="nb">{</span>document<span class="nb">}</span>

<span class="k">\maketitle</span>

<span class="c">% Abstract</span>
<span class="k">\begin</span><span class="nb">{</span>abstract<span class="nb">}</span>
Time series forecasting is a critical task in many domains including finance, weather prediction, and energy management. Recent advances in deep learning have led to powerful models such as Long Short-Term Memory (LSTM) networks and Transformers. This paper presents a comprehensive comparison of LSTM and Transformer architectures for time series forecasting on three benchmark datasets. We evaluate model performance using Mean Absolute Error (MAE) and Root Mean Squared Error (RMSE). Our experiments show that while Transformers achieve superior accuracy on datasets with long-range dependencies, LSTMs remain competitive on shorter sequences and require significantly less computational resources. We provide implementation details and hyperparameter configurations to facilitate reproducibility.
<span class="k">\end</span><span class="nb">{</span>abstract<span class="nb">}</span>

<span class="k">\begin</span><span class="nb">{</span>IEEEkeywords<span class="nb">}</span>
Time series forecasting, LSTM, Transformer, deep learning, sequence modeling
<span class="k">\end</span><span class="nb">{</span>IEEEkeywords<span class="nb">}</span>

<span class="c">% Introduction</span>
<span class="k">\section</span><span class="nb">{</span>Introduction<span class="nb">}</span>
<span class="k">\label</span><span class="nb">{</span>sec:intro<span class="nb">}</span>

Time series forecasting aims to predict future values based on historical observations. Traditional methods such as ARIMA <span class="k">\cite</span><span class="nb">{</span>box2015time<span class="nb">}</span> and exponential smoothing have been widely used, but recent deep learning approaches have demonstrated superior performance on complex, high-dimensional data.

Long Short-Term Memory (LSTM) networks <span class="k">\cite</span><span class="nb">{</span>hochreiter1997long<span class="nb">}</span>, introduced by Hochreiter and Schmidhuber, were specifically designed to capture long-term dependencies in sequential data. More recently, the Transformer architecture <span class="k">\cite</span><span class="nb">{</span>vaswani2017attention<span class="nb">}</span>, originally developed for natural language processing, has been adapted for time series tasks <span class="k">\cite</span><span class="nb">{</span>zhou2021informer<span class="nb">}</span>.

This paper makes the following contributions:
<span class="k">\begin</span><span class="nb">{</span>itemize<span class="nb">}</span>
  <span class="k">\item</span> A systematic comparison of LSTM and Transformer models on three benchmark datasets
  <span class="k">\item</span> Analysis of computational efficiency and scalability
  <span class="k">\item</span> Open-source implementation and trained model weights
  <span class="k">\item</span> Guidelines for practitioners on model selection
<span class="k">\end</span><span class="nb">{</span>itemize<span class="nb">}</span>

The rest of this paper is organized as follows. <span class="k">\Cref</span><span class="nb">{</span>sec:related<span class="nb">}</span> reviews related work. <span class="k">\Cref</span><span class="nb">{</span>sec:methods<span class="nb">}</span> describes the models and datasets. <span class="k">\Cref</span><span class="nb">{</span>sec:results<span class="nb">}</span> presents experimental results. <span class="k">\Cref</span><span class="nb">{</span>sec:conclusion<span class="nb">}</span> concludes the paper.

<span class="c">% Related Work</span>
<span class="k">\section</span><span class="nb">{</span>Related Work<span class="nb">}</span>
<span class="k">\label</span><span class="nb">{</span>sec:related<span class="nb">}</span>

<span class="k">\subsection</span><span class="nb">{</span>Classical Methods<span class="nb">}</span>

Classical time series forecasting methods include ARIMA <span class="k">\cite</span><span class="nb">{</span>box2015time<span class="nb">}</span>, which models temporal dependencies through autoregressive and moving average components. Holt-Winters exponential smoothing extends these ideas to handle trends and seasonality.

<span class="k">\subsection</span><span class="nb">{</span>Deep Learning Approaches<span class="nb">}</span>

Recurrent Neural Networks (RNNs) and their variants have become popular for sequence modeling. LSTM networks <span class="k">\cite</span><span class="nb">{</span>hochreiter1997long<span class="nb">}</span> address the vanishing gradient problem through gating mechanisms. Gated Recurrent Units (GRUs) <span class="k">\cite</span><span class="nb">{</span>cho2014learning<span class="nb">}</span> simplify LSTM architecture while maintaining performance.

<span class="k">\subsection</span><span class="nb">{</span>Attention Mechanisms<span class="nb">}</span>

The Transformer architecture <span class="k">\cite</span><span class="nb">{</span>vaswani2017attention<span class="nb">}</span> relies entirely on self-attention mechanisms, eliminating recurrence. Informer <span class="k">\cite</span><span class="nb">{</span>zhou2021informer<span class="nb">}</span> adapts Transformers for long-sequence time series forecasting with efficient attention mechanisms.

<span class="c">% Methodology</span>
<span class="k">\section</span><span class="nb">{</span>Methodology<span class="nb">}</span>
<span class="k">\label</span><span class="nb">{</span>sec:methods<span class="nb">}</span>

<span class="k">\subsection</span><span class="nb">{</span>Problem Formulation<span class="nb">}</span>

Let <span class="s">$</span><span class="nv">\mathbf</span><span class="nb">{x} </span><span class="o">=</span><span class="nb"> </span><span class="o">(</span><span class="nb">x_</span><span class="m">1</span><span class="nb">, x_</span><span class="m">2</span><span class="nb">, </span><span class="nv">\ldots</span><span class="nb">, x_T</span><span class="o">)</span><span class="nb"> </span><span class="nv">\in</span><span class="nb"> </span><span class="nv">\R</span><span class="nb">^T</span><span class="s">$</span> denote a univariate time series. The forecasting task is to predict future values <span class="s">$</span><span class="nv">\mathbf</span><span class="nb">{y} </span><span class="o">=</span><span class="nb"> </span><span class="o">(</span><span class="nb">y_</span><span class="m">1</span><span class="nb">, y_</span><span class="m">2</span><span class="nb">, </span><span class="nv">\ldots</span><span class="nb">, y_H</span><span class="o">)</span><span class="s">$</span> given historical observations:
<span class="k">\begin</span><span class="nb">{</span>equation<span class="nb">}</span>
  <span class="k">\mathbf</span><span class="nb">{</span>y<span class="nb">}</span> = f(<span class="k">\mathbf</span><span class="nb">{</span>x<span class="nb">}</span>; <span class="k">\theta</span>)
  <span class="k">\label</span><span class="nb">{</span>eq:forecast<span class="nb">}</span>
<span class="k">\end</span><span class="nb">{</span>equation<span class="nb">}</span>
where <span class="s">$</span><span class="nb">f</span><span class="s">$</span> is the model parameterized by <span class="s">$</span><span class="nv">\theta</span><span class="s">$</span>, and <span class="s">$</span><span class="nb">H</span><span class="s">$</span> is the forecast horizon.

<span class="k">\subsection</span><span class="nb">{</span>LSTM Model<span class="nb">}</span>

The LSTM cell computes hidden state <span class="s">$</span><span class="nv">\mathbf</span><span class="nb">{h}_t</span><span class="s">$</span> and cell state <span class="s">$</span><span class="nv">\mathbf</span><span class="nb">{c}_t</span><span class="s">$</span> as:
<span class="k">\begin</span><span class="nb">{</span>align<span class="nb">}</span>
  <span class="k">\mathbf</span><span class="nb">{</span>f<span class="nb">}_</span>t <span class="nb">&amp;</span>= <span class="k">\sigma</span>(<span class="k">\mathbf</span><span class="nb">{</span>W<span class="nb">}_</span>f [<span class="k">\mathbf</span><span class="nb">{</span>h<span class="nb">}_{</span>t-1<span class="nb">}</span>, x<span class="nb">_</span>t] + <span class="k">\mathbf</span><span class="nb">{</span>b<span class="nb">}_</span>f) <span class="k">\label</span><span class="nb">{</span>eq:forget-gate<span class="nb">}</span> <span class="k">\\</span>
  <span class="k">\mathbf</span><span class="nb">{</span>i<span class="nb">}_</span>t <span class="nb">&amp;</span>= <span class="k">\sigma</span>(<span class="k">\mathbf</span><span class="nb">{</span>W<span class="nb">}_</span>i [<span class="k">\mathbf</span><span class="nb">{</span>h<span class="nb">}_{</span>t-1<span class="nb">}</span>, x<span class="nb">_</span>t] + <span class="k">\mathbf</span><span class="nb">{</span>b<span class="nb">}_</span>i) <span class="k">\label</span><span class="nb">{</span>eq:input-gate<span class="nb">}</span> <span class="k">\\</span>
  <span class="k">\tilde</span><span class="nb">{</span><span class="k">\mathbf</span><span class="nb">{</span>c<span class="nb">}}_</span>t <span class="nb">&amp;</span>= <span class="k">\tanh</span>(<span class="k">\mathbf</span><span class="nb">{</span>W<span class="nb">}_</span>c [<span class="k">\mathbf</span><span class="nb">{</span>h<span class="nb">}_{</span>t-1<span class="nb">}</span>, x<span class="nb">_</span>t] + <span class="k">\mathbf</span><span class="nb">{</span>b<span class="nb">}_</span>c) <span class="k">\\</span>
  <span class="k">\mathbf</span><span class="nb">{</span>c<span class="nb">}_</span>t <span class="nb">&amp;</span>= <span class="k">\mathbf</span><span class="nb">{</span>f<span class="nb">}_</span>t <span class="k">\odot</span> <span class="k">\mathbf</span><span class="nb">{</span>c<span class="nb">}_{</span>t-1<span class="nb">}</span> + <span class="k">\mathbf</span><span class="nb">{</span>i<span class="nb">}_</span>t <span class="k">\odot</span> <span class="k">\tilde</span><span class="nb">{</span><span class="k">\mathbf</span><span class="nb">{</span>c<span class="nb">}}_</span>t <span class="k">\\</span>
  <span class="k">\mathbf</span><span class="nb">{</span>o<span class="nb">}_</span>t <span class="nb">&amp;</span>= <span class="k">\sigma</span>(<span class="k">\mathbf</span><span class="nb">{</span>W<span class="nb">}_</span>o [<span class="k">\mathbf</span><span class="nb">{</span>h<span class="nb">}_{</span>t-1<span class="nb">}</span>, x<span class="nb">_</span>t] + <span class="k">\mathbf</span><span class="nb">{</span>b<span class="nb">}_</span>o) <span class="k">\\</span>
  <span class="k">\mathbf</span><span class="nb">{</span>h<span class="nb">}_</span>t <span class="nb">&amp;</span>= <span class="k">\mathbf</span><span class="nb">{</span>o<span class="nb">}_</span>t <span class="k">\odot</span> <span class="k">\tanh</span>(<span class="k">\mathbf</span><span class="nb">{</span>c<span class="nb">}_</span>t)
<span class="k">\end</span><span class="nb">{</span>align<span class="nb">}</span>
where <span class="s">$</span><span class="nv">\sigma</span><span class="s">$</span> is the sigmoid function and <span class="s">$</span><span class="nv">\odot</span><span class="s">$</span> denotes element-wise multiplication.

<span class="k">\subsection</span><span class="nb">{</span>Transformer Model<span class="nb">}</span>

The Transformer uses multi-head self-attention:
<span class="k">\begin</span><span class="nb">{</span>equation<span class="nb">}</span>
  <span class="k">\text</span><span class="nb">{</span>Attention<span class="nb">}</span>(<span class="k">\mathbf</span><span class="nb">{</span>Q<span class="nb">}</span>, <span class="k">\mathbf</span><span class="nb">{</span>K<span class="nb">}</span>, <span class="k">\mathbf</span><span class="nb">{</span>V<span class="nb">}</span>) = <span class="k">\text</span><span class="nb">{</span>softmax<span class="nb">}</span><span class="k">\left</span>(<span class="k">\frac</span><span class="nb">{</span><span class="k">\mathbf</span><span class="nb">{</span>Q<span class="nb">}</span><span class="k">\mathbf</span><span class="nb">{</span>K<span class="nb">}^</span>T<span class="nb">}{</span><span class="k">\sqrt</span><span class="nb">{</span>d<span class="nb">_</span>k<span class="nb">}}</span><span class="k">\right</span>) <span class="k">\mathbf</span><span class="nb">{</span>V<span class="nb">}</span>
<span class="k">\end</span><span class="nb">{</span>equation<span class="nb">}</span>
where <span class="s">$</span><span class="nv">\mathbf</span><span class="nb">{Q}</span><span class="s">$</span>, <span class="s">$</span><span class="nv">\mathbf</span><span class="nb">{K}</span><span class="s">$</span>, <span class="s">$</span><span class="nv">\mathbf</span><span class="nb">{V}</span><span class="s">$</span> are query, key, and value matrices derived from input embeddings.

<span class="k">\subsection</span><span class="nb">{</span>Datasets<span class="nb">}</span>

We evaluate models on three benchmark datasets (<span class="k">\cref</span><span class="nb">{</span>tab:datasets<span class="nb">}</span>):

<span class="k">\begin</span><span class="nb">{</span>table<span class="nb">}</span>[htbp]
<span class="k">\caption</span><span class="nb">{</span>Dataset Statistics<span class="nb">}</span>
<span class="k">\label</span><span class="nb">{</span>tab:datasets<span class="nb">}</span>
<span class="k">\centering</span>
<span class="k">\begin</span><span class="nb">{</span>tabular<span class="nb">}{</span>@<span class="nb">{}</span>lccc@<span class="nb">{}}</span>
<span class="k">\toprule</span>
Dataset <span class="nb">&amp;</span> Samples <span class="nb">&amp;</span> Features <span class="nb">&amp;</span> Frequency <span class="k">\\</span>
<span class="k">\midrule</span>
ETTh1 <span class="nb">&amp;</span> 17,420 <span class="nb">&amp;</span> 7 <span class="nb">&amp;</span> Hourly <span class="k">\\</span>
Weather <span class="nb">&amp;</span> 52,696 <span class="nb">&amp;</span> 21 <span class="nb">&amp;</span> 10 min <span class="k">\\</span>
Electricity <span class="nb">&amp;</span> 26,304 <span class="nb">&amp;</span> 321 <span class="nb">&amp;</span> Hourly <span class="k">\\</span>
<span class="k">\bottomrule</span>
<span class="k">\end</span><span class="nb">{</span>tabular<span class="nb">}</span>
<span class="k">\end</span><span class="nb">{</span>table<span class="nb">}</span>

<span class="k">\subsection</span><span class="nb">{</span>Training Procedure<span class="nb">}</span>

We minimize the Mean Squared Error (MSE) loss:
<span class="k">\begin</span><span class="nb">{</span>equation<span class="nb">}</span>
  L(<span class="k">\theta</span>) = <span class="k">\frac</span><span class="nb">{</span>1<span class="nb">}{</span>N<span class="nb">}</span> <span class="k">\sum_</span><span class="nb">{</span>i=1<span class="nb">}^</span>N <span class="k">\norm</span><span class="nb">{</span><span class="k">\mathbf</span><span class="nb">{</span>y<span class="nb">}^{</span>(i)<span class="nb">}</span> - f(<span class="k">\mathbf</span><span class="nb">{</span>x<span class="nb">}^{</span>(i)<span class="nb">}</span>; <span class="k">\theta</span>)<span class="nb">}^</span>2
<span class="k">\end</span><span class="nb">{</span>equation<span class="nb">}</span>

Models are trained using Adam optimizer <span class="k">\cite</span><span class="nb">{</span>kingma2014adam<span class="nb">}</span> with learning rate <span class="s">$</span><span class="nv">\eta</span><span class="nb"> </span><span class="o">=</span><span class="nb"> </span><span class="m">10</span><span class="nb">^{</span><span class="o">-</span><span class="m">4</span><span class="nb">}</span><span class="s">$</span>. <span class="k">\Cref</span><span class="nb">{</span>alg:training<span class="nb">}</span> summarizes the training procedure.

<span class="k">\begin</span><span class="nb">{</span>algorithm<span class="nb">}</span>[htbp]
<span class="k">\caption</span><span class="nb">{</span>Model Training<span class="nb">}</span>
<span class="k">\label</span><span class="nb">{</span>alg:training<span class="nb">}</span>
<span class="k">\begin</span><span class="nb">{</span>algorithmic<span class="nb">}</span>[1]
<span class="k">\Require</span> Dataset <span class="s">$</span><span class="nv">\mathcal</span><span class="nb">{D} </span><span class="o">=</span><span class="nb"> </span><span class="nv">\{</span><span class="o">(</span><span class="nv">\mathbf</span><span class="nb">{x}^{</span><span class="o">(</span><span class="nb">i</span><span class="o">)</span><span class="nb">}, </span><span class="nv">\mathbf</span><span class="nb">{y}^{</span><span class="o">(</span><span class="nb">i</span><span class="o">)</span><span class="nb">}</span><span class="o">)</span><span class="nv">\}</span><span class="nb">_{i</span><span class="o">=</span><span class="m">1</span><span class="nb">}^N</span><span class="s">$</span>, epochs <span class="s">$</span><span class="nb">E</span><span class="s">$</span>
<span class="k">\State</span> Initialize parameters <span class="s">$</span><span class="nv">\theta</span><span class="s">$</span>
<span class="k">\For</span><span class="nb">{</span><span class="s">$</span><span class="nb">e </span><span class="o">=</span><span class="nb"> </span><span class="m">1</span><span class="s">$</span> to <span class="s">$</span><span class="nb">E</span><span class="s">$</span><span class="nb">}</span>
  <span class="k">\For</span><span class="nb">{</span>each batch <span class="s">$</span><span class="nv">\mathcal</span><span class="nb">{B} </span><span class="nv">\subset</span><span class="nb"> </span><span class="nv">\mathcal</span><span class="nb">{D}</span><span class="s">$</span><span class="nb">}</span>
    <span class="k">\State</span> Compute predictions <span class="s">$</span><span class="nv">\hat</span><span class="nb">{</span><span class="nv">\mathbf</span><span class="nb">{y}} </span><span class="o">=</span><span class="nb"> f</span><span class="o">(</span><span class="nv">\mathbf</span><span class="nb">{x}; </span><span class="nv">\theta</span><span class="o">)</span><span class="s">$</span> for <span class="s">$</span><span class="nv">\mathbf</span><span class="nb">{x} </span><span class="nv">\in</span><span class="nb"> </span><span class="nv">\mathcal</span><span class="nb">{B}</span><span class="s">$</span>
    <span class="k">\State</span> Compute loss <span class="s">$</span><span class="nb">L </span><span class="o">=</span><span class="nb"> </span><span class="nv">\frac</span><span class="nb">{</span><span class="m">1</span><span class="nb">}{|</span><span class="nv">\mathcal</span><span class="nb">{B}|} </span><span class="nv">\sum</span><span class="nb">_{</span><span class="o">(</span><span class="nv">\mathbf</span><span class="nb">{x}, </span><span class="nv">\mathbf</span><span class="nb">{y}</span><span class="o">)</span><span class="nb"> </span><span class="nv">\in</span><span class="nb"> </span><span class="nv">\mathcal</span><span class="nb">{B}} </span><span class="nv">\norm</span><span class="nb">{</span><span class="nv">\mathbf</span><span class="nb">{y} </span><span class="o">-</span><span class="nb"> </span><span class="nv">\hat</span><span class="nb">{</span><span class="nv">\mathbf</span><span class="nb">{y}}}^</span><span class="m">2</span><span class="s">$</span>
    <span class="k">\State</span> Update <span class="s">$</span><span class="nv">\theta</span><span class="nb"> </span><span class="nv">\leftarrow</span><span class="nb"> </span><span class="nv">\theta</span><span class="nb"> </span><span class="o">-</span><span class="nb"> </span><span class="nv">\eta</span><span class="nb"> </span><span class="nv">\nabla</span><span class="nb">_</span><span class="nv">\theta</span><span class="nb"> L</span><span class="s">$</span>
  <span class="k">\EndFor</span>
<span class="k">\EndFor</span>
<span class="k">\State</span> <span class="k">\Return</span> <span class="s">$</span><span class="nv">\theta</span><span class="s">$</span>
<span class="k">\end</span><span class="nb">{</span>algorithmic<span class="nb">}</span>
<span class="k">\end</span><span class="nb">{</span>algorithm<span class="nb">}</span>

<span class="c">% Results</span>
<span class="k">\section</span><span class="nb">{</span>Experimental Results<span class="nb">}</span>
<span class="k">\label</span><span class="nb">{</span>sec:results<span class="nb">}</span>

<span class="k">\subsection</span><span class="nb">{</span>Quantitative Comparison<span class="nb">}</span>

<span class="k">\Cref</span><span class="nb">{</span>fig:results<span class="nb">}</span> shows MAE and RMSE for both models across datasets. Transformers achieve lower error on ETTh1 and Weather datasets, while LSTMs perform comparably on Electricity with 50<span class="k">\%</span> fewer parameters.

<span class="k">\begin</span><span class="nb">{</span>figure<span class="nb">}</span>[htbp]
  <span class="k">\centering</span>
  <span class="k">\begin</span><span class="nb">{</span>subfigure<span class="nb">}</span>[b]<span class="nb">{</span>0.45<span class="k">\columnwidth</span><span class="nb">}</span>
    <span class="k">\centering</span>
    <span class="k">\includegraphics</span><span class="na">[width=\textwidth]</span><span class="nb">{</span>mae<span class="nb">_</span>comparison.pdf<span class="nb">}</span>
    <span class="k">\caption</span><span class="nb">{</span>Mean Absolute Error<span class="nb">}</span>
    <span class="k">\label</span><span class="nb">{</span>fig:mae<span class="nb">}</span>
  <span class="k">\end</span><span class="nb">{</span>subfigure<span class="nb">}</span>
  <span class="k">\hfill</span>
  <span class="k">\begin</span><span class="nb">{</span>subfigure<span class="nb">}</span>[b]<span class="nb">{</span>0.45<span class="k">\columnwidth</span><span class="nb">}</span>
    <span class="k">\centering</span>
    <span class="k">\includegraphics</span><span class="na">[width=\textwidth]</span><span class="nb">{</span>rmse<span class="nb">_</span>comparison.pdf<span class="nb">}</span>
    <span class="k">\caption</span><span class="nb">{</span>Root Mean Squared Error<span class="nb">}</span>
    <span class="k">\label</span><span class="nb">{</span>fig:rmse<span class="nb">}</span>
  <span class="k">\end</span><span class="nb">{</span>subfigure<span class="nb">}</span>
  <span class="k">\caption</span><span class="nb">{</span>Performance comparison on three datasets. Lower is better.<span class="nb">}</span>
  <span class="k">\label</span><span class="nb">{</span>fig:results<span class="nb">}</span>
<span class="k">\end</span><span class="nb">{</span>figure<span class="nb">}</span>

<span class="k">\subsection</span><span class="nb">{</span>Computational Efficiency<span class="nb">}</span>

Training time and memory usage are shown in <span class="k">\cref</span><span class="nb">{</span>tab:efficiency<span class="nb">}</span>. LSTMs train 2-3√ó faster than Transformers and use less GPU memory.

<span class="k">\begin</span><span class="nb">{</span>table<span class="nb">}</span>[htbp]
<span class="k">\caption</span><span class="nb">{</span>Computational Efficiency (batch size 32)<span class="nb">}</span>
<span class="k">\label</span><span class="nb">{</span>tab:efficiency<span class="nb">}</span>
<span class="k">\centering</span>
<span class="k">\begin</span><span class="nb">{</span>tabular<span class="nb">}{</span>@<span class="nb">{}</span>lcc@<span class="nb">{}}</span>
<span class="k">\toprule</span>
Model <span class="nb">&amp;</span> Training Time (s/epoch) <span class="nb">&amp;</span> GPU Memory (GB) <span class="k">\\</span>
<span class="k">\midrule</span>
LSTM <span class="nb">&amp;</span> 45.2 <span class="nb">&amp;</span> 3.8 <span class="k">\\</span>
Transformer <span class="nb">&amp;</span> 128.7 <span class="nb">&amp;</span> 7.2 <span class="k">\\</span>
<span class="k">\bottomrule</span>
<span class="k">\end</span><span class="nb">{</span>tabular<span class="nb">}</span>
<span class="k">\end</span><span class="nb">{</span>table<span class="nb">}</span>

<span class="c">% Conclusion</span>
<span class="k">\section</span><span class="nb">{</span>Conclusion<span class="nb">}</span>
<span class="k">\label</span><span class="nb">{</span>sec:conclusion<span class="nb">}</span>

This paper presented a comprehensive comparison of LSTM and Transformer models for time series forecasting. Our experiments demonstrate that Transformers excel at capturing long-range dependencies but require more computational resources. LSTMs remain a strong choice for resource-constrained scenarios and shorter sequences.

Future work will explore hybrid architectures combining LSTM and attention mechanisms, and evaluation on multivariate forecasting tasks with more complex dependencies.

<span class="c">% Bibliography</span>
<span class="k">\printbibliography</span>

<span class="k">\end</span><span class="nb">{</span>document<span class="nb">}</span>
</code></pre></div>

<p><strong>File: <code>references.bib</code></strong></p>
<div class="highlight"><pre><span></span><code><span class="nc">@book</span><span class="p">{</span><span class="nl">box2015time</span><span class="p">,</span>
<span class="w">  </span><span class="na">title</span><span class="p">=</span><span class="s">{Time series analysis: forecasting and control}</span><span class="p">,</span>
<span class="w">  </span><span class="na">author</span><span class="p">=</span><span class="s">{Box, George EP and Jenkins, Gwilym M and Reinsel, Gregory C and Ljung, Greta M}</span><span class="p">,</span>
<span class="w">  </span><span class="na">year</span><span class="p">=</span><span class="s">{2015}</span><span class="p">,</span>
<span class="w">  </span><span class="na">publisher</span><span class="p">=</span><span class="s">{John Wiley \&amp; Sons}</span>
<span class="p">}</span>

<span class="nc">@article</span><span class="p">{</span><span class="nl">hochreiter1997long</span><span class="p">,</span>
<span class="w">  </span><span class="na">title</span><span class="p">=</span><span class="s">{Long short-term memory}</span><span class="p">,</span>
<span class="w">  </span><span class="na">author</span><span class="p">=</span><span class="s">{Hochreiter, Sepp and Schmidhuber, J{\&quot;u}rgen}</span><span class="p">,</span>
<span class="w">  </span><span class="na">journal</span><span class="p">=</span><span class="s">{Neural computation}</span><span class="p">,</span>
<span class="w">  </span><span class="na">volume</span><span class="p">=</span><span class="s">{9}</span><span class="p">,</span>
<span class="w">  </span><span class="na">number</span><span class="p">=</span><span class="s">{8}</span><span class="p">,</span>
<span class="w">  </span><span class="na">pages</span><span class="p">=</span><span class="s">{1735--1780}</span><span class="p">,</span>
<span class="w">  </span><span class="na">year</span><span class="p">=</span><span class="s">{1997}</span><span class="p">,</span>
<span class="w">  </span><span class="na">publisher</span><span class="p">=</span><span class="s">{MIT Press}</span>
<span class="p">}</span>

<span class="nc">@article</span><span class="p">{</span><span class="nl">vaswani2017attention</span><span class="p">,</span>
<span class="w">  </span><span class="na">title</span><span class="p">=</span><span class="s">{Attention is all you need}</span><span class="p">,</span>
<span class="w">  </span><span class="na">author</span><span class="p">=</span><span class="s">{Vaswani, Ashish and Shazeer, Noam and Parmar, Niki and Uszkoreit, Jakob and Jones, Llion and Gomez, Aidan N and Kaiser, {\L}ukasz and Polosukhin, Illia}</span><span class="p">,</span>
<span class="w">  </span><span class="na">journal</span><span class="p">=</span><span class="s">{Advances in neural information processing systems}</span><span class="p">,</span>
<span class="w">  </span><span class="na">volume</span><span class="p">=</span><span class="s">{30}</span><span class="p">,</span>
<span class="w">  </span><span class="na">year</span><span class="p">=</span><span class="s">{2017}</span>
<span class="p">}</span>

<span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">zhou2021informer</span><span class="p">,</span>
<span class="w">  </span><span class="na">title</span><span class="p">=</span><span class="s">{Informer: Beyond efficient transformer for long sequence time-series forecasting}</span><span class="p">,</span>
<span class="w">  </span><span class="na">author</span><span class="p">=</span><span class="s">{Zhou, Haoyi and Zhang, Shanghang and Peng, Jieqi and Zhang, Shuai and Li, Jianxin and Xiong, Hui and Zhang, Wancai}</span><span class="p">,</span>
<span class="w">  </span><span class="na">booktitle</span><span class="p">=</span><span class="s">{Proceedings of AAAI}</span><span class="p">,</span>
<span class="w">  </span><span class="na">year</span><span class="p">=</span><span class="s">{2021}</span>
<span class="p">}</span>

<span class="nc">@article</span><span class="p">{</span><span class="nl">kingma2014adam</span><span class="p">,</span>
<span class="w">  </span><span class="na">title</span><span class="p">=</span><span class="s">{Adam: A method for stochastic optimization}</span><span class="p">,</span>
<span class="w">  </span><span class="na">author</span><span class="p">=</span><span class="s">{Kingma, Diederik P and Ba, Jimmy}</span><span class="p">,</span>
<span class="w">  </span><span class="na">journal</span><span class="p">=</span><span class="s">{arXiv preprint arXiv:1412.6980}</span><span class="p">,</span>
<span class="w">  </span><span class="na">year</span><span class="p">=</span><span class="s">{2014}</span>
<span class="p">}</span>

<span class="nc">@article</span><span class="p">{</span><span class="nl">cho2014learning</span><span class="p">,</span>
<span class="w">  </span><span class="na">title</span><span class="p">=</span><span class="s">{Learning phrase representations using RNN encoder-decoder for statistical machine translation}</span><span class="p">,</span>
<span class="w">  </span><span class="na">author</span><span class="p">=</span><span class="s">{Cho, Kyunghyun and Van Merri{\&quot;e}nboer, Bart and Gulcehre, Caglar and Bahdanau, Dzmitry and Bougares, Fethi and Schwenk, Holger and Bengio, Yoshua}</span><span class="p">,</span>
<span class="w">  </span><span class="na">journal</span><span class="p">=</span><span class="s">{arXiv preprint arXiv:1406.1078}</span><span class="p">,</span>
<span class="w">  </span><span class="na">year</span><span class="p">=</span><span class="s">{2014}</span>
<span class="p">}</span>
</code></pre></div>

<h3 id="compilation">Compilation<a class="header-link" href="#compilation" title="Permanent link">&para;</a></h3>
<div class="highlight"><pre><span></span><code>pdflatex<span class="w"> </span>paper.tex
biber<span class="w"> </span>paper
pdflatex<span class="w"> </span>paper.tex
pdflatex<span class="w"> </span>paper.tex
</code></pre></div>

<p>Or with <code>latexmk</code>:</p>
<div class="highlight"><pre><span></span><code>latexmk<span class="w"> </span>-pdf<span class="w"> </span>paper.tex
</code></pre></div>

<h3 id="common-pitfalls">Common Pitfalls<a class="header-link" href="#common-pitfalls" title="Permanent link">&para;</a></h3>
<p><strong>Problem</strong>: "Undefined references" or <code>??</code> in PDF
- <strong>Solution</strong>: Run <code>biber paper</code> (not <code>bibtex</code>), then compile twice more</p>
<p><strong>Problem</strong>: Figures don't appear
- <strong>Solution</strong>: Create placeholder PDFs or comment out <code>\includegraphics</code> lines</p>
<p><strong>Problem</strong>: Two-column equations overflow
- <strong>Solution</strong>: Use <code>equation*</code> with smaller font, or switch to one-column with <code>figure*</code></p>
<h3 id="customization-tips">Customization Tips<a class="header-link" href="#customization-tips" title="Permanent link">&para;</a></h3>
<ul>
<li><strong>Single column</strong>: Remove <code>conference</code> option: <code>\documentclass{IEEEtran}</code></li>
<li><strong>Different bibliography style</strong>: Change <code>style=ieee</code> to <code>style=apa</code>, <code>style=nature</code>, etc.</li>
<li><strong>Add line numbers</strong>: <code>\usepackage{lineno}</code> and <code>\linenumbers</code> before <code>\begin{document}</code></li>
<li><strong>Blind review</strong>: Comment out <code>\author{}</code>, use <code>\author{Anonymous}</code></li>
</ul>
<hr />
<h2 id="project-2-beamer-presentation">Project 2: Beamer Presentation<a class="header-link" href="#project-2-beamer-presentation" title="Permanent link">&para;</a></h2>
<h3 id="overview_1">Overview<a class="header-link" href="#overview_1" title="Permanent link">&para;</a></h3>
<p>A 15-slide conference presentation with:
- Custom color theme
- Section slides with progress indicators
- Content slides with overlays (incremental reveals)
- TikZ diagrams
- Code listings
- Speaker notes
- Handout generation</p>
<h3 id="complete-source-code_1">Complete Source Code<a class="header-link" href="#complete-source-code_1" title="Permanent link">&para;</a></h3>
<p><strong>File: <code>presentation.tex</code></strong></p>
<div class="highlight"><pre><span></span><code><span class="k">\documentclass</span><span class="na">[aspectratio=169]</span><span class="nb">{</span>beamer<span class="nb">}</span>

<span class="c">% Theme</span>
<span class="k">\usetheme</span><span class="nb">{</span>Madrid<span class="nb">}</span>
<span class="k">\usecolortheme</span><span class="nb">{</span>default<span class="nb">}</span>

<span class="c">% Custom colors</span>
<span class="k">\definecolor</span><span class="nb">{</span>primaryblue<span class="nb">}{</span>RGB<span class="nb">}{</span>0,82,155<span class="nb">}</span>
<span class="k">\definecolor</span><span class="nb">{</span>secondaryorange<span class="nb">}{</span>RGB<span class="nb">}{</span>255,127,0<span class="nb">}</span>
<span class="k">\setbeamercolor</span><span class="nb">{</span>structure<span class="nb">}{</span>fg=primaryblue<span class="nb">}</span>
<span class="k">\setbeamercolor</span><span class="nb">{</span>alerted text<span class="nb">}{</span>fg=secondaryorange<span class="nb">}</span>

<span class="c">% Packages</span>
<span class="k">\usepackage</span><span class="na">[utf8]</span><span class="nb">{</span>inputenc<span class="nb">}</span>
<span class="k">\usepackage</span><span class="nb">{</span>amsmath,amssymb<span class="nb">}</span>
<span class="k">\usepackage</span><span class="nb">{</span>graphicx<span class="nb">}</span>
<span class="k">\usepackage</span><span class="nb">{</span>tikz<span class="nb">}</span>
<span class="k">\usetikzlibrary</span><span class="nb">{</span>shapes,arrows,positioning<span class="nb">}</span>
<span class="k">\usepackage</span><span class="nb">{</span>listings<span class="nb">}</span>
<span class="k">\usepackage</span><span class="nb">{</span>booktabs<span class="nb">}</span>

<span class="c">% Listings style</span>
<span class="k">\lstset</span><span class="nb">{</span>
  basicstyle=<span class="k">\ttfamily\small</span>,
  keywordstyle=<span class="k">\color</span><span class="nb">{</span>primaryblue<span class="nb">}</span><span class="k">\bfseries</span>,
  commentstyle=<span class="k">\color</span><span class="nb">{</span>gray<span class="nb">}</span><span class="k">\itshape</span>,
  stringstyle=<span class="k">\color</span><span class="nb">{</span>secondaryorange<span class="nb">}</span>,
  showstringspaces=false,
  frame=single
<span class="nb">}</span>

<span class="c">% Title</span>
<span class="k">\title</span><span class="nb">{</span>Deep Learning for Time Series Forecasting<span class="nb">}</span>
<span class="k">\subtitle</span><span class="nb">{</span>LSTM vs. Transformer: A Comparative Study<span class="nb">}</span>
<span class="k">\author</span><span class="nb">{</span>Alice Johnson<span class="nb">}</span>
<span class="k">\institute</span><span class="nb">{</span>University of Example<span class="nb">}</span>
<span class="k">\date</span><span class="nb">{</span>March 15, 2026<span class="nb">}</span>

<span class="c">% Remove navigation symbols</span>
<span class="k">\setbeamertemplate</span><span class="nb">{</span>navigation symbols<span class="nb">}{}</span>

<span class="c">% Footer with progress</span>
<span class="k">\setbeamertemplate</span><span class="nb">{</span>footline<span class="nb">}</span>[frame number]

<span class="k">\begin</span><span class="nb">{</span>document<span class="nb">}</span>

<span class="c">% Title slide</span>
<span class="k">\begin</span><span class="nb">{</span>frame<span class="nb">}</span>
  <span class="k">\titlepage</span>
  <span class="k">\note</span><span class="nb">{</span>Welcome everyone. Today I&#39;ll present our work on time series forecasting.<span class="nb">}</span>
<span class="k">\end</span><span class="nb">{</span>frame<span class="nb">}</span>

<span class="c">% Outline</span>
<span class="k">\begin</span><span class="nb">{</span>frame<span class="nb">}{</span>Outline<span class="nb">}</span>
  <span class="k">\tableofcontents</span>
  <span class="k">\note</span><span class="nb">{</span>Here&#39;s what we&#39;ll cover in the next 15 minutes.<span class="nb">}</span>
<span class="k">\end</span><span class="nb">{</span>frame<span class="nb">}</span>

<span class="c">% Section 1</span>
<span class="k">\section</span><span class="nb">{</span>Introduction<span class="nb">}</span>

<span class="k">\begin</span><span class="nb">{</span>frame<span class="nb">}{</span>Motivation<span class="nb">}</span>
  <span class="k">\begin</span><span class="nb">{</span>block<span class="nb">}{</span>Time Series Forecasting<span class="nb">}</span>
    Predicting future values based on historical observations
  <span class="k">\end</span><span class="nb">{</span>block<span class="nb">}</span>

  <span class="k">\vspace</span><span class="nb">{</span>0.5cm<span class="nb">}</span>

  <span class="k">\textbf</span><span class="nb">{</span>Applications<span class="nb">}</span>:
  <span class="k">\begin</span><span class="nb">{</span>itemize<span class="nb">}</span>
    <span class="k">\item</span>&lt;2-&gt; Finance: Stock price prediction
    <span class="k">\item</span>&lt;3-&gt; Energy: Electricity demand forecasting
    <span class="k">\item</span>&lt;4-&gt; Weather: Temperature and precipitation
    <span class="k">\item</span>&lt;5-&gt; Healthcare: Patient monitoring and early warning
  <span class="k">\end</span><span class="nb">{</span>itemize<span class="nb">}</span>

  <span class="k">\note</span>&lt;1-&gt;<span class="nb">{</span>Introduce the problem<span class="nb">}</span>
  <span class="k">\note</span>&lt;2-&gt;<span class="nb">{</span>Financial applications are critical<span class="nb">}</span>
  <span class="k">\note</span>&lt;3-&gt;<span class="nb">{</span>Energy sector needs accurate forecasts for grid management<span class="nb">}</span>
<span class="k">\end</span><span class="nb">{</span>frame<span class="nb">}</span>

<span class="k">\begin</span><span class="nb">{</span>frame<span class="nb">}{</span>Research Questions<span class="nb">}</span>
  <span class="k">\begin</span><span class="nb">{</span>enumerate<span class="nb">}</span>
    <span class="k">\item</span> How do LSTM and Transformer models compare in accuracy?
    <span class="k">\item</span> What are the computational trade-offs?
    <span class="k">\item</span> Which model should practitioners choose?
  <span class="k">\end</span><span class="nb">{</span>enumerate<span class="nb">}</span>

  <span class="k">\vspace</span><span class="nb">{</span>1cm<span class="nb">}</span>

  <span class="k">\pause</span>

  <span class="k">\begin</span><span class="nb">{</span>alertblock<span class="nb">}{</span>Hypothesis<span class="nb">}</span>
    Transformers achieve better accuracy on long sequences, but LSTMs are more efficient.
  <span class="k">\end</span><span class="nb">{</span>alertblock<span class="nb">}</span>
<span class="k">\end</span><span class="nb">{</span>frame<span class="nb">}</span>

<span class="c">% Section 2</span>
<span class="k">\section</span><span class="nb">{</span>Methodology<span class="nb">}</span>

<span class="k">\begin</span><span class="nb">{</span>frame<span class="nb">}{</span>Model Architectures<span class="nb">}</span>
  <span class="k">\begin</span><span class="nb">{</span>columns<span class="nb">}</span>[T]
    <span class="k">\begin</span><span class="nb">{</span>column<span class="nb">}{</span>0.48<span class="k">\textwidth</span><span class="nb">}</span>
      <span class="k">\centering</span>
      <span class="k">\textbf</span><span class="nb">{</span>LSTM<span class="nb">}</span>
      <span class="k">\vspace</span><span class="nb">{</span>0.3cm<span class="nb">}</span>

      <span class="k">\begin</span><span class="nb">{</span>tikzpicture<span class="nb">}</span>[scale=0.7,
        node/.style=<span class="nb">{</span>rectangle,draw,minimum width=1.5cm,minimum height=0.8cm<span class="nb">}</span>]
        <span class="k">\node</span><span class="na">[node]</span> (x) at (0,0) <span class="nb">{</span>Input<span class="nb">}</span>;
        <span class="k">\node</span><span class="na">[node]</span> (lstm1) at (0,1.5) <span class="nb">{</span>LSTM<span class="nb">}</span>;
        <span class="k">\node</span><span class="na">[node]</span> (lstm2) at (0,3) <span class="nb">{</span>LSTM<span class="nb">}</span>;
        <span class="k">\node</span><span class="na">[node]</span> (fc) at (0,4.5) <span class="nb">{</span>Dense<span class="nb">}</span>;
        <span class="k">\node</span><span class="na">[node]</span> (y) at (0,6) <span class="nb">{</span>Output<span class="nb">}</span>;

        <span class="k">\draw</span><span class="na">[-&gt;]</span> (x) -- (lstm1);
        <span class="k">\draw</span><span class="na">[-&gt;]</span> (lstm1) -- (lstm2);
        <span class="k">\draw</span><span class="na">[-&gt;]</span> (lstm2) -- (fc);
        <span class="k">\draw</span><span class="na">[-&gt;]</span> (fc) -- (y);
      <span class="k">\end</span><span class="nb">{</span>tikzpicture<span class="nb">}</span>
    <span class="k">\end</span><span class="nb">{</span>column<span class="nb">}</span>

    <span class="k">\begin</span><span class="nb">{</span>column<span class="nb">}{</span>0.48<span class="k">\textwidth</span><span class="nb">}</span>
      <span class="k">\centering</span>
      <span class="k">\textbf</span><span class="nb">{</span>Transformer<span class="nb">}</span>
      <span class="k">\vspace</span><span class="nb">{</span>0.3cm<span class="nb">}</span>

      <span class="k">\begin</span><span class="nb">{</span>tikzpicture<span class="nb">}</span>[scale=0.7,
        node/.style=<span class="nb">{</span>rectangle,draw,minimum width=1.5cm,minimum height=0.8cm<span class="nb">}</span>]
        <span class="k">\node</span><span class="na">[node]</span> (x) at (0,0) <span class="nb">{</span>Input<span class="nb">}</span>;
        <span class="k">\node</span><span class="na">[node]</span> (emb) at (0,1.5) <span class="nb">{</span>Embedding<span class="nb">}</span>;
        <span class="k">\node</span><span class="na">[node]</span> (att1) at (0,3) <span class="nb">{</span>Attention<span class="nb">}</span>;
        <span class="k">\node</span><span class="na">[node]</span> (att2) at (0,4.5) <span class="nb">{</span>Attention<span class="nb">}</span>;
        <span class="k">\node</span><span class="na">[node]</span> (y) at (0,6) <span class="nb">{</span>Output<span class="nb">}</span>;

        <span class="k">\draw</span><span class="na">[-&gt;]</span> (x) -- (emb);
        <span class="k">\draw</span><span class="na">[-&gt;]</span> (emb) -- (att1);
        <span class="k">\draw</span><span class="na">[-&gt;]</span> (att1) -- (att2);
        <span class="k">\draw</span><span class="na">[-&gt;]</span> (att2) -- (y);
      <span class="k">\end</span><span class="nb">{</span>tikzpicture<span class="nb">}</span>
    <span class="k">\end</span><span class="nb">{</span>column<span class="nb">}</span>
  <span class="k">\end</span><span class="nb">{</span>columns<span class="nb">}</span>

  <span class="k">\note</span><span class="nb">{</span>LSTM processes sequences recurrently; Transformer uses parallel attention.<span class="nb">}</span>
<span class="k">\end</span><span class="nb">{</span>frame<span class="nb">}</span>

<span class="k">\begin</span><span class="nb">{</span>frame<span class="nb">}</span>[fragile]<span class="nb">{</span>LSTM Equations<span class="nb">}</span>
  The LSTM cell updates hidden state via gating:

  <span class="k">\begin</span><span class="nb">{</span>align*<span class="nb">}</span>
    <span class="k">\mathbf</span><span class="nb">{</span>f<span class="nb">}_</span>t <span class="nb">&amp;</span>= <span class="k">\sigma</span>(<span class="k">\mathbf</span><span class="nb">{</span>W<span class="nb">}_</span>f [<span class="k">\mathbf</span><span class="nb">{</span>h<span class="nb">}_{</span>t-1<span class="nb">}</span>, x<span class="nb">_</span>t] + <span class="k">\mathbf</span><span class="nb">{</span>b<span class="nb">}_</span>f) <span class="k">\\</span>
    <span class="k">\mathbf</span><span class="nb">{</span>i<span class="nb">}_</span>t <span class="nb">&amp;</span>= <span class="k">\sigma</span>(<span class="k">\mathbf</span><span class="nb">{</span>W<span class="nb">}_</span>i [<span class="k">\mathbf</span><span class="nb">{</span>h<span class="nb">}_{</span>t-1<span class="nb">}</span>, x<span class="nb">_</span>t] + <span class="k">\mathbf</span><span class="nb">{</span>b<span class="nb">}_</span>i) <span class="k">\\</span>
    <span class="k">\mathbf</span><span class="nb">{</span>c<span class="nb">}_</span>t <span class="nb">&amp;</span>= <span class="k">\mathbf</span><span class="nb">{</span>f<span class="nb">}_</span>t <span class="k">\odot</span> <span class="k">\mathbf</span><span class="nb">{</span>c<span class="nb">}_{</span>t-1<span class="nb">}</span> + <span class="k">\mathbf</span><span class="nb">{</span>i<span class="nb">}_</span>t <span class="k">\odot</span> <span class="k">\tilde</span><span class="nb">{</span><span class="k">\mathbf</span><span class="nb">{</span>c<span class="nb">}}_</span>t
  <span class="k">\end</span><span class="nb">{</span>align*<span class="nb">}</span>

  <span class="k">\vspace</span><span class="nb">{</span>0.5cm<span class="nb">}</span>

  <span class="k">\pause</span>

  <span class="k">\begin</span><span class="nb">{</span>lstlisting<span class="nb">}</span>[language=Python]
import torch.nn as nn

lstm = nn.LSTM(input<span class="nb">_</span>size=10, hidden<span class="nb">_</span>size=128,
               num<span class="nb">_</span>layers=2, batch<span class="nb">_</span>first=True)
  <span class="k">\end</span><span class="nb">{</span>lstlisting<span class="nb">}</span>
<span class="k">\end</span><span class="nb">{</span>frame<span class="nb">}</span>

<span class="k">\begin</span><span class="nb">{</span>frame<span class="nb">}{</span>Datasets<span class="nb">}</span>
  <span class="k">\begin</span><span class="nb">{</span>table<span class="nb">}</span>
    <span class="k">\centering</span>
    <span class="k">\begin</span><span class="nb">{</span>tabular<span class="nb">}{</span>@<span class="nb">{}</span>lccc@<span class="nb">{}}</span>
      <span class="k">\toprule</span>
      Dataset <span class="nb">&amp;</span> Samples <span class="nb">&amp;</span> Features <span class="nb">&amp;</span> Frequency <span class="k">\\</span>
      <span class="k">\midrule</span>
      ETTh1 <span class="nb">&amp;</span> 17,420 <span class="nb">&amp;</span> 7 <span class="nb">&amp;</span> Hourly <span class="k">\\</span>
      Weather <span class="nb">&amp;</span> 52,696 <span class="nb">&amp;</span> 21 <span class="nb">&amp;</span> 10 min <span class="k">\\</span>
      Electricity <span class="nb">&amp;</span> 26,304 <span class="nb">&amp;</span> 321 <span class="nb">&amp;</span> Hourly <span class="k">\\</span>
      <span class="k">\bottomrule</span>
    <span class="k">\end</span><span class="nb">{</span>tabular<span class="nb">}</span>
  <span class="k">\end</span><span class="nb">{</span>table<span class="nb">}</span>

  <span class="k">\vspace</span><span class="nb">{</span>0.5cm<span class="nb">}</span>

  <span class="k">\begin</span><span class="nb">{</span>itemize<span class="nb">}</span>
    <span class="k">\item</span> Split: 70<span class="k">\%</span> train, 15<span class="k">\%</span> validation, 15<span class="k">\%</span> test
    <span class="k">\item</span> Metrics: MAE, RMSE
  <span class="k">\end</span><span class="nb">{</span>itemize<span class="nb">}</span>
<span class="k">\end</span><span class="nb">{</span>frame<span class="nb">}</span>

<span class="c">% Section 3</span>
<span class="k">\section</span><span class="nb">{</span>Results<span class="nb">}</span>

<span class="k">\begin</span><span class="nb">{</span>frame<span class="nb">}{</span>Accuracy Comparison<span class="nb">}</span>
  <span class="k">\centering</span>
  <span class="k">\includegraphics</span><span class="na">[width=0.7\textwidth]</span><span class="nb">{</span>mae<span class="nb">_</span>comparison.pdf<span class="nb">}</span>

  <span class="k">\vspace</span><span class="nb">{</span>0.3cm<span class="nb">}</span>

  <span class="k">\begin</span><span class="nb">{</span>itemize<span class="nb">}</span>
    <span class="k">\item</span> Transformer: 12<span class="k">\%</span> lower MAE on ETTh1
    <span class="k">\item</span> LSTM: Competitive on Electricity dataset
  <span class="k">\end</span><span class="nb">{</span>itemize<span class="nb">}</span>

  <span class="k">\note</span><span class="nb">{</span>Highlight the accuracy advantage of Transformers on long sequences.<span class="nb">}</span>
<span class="k">\end</span><span class="nb">{</span>frame<span class="nb">}</span>

<span class="k">\begin</span><span class="nb">{</span>frame<span class="nb">}{</span>Computational Efficiency<span class="nb">}</span>
  <span class="k">\begin</span><span class="nb">{</span>columns<span class="nb">}</span>[T]
    <span class="k">\begin</span><span class="nb">{</span>column<span class="nb">}{</span>0.5<span class="k">\textwidth</span><span class="nb">}</span>
      <span class="k">\textbf</span><span class="nb">{</span>Training Time<span class="nb">}</span>
      <span class="k">\begin</span><span class="nb">{</span>itemize<span class="nb">}</span>
        <span class="k">\item</span> LSTM: 45 s/epoch
        <span class="k">\item</span> Transformer: 129 s/epoch
        <span class="k">\item</span> <span class="k">\alert</span><span class="nb">{</span>2.9√ó faster<span class="nb">}</span>
      <span class="k">\end</span><span class="nb">{</span>itemize<span class="nb">}</span>
    <span class="k">\end</span><span class="nb">{</span>column<span class="nb">}</span>

    <span class="k">\begin</span><span class="nb">{</span>column<span class="nb">}{</span>0.5<span class="k">\textwidth</span><span class="nb">}</span>
      <span class="k">\textbf</span><span class="nb">{</span>GPU Memory<span class="nb">}</span>
      <span class="k">\begin</span><span class="nb">{</span>itemize<span class="nb">}</span>
        <span class="k">\item</span> LSTM: 3.8 GB
        <span class="k">\item</span> Transformer: 7.2 GB
        <span class="k">\item</span> <span class="k">\alert</span><span class="nb">{</span>1.9√ó less memory<span class="nb">}</span>
      <span class="k">\end</span><span class="nb">{</span>itemize<span class="nb">}</span>
    <span class="k">\end</span><span class="nb">{</span>column<span class="nb">}</span>
  <span class="k">\end</span><span class="nb">{</span>columns<span class="nb">}</span>

  <span class="k">\vspace</span><span class="nb">{</span>1cm<span class="nb">}</span>

  <span class="k">\begin</span><span class="nb">{</span>block<span class="nb">}{</span>Key Insight<span class="nb">}</span>
    LSTMs offer significant computational savings with minor accuracy trade-offs.
  <span class="k">\end</span><span class="nb">{</span>block<span class="nb">}</span>
<span class="k">\end</span><span class="nb">{</span>frame<span class="nb">}</span>

<span class="c">% Section 4</span>
<span class="k">\section</span><span class="nb">{</span>Conclusion<span class="nb">}</span>

<span class="k">\begin</span><span class="nb">{</span>frame<span class="nb">}{</span>Summary<span class="nb">}</span>
  <span class="k">\textbf</span><span class="nb">{</span>Contributions<span class="nb">}</span>:
  <span class="k">\begin</span><span class="nb">{</span>itemize<span class="nb">}</span>
    <span class="k">\item</span> Systematic comparison on 3 benchmark datasets
    <span class="k">\item</span> Analysis of accuracy vs. efficiency trade-offs
    <span class="k">\item</span> Open-source code and trained models
  <span class="k">\end</span><span class="nb">{</span>itemize<span class="nb">}</span>

  <span class="k">\vspace</span><span class="nb">{</span>0.8cm<span class="nb">}</span>

  <span class="k">\pause</span>

  <span class="k">\textbf</span><span class="nb">{</span>Recommendations<span class="nb">}</span>:
  <span class="k">\begin</span><span class="nb">{</span>itemize<span class="nb">}</span>
    <span class="k">\item</span> Use <span class="k">\alert</span><span class="nb">{</span>Transformer<span class="nb">}</span> for maximum accuracy on long sequences
    <span class="k">\item</span> Use <span class="k">\alert</span><span class="nb">{</span>LSTM<span class="nb">}</span> for resource-constrained environments
    <span class="k">\item</span> Consider hybrid models for best of both worlds
  <span class="k">\end</span><span class="nb">{</span>itemize<span class="nb">}</span>
<span class="k">\end</span><span class="nb">{</span>frame<span class="nb">}</span>

<span class="k">\begin</span><span class="nb">{</span>frame<span class="nb">}{</span>Future Work<span class="nb">}</span>
  <span class="k">\begin</span><span class="nb">{</span>enumerate<span class="nb">}</span>
    <span class="k">\item</span> Hybrid LSTM-Transformer architectures
    <span class="k">\item</span> Multivariate forecasting with graph neural networks
    <span class="k">\item</span> Real-time inference optimization
    <span class="k">\item</span> Application to finance and healthcare domains
  <span class="k">\end</span><span class="nb">{</span>enumerate<span class="nb">}</span>

  <span class="k">\vspace</span><span class="nb">{</span>1cm<span class="nb">}</span>

  <span class="k">\centering</span>
  <span class="k">\Large</span> <span class="k">\textbf</span><span class="nb">{</span>Thank you!<span class="nb">}</span>

  <span class="k">\vspace</span><span class="nb">{</span>0.5cm<span class="nb">}</span>

  <span class="k">\normalsize</span>
  Questions? <span class="k">\\</span>
  <span class="k">\texttt</span><span class="nb">{</span>alice.johnson@example.edu<span class="nb">}</span>
<span class="k">\end</span><span class="nb">{</span>frame<span class="nb">}</span>

<span class="c">% Backup slides</span>
<span class="k">\appendix</span>

<span class="k">\begin</span><span class="nb">{</span>frame<span class="nb">}{</span>Backup: Hyperparameters<span class="nb">}</span>
  <span class="k">\begin</span><span class="nb">{</span>table<span class="nb">}</span>
    <span class="k">\centering</span>
    <span class="k">\small</span>
    <span class="k">\begin</span><span class="nb">{</span>tabular<span class="nb">}{</span>@<span class="nb">{}</span>lcc@<span class="nb">{}}</span>
      <span class="k">\toprule</span>
      Parameter <span class="nb">&amp;</span> LSTM <span class="nb">&amp;</span> Transformer <span class="k">\\</span>
      <span class="k">\midrule</span>
      Hidden size <span class="nb">&amp;</span> 128 <span class="nb">&amp;</span> 256 <span class="k">\\</span>
      Layers <span class="nb">&amp;</span> 2 <span class="nb">&amp;</span> 4 <span class="k">\\</span>
      Dropout <span class="nb">&amp;</span> 0.2 <span class="nb">&amp;</span> 0.1 <span class="k">\\</span>
      Learning rate <span class="nb">&amp;</span> <span class="s">$</span><span class="m">10</span><span class="nb">^{</span><span class="o">-</span><span class="m">4</span><span class="nb">}</span><span class="s">$</span> <span class="nb">&amp;</span> <span class="s">$</span><span class="m">10</span><span class="nb">^{</span><span class="o">-</span><span class="m">4</span><span class="nb">}</span><span class="s">$</span> <span class="k">\\</span>
      Batch size <span class="nb">&amp;</span> 32 <span class="nb">&amp;</span> 32 <span class="k">\\</span>
      <span class="k">\bottomrule</span>
    <span class="k">\end</span><span class="nb">{</span>tabular<span class="nb">}</span>
  <span class="k">\end</span><span class="nb">{</span>table<span class="nb">}</span>
<span class="k">\end</span><span class="nb">{</span>frame<span class="nb">}</span>

<span class="k">\end</span><span class="nb">{</span>document<span class="nb">}</span>
</code></pre></div>

<h3 id="compilation_1">Compilation<a class="header-link" href="#compilation_1" title="Permanent link">&para;</a></h3>
<div class="highlight"><pre><span></span><code>pdflatex<span class="w"> </span>presentation.tex
pdflatex<span class="w"> </span>presentation.tex
</code></pre></div>

<h3 id="creating-handouts">Creating Handouts<a class="header-link" href="#creating-handouts" title="Permanent link">&para;</a></h3>
<p>Add this option:</p>
<div class="highlight"><pre><span></span><code><span class="k">\documentclass</span><span class="na">[aspectratio=169,handout]</span><span class="nb">{</span>beamer<span class="nb">}</span>
</code></pre></div>

<p>Compile as usual. Overlays will be collapsed.</p>
<h3 id="speaker-notes">Speaker Notes<a class="header-link" href="#speaker-notes" title="Permanent link">&para;</a></h3>
<p>View notes in PDF viewer supporting annotations, or use:</p>
<div class="highlight"><pre><span></span><code>pdfpc<span class="w"> </span>presentation.pdf
</code></pre></div>

<p>(Requires <code>pdfpc</code> tool)</p>
<h3 id="common-pitfalls_1">Common Pitfalls<a class="header-link" href="#common-pitfalls_1" title="Permanent link">&para;</a></h3>
<p><strong>Problem</strong>: Overlays don't work
- <strong>Solution</strong>: Use <code>\pause</code>, <code>\only&lt;2-&gt;</code>, <code>\item&lt;3-&gt;</code> syntax correctly</p>
<p><strong>Problem</strong>: Too much text per slide
- <strong>Solution</strong>: Follow "6√ó6 rule": max 6 bullets, 6 words each</p>
<p><strong>Problem</strong>: TikZ diagrams too complex
- <strong>Solution</strong>: Simplify or create in external tool, import as PDF</p>
<hr />
<h2 id="project-3-tikz-scientific-poster">Project 3: TikZ Scientific Poster<a class="header-link" href="#project-3-tikz-scientific-poster" title="Permanent link">&para;</a></h2>
<h3 id="overview_2">Overview<a class="header-link" href="#overview_2" title="Permanent link">&para;</a></h3>
<p>An A0 poster (841 √ó 1189 mm) for a conference, featuring:
- Multi-column layout (3 columns)
- Title banner with logos
- Introduction, methods, results, conclusion blocks
- PGFPlots for data visualization
- TikZ flowchart
- QR code for references
- Custom color scheme</p>
<h3 id="complete-source-code_2">Complete Source Code<a class="header-link" href="#complete-source-code_2" title="Permanent link">&para;</a></h3>
<p><strong>File: <code>poster.tex</code></strong></p>
<div class="highlight"><pre><span></span><code><span class="k">\documentclass</span><span class="na">[a0paper,portrait]</span><span class="nb">{</span>tikzposter<span class="nb">}</span>

<span class="c">% Packages</span>
<span class="k">\usepackage</span><span class="na">[utf8]</span><span class="nb">{</span>inputenc<span class="nb">}</span>
<span class="k">\usepackage</span><span class="nb">{</span>amsmath,amssymb<span class="nb">}</span>
<span class="k">\usepackage</span><span class="nb">{</span>graphicx<span class="nb">}</span>
<span class="k">\usepackage</span><span class="nb">{</span>booktabs<span class="nb">}</span>
<span class="k">\usepackage</span><span class="nb">{</span>pgfplots<span class="nb">}</span>
<span class="k">\pgfplotsset</span><span class="nb">{</span>compat=1.18<span class="nb">}</span>
<span class="k">\usepackage</span><span class="nb">{</span>qrcode<span class="nb">}</span>

<span class="c">% Theme</span>
<span class="k">\usetheme</span><span class="nb">{</span>Default<span class="nb">}</span>
<span class="k">\usecolorstyle</span><span class="nb">{</span>Denmark<span class="nb">}</span>

<span class="c">% Custom colors</span>
<span class="k">\definecolor</span><span class="nb">{</span>primaryblue<span class="nb">}{</span>RGB<span class="nb">}{</span>0,82,155<span class="nb">}</span>
<span class="k">\definecolor</span><span class="nb">{</span>lightblue<span class="nb">}{</span>RGB<span class="nb">}{</span>204,229,255<span class="nb">}</span>
<span class="k">\definecolor</span><span class="nb">{</span>darkgray<span class="nb">}{</span>RGB<span class="nb">}{</span>51,51,51<span class="nb">}</span>

<span class="k">\colorlet</span><span class="nb">{</span>backgroundcolor<span class="nb">}{</span>lightblue!30<span class="nb">}</span>
<span class="k">\colorlet</span><span class="nb">{</span>blocktitlefgcolor<span class="nb">}{</span>white<span class="nb">}</span>
<span class="k">\colorlet</span><span class="nb">{</span>blocktitlebgcolor<span class="nb">}{</span>primaryblue<span class="nb">}</span>
<span class="k">\colorlet</span><span class="nb">{</span>blockbodyfgcolor<span class="nb">}{</span>darkgray<span class="nb">}</span>
<span class="k">\colorlet</span><span class="nb">{</span>blockbodybgcolor<span class="nb">}{</span>white<span class="nb">}</span>

<span class="c">% Title</span>
<span class="k">\title</span><span class="nb">{</span><span class="k">\parbox</span><span class="nb">{</span>0.8<span class="k">\linewidth</span><span class="nb">}{</span><span class="k">\centering</span> Deep Learning for Time Series Forecasting: LSTM vs. Transformer<span class="nb">}}</span>
<span class="k">\author</span><span class="nb">{</span>Alice Johnson<span class="s">$</span><span class="nb">^</span><span class="m">1</span><span class="s">$</span>, Bob Smith<span class="s">$</span><span class="nb">^</span><span class="m">2</span><span class="s">$</span><span class="nb">}</span>
<span class="k">\institute</span><span class="nb">{</span><span class="s">$</span><span class="nb">^</span><span class="m">1</span><span class="s">$</span>University of Example, <span class="s">$</span><span class="nb">^</span><span class="m">2</span><span class="s">$</span>Research Lab XYZ<span class="nb">}</span>

<span class="c">% Logos (placeholders - replace with actual logos)</span>
<span class="k">\titlegraphic</span><span class="nb">{</span>
  <span class="k">\includegraphics</span><span class="na">[width=0.1\textwidth]</span><span class="nb">{</span>logo1.pdf<span class="nb">}</span>
  <span class="k">\hspace</span><span class="nb">{</span>2cm<span class="nb">}</span>
  <span class="k">\includegraphics</span><span class="na">[width=0.1\textwidth]</span><span class="nb">{</span>logo2.pdf<span class="nb">}</span>
<span class="nb">}</span>

<span class="k">\begin</span><span class="nb">{</span>document<span class="nb">}</span>

<span class="k">\maketitle</span>

<span class="k">\begin</span><span class="nb">{</span>columns<span class="nb">}</span>
  <span class="c">% Column 1</span>
  <span class="k">\column</span><span class="nb">{</span>0.33<span class="nb">}</span>

  <span class="k">\block</span><span class="nb">{</span>Introduction<span class="nb">}{</span>
    <span class="k">\textbf</span><span class="nb">{</span>Motivation:<span class="nb">}</span> Time series forecasting is critical in finance, energy, and healthcare.

    <span class="k">\vspace</span><span class="nb">{</span>0.5cm<span class="nb">}</span>

    <span class="k">\textbf</span><span class="nb">{</span>Problem:<span class="nb">}</span> Traditional methods (ARIMA, exponential smoothing) struggle with complex, high-dimensional data.

    <span class="k">\vspace</span><span class="nb">{</span>0.5cm<span class="nb">}</span>

    <span class="k">\textbf</span><span class="nb">{</span>Solution:<span class="nb">}</span> Deep learning models (LSTM, Transformer) capture nonlinear dependencies.

    <span class="k">\vspace</span><span class="nb">{</span>0.5cm<span class="nb">}</span>

    <span class="k">\textbf</span><span class="nb">{</span>Research Questions:<span class="nb">}</span>
    <span class="k">\begin</span><span class="nb">{</span>itemize<span class="nb">}</span>
      <span class="k">\item</span> How do LSTM and Transformer compare in accuracy?
      <span class="k">\item</span> What are computational trade-offs?
      <span class="k">\item</span> Which model should practitioners choose?
    <span class="k">\end</span><span class="nb">{</span>itemize<span class="nb">}</span>
  <span class="nb">}</span>

  <span class="k">\block</span><span class="nb">{</span>Model Architectures<span class="nb">}{</span>
    <span class="k">\textbf</span><span class="nb">{</span>LSTM:<span class="nb">}</span> Recurrent architecture with gating mechanisms
    <span class="sb">\[</span>
<span class="nb">      </span><span class="nv">\mathbf</span><span class="nb">{c}_t </span><span class="o">=</span><span class="nb"> </span><span class="nv">\mathbf</span><span class="nb">{f}_t </span><span class="nv">\odot</span><span class="nb"> </span><span class="nv">\mathbf</span><span class="nb">{c}_{t</span><span class="o">-</span><span class="m">1</span><span class="nb">} </span><span class="o">+</span><span class="nb"> </span><span class="nv">\mathbf</span><span class="nb">{i}_t </span><span class="nv">\odot</span><span class="nb"> </span><span class="nv">\tilde</span><span class="nb">{</span><span class="nv">\mathbf</span><span class="nb">{c}}_t</span>
<span class="nb">    </span><span class="s">\]</span>

    <span class="k">\vspace</span><span class="nb">{</span>1cm<span class="nb">}</span>

    <span class="k">\textbf</span><span class="nb">{</span>Transformer:<span class="nb">}</span> Self-attention for parallel sequence processing
    <span class="sb">\[</span>
<span class="nb">      </span><span class="nv">\text</span><span class="nb">{Attention}</span><span class="o">(</span><span class="nv">\mathbf</span><span class="nb">{Q}, </span><span class="nv">\mathbf</span><span class="nb">{K}, </span><span class="nv">\mathbf</span><span class="nb">{V}</span><span class="o">)</span><span class="nb"> </span><span class="o">=</span><span class="nb"> </span><span class="nv">\text</span><span class="nb">{softmax}</span><span class="nv">\left</span><span class="o">(</span><span class="nv">\frac</span><span class="nb">{</span><span class="nv">\mathbf</span><span class="nb">{Q}</span><span class="nv">\mathbf</span><span class="nb">{K}^T}{</span><span class="nv">\sqrt</span><span class="nb">{d_k}}</span><span class="nv">\right</span><span class="o">)</span><span class="nb"> </span><span class="nv">\mathbf</span><span class="nb">{V}</span>
<span class="nb">    </span><span class="s">\]</span>

    <span class="k">\vspace</span><span class="nb">{</span>1cm<span class="nb">}</span>

    <span class="k">\begin</span><span class="nb">{</span>tikzfigure<span class="nb">}</span>[Comparison of architectures]
      <span class="k">\begin</span><span class="nb">{</span>tikzpicture<span class="nb">}</span>[scale=1.5,
        node/.style=<span class="nb">{</span>rectangle,draw,minimum width=2.5cm,minimum height=1cm,fill=white<span class="nb">}</span>]

        <span class="c">% LSTM</span>
        <span class="k">\node</span><span class="na">[node]</span> (x1) at (0,0) <span class="nb">{</span>Input<span class="nb">}</span>;
        <span class="k">\node</span><span class="na">[node]</span> (lstm1) at (0,2) <span class="nb">{</span>LSTM Layer 1<span class="nb">}</span>;
        <span class="k">\node</span><span class="na">[node]</span> (lstm2) at (0,4) <span class="nb">{</span>LSTM Layer 2<span class="nb">}</span>;
        <span class="k">\node</span><span class="na">[node]</span> (fc1) at (0,6) <span class="nb">{</span>Dense<span class="nb">}</span>;
        <span class="k">\node</span><span class="na">[node]</span> (y1) at (0,8) <span class="nb">{</span>Output<span class="nb">}</span>;

        <span class="k">\draw</span><span class="na">[-&gt;,thick]</span> (x1) -- (lstm1);
        <span class="k">\draw</span><span class="na">[-&gt;,thick]</span> (lstm1) -- (lstm2);
        <span class="k">\draw</span><span class="na">[-&gt;,thick]</span> (lstm2) -- (fc1);
        <span class="k">\draw</span><span class="na">[-&gt;,thick]</span> (fc1) -- (y1);

        <span class="k">\node</span> at (0,-1) <span class="nb">{</span><span class="k">\textbf</span><span class="nb">{</span>LSTM<span class="nb">}}</span>;

        <span class="c">% Transformer</span>
        <span class="k">\node</span><span class="na">[node]</span> (x2) at (6,0) <span class="nb">{</span>Input<span class="nb">}</span>;
        <span class="k">\node</span><span class="na">[node]</span> (emb) at (6,2) <span class="nb">{</span>Embedding<span class="nb">}</span>;
        <span class="k">\node</span><span class="na">[node]</span> (att1) at (6,4) <span class="nb">{</span>Attention <span class="s">$</span><span class="nv">\times</span><span class="s">$</span> 4<span class="nb">}</span>;
        <span class="k">\node</span><span class="na">[node]</span> (fc2) at (6,6) <span class="nb">{</span>Dense<span class="nb">}</span>;
        <span class="k">\node</span><span class="na">[node]</span> (y2) at (6,8) <span class="nb">{</span>Output<span class="nb">}</span>;

        <span class="k">\draw</span><span class="na">[-&gt;,thick]</span> (x2) -- (emb);
        <span class="k">\draw</span><span class="na">[-&gt;,thick]</span> (emb) -- (att1);
        <span class="k">\draw</span><span class="na">[-&gt;,thick]</span> (att1) -- (fc2);
        <span class="k">\draw</span><span class="na">[-&gt;,thick]</span> (fc2) -- (y2);

        <span class="k">\node</span> at (6,-1) <span class="nb">{</span><span class="k">\textbf</span><span class="nb">{</span>Transformer<span class="nb">}}</span>;
      <span class="k">\end</span><span class="nb">{</span>tikzpicture<span class="nb">}</span>
    <span class="k">\end</span><span class="nb">{</span>tikzfigure<span class="nb">}</span>
  <span class="nb">}</span>

  <span class="c">% Column 2</span>
  <span class="k">\column</span><span class="nb">{</span>0.33<span class="nb">}</span>

  <span class="k">\block</span><span class="nb">{</span>Datasets<span class="nb">}{</span>
    <span class="k">\begin</span><span class="nb">{</span>tabular<span class="nb">}{</span>@<span class="nb">{}</span>lccc@<span class="nb">{}}</span>
      <span class="k">\toprule</span>
      Dataset <span class="nb">&amp;</span> Samples <span class="nb">&amp;</span> Features <span class="nb">&amp;</span> Frequency <span class="k">\\</span>
      <span class="k">\midrule</span>
      ETTh1 <span class="nb">&amp;</span> 17,420 <span class="nb">&amp;</span> 7 <span class="nb">&amp;</span> Hourly <span class="k">\\</span>
      Weather <span class="nb">&amp;</span> 52,696 <span class="nb">&amp;</span> 21 <span class="nb">&amp;</span> 10 min <span class="k">\\</span>
      Electricity <span class="nb">&amp;</span> 26,304 <span class="nb">&amp;</span> 321 <span class="nb">&amp;</span> Hourly <span class="k">\\</span>
      <span class="k">\bottomrule</span>
    <span class="k">\end</span><span class="nb">{</span>tabular<span class="nb">}</span>

    <span class="k">\vspace</span><span class="nb">{</span>0.5cm<span class="nb">}</span>

    <span class="k">\textbf</span><span class="nb">{</span>Split:<span class="nb">}</span> 70<span class="k">\%</span> train, 15<span class="k">\%</span> validation, 15<span class="k">\%</span> test

    <span class="k">\textbf</span><span class="nb">{</span>Metrics:<span class="nb">}</span> Mean Absolute Error (MAE), Root Mean Squared Error (RMSE)
  <span class="nb">}</span>

  <span class="k">\block</span><span class="nb">{</span>Results: Accuracy<span class="nb">}{</span>
    <span class="k">\begin</span><span class="nb">{</span>tikzfigure<span class="nb">}</span>[MAE comparison across datasets]
      <span class="k">\begin</span><span class="nb">{</span>tikzpicture<span class="nb">}</span>
        <span class="k">\begin</span><span class="nb">{</span>axis<span class="nb">}</span>[
          ybar,
          width=0.9<span class="k">\linewidth</span>,
          height=10cm,
          ylabel=<span class="nb">{</span>Mean Absolute Error (MAE)<span class="nb">}</span>,
          xlabel=<span class="nb">{</span>Dataset<span class="nb">}</span>,
          symbolic x coords=<span class="nb">{</span>ETTh1, Weather, Electricity<span class="nb">}</span>,
          xtick=data,
          legend pos=north west,
          ymajorgrids=true,
          bar width=0.8cm,
          enlarge x limits=0.2,
          ymin=0
        ]
        <span class="k">\addplot</span> coordinates <span class="nb">{</span>(ETTh1,0.42) (Weather,0.31) (Electricity,0.18)<span class="nb">}</span>;
        <span class="k">\addplot</span> coordinates <span class="nb">{</span>(ETTh1,0.37) (Weather,0.28) (Electricity,0.19)<span class="nb">}</span>;
        <span class="k">\legend</span><span class="nb">{</span>LSTM, Transformer<span class="nb">}</span>
        <span class="k">\end</span><span class="nb">{</span>axis<span class="nb">}</span>
      <span class="k">\end</span><span class="nb">{</span>tikzpicture<span class="nb">}</span>
    <span class="k">\end</span><span class="nb">{</span>tikzfigure<span class="nb">}</span>

    <span class="k">\textbf</span><span class="nb">{</span>Key Finding:<span class="nb">}</span> Transformer achieves 12<span class="k">\%</span> lower MAE on ETTh1 (long sequences)
  <span class="nb">}</span>

  <span class="c">% Column 3</span>
  <span class="k">\column</span><span class="nb">{</span>0.33<span class="nb">}</span>

  <span class="k">\block</span><span class="nb">{</span>Results: Efficiency<span class="nb">}{</span>
    <span class="k">\begin</span><span class="nb">{</span>tikzfigure<span class="nb">}</span>[Training time and memory usage]
      <span class="k">\begin</span><span class="nb">{</span>tikzpicture<span class="nb">}</span>
        <span class="k">\begin</span><span class="nb">{</span>axis<span class="nb">}</span>[
          ybar,
          width=0.9<span class="k">\linewidth</span>,
          height=8cm,
          ylabel=<span class="nb">{</span>Training Time (s/epoch)<span class="nb">}</span>,
          symbolic x coords=<span class="nb">{</span>LSTM, Transformer<span class="nb">}</span>,
          xtick=data,
          bar width=1.5cm,
          ymin=0,
          ymajorgrids=true,
          nodes near coords,
          enlarge x limits=0.5
        ]
        <span class="k">\addplot</span> coordinates <span class="nb">{</span>(LSTM,45.2) (Transformer,128.7)<span class="nb">}</span>;
        <span class="k">\end</span><span class="nb">{</span>axis<span class="nb">}</span>
      <span class="k">\end</span><span class="nb">{</span>tikzpicture<span class="nb">}</span>
    <span class="k">\end</span><span class="nb">{</span>tikzfigure<span class="nb">}</span>

    <span class="k">\vspace</span><span class="nb">{</span>1cm<span class="nb">}</span>

    <span class="k">\textbf</span><span class="nb">{</span>Computational Trade-offs:<span class="nb">}</span>
    <span class="k">\begin</span><span class="nb">{</span>itemize<span class="nb">}</span>
      <span class="k">\item</span> LSTM: 2.9<span class="s">$</span><span class="nv">\times</span><span class="s">$</span> faster training
      <span class="k">\item</span> LSTM: 1.9<span class="s">$</span><span class="nv">\times</span><span class="s">$</span> less GPU memory
      <span class="k">\item</span> Transformer: Better accuracy on long sequences
    <span class="k">\end</span><span class="nb">{</span>itemize<span class="nb">}</span>
  <span class="nb">}</span>

  <span class="k">\block</span><span class="nb">{</span>Conclusion<span class="nb">}{</span>
    <span class="k">\textbf</span><span class="nb">{</span>Contributions:<span class="nb">}</span>
    <span class="k">\begin</span><span class="nb">{</span>itemize<span class="nb">}</span>
      <span class="k">\item</span> Comprehensive comparison on 3 datasets
      <span class="k">\item</span> Accuracy vs. efficiency trade-off analysis
      <span class="k">\item</span> Open-source implementation
    <span class="k">\end</span><span class="nb">{</span>itemize<span class="nb">}</span>

    <span class="k">\vspace</span><span class="nb">{</span>0.5cm<span class="nb">}</span>

    <span class="k">\textbf</span><span class="nb">{</span>Recommendations:<span class="nb">}</span>
    <span class="k">\begin</span><span class="nb">{</span>itemize<span class="nb">}</span>
      <span class="k">\item</span> <span class="k">\textbf</span><span class="nb">{</span>Transformer:<span class="nb">}</span> Maximum accuracy, long sequences
      <span class="k">\item</span> <span class="k">\textbf</span><span class="nb">{</span>LSTM:<span class="nb">}</span> Resource-constrained environments
    <span class="k">\end</span><span class="nb">{</span>itemize<span class="nb">}</span>

    <span class="k">\vspace</span><span class="nb">{</span>1cm<span class="nb">}</span>

    <span class="k">\textbf</span><span class="nb">{</span>Future Work:<span class="nb">}</span> Hybrid architectures, multivariate forecasting, real-time inference

    <span class="k">\vspace</span><span class="nb">{</span>1cm<span class="nb">}</span>

    <span class="k">\begin</span><span class="nb">{</span>center<span class="nb">}</span>
      <span class="k">\textbf</span><span class="nb">{</span>Code <span class="k">\&amp;</span> Data:<span class="nb">}</span><span class="k">\\</span>
      <span class="k">\qrcode</span><span class="na">[height=3cm]</span><span class="nb">{</span>https://github.com/example/time-series-forecast<span class="nb">}</span>
    <span class="k">\end</span><span class="nb">{</span>center<span class="nb">}</span>

    <span class="k">\vspace</span><span class="nb">{</span>0.5cm<span class="nb">}</span>

    <span class="k">\begin</span><span class="nb">{</span>center<span class="nb">}</span>
      <span class="k">\textbf</span><span class="nb">{</span>Contact:<span class="nb">}</span> <span class="k">\texttt</span><span class="nb">{</span>alice.johnson@example.edu<span class="nb">}</span>
    <span class="k">\end</span><span class="nb">{</span>center<span class="nb">}</span>
  <span class="nb">}</span>

<span class="k">\end</span><span class="nb">{</span>columns<span class="nb">}</span>

<span class="k">\end</span><span class="nb">{</span>document<span class="nb">}</span>
</code></pre></div>

<h3 id="compilation_2">Compilation<a class="header-link" href="#compilation_2" title="Permanent link">&para;</a></h3>
<div class="highlight"><pre><span></span><code>pdflatex<span class="w"> </span>poster.tex
</code></pre></div>

<p><strong>Note</strong>: Compile may take longer due to TikZ complexity.</p>
<h3 id="printing">Printing<a class="header-link" href="#printing" title="Permanent link">&para;</a></h3>
<p>For actual conference poster:
1. Export to PDF
2. Send to professional poster printing service
3. Specify: A0 size, portrait, high-quality (600 dpi)</p>
<h3 id="common-pitfalls_2">Common Pitfalls<a class="header-link" href="#common-pitfalls_2" title="Permanent link">&para;</a></h3>
<p><strong>Problem</strong>: Text too small when printed
- <strong>Solution</strong>: Use larger font sizes in <code>\tikzposter</code> options</p>
<p><strong>Problem</strong>: QR code doesn't scan
- <strong>Solution</strong>: Increase <code>height</code> parameter, test before printing</p>
<p><strong>Problem</strong>: Colors look different on screen vs. print
- <strong>Solution</strong>: Use CMYK color space, preview with print simulation</p>
<hr />
<h2 id="combining-lessons-integration-map">Combining Lessons: Integration Map<a class="header-link" href="#combining-lessons-integration-map" title="Permanent link">&para;</a></h2>
<p>All three projects use concepts from previous lessons:</p>
<table>
<thead>
<tr>
<th>Lesson</th>
<th>Project 1 (Paper)</th>
<th>Project 2 (Beamer)</th>
<th>Project 3 (Poster)</th>
</tr>
</thead>
<tbody>
<tr>
<td>L01-02</td>
<td>Document structure</td>
<td>Frame structure</td>
<td>Block structure</td>
</tr>
<tr>
<td>L03</td>
<td>Text formatting</td>
<td>Theme colors</td>
<td>Custom colors</td>
</tr>
<tr>
<td>L05</td>
<td>Tables (booktabs)</td>
<td>Tables</td>
<td>Tables</td>
</tr>
<tr>
<td>L06</td>
<td>Figures, subfigures</td>
<td>Images</td>
<td>TikZ figures</td>
</tr>
<tr>
<td>L07-08</td>
<td>Equations, align</td>
<td>Math in slides</td>
<td>Math in blocks</td>
</tr>
<tr>
<td>L09</td>
<td>Cross-references</td>
<td>Frame references</td>
<td>‚Äî</td>
</tr>
<tr>
<td>L10</td>
<td>BibLaTeX</td>
<td>Citations</td>
<td>‚Äî</td>
</tr>
<tr>
<td>L11</td>
<td>‚Äî</td>
<td>Beamer themes, overlays</td>
<td>‚Äî</td>
</tr>
<tr>
<td>L12</td>
<td>‚Äî</td>
<td>TikZ diagrams</td>
<td>PGFPlots, TikZ</td>
</tr>
<tr>
<td>L13</td>
<td>Custom commands</td>
<td>‚Äî</td>
<td>‚Äî</td>
</tr>
<tr>
<td>L14</td>
<td>IEEE class</td>
<td>Beamer class</td>
<td>tikzposter class</td>
</tr>
<tr>
<td>L15</td>
<td>latexmk</td>
<td>latexmk</td>
<td>‚Äî</td>
</tr>
</tbody>
</table>
<hr />
<h2 id="next-steps">Next Steps<a class="header-link" href="#next-steps" title="Permanent link">&para;</a></h2>
<h3 id="explore-advanced-topics">Explore Advanced Topics<a class="header-link" href="#explore-advanced-topics" title="Permanent link">&para;</a></h3>
<ol>
<li><strong>LuaLaTeX programming</strong>: Automate complex document generation</li>
<li><strong>Externalization</strong>: Speed up TikZ compilation</li>
<li><strong>ConTeXt</strong>: Alternative to LaTeX for advanced typography</li>
<li><strong>arXiv submission</strong>: Prepare papers for preprint servers</li>
<li><strong>Journal-specific templates</strong>: IEEE, ACM, Springer, Elsevier</li>
</ol>
<h3 id="join-the-community">Join the Community<a class="header-link" href="#join-the-community" title="Permanent link">&para;</a></h3>
<ul>
<li><strong>TeX StackExchange</strong>: Q&amp;A for troubleshooting</li>
<li><strong>LaTeX Project</strong>: Official news and releases</li>
<li><strong>CTAN</strong>: Explore 6000+ packages</li>
<li><strong>Overleaf tutorials</strong>: Video guides and webinars</li>
<li><strong>Local TeX user groups</strong>: TUG, UK-TUG, etc.</li>
</ul>
<h3 id="practice-projects">Practice Projects<a class="header-link" href="#practice-projects" title="Permanent link">&para;</a></h3>
<ul>
<li>Write your CV in LaTeX</li>
<li>Create presentation for upcoming talk</li>
<li>Typeset notes or documentation</li>
<li>Contribute to open-source LaTeX packages</li>
</ul>
<hr />
<h2 id="summary">Summary<a class="header-link" href="#summary" title="Permanent link">&para;</a></h2>
<p>This lesson presented three complete, real-world LaTeX projects:</p>
<ol>
<li><strong>Academic Paper</strong>: IEEE-style conference paper with figures, tables, math, bibliography</li>
<li><strong>Beamer Presentation</strong>: 15-slide talk with overlays, TikZ, custom theme</li>
<li><strong>Scientific Poster</strong>: A0 poster with multi-column layout, plots, QR code</li>
</ol>
<p><strong>Key skills demonstrated</strong>:
- Document class selection and configuration
- Package integration (graphics, math, bibliography, TikZ)
- Custom commands and environments
- Cross-referencing and citations
- Visual design (colors, layouts, themes)
- Compilation workflows</p>
<p><strong>Congratulations!</strong> You've completed all 16 lessons of the LaTeX course. You now have the skills to create professional documents, presentations, and posters for academic and professional contexts.</p>
<hr />
<p><strong>Navigation</strong></p>
<ul>
<li>Previous: <a href="15_Automation_and_Build.md">15_Automation_and_Build.md</a></li>
<li>End of Course</li>
</ul>
    </div>

    
<div class="lesson-toolbar lesson-toolbar--bottom">
    <div class="toolbar-nav toolbar-nav--prev">
        
        <a href="/study/en/LaTeX/15_Automation_and_Build.html" class="nav-prev">
            <span class="nav-label">Previous</span>
            <span class="nav-title">Build Systems & Automation</span>
        </a>
        
    </div>
    <div class="toolbar-actions">
        <button class="btn btn-copy-link" data-action="copy-link" title="Copy link">
            <span class="icon">üîó</span>
            <span class="text">Copy link</span>
        </button>
        <a href="/study/en/LaTeX/" class="btn btn-topic-link" title="Back to topic list">
            <span class="icon">üìã</span>
            <span class="text">Topic list</span>
        </a>
    </div>
    <div class="toolbar-nav toolbar-nav--next">
        
    </div>
</div>


    <div class="toolbar-keyboard-hint">
        <kbd>&larr;</kbd> <kbd>&rarr;</kbd> to navigate between lessons
    </div>
</article>

<!-- Scroll to top floating button -->
<button class="scroll-to-top" id="scroll-to-top" title="Scroll to top" aria-label="Scroll to top">‚Üë</button>

            </div>
        </main>
    </div>

    <script src="/study/static/js/app.js"></script>
    <script>
        function switchLanguage(newLang) {
            const path = window.location.pathname;
            const base = '/study';
            const currentLang = 'en';
            const newPath = path.replace(base + '/' + currentLang + '/', base + '/' + newLang + '/');
            window.location.href = newPath + window.location.search;
        }
    </script>
    
<script>
    document.addEventListener('DOMContentLoaded', function() {
        // Copy link - independent feedback per button
        document.querySelectorAll('[data-action="copy-link"]').forEach(function(btn) {
            btn.addEventListener('click', function() {
                navigator.clipboard.writeText(window.location.href);
                var textEl = this.querySelector('.text');
                var originalText = textEl.textContent;
                textEl.textContent = 'Copied!';
                setTimeout(function() { textEl.textContent = originalText; }, 2000);
            });
        });

        // Add copy buttons to code blocks
        document.querySelectorAll('.lesson-content pre').forEach(function(pre) {
            var copyBtn = document.createElement('button');
            copyBtn.className = 'code-copy-btn';
            copyBtn.textContent = 'Copy';
            copyBtn.addEventListener('click', function() {
                var code = pre.querySelector('code');
                var text = code ? code.textContent : pre.textContent;
                navigator.clipboard.writeText(text);
                copyBtn.textContent = 'Copied!';
                setTimeout(function() { copyBtn.textContent = 'Copy'; }, 2000);
            });
            pre.style.position = 'relative';
            pre.appendChild(copyBtn);
        });

        // Scroll to top button
        var scrollBtn = document.getElementById('scroll-to-top');
        window.addEventListener('scroll', function() {
            scrollBtn.classList.toggle('visible', window.scrollY > 300);
        });
        scrollBtn.addEventListener('click', function() {
            window.scrollTo({ top: 0, behavior: 'smooth' });
        });

        // Keyboard shortcuts: left/right arrows for lesson navigation
        document.addEventListener('keydown', function(e) {
            var tag = document.activeElement.tagName;
            if (tag === 'INPUT' || tag === 'TEXTAREA' || tag === 'SELECT') return;

            if (e.key === 'ArrowLeft') {
                var prevLink = document.querySelector('.nav-prev');
                if (prevLink) prevLink.click();
            } else if (e.key === 'ArrowRight') {
                var nextLink = document.querySelector('.nav-next');
                if (nextLink) nextLink.click();
            }
        });
    });
</script>

</body>
</html>
{% endraw %}