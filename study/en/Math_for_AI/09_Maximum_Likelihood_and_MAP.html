{% raw %}
<!DOCTYPE html>
<html lang="en" data-theme="light">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>09. Maximum Likelihood and MAP - Study Materials</title>
    <link rel="stylesheet" href="/study/static/css/style.css">
    <link rel="stylesheet" href="/study/static/css/highlight.css">
    <!-- KaTeX for LaTeX math rendering -->
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.11/dist/katex.min.css">
    <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.11/dist/katex.min.js"></script>
    <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.11/dist/contrib/auto-render.min.js"
        onload="renderMathInElement(document.body, {
            delimiters: [
                {left: '$$', right: '$$', display: true},
                {left: '$', right: '$', display: false}
            ],
            throwOnError: false
        });"></script>
    
</head>
<body>
    <div class="app-container">
        <!-- Sidebar -->
        <aside class="sidebar" id="sidebar">
            <div class="sidebar-header">
                <a href="/study/en/" class="logo">Study Materials</a>
                <button class="sidebar-toggle" id="sidebar-toggle" aria-label="Toggle sidebar">
                    <span></span>
                </button>
            </div>

            <nav class="sidebar-nav">
                <a href="/study/en/" class="nav-item ">
                    <span class="nav-icon">üè†</span>
                    <span class="nav-text">Home</span>
                </a>
                <a href="/study/examples/" class="nav-item ">
                    <span class="nav-icon">üíª</span>
                    <span class="nav-text">Examples</span>
                </a>
            </nav>

            <div class="sidebar-search">
                <form action="/study/en/search.html" method="get" id="search-form">
                    <input type="search" name="q" placeholder="Search..." id="search-sidebar-input">
                </form>
            </div>

            <!-- Language Selector -->
            <div class="sidebar-lang">
                <select id="lang-select" class="lang-selector" onchange="switchLanguage(this.value)">
                    
                    <option value="en" selected>
                        English
                    </option>
                    
                    <option value="ko" >
                        ÌïúÍµ≠Ïñ¥
                    </option>
                    
                </select>
            </div>

            <div class="sidebar-footer">
                <button class="theme-toggle" id="theme-toggle" aria-label="Toggle theme">
                    <span class="theme-icon light">‚òÄÔ∏è</span>
                    <span class="theme-icon dark">üåô</span>
                </button>
            </div>
        </aside>

        <!-- Main Content -->
        <main class="main-content">
            <header class="main-header">
                <button class="menu-toggle" id="menu-toggle" aria-label="Open menu">
                    <span></span>
                </button>
                
<nav class="breadcrumb">
    <a href="/study/en/">Topics</a>
    <span class="separator">/</span>
    <a href="/study/en/Math_for_AI/">Math for AI</a>
    <span class="separator">/</span>
    <span class="current">09. Maximum Likelihood and MAP</span>
</nav>

            </header>

            <div class="content">
                
<article class="lesson-article">
    <header class="lesson-header">
        <h1>09. Maximum Likelihood and MAP</h1>
    </header>

    
<div class="lesson-toolbar lesson-toolbar--top">
    <div class="toolbar-nav toolbar-nav--prev">
        
        <a href="/study/en/Math_for_AI/08_Probability_for_ML.html" class="nav-prev">
            <span class="nav-label">Previous</span>
            <span class="nav-title">08. Probability for Machine Learning</span>
        </a>
        
    </div>
    <div class="toolbar-actions">
        <button class="btn btn-copy-link" data-action="copy-link" title="Copy link">
            <span class="icon">üîó</span>
            <span class="text">Copy link</span>
        </button>
        <a href="/study/en/Math_for_AI/" class="btn btn-topic-link" title="Back to topic list">
            <span class="icon">üìã</span>
            <span class="text">Topic list</span>
        </a>
    </div>
    <div class="toolbar-nav toolbar-nav--next">
        
        <a href="/study/en/Math_for_AI/10_Information_Theory.html" class="nav-next">
            <span class="nav-label">Next</span>
            <span class="nav-title">10. Information Theory</span>
        </a>
        
    </div>
</div>


    
    <nav class="toc" id="toc">
        <h2>Table of Contents</h2>
        <div class="toc">
<ul>
<li><a href="#learning-objectives">Learning Objectives</a></li>
<li><a href="#1-likelihood-function">1. Likelihood Function</a><ul>
<li><a href="#11-likelihood-vs-probability">1.1 Likelihood vs Probability</a></li>
<li><a href="#12-log-likelihood">1.2 Log-Likelihood</a></li>
<li><a href="#13-iid-assumption-and-productsum-conversion">1.3 i.i.d. Assumption and Product‚ÜíSum Conversion</a></li>
</ul>
</li>
<li><a href="#2-maximum-likelihood-estimation-mle">2. Maximum Likelihood Estimation (MLE)</a><ul>
<li><a href="#21-mle-definition">2.1 MLE Definition</a></li>
<li><a href="#22-mle-for-normal-distribution">2.2 MLE for Normal Distribution</a></li>
<li><a href="#23-mle-for-bernoulli-distribution">2.3 MLE for Bernoulli Distribution</a></li>
<li><a href="#24-properties-of-mle">2.4 Properties of MLE</a></li>
</ul>
</li>
<li><a href="#3-map-estimation-maximum-a-posteriori">3. MAP Estimation (Maximum A Posteriori)</a><ul>
<li><a href="#31-bayes-theorem-and-posterior-distribution">3.1 Bayes' Theorem and Posterior Distribution</a></li>
<li><a href="#32-map-definition">3.2 MAP Definition</a></li>
<li><a href="#33-mle-vs-map-comparison">3.3 MLE vs MAP Comparison</a></li>
<li><a href="#34-role-of-prior-distribution">3.4 Role of Prior Distribution</a></li>
</ul>
</li>
<li><a href="#4-regularization-prior-distribution">4. Regularization = Prior Distribution</a><ul>
<li><a href="#41-l2-regularization-and-gaussian-prior">4.1 L2 Regularization and Gaussian Prior</a></li>
<li><a href="#42-l1-regularization-and-laplace-prior">4.2 L1 Regularization and Laplace Prior</a></li>
</ul>
</li>
<li><a href="#5-em-algorithm-expectation-maximization">5. EM Algorithm (Expectation-Maximization)</a><ul>
<li><a href="#51-latent-variable-problem">5.1 Latent Variable Problem</a></li>
<li><a href="#52-elbo-and-em-derivation">5.2 ELBO and EM Derivation</a></li>
<li><a href="#53-gaussian-mixture-model-gmm">5.3 Gaussian Mixture Model (GMM)</a></li>
</ul>
</li>
<li><a href="#6-machine-learning-applications">6. Machine Learning Applications</a><ul>
<li><a href="#61-logistic-regression-bernoulli-mle">6.1 Logistic Regression = Bernoulli MLE</a></li>
<li><a href="#62-neural-network-training-mle">6.2 Neural Network Training = MLE</a></li>
<li><a href="#63-introduction-to-bayesian-neural-networks">6.3 Introduction to Bayesian Neural Networks</a></li>
</ul>
</li>
<li><a href="#practice-problems">Practice Problems</a><ul>
<li><a href="#problem-1-mle-for-exponential-distribution">Problem 1: MLE for Exponential Distribution</a></li>
<li><a href="#problem-2-map-with-different-priors">Problem 2: MAP with Different Priors</a></li>
<li><a href="#problem-3-em-for-coin-toss">Problem 3: EM for Coin Toss</a></li>
<li><a href="#problem-4-fisher-information">Problem 4: Fisher Information</a></li>
<li><a href="#problem-5-em-for-mixture-of-exponentials">Problem 5: EM for Mixture of Exponentials</a></li>
</ul>
</li>
<li><a href="#references">References</a></li>
</ul>
</div>

    </nav>
    

    <div class="lesson-content markdown-body">
        <h1 id="09-maximum-likelihood-and-map">09. Maximum Likelihood and MAP<a class="header-link" href="#09-maximum-likelihood-and-map" title="Permanent link">&para;</a></h1>
<h2 id="learning-objectives">Learning Objectives<a class="header-link" href="#learning-objectives" title="Permanent link">&para;</a></h2>
<ul>
<li>Understand the difference between likelihood and probability, and grasp the role of log-likelihood</li>
<li>Define Maximum Likelihood Estimation (MLE) and derive MLE for various probability distributions</li>
<li>Understand the principles of MAP estimation and explain the differences from MLE</li>
<li>Mathematically derive how regularization terms connect to prior distributions</li>
<li>Understand the principles of the EM algorithm and apply it to models with latent variables</li>
<li>Learn how MLE and MAP are utilized in machine learning through practical examples</li>
</ul>
<hr />
<h2 id="1-likelihood-function">1. Likelihood Function<a class="header-link" href="#1-likelihood-function" title="Permanent link">&para;</a></h2>
<h3 id="11-likelihood-vs-probability">1.1 Likelihood vs Probability<a class="header-link" href="#11-likelihood-vs-probability" title="Permanent link">&para;</a></h3>
<p><strong>Probability</strong> and <strong>Likelihood</strong> are computed using the same formula, but have different meanings.</p>
<ul>
<li>
<p><strong>Probability</strong>: When parameter $\theta$ is fixed, the probability of data $D$ occurring
  $$P(D|\theta)$$</p>
</li>
<li>
<p><strong>Likelihood</strong>: After data $D$ is observed, a function of parameter $\theta$
  $$\mathcal{L}(\theta|D) = P(D|\theta)$$</p>
</li>
</ul>
<p><strong>Key difference</strong>:
- Probability: $\theta$ fixed, $D$ variable ‚Üí $\sum_D P(D|\theta) = 1$
- Likelihood: $D$ fixed, $\theta$ variable ‚Üí $\int \mathcal{L}(\theta|D) d\theta \neq 1$ (in general)</p>
<p><strong>Example</strong>: Coin toss
- The <strong>probability</strong> of getting 7 heads in 10 tosses when the coin's head probability is $\theta = 0.7$
- The <strong>likelihood</strong> that $\theta$ is 0.7 when 7 heads are observed in 10 tosses</p>
<h3 id="12-log-likelihood">1.2 Log-Likelihood<a class="header-link" href="#12-log-likelihood" title="Permanent link">&para;</a></h3>
<p>In practice, we use <strong>log-likelihood</strong>:</p>
<p>$$\ell(\theta|D) = \log \mathcal{L}(\theta|D) = \log P(D|\theta)$$</p>
<p><strong>Reasons for using log-likelihood</strong>:</p>
<ol>
<li><strong>Numerical stability</strong>: Products of probabilities become very small, causing underflow</li>
<li><strong>Convert products to sums</strong>: $\log(ab) = \log a + \log b$</li>
<li><strong>Easier differentiation</strong>: Derivatives of exponential functions become simpler</li>
<li><strong>Same optimization</strong>: $\log$ is monotonically increasing, so argmax is identical</li>
</ol>
<h3 id="13-iid-assumption-and-productsum-conversion">1.3 i.i.d. Assumption and Product‚ÜíSum Conversion<a class="header-link" href="#13-iid-assumption-and-productsum-conversion" title="Permanent link">&para;</a></h3>
<p>If data $D = \{x_1, x_2, \ldots, x_n\}$ follows <strong>independent and identically distributed (i.i.d.)</strong>:</p>
<p>$$P(D|\theta) = \prod_{i=1}^n P(x_i|\theta)$$</p>
<p>Taking the log:</p>
<p>$$\ell(\theta|D) = \log \prod_{i=1}^n P(x_i|\theta) = \sum_{i=1}^n \log P(x_i|\theta)$$</p>
<p>This transformation is the core of MLE.</p>
<h2 id="2-maximum-likelihood-estimation-mle">2. Maximum Likelihood Estimation (MLE)<a class="header-link" href="#2-maximum-likelihood-estimation-mle" title="Permanent link">&para;</a></h2>
<h3 id="21-mle-definition">2.1 MLE Definition<a class="header-link" href="#21-mle-definition" title="Permanent link">&para;</a></h3>
<p><strong>Maximum Likelihood Estimation (MLE)</strong> is the method of finding the parameter that maximizes the likelihood of observed data:</p>
<p>$$\theta_{\text{MLE}} = \arg\max_{\theta} \mathcal{L}(\theta|D) = \arg\max_{\theta} \log P(D|\theta)$$</p>
<p><strong>Computation method</strong>:
1. Write the likelihood function $\mathcal{L}(\theta|D)$
2. Compute log-likelihood $\ell(\theta|D)$
3. Solve $\frac{\partial \ell}{\partial \theta} = 0$
4. Verify maximum with second derivative</p>
<h3 id="22-mle-for-normal-distribution">2.2 MLE for Normal Distribution<a class="header-link" href="#22-mle-for-normal-distribution" title="Permanent link">&para;</a></h3>
<p><strong>Problem</strong>: Given $n$ data points $\{x_1, \ldots, x_n\}$ from $\mathcal{N}(\mu, \sigma^2)$, find the MLE of $\mu$ and $\sigma^2$.</p>
<p><strong>Solution</strong>:</p>
<p>Log-likelihood:
$$\ell(\mu, \sigma^2) = \sum_{i=1}^n \log \frac{1}{\sqrt{2\pi\sigma^2}} \exp\left(-\frac{(x_i-\mu)^2}{2\sigma^2}\right)$$</p>
<p>$$= -\frac{n}{2}\log(2\pi) - \frac{n}{2}\log(\sigma^2) - \frac{1}{2\sigma^2}\sum_{i=1}^n (x_i-\mu)^2$$</p>
<p><strong>Optimization w.r.t. $\mu$</strong>:</p>
<p>$$\frac{\partial \ell}{\partial \mu} = \frac{1}{\sigma^2}\sum_{i=1}^n (x_i - \mu) = 0$$</p>
<p>$$\Rightarrow \mu_{\text{MLE}} = \frac{1}{n}\sum_{i=1}^n x_i$$</p>
<p><strong>Optimization w.r.t. $\sigma^2$</strong>:</p>
<p>$$\frac{\partial \ell}{\partial \sigma^2} = -\frac{n}{2\sigma^2} + \frac{1}{2(\sigma^2)^2}\sum_{i=1}^n (x_i-\mu)^2 = 0$$</p>
<p>$$\Rightarrow \sigma^2_{\text{MLE}} = \frac{1}{n}\sum_{i=1}^n (x_i - \mu_{\text{MLE}})^2$$</p>
<p><strong>Result</strong>: Sample mean and sample variance!</p>
<div class="highlight"><pre><span></span><code><span class="kn">import</span><span class="w"> </span><span class="nn">numpy</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">np</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">matplotlib.pyplot</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">plt</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">scipy</span><span class="w"> </span><span class="kn">import</span> <span class="n">stats</span>

<span class="c1"># Îç∞Ïù¥ÌÑ∞ ÏÉùÏÑ±</span>
<span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">seed</span><span class="p">(</span><span class="mi">42</span><span class="p">)</span>
<span class="n">true_mu</span> <span class="o">=</span> <span class="mf">5.0</span>
<span class="n">true_sigma</span> <span class="o">=</span> <span class="mf">2.0</span>
<span class="n">n_samples</span> <span class="o">=</span> <span class="mi">100</span>
<span class="n">data</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">normal</span><span class="p">(</span><span class="n">true_mu</span><span class="p">,</span> <span class="n">true_sigma</span><span class="p">,</span> <span class="n">n_samples</span><span class="p">)</span>

<span class="c1"># MLE Í≥ÑÏÇ∞</span>
<span class="n">mu_mle</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">data</span><span class="p">)</span>
<span class="n">sigma2_mle</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">var</span><span class="p">(</span><span class="n">data</span><span class="p">,</span> <span class="n">ddof</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>  <span class="c1"># ddof=0: MLE (biased)</span>
<span class="n">sigma_mle</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="n">sigma2_mle</span><span class="p">)</span>

<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;True parameters: Œº=</span><span class="si">{</span><span class="n">true_mu</span><span class="si">}</span><span class="s2">, œÉ=</span><span class="si">{</span><span class="n">true_sigma</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;MLE estimates: Œº=</span><span class="si">{</span><span class="n">mu_mle</span><span class="si">:</span><span class="s2">.3f</span><span class="si">}</span><span class="s2">, œÉ=</span><span class="si">{</span><span class="n">sigma_mle</span><span class="si">:</span><span class="s2">.3f</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>

<span class="c1"># Î°úÍ∑∏ Ïö∞ÎèÑ Ìï®Ïàò ÏãúÍ∞ÅÌôî</span>
<span class="k">def</span><span class="w"> </span><span class="nf">log_likelihood</span><span class="p">(</span><span class="n">mu</span><span class="p">,</span> <span class="n">sigma</span><span class="p">,</span> <span class="n">data</span><span class="p">):</span>
    <span class="n">n</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">data</span><span class="p">)</span>
    <span class="k">return</span> <span class="o">-</span><span class="n">n</span><span class="o">/</span><span class="mi">2</span> <span class="o">*</span> <span class="n">np</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="mi">2</span><span class="o">*</span><span class="n">np</span><span class="o">.</span><span class="n">pi</span><span class="p">)</span> <span class="o">-</span> <span class="n">n</span> <span class="o">*</span> <span class="n">np</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="n">sigma</span><span class="p">)</span> <span class="o">-</span> \
           <span class="n">np</span><span class="o">.</span><span class="n">sum</span><span class="p">((</span><span class="n">data</span> <span class="o">-</span> <span class="n">mu</span><span class="p">)</span><span class="o">**</span><span class="mi">2</span><span class="p">)</span> <span class="o">/</span> <span class="p">(</span><span class="mi">2</span> <span class="o">*</span> <span class="n">sigma</span><span class="o">**</span><span class="mi">2</span><span class="p">)</span>

<span class="c1"># ŒºÏóê ÎåÄÌïú Î°úÍ∑∏ Ïö∞ÎèÑ</span>
<span class="n">mu_range</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="mi">7</span><span class="p">,</span> <span class="mi">100</span><span class="p">)</span>
<span class="n">ll_mu</span> <span class="o">=</span> <span class="p">[</span><span class="n">log_likelihood</span><span class="p">(</span><span class="n">m</span><span class="p">,</span> <span class="n">true_sigma</span><span class="p">,</span> <span class="n">data</span><span class="p">)</span> <span class="k">for</span> <span class="n">m</span> <span class="ow">in</span> <span class="n">mu_range</span><span class="p">]</span>

<span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">12</span><span class="p">,</span> <span class="mi">4</span><span class="p">))</span>

<span class="n">plt</span><span class="o">.</span><span class="n">subplot</span><span class="p">(</span><span class="mi">131</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">mu_range</span><span class="p">,</span> <span class="n">ll_mu</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">axvline</span><span class="p">(</span><span class="n">mu_mle</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s1">&#39;r&#39;</span><span class="p">,</span> <span class="n">linestyle</span><span class="o">=</span><span class="s1">&#39;--&#39;</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="sa">f</span><span class="s1">&#39;MLE=</span><span class="si">{</span><span class="n">mu_mle</span><span class="si">:</span><span class="s1">.2f</span><span class="si">}</span><span class="s1">&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s1">&#39;Œº&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s1">&#39;Log-Likelihood&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s1">&#39;Log-Likelihood vs Œº&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">legend</span><span class="p">()</span>
<span class="n">plt</span><span class="o">.</span><span class="n">grid</span><span class="p">(</span><span class="kc">True</span><span class="p">)</span>

<span class="c1"># œÉÏóê ÎåÄÌïú Î°úÍ∑∏ Ïö∞ÎèÑ</span>
<span class="n">sigma_range</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="mf">0.5</span><span class="p">,</span> <span class="mi">4</span><span class="p">,</span> <span class="mi">100</span><span class="p">)</span>
<span class="n">ll_sigma</span> <span class="o">=</span> <span class="p">[</span><span class="n">log_likelihood</span><span class="p">(</span><span class="n">true_mu</span><span class="p">,</span> <span class="n">s</span><span class="p">,</span> <span class="n">data</span><span class="p">)</span> <span class="k">for</span> <span class="n">s</span> <span class="ow">in</span> <span class="n">sigma_range</span><span class="p">]</span>

<span class="n">plt</span><span class="o">.</span><span class="n">subplot</span><span class="p">(</span><span class="mi">132</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">sigma_range</span><span class="p">,</span> <span class="n">ll_sigma</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">axvline</span><span class="p">(</span><span class="n">sigma_mle</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s1">&#39;r&#39;</span><span class="p">,</span> <span class="n">linestyle</span><span class="o">=</span><span class="s1">&#39;--&#39;</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="sa">f</span><span class="s1">&#39;MLE=</span><span class="si">{</span><span class="n">sigma_mle</span><span class="si">:</span><span class="s1">.2f</span><span class="si">}</span><span class="s1">&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s1">&#39;œÉ&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s1">&#39;Log-Likelihood&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s1">&#39;Log-Likelihood vs œÉ&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">legend</span><span class="p">()</span>
<span class="n">plt</span><span class="o">.</span><span class="n">grid</span><span class="p">(</span><span class="kc">True</span><span class="p">)</span>

<span class="c1"># ÌûàÏä§ÌÜ†Í∑∏Îû®Í≥º MLE Î∂ÑÌè¨</span>
<span class="n">plt</span><span class="o">.</span><span class="n">subplot</span><span class="p">(</span><span class="mi">133</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">hist</span><span class="p">(</span><span class="n">data</span><span class="p">,</span> <span class="n">bins</span><span class="o">=</span><span class="mi">30</span><span class="p">,</span> <span class="n">density</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.7</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s1">&#39;Data&#39;</span><span class="p">)</span>
<span class="n">x</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="n">data</span><span class="o">.</span><span class="n">min</span><span class="p">(),</span> <span class="n">data</span><span class="o">.</span><span class="n">max</span><span class="p">(),</span> <span class="mi">100</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">stats</span><span class="o">.</span><span class="n">norm</span><span class="o">.</span><span class="n">pdf</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">mu_mle</span><span class="p">,</span> <span class="n">sigma_mle</span><span class="p">),</span> <span class="s1">&#39;r-&#39;</span><span class="p">,</span>
         <span class="n">linewidth</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s1">&#39;MLE fit&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s1">&#39;x&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s1">&#39;Density&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s1">&#39;Data vs MLE Distribution&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">legend</span><span class="p">()</span>
<span class="n">plt</span><span class="o">.</span><span class="n">grid</span><span class="p">(</span><span class="kc">True</span><span class="p">)</span>

<span class="n">plt</span><span class="o">.</span><span class="n">tight_layout</span><span class="p">()</span>
<span class="n">plt</span><span class="o">.</span><span class="n">savefig</span><span class="p">(</span><span class="s1">&#39;mle_gaussian.png&#39;</span><span class="p">,</span> <span class="n">dpi</span><span class="o">=</span><span class="mi">150</span><span class="p">,</span> <span class="n">bbox_inches</span><span class="o">=</span><span class="s1">&#39;tight&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</code></pre></div>

<h3 id="23-mle-for-bernoulli-distribution">2.3 MLE for Bernoulli Distribution<a class="header-link" href="#23-mle-for-bernoulli-distribution" title="Permanent link">&para;</a></h3>
<p><strong>Problem</strong>: In $n$ coin tosses resulting in $k$ heads, what is the MLE of head probability $\theta$?</p>
<p><strong>Solution</strong>:</p>
<p>$$\mathcal{L}(\theta) = \theta^k (1-\theta)^{n-k}$$</p>
<p>$$\ell(\theta) = k \log \theta + (n-k) \log(1-\theta)$$</p>
<p>$$\frac{d\ell}{d\theta} = \frac{k}{\theta} - \frac{n-k}{1-\theta} = 0$$</p>
<p>$$\Rightarrow \theta_{\text{MLE}} = \frac{k}{n}$$</p>
<p><strong>Intuition</strong>: The observed relative frequency!</p>
<h3 id="24-properties-of-mle">2.4 Properties of MLE<a class="header-link" href="#24-properties-of-mle" title="Permanent link">&para;</a></h3>
<ol>
<li>
<p><strong>Consistency</strong>: As $n \to \infty$, $\theta_{\text{MLE}} \to \theta_{\text{true}}$ (in probability)</p>
</li>
<li>
<p><strong>Asymptotic Normality</strong>:
   $$\sqrt{n}(\theta_{\text{MLE}} - \theta_{\text{true}}) \xrightarrow{d} \mathcal{N}(0, I(\theta)^{-1})$$
   where $I(\theta)$ is the Fisher information matrix</p>
</li>
<li>
<p><strong>Unbiasedness not guaranteed</strong>: For example, $\sigma^2_{\text{MLE}}$ is a biased estimator (ddof=0)</p>
</li>
<li>
<p><strong>Invariance</strong>: If $\theta_{\text{MLE}}$ is the MLE of $\theta$, then $g(\theta_{\text{MLE}})$ is the MLE of $g(\theta)$</p>
</li>
</ol>
<h2 id="3-map-estimation-maximum-a-posteriori">3. MAP Estimation (Maximum A Posteriori)<a class="header-link" href="#3-map-estimation-maximum-a-posteriori" title="Permanent link">&para;</a></h2>
<h3 id="31-bayes-theorem-and-posterior-distribution">3.1 Bayes' Theorem and Posterior Distribution<a class="header-link" href="#31-bayes-theorem-and-posterior-distribution" title="Permanent link">&para;</a></h3>
<p>Bayes' theorem:</p>
<p>$$P(\theta|D) = \frac{P(D|\theta)P(\theta)}{P(D)}$$</p>
<ul>
<li>$P(\theta|D)$: Posterior distribution</li>
<li>$P(D|\theta)$: Likelihood</li>
<li>$P(\theta)$: Prior distribution</li>
<li>$P(D)$: Evidence, normalization constant</li>
</ul>
<h3 id="32-map-definition">3.2 MAP Definition<a class="header-link" href="#32-map-definition" title="Permanent link">&para;</a></h3>
<p><strong>MAP estimation (Maximum A Posteriori)</strong> finds the parameter that maximizes the posterior distribution:</p>
<p>$$\theta_{\text{MAP}} = \arg\max_{\theta} P(\theta|D)$$</p>
<p>$$= \arg\max_{\theta} P(D|\theta)P(\theta)$$</p>
<p>$$= \arg\max_{\theta} \left[\log P(D|\theta) + \log P(\theta)\right]$$</p>
<p><strong>Relationship with MLE</strong>:
- MLE: $\arg\max_{\theta} \log P(D|\theta)$
- MAP: $\arg\max_{\theta} \left[\log P(D|\theta) + \log P(\theta)\right]$</p>
<p>MAP = MLE + prior term</p>
<h3 id="33-mle-vs-map-comparison">3.3 MLE vs MAP Comparison<a class="header-link" href="#33-mle-vs-map-comparison" title="Permanent link">&para;</a></h3>
<div class="highlight"><pre><span></span><code><span class="kn">import</span><span class="w"> </span><span class="nn">numpy</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">np</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">matplotlib.pyplot</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">plt</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">scipy</span><span class="w"> </span><span class="kn">import</span> <span class="n">stats</span>

<span class="c1"># Îç∞Ïù¥ÌÑ∞: 10Î≤à Ï§ë 8Î≤à ÏïûÎ©¥</span>
<span class="n">n</span><span class="p">,</span> <span class="n">k</span> <span class="o">=</span> <span class="mi">10</span><span class="p">,</span> <span class="mi">8</span>

<span class="c1"># Ïö∞ÎèÑ Ìï®Ïàò</span>
<span class="n">theta_range</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="mf">0.01</span><span class="p">,</span> <span class="mf">0.99</span><span class="p">,</span> <span class="mi">100</span><span class="p">)</span>
<span class="n">likelihood</span> <span class="o">=</span> <span class="n">stats</span><span class="o">.</span><span class="n">binom</span><span class="o">.</span><span class="n">pmf</span><span class="p">(</span><span class="n">k</span><span class="p">,</span> <span class="n">n</span><span class="p">,</span> <span class="n">theta_range</span><span class="p">)</span>

<span class="c1"># MLE</span>
<span class="n">theta_mle</span> <span class="o">=</span> <span class="n">k</span> <span class="o">/</span> <span class="n">n</span>

<span class="c1"># MAP with Beta prior (Œ±, Œ≤)</span>
<span class="n">alpha</span><span class="p">,</span> <span class="n">beta</span> <span class="o">=</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">2</span>  <span class="c1"># ÏïΩÌïú ÏÇ¨Ï†ÑÎ∂ÑÌè¨</span>
<span class="n">prior</span> <span class="o">=</span> <span class="n">stats</span><span class="o">.</span><span class="n">beta</span><span class="o">.</span><span class="n">pdf</span><span class="p">(</span><span class="n">theta_range</span><span class="p">,</span> <span class="n">alpha</span><span class="p">,</span> <span class="n">beta</span><span class="p">)</span>
<span class="n">posterior</span> <span class="o">=</span> <span class="n">likelihood</span> <span class="o">*</span> <span class="n">prior</span>
<span class="n">posterior</span> <span class="o">=</span> <span class="n">posterior</span> <span class="o">/</span> <span class="n">np</span><span class="o">.</span><span class="n">trapz</span><span class="p">(</span><span class="n">posterior</span><span class="p">,</span> <span class="n">theta_range</span><span class="p">)</span>  <span class="c1"># Ï†ïÍ∑úÌôî</span>
<span class="n">theta_map</span> <span class="o">=</span> <span class="n">theta_range</span><span class="p">[</span><span class="n">np</span><span class="o">.</span><span class="n">argmax</span><span class="p">(</span><span class="n">posterior</span><span class="p">)]</span>

<span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">14</span><span class="p">,</span> <span class="mi">4</span><span class="p">))</span>

<span class="c1"># Ïö∞ÎèÑ</span>
<span class="n">plt</span><span class="o">.</span><span class="n">subplot</span><span class="p">(</span><span class="mi">131</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">theta_range</span><span class="p">,</span> <span class="n">likelihood</span> <span class="o">/</span> <span class="n">np</span><span class="o">.</span><span class="n">max</span><span class="p">(</span><span class="n">likelihood</span><span class="p">),</span> <span class="n">label</span><span class="o">=</span><span class="s1">&#39;Likelihood&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">axvline</span><span class="p">(</span><span class="n">theta_mle</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s1">&#39;r&#39;</span><span class="p">,</span> <span class="n">linestyle</span><span class="o">=</span><span class="s1">&#39;--&#39;</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="sa">f</span><span class="s1">&#39;MLE=</span><span class="si">{</span><span class="n">theta_mle</span><span class="si">:</span><span class="s1">.2f</span><span class="si">}</span><span class="s1">&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s1">&#39;Œ∏&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s1">&#39;Normalized Likelihood&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s1">&#39;Likelihood Function&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">legend</span><span class="p">()</span>
<span class="n">plt</span><span class="o">.</span><span class="n">grid</span><span class="p">(</span><span class="kc">True</span><span class="p">)</span>

<span class="c1"># ÏÇ¨Ï†ÑÎ∂ÑÌè¨</span>
<span class="n">plt</span><span class="o">.</span><span class="n">subplot</span><span class="p">(</span><span class="mi">132</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">theta_range</span><span class="p">,</span> <span class="n">prior</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="sa">f</span><span class="s1">&#39;Beta(</span><span class="si">{</span><span class="n">alpha</span><span class="si">}</span><span class="s1">,</span><span class="si">{</span><span class="n">beta</span><span class="si">}</span><span class="s1">)&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s1">&#39;Œ∏&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s1">&#39;Density&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s1">&#39;Prior Distribution&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">legend</span><span class="p">()</span>
<span class="n">plt</span><span class="o">.</span><span class="n">grid</span><span class="p">(</span><span class="kc">True</span><span class="p">)</span>

<span class="c1"># ÏÇ¨ÌõÑÎ∂ÑÌè¨</span>
<span class="n">plt</span><span class="o">.</span><span class="n">subplot</span><span class="p">(</span><span class="mi">133</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">theta_range</span><span class="p">,</span> <span class="n">posterior</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s1">&#39;Posterior&#39;</span><span class="p">,</span> <span class="n">linewidth</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">axvline</span><span class="p">(</span><span class="n">theta_mle</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s1">&#39;r&#39;</span><span class="p">,</span> <span class="n">linestyle</span><span class="o">=</span><span class="s1">&#39;--&#39;</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.7</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="sa">f</span><span class="s1">&#39;MLE=</span><span class="si">{</span><span class="n">theta_mle</span><span class="si">:</span><span class="s1">.2f</span><span class="si">}</span><span class="s1">&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">axvline</span><span class="p">(</span><span class="n">theta_map</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s1">&#39;g&#39;</span><span class="p">,</span> <span class="n">linestyle</span><span class="o">=</span><span class="s1">&#39;--&#39;</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.7</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="sa">f</span><span class="s1">&#39;MAP=</span><span class="si">{</span><span class="n">theta_map</span><span class="si">:</span><span class="s1">.2f</span><span class="si">}</span><span class="s1">&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s1">&#39;Œ∏&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s1">&#39;Density&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s1">&#39;Posterior Distribution&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">legend</span><span class="p">()</span>
<span class="n">plt</span><span class="o">.</span><span class="n">grid</span><span class="p">(</span><span class="kc">True</span><span class="p">)</span>

<span class="n">plt</span><span class="o">.</span><span class="n">tight_layout</span><span class="p">()</span>
<span class="n">plt</span><span class="o">.</span><span class="n">savefig</span><span class="p">(</span><span class="s1">&#39;mle_vs_map.png&#39;</span><span class="p">,</span> <span class="n">dpi</span><span class="o">=</span><span class="mi">150</span><span class="p">,</span> <span class="n">bbox_inches</span><span class="o">=</span><span class="s1">&#39;tight&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>

<span class="c1"># Îç∞Ïù¥ÌÑ∞Í∞Ä Ï†ÅÏùÑ ÎïåÏùò Ï∞®Ïù¥</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Îç∞Ïù¥ÌÑ∞Í∞Ä Ï†ÅÏùÑ Îïå (n=10, k=8):&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;MLE: </span><span class="si">{</span><span class="n">theta_mle</span><span class="si">:</span><span class="s2">.3f</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;MAP: </span><span class="si">{</span><span class="n">theta_map</span><span class="si">:</span><span class="s2">.3f</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>

<span class="c1"># Îç∞Ïù¥ÌÑ∞Í∞Ä ÎßéÏùÑ Îïå</span>
<span class="n">n2</span><span class="p">,</span> <span class="n">k2</span> <span class="o">=</span> <span class="mi">1000</span><span class="p">,</span> <span class="mi">800</span>
<span class="n">theta_mle2</span> <span class="o">=</span> <span class="n">k2</span> <span class="o">/</span> <span class="n">n2</span>
<span class="n">likelihood2</span> <span class="o">=</span> <span class="n">stats</span><span class="o">.</span><span class="n">binom</span><span class="o">.</span><span class="n">pmf</span><span class="p">(</span><span class="n">k2</span><span class="p">,</span> <span class="n">n2</span><span class="p">,</span> <span class="n">theta_range</span><span class="p">)</span>
<span class="n">posterior2</span> <span class="o">=</span> <span class="n">likelihood2</span> <span class="o">*</span> <span class="n">prior</span>
<span class="n">theta_map2</span> <span class="o">=</span> <span class="n">theta_range</span><span class="p">[</span><span class="n">np</span><span class="o">.</span><span class="n">argmax</span><span class="p">(</span><span class="n">posterior2</span><span class="p">)]</span>

<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;</span><span class="se">\n</span><span class="s2">Îç∞Ïù¥ÌÑ∞Í∞Ä ÎßéÏùÑ Îïå (n=1000, k=800):&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;MLE: </span><span class="si">{</span><span class="n">theta_mle2</span><span class="si">:</span><span class="s2">.3f</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;MAP: </span><span class="si">{</span><span class="n">theta_map2</span><span class="si">:</span><span class="s2">.3f</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;‚Üí Îç∞Ïù¥ÌÑ∞Í∞Ä ÎßéÏúºÎ©¥ MLEÏôÄ MAPÍ∞Ä ÏàòÎ†¥&quot;</span><span class="p">)</span>
</code></pre></div>

<h3 id="34-role-of-prior-distribution">3.4 Role of Prior Distribution<a class="header-link" href="#34-role-of-prior-distribution" title="Permanent link">&para;</a></h3>
<ul>
<li><strong>With little data</strong>: Prior has significant influence (regularization effect)</li>
<li><strong>With much data</strong>: Likelihood dominates, MAP ‚âà MLE</li>
<li><strong>Appropriate prior</strong>: Reflects domain knowledge, prevents overfitting</li>
<li><strong>Uninformative prior</strong>: $P(\theta) \propto 1$ ‚Üí MAP = MLE</li>
</ul>
<h2 id="4-regularization-prior-distribution">4. Regularization = Prior Distribution<a class="header-link" href="#4-regularization-prior-distribution" title="Permanent link">&para;</a></h2>
<h3 id="41-l2-regularization-and-gaussian-prior">4.1 L2 Regularization and Gaussian Prior<a class="header-link" href="#41-l2-regularization-and-gaussian-prior" title="Permanent link">&para;</a></h3>
<p><strong>L2 regularization in linear regression</strong> (Ridge):</p>
<p>$$\min_w \left[\frac{1}{2}\sum_{i=1}^n (y_i - w^T x_i)^2 + \frac{\lambda}{2}\|w\|^2\right]$$</p>
<p>This is equivalent to the following MAP:</p>
<p><strong>Prior</strong>: $w \sim \mathcal{N}(0, \sigma_w^2 I)$</p>
<p>$$\log P(w) = -\frac{1}{2\sigma_w^2}\|w\|^2 + \text{const}$$</p>
<p><strong>Likelihood</strong> (Gaussian noise): $y|x,w \sim \mathcal{N}(w^T x, \sigma^2)$</p>
<p>$$\log P(D|w) = -\frac{1}{2\sigma^2}\sum_{i=1}^n (y_i - w^T x_i)^2 + \text{const}$$</p>
<p><strong>Log posterior</strong>:</p>
<p>$$\log P(w|D) = \log P(D|w) + \log P(w)$$</p>
<p>$$= -\frac{1}{2\sigma^2}\sum_{i=1}^n (y_i - w^T x_i)^2 - \frac{1}{2\sigma_w^2}\|w\|^2 + \text{const}$$</p>
<p><strong>Maximization = Minimization</strong>:</p>
<p>$$\arg\max_w \log P(w|D) = \arg\min_w \left[\sum_{i=1}^n (y_i - w^T x_i)^2 + \frac{\sigma^2}{\sigma_w^2}\|w\|^2\right]$$</p>
<p>Therefore $\lambda = \frac{\sigma^2}{\sigma_w^2}$</p>
<p><strong>Conclusion</strong>: L2 regularization = MAP with Gaussian prior</p>
<h3 id="42-l1-regularization-and-laplace-prior">4.2 L1 Regularization and Laplace Prior<a class="header-link" href="#42-l1-regularization-and-laplace-prior" title="Permanent link">&para;</a></h3>
<p><strong>Lasso regression</strong>:</p>
<p>$$\min_w \left[\frac{1}{2}\sum_{i=1}^n (y_i - w^T x_i)^2 + \lambda\|w\|_1\right]$$</p>
<p><strong>Laplace prior</strong>: $P(w_j) = \frac{1}{2b}\exp\left(-\frac{|w_j|}{b}\right)$</p>
<p>$$\log P(w) = -\frac{1}{b}\|w\|_1 + \text{const}$$</p>
<p>Therefore $\lambda = \frac{\sigma^2}{b}$</p>
<p><strong>Conclusion</strong>: L1 regularization = MAP with Laplace prior</p>
<div class="highlight"><pre><span></span><code><span class="kn">import</span><span class="w"> </span><span class="nn">numpy</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">np</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">matplotlib.pyplot</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">plt</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">scipy</span><span class="w"> </span><span class="kn">import</span> <span class="n">stats</span>

<span class="c1"># ÏÇ¨Ï†ÑÎ∂ÑÌè¨ ÏãúÍ∞ÅÌôî</span>
<span class="n">w_range</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="o">-</span><span class="mi">3</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">1000</span><span class="p">)</span>

<span class="c1"># Í∞ÄÏö∞ÏãúÏïà ÏÇ¨Ï†ÑÎ∂ÑÌè¨ (L2)</span>
<span class="n">sigma_w</span> <span class="o">=</span> <span class="mf">1.0</span>
<span class="n">gaussian_prior</span> <span class="o">=</span> <span class="n">stats</span><span class="o">.</span><span class="n">norm</span><span class="o">.</span><span class="n">pdf</span><span class="p">(</span><span class="n">w_range</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="n">sigma_w</span><span class="p">)</span>
<span class="n">log_gaussian</span> <span class="o">=</span> <span class="n">stats</span><span class="o">.</span><span class="n">norm</span><span class="o">.</span><span class="n">logpdf</span><span class="p">(</span><span class="n">w_range</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="n">sigma_w</span><span class="p">)</span>

<span class="c1"># ÎùºÌîåÎùºÏä§ ÏÇ¨Ï†ÑÎ∂ÑÌè¨ (L1)</span>
<span class="n">b</span> <span class="o">=</span> <span class="mf">1.0</span>
<span class="n">laplace_prior</span> <span class="o">=</span> <span class="n">stats</span><span class="o">.</span><span class="n">laplace</span><span class="o">.</span><span class="n">pdf</span><span class="p">(</span><span class="n">w_range</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="n">b</span><span class="p">)</span>
<span class="n">log_laplace</span> <span class="o">=</span> <span class="n">stats</span><span class="o">.</span><span class="n">laplace</span><span class="o">.</span><span class="n">logpdf</span><span class="p">(</span><span class="n">w_range</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="n">b</span><span class="p">)</span>

<span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">12</span><span class="p">,</span> <span class="mi">4</span><span class="p">))</span>

<span class="n">plt</span><span class="o">.</span><span class="n">subplot</span><span class="p">(</span><span class="mi">131</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">w_range</span><span class="p">,</span> <span class="n">gaussian_prior</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s1">&#39;Gaussian (L2)&#39;</span><span class="p">,</span> <span class="n">linewidth</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">w_range</span><span class="p">,</span> <span class="n">laplace_prior</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s1">&#39;Laplace (L1)&#39;</span><span class="p">,</span> <span class="n">linewidth</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s1">&#39;w&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s1">&#39;Density&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s1">&#39;Prior Distributions&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">legend</span><span class="p">()</span>
<span class="n">plt</span><span class="o">.</span><span class="n">grid</span><span class="p">(</span><span class="kc">True</span><span class="p">)</span>

<span class="n">plt</span><span class="o">.</span><span class="n">subplot</span><span class="p">(</span><span class="mi">132</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">w_range</span><span class="p">,</span> <span class="n">log_gaussian</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s1">&#39;log Gaussian&#39;</span><span class="p">,</span> <span class="n">linewidth</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">w_range</span><span class="p">,</span> <span class="n">log_laplace</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s1">&#39;log Laplace&#39;</span><span class="p">,</span> <span class="n">linewidth</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s1">&#39;w&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s1">&#39;Log Density&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s1">&#39;Log Prior (Regularization Term)&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">legend</span><span class="p">()</span>
<span class="n">plt</span><span class="o">.</span><span class="n">grid</span><span class="p">(</span><span class="kc">True</span><span class="p">)</span>

<span class="n">plt</span><span class="o">.</span><span class="n">subplot</span><span class="p">(</span><span class="mi">133</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">w_range</span><span class="p">,</span> <span class="o">-</span><span class="n">w_range</span><span class="o">**</span><span class="mi">2</span> <span class="o">/</span> <span class="p">(</span><span class="mi">2</span><span class="o">*</span><span class="n">sigma_w</span><span class="o">**</span><span class="mi">2</span><span class="p">),</span>
         <span class="n">label</span><span class="o">=</span><span class="sa">r</span><span class="s1">&#39;$-\frac{w^2}{2\sigma_w^2}$ (L2)&#39;</span><span class="p">,</span> <span class="n">linewidth</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">w_range</span><span class="p">,</span> <span class="o">-</span><span class="n">np</span><span class="o">.</span><span class="n">abs</span><span class="p">(</span><span class="n">w_range</span><span class="p">)</span> <span class="o">/</span> <span class="n">b</span><span class="p">,</span>
         <span class="n">label</span><span class="o">=</span><span class="sa">r</span><span class="s1">&#39;$-\frac{|w|}</span><span class="si">{b}</span><span class="s1">$ (L1)&#39;</span><span class="p">,</span> <span class="n">linewidth</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s1">&#39;w&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s1">&#39;Regularization Penalty&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s1">&#39;Penalty Terms&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">legend</span><span class="p">()</span>
<span class="n">plt</span><span class="o">.</span><span class="n">grid</span><span class="p">(</span><span class="kc">True</span><span class="p">)</span>

<span class="n">plt</span><span class="o">.</span><span class="n">tight_layout</span><span class="p">()</span>
<span class="n">plt</span><span class="o">.</span><span class="n">savefig</span><span class="p">(</span><span class="s1">&#39;regularization_prior.png&#39;</span><span class="p">,</span> <span class="n">dpi</span><span class="o">=</span><span class="mi">150</span><span class="p">,</span> <span class="n">bbox_inches</span><span class="o">=</span><span class="s1">&#39;tight&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>

<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;L2 (Gaussian)Îäî ÌÅ∞ Í∞ÄÏ§ëÏπòÏóê quadratic penalty&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;L1 (Laplace)Îäî Î™®Îì† Í∞ÄÏ§ëÏπòÏóê linear penalty ‚Üí Ìù¨ÏÜåÏÑ±&quot;</span><span class="p">)</span>
</code></pre></div>

<h2 id="5-em-algorithm-expectation-maximization">5. EM Algorithm (Expectation-Maximization)<a class="header-link" href="#5-em-algorithm-expectation-maximization" title="Permanent link">&para;</a></h2>
<h3 id="51-latent-variable-problem">5.1 Latent Variable Problem<a class="header-link" href="#51-latent-variable-problem" title="Permanent link">&para;</a></h3>
<p>With <strong>observed variable</strong> $X$ and <strong>latent variable</strong> $Z$:</p>
<p>$$P(X|\theta) = \sum_Z P(X, Z|\theta)$$</p>
<p>Direct maximization is difficult (log of sum).</p>
<p><strong>EM algorithm</strong> iteratively maximizes a lower bound.</p>
<h3 id="52-elbo-and-em-derivation">5.2 ELBO and EM Derivation<a class="header-link" href="#52-elbo-and-em-derivation" title="Permanent link">&para;</a></h3>
<p>Using Jensen's inequality:</p>
<p>$$\log P(X|\theta) = \log \sum_Z P(X, Z|\theta)$$</p>
<p>$$= \log \sum_Z Q(Z) \frac{P(X, Z|\theta)}{Q(Z)}$$</p>
<p>$$\geq \sum_Z Q(Z) \log \frac{P(X, Z|\theta)}{Q(Z)}$$</p>
<p>$$= \mathbb{E}_{Q(Z)} [\log P(X, Z|\theta)] + H(Q)$$</p>
<p>This is called the <strong>ELBO</strong> (Evidence Lower BOund).</p>
<p><strong>E-step</strong>: Optimize $Q(Z)$ for current $\theta^{(t)}$
$$Q^{(t+1)}(Z) = P(Z|X, \theta^{(t)})$$</p>
<p><strong>M-step</strong>: Optimize $\theta$ for current $Q^{(t+1)}$
$$\theta^{(t+1)} = \arg\max_{\theta} \mathbb{E}_{Q^{(t+1)}(Z)} [\log P(X, Z|\theta)]$$</p>
<h3 id="53-gaussian-mixture-model-gmm">5.3 Gaussian Mixture Model (GMM)<a class="header-link" href="#53-gaussian-mixture-model-gmm" title="Permanent link">&para;</a></h3>
<p><strong>Model</strong>:
- $K$ Gaussian components
- Latent variable $z_i \in \{1, \ldots, K\}$: component of data $x_i$
- Parameters: $\pi_k$ (mixing proportions), $\mu_k, \Sigma_k$ (mean, covariance of each Gaussian)</p>
<p><strong>Generative process</strong>:
1. $z_i \sim \text{Categorical}(\pi)$
2. $x_i | z_i=k \sim \mathcal{N}(\mu_k, \Sigma_k)$</p>
<p><strong>E-step</strong>: Compute responsibility</p>
<p>$$\gamma_{ik} = P(z_i=k | x_i, \theta^{(t)}) = \frac{\pi_k \mathcal{N}(x_i|\mu_k, \Sigma_k)}{\sum_{j=1}^K \pi_j \mathcal{N}(x_i|\mu_j, \Sigma_j)}$$</p>
<p><strong>M-step</strong>: Update parameters</p>
<p>$$\pi_k^{\text{new}} = \frac{1}{n}\sum_{i=1}^n \gamma_{ik}$$</p>
<p>$$\mu_k^{\text{new}} = \frac{\sum_{i=1}^n \gamma_{ik} x_i}{\sum_{i=1}^n \gamma_{ik}}$$</p>
<p>$$\Sigma_k^{\text{new}} = \frac{\sum_{i=1}^n \gamma_{ik} (x_i - \mu_k^{\text{new}})(x_i - \mu_k^{\text{new}})^T}{\sum_{i=1}^n \gamma_{ik}}$$</p>
<div class="highlight"><pre><span></span><code><span class="kn">import</span><span class="w"> </span><span class="nn">numpy</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">np</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">matplotlib.pyplot</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">plt</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">scipy</span><span class="w"> </span><span class="kn">import</span> <span class="n">stats</span>

<span class="k">class</span><span class="w"> </span><span class="nc">GMM</span><span class="p">:</span>
    <span class="k">def</span><span class="w"> </span><span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">K</span><span class="p">,</span> <span class="n">max_iter</span><span class="o">=</span><span class="mi">100</span><span class="p">,</span> <span class="n">tol</span><span class="o">=</span><span class="mf">1e-4</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">K</span> <span class="o">=</span> <span class="n">K</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">max_iter</span> <span class="o">=</span> <span class="n">max_iter</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">tol</span> <span class="o">=</span> <span class="n">tol</span>

    <span class="k">def</span><span class="w"> </span><span class="nf">fit</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">X</span><span class="p">):</span>
        <span class="n">n</span><span class="p">,</span> <span class="n">d</span> <span class="o">=</span> <span class="n">X</span><span class="o">.</span><span class="n">shape</span>

        <span class="c1"># Ï¥àÍ∏∞Ìôî: k-means++</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">pi</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">ones</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">K</span><span class="p">)</span> <span class="o">/</span> <span class="bp">self</span><span class="o">.</span><span class="n">K</span>
        <span class="c1"># ÎûúÎç§ Ï¥àÍ∏∞Ìôî</span>
        <span class="n">idx</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">choice</span><span class="p">(</span><span class="n">n</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">K</span><span class="p">,</span> <span class="n">replace</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">mu</span> <span class="o">=</span> <span class="n">X</span><span class="p">[</span><span class="n">idx</span><span class="p">]</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">Sigma</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="n">np</span><span class="o">.</span><span class="n">eye</span><span class="p">(</span><span class="n">d</span><span class="p">)</span> <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">K</span><span class="p">)])</span>

        <span class="n">log_likelihood_old</span> <span class="o">=</span> <span class="o">-</span><span class="n">np</span><span class="o">.</span><span class="n">inf</span>

        <span class="k">for</span> <span class="n">iteration</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">max_iter</span><span class="p">):</span>
            <span class="c1"># E-step</span>
            <span class="n">gamma</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_e_step</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>

            <span class="c1"># M-step</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">_m_step</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">gamma</span><span class="p">)</span>

            <span class="c1"># Î°úÍ∑∏ Ïö∞ÎèÑ Í≥ÑÏÇ∞</span>
            <span class="n">log_likelihood</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_compute_log_likelihood</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>

            <span class="k">if</span> <span class="n">iteration</span> <span class="o">%</span> <span class="mi">10</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
                <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Iteration </span><span class="si">{</span><span class="n">iteration</span><span class="si">}</span><span class="s2">: log-likelihood = </span><span class="si">{</span><span class="n">log_likelihood</span><span class="si">:</span><span class="s2">.2f</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>

            <span class="c1"># ÏàòÎ†¥ ÌôïÏù∏</span>
            <span class="k">if</span> <span class="nb">abs</span><span class="p">(</span><span class="n">log_likelihood</span> <span class="o">-</span> <span class="n">log_likelihood_old</span><span class="p">)</span> <span class="o">&lt;</span> <span class="bp">self</span><span class="o">.</span><span class="n">tol</span><span class="p">:</span>
                <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Converged at iteration </span><span class="si">{</span><span class="n">iteration</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
                <span class="k">break</span>

            <span class="n">log_likelihood_old</span> <span class="o">=</span> <span class="n">log_likelihood</span>

        <span class="k">return</span> <span class="bp">self</span>

    <span class="k">def</span><span class="w"> </span><span class="nf">_e_step</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">X</span><span class="p">):</span>
        <span class="n">n</span> <span class="o">=</span> <span class="n">X</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
        <span class="n">gamma</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">((</span><span class="n">n</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">K</span><span class="p">))</span>

        <span class="k">for</span> <span class="n">k</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">K</span><span class="p">):</span>
            <span class="n">gamma</span><span class="p">[:,</span> <span class="n">k</span><span class="p">]</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">pi</span><span class="p">[</span><span class="n">k</span><span class="p">]</span> <span class="o">*</span> <span class="n">stats</span><span class="o">.</span><span class="n">multivariate_normal</span><span class="o">.</span><span class="n">pdf</span><span class="p">(</span>
                <span class="n">X</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">mu</span><span class="p">[</span><span class="n">k</span><span class="p">],</span> <span class="bp">self</span><span class="o">.</span><span class="n">Sigma</span><span class="p">[</span><span class="n">k</span><span class="p">])</span>

        <span class="c1"># Ï†ïÍ∑úÌôî</span>
        <span class="n">gamma</span> <span class="o">/=</span> <span class="n">gamma</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">keepdims</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">gamma</span>

    <span class="k">def</span><span class="w"> </span><span class="nf">_m_step</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">X</span><span class="p">,</span> <span class="n">gamma</span><span class="p">):</span>
        <span class="n">n</span> <span class="o">=</span> <span class="n">X</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>

        <span class="c1"># Effective number of points assigned to each cluster</span>
        <span class="n">Nk</span> <span class="o">=</span> <span class="n">gamma</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>

        <span class="c1"># ÌòºÌï© ÎπÑÏú® ÏóÖÎç∞Ïù¥Ìä∏</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">pi</span> <span class="o">=</span> <span class="n">Nk</span> <span class="o">/</span> <span class="n">n</span>

        <span class="c1"># ÌèâÍ∑† ÏóÖÎç∞Ïù¥Ìä∏</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">mu</span> <span class="o">=</span> <span class="p">(</span><span class="n">gamma</span><span class="o">.</span><span class="n">T</span> <span class="o">@</span> <span class="n">X</span><span class="p">)</span> <span class="o">/</span> <span class="n">Nk</span><span class="p">[:,</span> <span class="n">np</span><span class="o">.</span><span class="n">newaxis</span><span class="p">]</span>

        <span class="c1"># Í≥µÎ∂ÑÏÇ∞ ÏóÖÎç∞Ïù¥Ìä∏</span>
        <span class="k">for</span> <span class="n">k</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">K</span><span class="p">):</span>
            <span class="n">diff</span> <span class="o">=</span> <span class="n">X</span> <span class="o">-</span> <span class="bp">self</span><span class="o">.</span><span class="n">mu</span><span class="p">[</span><span class="n">k</span><span class="p">]</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">Sigma</span><span class="p">[</span><span class="n">k</span><span class="p">]</span> <span class="o">=</span> <span class="p">(</span><span class="n">gamma</span><span class="p">[:,</span> <span class="n">k</span><span class="p">:</span><span class="n">k</span><span class="o">+</span><span class="mi">1</span><span class="p">]</span> <span class="o">*</span> <span class="n">diff</span><span class="p">)</span><span class="o">.</span><span class="n">T</span> <span class="o">@</span> <span class="n">diff</span> <span class="o">/</span> <span class="n">Nk</span><span class="p">[</span><span class="n">k</span><span class="p">]</span>
            <span class="c1"># ÏàòÏπò ÏïàÏ†ïÏÑ±</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">Sigma</span><span class="p">[</span><span class="n">k</span><span class="p">]</span> <span class="o">+=</span> <span class="mf">1e-6</span> <span class="o">*</span> <span class="n">np</span><span class="o">.</span><span class="n">eye</span><span class="p">(</span><span class="n">X</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">])</span>

    <span class="k">def</span><span class="w"> </span><span class="nf">_compute_log_likelihood</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">X</span><span class="p">):</span>
        <span class="n">n</span> <span class="o">=</span> <span class="n">X</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
        <span class="n">log_likelihood</span> <span class="o">=</span> <span class="mi">0</span>

        <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">n</span><span class="p">):</span>
            <span class="n">prob</span> <span class="o">=</span> <span class="mi">0</span>
            <span class="k">for</span> <span class="n">k</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">K</span><span class="p">):</span>
                <span class="n">prob</span> <span class="o">+=</span> <span class="bp">self</span><span class="o">.</span><span class="n">pi</span><span class="p">[</span><span class="n">k</span><span class="p">]</span> <span class="o">*</span> <span class="n">stats</span><span class="o">.</span><span class="n">multivariate_normal</span><span class="o">.</span><span class="n">pdf</span><span class="p">(</span>
                    <span class="n">X</span><span class="p">[</span><span class="n">i</span><span class="p">],</span> <span class="bp">self</span><span class="o">.</span><span class="n">mu</span><span class="p">[</span><span class="n">k</span><span class="p">],</span> <span class="bp">self</span><span class="o">.</span><span class="n">Sigma</span><span class="p">[</span><span class="n">k</span><span class="p">])</span>
            <span class="n">log_likelihood</span> <span class="o">+=</span> <span class="n">np</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="n">prob</span> <span class="o">+</span> <span class="mf">1e-10</span><span class="p">)</span>

        <span class="k">return</span> <span class="n">log_likelihood</span>

    <span class="k">def</span><span class="w"> </span><span class="nf">predict</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">X</span><span class="p">):</span>
        <span class="n">gamma</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_e_step</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">np</span><span class="o">.</span><span class="n">argmax</span><span class="p">(</span><span class="n">gamma</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>

<span class="c1"># ÌÖåÏä§Ìä∏: 3Í∞úÏùò Í∞ÄÏö∞ÏãúÏïà ÌòºÌï©</span>
<span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">seed</span><span class="p">(</span><span class="mi">42</span><span class="p">)</span>
<span class="n">n_samples</span> <span class="o">=</span> <span class="mi">300</span>

<span class="c1"># 3Í∞úÏùò ÌÅ¥Îü¨Ïä§ÌÑ∞ ÏÉùÏÑ±</span>
<span class="n">X1</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="n">n_samples</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span> <span class="o">+</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">])</span>
<span class="n">X2</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="n">n_samples</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span> <span class="o">+</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="mi">5</span><span class="p">,</span> <span class="mi">5</span><span class="p">])</span>
<span class="n">X3</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="n">n_samples</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span> <span class="o">+</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="mi">0</span><span class="p">,</span> <span class="mi">5</span><span class="p">])</span>
<span class="n">X</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">vstack</span><span class="p">([</span><span class="n">X1</span><span class="p">,</span> <span class="n">X2</span><span class="p">,</span> <span class="n">X3</span><span class="p">])</span>

<span class="c1"># GMM ÌïôÏäµ</span>
<span class="n">gmm</span> <span class="o">=</span> <span class="n">GMM</span><span class="p">(</span><span class="n">K</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span> <span class="n">max_iter</span><span class="o">=</span><span class="mi">100</span><span class="p">)</span>
<span class="n">gmm</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>

<span class="c1"># ÏòàÏ∏°</span>
<span class="n">labels</span> <span class="o">=</span> <span class="n">gmm</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>

<span class="c1"># ÏãúÍ∞ÅÌôî</span>
<span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">12</span><span class="p">,</span> <span class="mi">4</span><span class="p">))</span>

<span class="n">plt</span><span class="o">.</span><span class="n">subplot</span><span class="p">(</span><span class="mi">131</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">X</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">],</span> <span class="n">X</span><span class="p">[:,</span> <span class="mi">1</span><span class="p">],</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.5</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s1">&#39;Original Data&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s1">&#39;x1&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s1">&#39;x2&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">grid</span><span class="p">(</span><span class="kc">True</span><span class="p">)</span>

<span class="n">plt</span><span class="o">.</span><span class="n">subplot</span><span class="p">(</span><span class="mi">132</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">X</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">],</span> <span class="n">X</span><span class="p">[:,</span> <span class="mi">1</span><span class="p">],</span> <span class="n">c</span><span class="o">=</span><span class="n">labels</span><span class="p">,</span> <span class="n">cmap</span><span class="o">=</span><span class="s1">&#39;viridis&#39;</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.5</span><span class="p">)</span>
<span class="k">for</span> <span class="n">k</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">gmm</span><span class="o">.</span><span class="n">K</span><span class="p">):</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">gmm</span><span class="o">.</span><span class="n">mu</span><span class="p">[</span><span class="n">k</span><span class="p">,</span> <span class="mi">0</span><span class="p">],</span> <span class="n">gmm</span><span class="o">.</span><span class="n">mu</span><span class="p">[</span><span class="n">k</span><span class="p">,</span> <span class="mi">1</span><span class="p">],</span>
                <span class="n">marker</span><span class="o">=</span><span class="s1">&#39;x&#39;</span><span class="p">,</span> <span class="n">s</span><span class="o">=</span><span class="mi">200</span><span class="p">,</span> <span class="n">linewidths</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s1">&#39;red&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s1">&#39;GMM Clustering&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s1">&#39;x1&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s1">&#39;x2&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">grid</span><span class="p">(</span><span class="kc">True</span><span class="p">)</span>

<span class="c1"># Í≤∞Ï†ï Í≤ΩÍ≥Ñ</span>
<span class="n">plt</span><span class="o">.</span><span class="n">subplot</span><span class="p">(</span><span class="mi">133</span><span class="p">)</span>
<span class="n">x_min</span><span class="p">,</span> <span class="n">x_max</span> <span class="o">=</span> <span class="n">X</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">min</span><span class="p">()</span> <span class="o">-</span> <span class="mi">1</span><span class="p">,</span> <span class="n">X</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">max</span><span class="p">()</span> <span class="o">+</span> <span class="mi">1</span>
<span class="n">y_min</span><span class="p">,</span> <span class="n">y_max</span> <span class="o">=</span> <span class="n">X</span><span class="p">[:,</span> <span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">min</span><span class="p">()</span> <span class="o">-</span> <span class="mi">1</span><span class="p">,</span> <span class="n">X</span><span class="p">[:,</span> <span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">max</span><span class="p">()</span> <span class="o">+</span> <span class="mi">1</span>
<span class="n">xx</span><span class="p">,</span> <span class="n">yy</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">meshgrid</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="n">x_min</span><span class="p">,</span> <span class="n">x_max</span><span class="p">,</span> <span class="mi">100</span><span class="p">),</span>
                     <span class="n">np</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="n">y_min</span><span class="p">,</span> <span class="n">y_max</span><span class="p">,</span> <span class="mi">100</span><span class="p">))</span>
<span class="n">Z</span> <span class="o">=</span> <span class="n">gmm</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">c_</span><span class="p">[</span><span class="n">xx</span><span class="o">.</span><span class="n">ravel</span><span class="p">(),</span> <span class="n">yy</span><span class="o">.</span><span class="n">ravel</span><span class="p">()])</span>
<span class="n">Z</span> <span class="o">=</span> <span class="n">Z</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="n">xx</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>

<span class="n">plt</span><span class="o">.</span><span class="n">contourf</span><span class="p">(</span><span class="n">xx</span><span class="p">,</span> <span class="n">yy</span><span class="p">,</span> <span class="n">Z</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.3</span><span class="p">,</span> <span class="n">cmap</span><span class="o">=</span><span class="s1">&#39;viridis&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">X</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">],</span> <span class="n">X</span><span class="p">[:,</span> <span class="mi">1</span><span class="p">],</span> <span class="n">c</span><span class="o">=</span><span class="n">labels</span><span class="p">,</span> <span class="n">cmap</span><span class="o">=</span><span class="s1">&#39;viridis&#39;</span><span class="p">,</span>
            <span class="n">alpha</span><span class="o">=</span><span class="mf">0.7</span><span class="p">,</span> <span class="n">edgecolors</span><span class="o">=</span><span class="s1">&#39;k&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s1">&#39;Decision Boundaries&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s1">&#39;x1&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s1">&#39;x2&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">grid</span><span class="p">(</span><span class="kc">True</span><span class="p">)</span>

<span class="n">plt</span><span class="o">.</span><span class="n">tight_layout</span><span class="p">()</span>
<span class="n">plt</span><span class="o">.</span><span class="n">savefig</span><span class="p">(</span><span class="s1">&#39;gmm_em.png&#39;</span><span class="p">,</span> <span class="n">dpi</span><span class="o">=</span><span class="mi">150</span><span class="p">,</span> <span class="n">bbox_inches</span><span class="o">=</span><span class="s1">&#39;tight&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</code></pre></div>

<h2 id="6-machine-learning-applications">6. Machine Learning Applications<a class="header-link" href="#6-machine-learning-applications" title="Permanent link">&para;</a></h2>
<h3 id="61-logistic-regression-bernoulli-mle">6.1 Logistic Regression = Bernoulli MLE<a class="header-link" href="#61-logistic-regression-bernoulli-mle" title="Permanent link">&para;</a></h3>
<p><strong>Model</strong>: $P(y=1|x,w) = \sigma(w^T x)$, where $\sigma(z) = \frac{1}{1+e^{-z}}$</p>
<p><strong>Log-likelihood</strong>:</p>
<p>$$\ell(w) = \sum_{i=1}^n \left[y_i \log \sigma(w^T x_i) + (1-y_i) \log(1-\sigma(w^T x_i))\right]$$</p>
<p>This is the log-likelihood of the Bernoulli distribution!</p>
<p><strong>Optimization</strong>: Gradient descent</p>
<p>$$\nabla_w \ell = \sum_{i=1}^n (y_i - \sigma(w^T x_i)) x_i$$</p>
<h3 id="62-neural-network-training-mle">6.2 Neural Network Training = MLE<a class="header-link" href="#62-neural-network-training-mle" title="Permanent link">&para;</a></h3>
<p>The neural network output for <strong>classification problems</strong> is interpreted as softmax:</p>
<p>$$P(y=k|x,\theta) = \frac{\exp(f_k(x;\theta))}{\sum_j \exp(f_j(x;\theta))}$$</p>
<p><strong>Cross-entropy loss</strong> = <strong>Negative log-likelihood</strong>:</p>
<p>$$\mathcal{L}(\theta) = -\sum_{i=1}^n \log P(y_i|x_i, \theta)$$</p>
<p>Therefore, neural network training = MLE!</p>
<h3 id="63-introduction-to-bayesian-neural-networks">6.3 Introduction to Bayesian Neural Networks<a class="header-link" href="#63-introduction-to-bayesian-neural-networks" title="Permanent link">&para;</a></h3>
<p><strong>Problem</strong>: Neural networks only provide point estimates, not uncertainty</p>
<p><strong>Bayesian neural networks</strong>: Infer posterior distribution over weights</p>
<p>$$P(\theta|D) = \frac{P(D|\theta)P(\theta)}{P(D)}$$</p>
<p><strong>Prediction</strong>: Integration over posterior</p>
<p>$$P(y|x, D) = \int P(y|x, \theta) P(\theta|D) d\theta$$</p>
<p><strong>Challenge</strong>: High-dimensional integration ‚Üí requires variational inference, MCMC</p>
<div class="highlight"><pre><span></span><code><span class="kn">import</span><span class="w"> </span><span class="nn">torch</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">torch.nn</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">nn</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">torch.optim</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">optim</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">numpy</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">np</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">matplotlib.pyplot</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">plt</span>

<span class="c1"># Í∞ÑÎã®Ìïú Î°úÏßÄÏä§Ìã± ÌöåÍ∑Ä ÏòàÏ†ú</span>
<span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">seed</span><span class="p">(</span><span class="mi">42</span><span class="p">)</span>
<span class="n">torch</span><span class="o">.</span><span class="n">manual_seed</span><span class="p">(</span><span class="mi">42</span><span class="p">)</span>

<span class="c1"># Îç∞Ïù¥ÌÑ∞ ÏÉùÏÑ±</span>
<span class="n">n</span> <span class="o">=</span> <span class="mi">200</span>
<span class="n">X</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="n">n</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>
<span class="n">y</span> <span class="o">=</span> <span class="p">(</span><span class="n">X</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">]</span> <span class="o">+</span> <span class="n">X</span><span class="p">[:,</span> <span class="mi">1</span><span class="p">]</span> <span class="o">&gt;</span> <span class="mi">0</span><span class="p">)</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>

<span class="n">X_tensor</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">FloatTensor</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>
<span class="n">y_tensor</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">FloatTensor</span><span class="p">(</span><span class="n">y</span><span class="p">)</span>

<span class="c1"># Î°úÏßÄÏä§Ìã± ÌöåÍ∑Ä Î™®Îç∏</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
<span class="n">criterion</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">BCEWithLogitsLoss</span><span class="p">()</span>  <span class="c1"># Ïù¥ÏßÑ ÍµêÏ∞® ÏóîÌä∏Î°úÌîº = Î≤†Î•¥ÎàÑÏù¥ MLE</span>
<span class="n">optimizer</span> <span class="o">=</span> <span class="n">optim</span><span class="o">.</span><span class="n">SGD</span><span class="p">(</span><span class="n">model</span><span class="o">.</span><span class="n">parameters</span><span class="p">(),</span> <span class="n">lr</span><span class="o">=</span><span class="mf">0.1</span><span class="p">)</span>

<span class="c1"># ÌïôÏäµ</span>
<span class="n">losses</span> <span class="o">=</span> <span class="p">[]</span>
<span class="k">for</span> <span class="n">epoch</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">1000</span><span class="p">):</span>
    <span class="n">optimizer</span><span class="o">.</span><span class="n">zero_grad</span><span class="p">()</span>
    <span class="n">output</span> <span class="o">=</span> <span class="n">model</span><span class="p">(</span><span class="n">X_tensor</span><span class="p">)</span><span class="o">.</span><span class="n">squeeze</span><span class="p">()</span>
    <span class="n">loss</span> <span class="o">=</span> <span class="n">criterion</span><span class="p">(</span><span class="n">output</span><span class="p">,</span> <span class="n">y_tensor</span><span class="p">)</span>
    <span class="n">loss</span><span class="o">.</span><span class="n">backward</span><span class="p">()</span>
    <span class="n">optimizer</span><span class="o">.</span><span class="n">step</span><span class="p">()</span>
    <span class="n">losses</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">loss</span><span class="o">.</span><span class="n">item</span><span class="p">())</span>

<span class="c1"># ÏãúÍ∞ÅÌôî</span>
<span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">12</span><span class="p">,</span> <span class="mi">4</span><span class="p">))</span>

<span class="n">plt</span><span class="o">.</span><span class="n">subplot</span><span class="p">(</span><span class="mi">131</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">losses</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s1">&#39;Epoch&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s1">&#39;Loss (Negative Log-Likelihood)&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s1">&#39;Training Loss = -Log-Likelihood&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">grid</span><span class="p">(</span><span class="kc">True</span><span class="p">)</span>

<span class="n">plt</span><span class="o">.</span><span class="n">subplot</span><span class="p">(</span><span class="mi">132</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">X</span><span class="p">[</span><span class="n">y</span><span class="o">==</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">],</span> <span class="n">X</span><span class="p">[</span><span class="n">y</span><span class="o">==</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">],</span> <span class="n">label</span><span class="o">=</span><span class="s1">&#39;Class 0&#39;</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.6</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">X</span><span class="p">[</span><span class="n">y</span><span class="o">==</span><span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">],</span> <span class="n">X</span><span class="p">[</span><span class="n">y</span><span class="o">==</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">],</span> <span class="n">label</span><span class="o">=</span><span class="s1">&#39;Class 1&#39;</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.6</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s1">&#39;x1&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s1">&#39;x2&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s1">&#39;Data&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">legend</span><span class="p">()</span>
<span class="n">plt</span><span class="o">.</span><span class="n">grid</span><span class="p">(</span><span class="kc">True</span><span class="p">)</span>

<span class="c1"># Í≤∞Ï†ï Í≤ΩÍ≥Ñ</span>
<span class="n">plt</span><span class="o">.</span><span class="n">subplot</span><span class="p">(</span><span class="mi">133</span><span class="p">)</span>
<span class="n">x_min</span><span class="p">,</span> <span class="n">x_max</span> <span class="o">=</span> <span class="n">X</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">min</span><span class="p">()</span> <span class="o">-</span> <span class="mi">1</span><span class="p">,</span> <span class="n">X</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">max</span><span class="p">()</span> <span class="o">+</span> <span class="mi">1</span>
<span class="n">y_min</span><span class="p">,</span> <span class="n">y_max</span> <span class="o">=</span> <span class="n">X</span><span class="p">[:,</span> <span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">min</span><span class="p">()</span> <span class="o">-</span> <span class="mi">1</span><span class="p">,</span> <span class="n">X</span><span class="p">[:,</span> <span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">max</span><span class="p">()</span> <span class="o">+</span> <span class="mi">1</span>
<span class="n">xx</span><span class="p">,</span> <span class="n">yy</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">meshgrid</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="n">x_min</span><span class="p">,</span> <span class="n">x_max</span><span class="p">,</span> <span class="mi">100</span><span class="p">),</span>
                     <span class="n">np</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="n">y_min</span><span class="p">,</span> <span class="n">y_max</span><span class="p">,</span> <span class="mi">100</span><span class="p">))</span>
<span class="k">with</span> <span class="n">torch</span><span class="o">.</span><span class="n">no_grad</span><span class="p">():</span>
    <span class="n">Z</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">sigmoid</span><span class="p">(</span><span class="n">model</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">FloatTensor</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">c_</span><span class="p">[</span><span class="n">xx</span><span class="o">.</span><span class="n">ravel</span><span class="p">(),</span> <span class="n">yy</span><span class="o">.</span><span class="n">ravel</span><span class="p">()])))</span><span class="o">.</span><span class="n">numpy</span><span class="p">()</span>
<span class="n">Z</span> <span class="o">=</span> <span class="n">Z</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="n">xx</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>

<span class="n">plt</span><span class="o">.</span><span class="n">contourf</span><span class="p">(</span><span class="n">xx</span><span class="p">,</span> <span class="n">yy</span><span class="p">,</span> <span class="n">Z</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.3</span><span class="p">,</span> <span class="n">levels</span><span class="o">=</span><span class="mi">20</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">X</span><span class="p">[</span><span class="n">y</span><span class="o">==</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">],</span> <span class="n">X</span><span class="p">[</span><span class="n">y</span><span class="o">==</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">],</span> <span class="n">label</span><span class="o">=</span><span class="s1">&#39;Class 0&#39;</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.7</span><span class="p">,</span> <span class="n">edgecolors</span><span class="o">=</span><span class="s1">&#39;k&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">X</span><span class="p">[</span><span class="n">y</span><span class="o">==</span><span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">],</span> <span class="n">X</span><span class="p">[</span><span class="n">y</span><span class="o">==</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">],</span> <span class="n">label</span><span class="o">=</span><span class="s1">&#39;Class 1&#39;</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.7</span><span class="p">,</span> <span class="n">edgecolors</span><span class="o">=</span><span class="s1">&#39;k&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">colorbar</span><span class="p">(</span><span class="n">label</span><span class="o">=</span><span class="s1">&#39;P(y=1|x)&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s1">&#39;MLE Decision Boundary&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s1">&#39;x1&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s1">&#39;x2&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">legend</span><span class="p">()</span>
<span class="n">plt</span><span class="o">.</span><span class="n">grid</span><span class="p">(</span><span class="kc">True</span><span class="p">)</span>

<span class="n">plt</span><span class="o">.</span><span class="n">tight_layout</span><span class="p">()</span>
<span class="n">plt</span><span class="o">.</span><span class="n">savefig</span><span class="p">(</span><span class="s1">&#39;logistic_mle.png&#39;</span><span class="p">,</span> <span class="n">dpi</span><span class="o">=</span><span class="mi">150</span><span class="p">,</span> <span class="n">bbox_inches</span><span class="o">=</span><span class="s1">&#39;tight&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>

<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Learned weights: </span><span class="si">{</span><span class="n">model</span><span class="o">.</span><span class="n">weight</span><span class="o">.</span><span class="n">data</span><span class="o">.</span><span class="n">numpy</span><span class="p">()</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Learned bias: </span><span class="si">{</span><span class="n">model</span><span class="o">.</span><span class="n">bias</span><span class="o">.</span><span class="n">data</span><span class="o">.</span><span class="n">numpy</span><span class="p">()</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
</code></pre></div>

<h2 id="practice-problems">Practice Problems<a class="header-link" href="#practice-problems" title="Permanent link">&para;</a></h2>
<h3 id="problem-1-mle-for-exponential-distribution">Problem 1: MLE for Exponential Distribution<a class="header-link" href="#problem-1-mle-for-exponential-distribution" title="Permanent link">&para;</a></h3>
<p>The PDF of the exponential distribution $\text{Exp}(\lambda)$ is $p(x|\lambda) = \lambda e^{-\lambda x}$ (for $x \geq 0$).</p>
<p>Given $n$ i.i.d. samples $\{x_1, \ldots, x_n\}$:</p>
<p>(a) Derive the log-likelihood function.
(b) Find the MLE of $\lambda$.
(c) Verify with Python (generate simulation data and compute MLE).</p>
<h3 id="problem-2-map-with-different-priors">Problem 2: MAP with Different Priors<a class="header-link" href="#problem-2-map-with-different-priors" title="Permanent link">&para;</a></h3>
<p>In a linear regression problem:
- Data: $(x_1, y_1), \ldots, (x_n, y_n)$
- Model: $y = wx + b + \epsilon$, $\epsilon \sim \mathcal{N}(0, \sigma^2)$</p>
<p>Compare MAP for the following two cases:</p>
<p>(a) Gaussian prior: $w \sim \mathcal{N}(0, \sigma_w^2)$ ‚Üí L2 regularization
(b) Laplace prior: $p(w) \propto \exp(-|w|/b)$ ‚Üí L1 regularization</p>
<p>Write the optimization problems for each and implement in Python to visualize the differences.</p>
<h3 id="problem-3-em-for-coin-toss">Problem 3: EM for Coin Toss<a class="header-link" href="#problem-3-em-for-coin-toss" title="Permanent link">&para;</a></h3>
<p>Two coins A and B have head probabilities $\theta_A, \theta_B$ respectively.</p>
<p>Experiment:
- In 5 trials, each trial randomly selects a coin (unobservable)
- Results: {H, T, T, H, H}</p>
<p>Use EM algorithm to estimate $\theta_A, \theta_B$:</p>
<p>(a) E-step: Compute probability of coin A for each trial
(b) M-step: Update $\theta_A, \theta_B$
(c) Implement in Python and visualize convergence process</p>
<h3 id="problem-4-fisher-information">Problem 4: Fisher Information<a class="header-link" href="#problem-4-fisher-information" title="Permanent link">&para;</a></h3>
<p>Fisher information is defined as:</p>
<p>$$I(\theta) = -\mathbb{E}\left[\frac{\partial^2 \log p(X|\theta)}{\partial \theta^2}\right]$$</p>
<p>For Bernoulli distribution $p(x|\theta) = \theta^x (1-\theta)^{1-x}$:</p>
<p>(a) Compute Fisher information $I(\theta)$.
(b) Verify the Cram√©r-Rao lower bound $\text{Var}(\hat{\theta}) \geq \frac{1}{nI(\theta)}$.
(c) Show that the variance of MLE $\hat{\theta} = \frac{k}{n}$ reaches this lower bound.</p>
<h3 id="problem-5-em-for-mixture-of-exponentials">Problem 5: EM for Mixture of Exponentials<a class="header-link" href="#problem-5-em-for-mixture-of-exponentials" title="Permanent link">&para;</a></h3>
<p>Mixture model of exponential distributions:</p>
<p>$$p(x) = \pi \lambda_1 e^{-\lambda_1 x} + (1-\pi) \lambda_2 e^{-\lambda_2 x}$$</p>
<p>Derive EM algorithm and implement in Python:</p>
<p>(a) Derive E-step responsibility formula
(b) Derive M-step update formulas for $\pi, \lambda_1, \lambda_2$
(c) Verify with simulation data (recover true parameters from known ground truth)</p>
<h2 id="references">References<a class="header-link" href="#references" title="Permanent link">&para;</a></h2>
<ol>
<li><strong>Bishop, C. M. (2006).</strong> <em>Pattern Recognition and Machine Learning</em>. Chapter 9 (EM Algorithm).</li>
<li><strong>Murphy, K. P. (2022).</strong> <em>Probabilistic Machine Learning: An Introduction</em>. Chapter 8 (MLE), Chapter 10 (MAP).</li>
<li><strong>Deisenroth, M. P., Faisal, A. A., &amp; Ong, C. S. (2020).</strong> <em>Mathematics for Machine Learning</em>. Chapter 8.</li>
<li><strong>MacKay, D. J. C. (2003).</strong> <em>Information Theory, Inference, and Learning Algorithms</em>. Chapter 22 (EM).</li>
<li><strong>Paper</strong>: Dempster, A. P., Laird, N. M., &amp; Rubin, D. B. (1977). "Maximum Likelihood from Incomplete Data via the EM Algorithm". <em>Journal of the Royal Statistical Society</em>.</li>
<li><strong>scikit-learn documentation</strong>: Gaussian Mixture Models - https://scikit-learn.org/stable/modules/mixture.html</li>
<li><strong>PyTorch documentation</strong>: Loss Functions - https://pytorch.org/docs/stable/nn.html#loss-functions</li>
</ol>
    </div>

    
<div class="lesson-toolbar lesson-toolbar--bottom">
    <div class="toolbar-nav toolbar-nav--prev">
        
        <a href="/study/en/Math_for_AI/08_Probability_for_ML.html" class="nav-prev">
            <span class="nav-label">Previous</span>
            <span class="nav-title">08. Probability for Machine Learning</span>
        </a>
        
    </div>
    <div class="toolbar-actions">
        <button class="btn btn-copy-link" data-action="copy-link" title="Copy link">
            <span class="icon">üîó</span>
            <span class="text">Copy link</span>
        </button>
        <a href="/study/en/Math_for_AI/" class="btn btn-topic-link" title="Back to topic list">
            <span class="icon">üìã</span>
            <span class="text">Topic list</span>
        </a>
    </div>
    <div class="toolbar-nav toolbar-nav--next">
        
        <a href="/study/en/Math_for_AI/10_Information_Theory.html" class="nav-next">
            <span class="nav-label">Next</span>
            <span class="nav-title">10. Information Theory</span>
        </a>
        
    </div>
</div>


    <div class="toolbar-keyboard-hint">
        <kbd>&larr;</kbd> <kbd>&rarr;</kbd> to navigate between lessons
    </div>
</article>

<!-- Scroll to top floating button -->
<button class="scroll-to-top" id="scroll-to-top" title="Scroll to top" aria-label="Scroll to top">‚Üë</button>

            </div>
        </main>
    </div>

    <script src="/study/static/js/app.js"></script>
    <script>
        function switchLanguage(newLang) {
            const path = window.location.pathname;
            const base = '/study';
            const currentLang = 'en';
            const newPath = path.replace(base + '/' + currentLang + '/', base + '/' + newLang + '/');
            window.location.href = newPath + window.location.search;
        }
    </script>
    
<script>
    document.addEventListener('DOMContentLoaded', function() {
        // Copy link - independent feedback per button
        document.querySelectorAll('[data-action="copy-link"]').forEach(function(btn) {
            btn.addEventListener('click', function() {
                navigator.clipboard.writeText(window.location.href);
                var textEl = this.querySelector('.text');
                var originalText = textEl.textContent;
                textEl.textContent = 'Copied!';
                setTimeout(function() { textEl.textContent = originalText; }, 2000);
            });
        });

        // Add copy buttons to code blocks
        document.querySelectorAll('.lesson-content pre').forEach(function(pre) {
            var copyBtn = document.createElement('button');
            copyBtn.className = 'code-copy-btn';
            copyBtn.textContent = 'Copy';
            copyBtn.addEventListener('click', function() {
                var code = pre.querySelector('code');
                var text = code ? code.textContent : pre.textContent;
                navigator.clipboard.writeText(text);
                copyBtn.textContent = 'Copied!';
                setTimeout(function() { copyBtn.textContent = 'Copy'; }, 2000);
            });
            pre.style.position = 'relative';
            pre.appendChild(copyBtn);
        });

        // Scroll to top button
        var scrollBtn = document.getElementById('scroll-to-top');
        window.addEventListener('scroll', function() {
            scrollBtn.classList.toggle('visible', window.scrollY > 300);
        });
        scrollBtn.addEventListener('click', function() {
            window.scrollTo({ top: 0, behavior: 'smooth' });
        });

        // Keyboard shortcuts: left/right arrows for lesson navigation
        document.addEventListener('keydown', function(e) {
            var tag = document.activeElement.tagName;
            if (tag === 'INPUT' || tag === 'TEXTAREA' || tag === 'SELECT') return;

            if (e.key === 'ArrowLeft') {
                var prevLink = document.querySelector('.nav-prev');
                if (prevLink) prevLink.click();
            } else if (e.key === 'ArrowRight') {
                var nextLink = document.querySelector('.nav-next');
                if (nextLink) nextLink.click();
            }
        });
    });
</script>

</body>
</html>
{% endraw %}