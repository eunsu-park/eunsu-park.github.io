{% raw %}
<!DOCTYPE html>
<html lang="en" data-theme="light">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>12. Sampling and Monte Carlo Methods - Study Materials</title>
    <link rel="stylesheet" href="/study/static/css/style.css">
    <link rel="stylesheet" href="/study/static/css/highlight.css">
    <!-- KaTeX for LaTeX math rendering -->
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.11/dist/katex.min.css">
    <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.11/dist/katex.min.js"></script>
    <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.11/dist/contrib/auto-render.min.js"
        onload="renderMathInElement(document.body, {
            delimiters: [
                {left: '$$', right: '$$', display: true},
                {left: '$', right: '$', display: false}
            ],
            throwOnError: false
        });"></script>
    
</head>
<body>
    <div class="app-container">
        <!-- Sidebar -->
        <aside class="sidebar" id="sidebar">
            <div class="sidebar-header">
                <a href="/study/en/" class="logo">Study Materials</a>
                <button class="sidebar-toggle" id="sidebar-toggle" aria-label="Toggle sidebar">
                    <span></span>
                </button>
            </div>

            <nav class="sidebar-nav">
                <a href="/study/en/" class="nav-item ">
                    <span class="nav-icon">ğŸ </span>
                    <span class="nav-text">Home</span>
                </a>
                <a href="/study/examples/" class="nav-item ">
                    <span class="nav-icon">ğŸ’»</span>
                    <span class="nav-text">Examples</span>
                </a>
            </nav>

            <div class="sidebar-search">
                <form action="/study/en/search.html" method="get" id="search-form">
                    <input type="search" name="q" placeholder="Search..." id="search-sidebar-input">
                </form>
            </div>

            <!-- Language Selector -->
            <div class="sidebar-lang">
                <select id="lang-select" class="lang-selector" onchange="switchLanguage(this.value)">
                    
                    <option value="en" selected>
                        English
                    </option>
                    
                    <option value="ko" >
                        í•œêµ­ì–´
                    </option>
                    
                </select>
            </div>

            <div class="sidebar-footer">
                <button class="theme-toggle" id="theme-toggle" aria-label="Toggle theme">
                    <span class="theme-icon light">â˜€ï¸</span>
                    <span class="theme-icon dark">ğŸŒ™</span>
                </button>
            </div>
        </aside>

        <!-- Main Content -->
        <main class="main-content">
            <header class="main-header">
                <button class="menu-toggle" id="menu-toggle" aria-label="Open menu">
                    <span></span>
                </button>
                
<nav class="breadcrumb">
    <a href="/study/en/">Topics</a>
    <span class="separator">/</span>
    <a href="/study/en/Math_for_AI/">Math for AI</a>
    <span class="separator">/</span>
    <span class="current">12. Sampling and Monte Carlo Methods</span>
</nav>

            </header>

            <div class="content">
                
<article class="lesson-article">
    <header class="lesson-header">
        <h1>12. Sampling and Monte Carlo Methods</h1>
    </header>

    
<div class="lesson-toolbar lesson-toolbar--top">
    <div class="toolbar-nav toolbar-nav--prev">
        
        <a href="/study/en/Math_for_AI/11_Probability_Distributions_Advanced.html" class="nav-prev">
            <span class="nav-label">Previous</span>
            <span class="nav-title">11. Advanced Probability Distributions</span>
        </a>
        
    </div>
    <div class="toolbar-actions">
        <button class="btn btn-copy-link" data-action="copy-link" title="Copy link">
            <span class="icon">ğŸ”—</span>
            <span class="text">Copy link</span>
        </button>
        <a href="/study/en/Math_for_AI/" class="btn btn-topic-link" title="Back to topic list">
            <span class="icon">ğŸ“‹</span>
            <span class="text">Topic list</span>
        </a>
    </div>
    <div class="toolbar-nav toolbar-nav--next">
        
        <a href="/study/en/Math_for_AI/13_Linear_Algebra_for_Deep_Learning.html" class="nav-next">
            <span class="nav-label">Next</span>
            <span class="nav-title">13. Linear Algebra for Deep Learning</span>
        </a>
        
    </div>
</div>


    
    <nav class="toc" id="toc">
        <h2>Table of Contents</h2>
        <div class="toc">
<ul>
<li><a href="#learning-objectives">Learning Objectives</a></li>
<li><a href="#1-why-sampling">1. Why Sampling?</a><ul>
<li><a href="#11-integrals-that-cannot-be-computed-analytically">1.1 Integrals that Cannot Be Computed Analytically</a></li>
<li><a href="#12-monte-carlo-estimation">1.2 Monte Carlo Estimation</a></li>
<li><a href="#13-convergence-rate">1.3 Convergence Rate</a></li>
</ul>
</li>
<li><a href="#2-basic-sampling-methods">2. Basic Sampling Methods</a><ul>
<li><a href="#21-inverse-transform-sampling">2.1 Inverse Transform Sampling</a></li>
<li><a href="#22-rejection-sampling">2.2 Rejection Sampling</a></li>
<li><a href="#23-limitations-in-high-dimensions">2.3 Limitations in High Dimensions</a></li>
</ul>
</li>
<li><a href="#3-importance-sampling">3. Importance Sampling</a><ul>
<li><a href="#31-basic-principle">3.1 Basic Principle</a></li>
<li><a href="#32-choosing-the-proposal-distribution">3.2 Choosing the Proposal Distribution</a></li>
<li><a href="#33-self-normalized-importance-sampling">3.3 Self-Normalized Importance Sampling</a></li>
<li><a href="#34-connection-to-reinforcement-learning">3.4 Connection to Reinforcement Learning</a></li>
</ul>
</li>
<li><a href="#4-markov-chain-monte-carlo-mcmc">4. Markov Chain Monte Carlo (MCMC)</a><ul>
<li><a href="#41-markov-chain-basics">4.1 Markov Chain Basics</a></li>
<li><a href="#42-metropolis-hastings-algorithm">4.2 Metropolis-Hastings Algorithm</a></li>
<li><a href="#43-gibbs-sampling">4.3 Gibbs Sampling</a></li>
<li><a href="#44-burn-in-and-autocorrelation">4.4 Burn-in and Autocorrelation</a></li>
</ul>
</li>
<li><a href="#5-reparameterization-trick">5. Reparameterization Trick</a><ul>
<li><a href="#51-problem-backpropagating-through-stochastic-nodes">5.1 Problem: Backpropagating Through Stochastic Nodes</a></li>
<li><a href="#52-reparameterization-solution">5.2 Reparameterization Solution</a></li>
<li><a href="#53-reparameterization-for-other-distributions">5.3 Reparameterization for Other Distributions</a></li>
<li><a href="#54-vae-elbo">5.4 VAE ELBO</a></li>
</ul>
</li>
<li><a href="#6-machine-learning-applications">6. Machine Learning Applications</a><ul>
<li><a href="#61-vae-elbo-optimization">6.1 VAE: ELBO Optimization</a></li>
<li><a href="#62-reinforcement-learning-reinforce-monte-carlo">6.2 Reinforcement Learning: REINFORCE = Monte Carlo</a></li>
<li><a href="#63-bayesian-inference-mcmc-vs-variational-inference">6.3 Bayesian Inference: MCMC vs Variational Inference</a></li>
<li><a href="#64-dropout-bernoulli-sampling">6.4 Dropout = Bernoulli Sampling</a></li>
</ul>
</li>
<li><a href="#practice-problems">Practice Problems</a><ul>
<li><a href="#problem-1-monte-carlo-integration">Problem 1: Monte Carlo Integration</a></li>
<li><a href="#problem-2-importance-sampling">Problem 2: Importance Sampling</a></li>
<li><a href="#problem-3-mcmc-diagnostics">Problem 3: MCMC Diagnostics</a></li>
<li><a href="#problem-4-reparameterization-trick">Problem 4: Reparameterization Trick</a></li>
<li><a href="#problem-5-gibbs-sampling-extension">Problem 5: Gibbs Sampling Extension</a></li>
</ul>
</li>
<li><a href="#references">References</a><ul>
<li><a href="#books">Books</a></li>
<li><a href="#papers">Papers</a></li>
<li><a href="#online-resources">Online Resources</a></li>
<li><a href="#libraries">Libraries</a></li>
</ul>
</li>
</ul>
</div>

    </nav>
    

    <div class="lesson-content markdown-body">
        <h1 id="12-sampling-and-monte-carlo-methods">12. Sampling and Monte Carlo Methods<a class="header-link" href="#12-sampling-and-monte-carlo-methods" title="Permanent link">&para;</a></h1>
<h2 id="learning-objectives">Learning Objectives<a class="header-link" href="#learning-objectives" title="Permanent link">&para;</a></h2>
<ul>
<li>Understand the fundamental principles of Monte Carlo methods and their convergence rate</li>
<li>Implement basic sampling methods (inverse transform, rejection sampling) and recognize their limitations</li>
<li>Master variance reduction techniques through importance sampling</li>
<li>Understand the principles and implementation of MCMC (Metropolis-Hastings, Gibbs sampling)</li>
<li>Grasp the mathematical background of the reparameterization trick and its role in VAEs</li>
<li>Learn various use cases of sampling in machine learning</li>
</ul>
<hr />
<h2 id="1-why-sampling">1. Why Sampling?<a class="header-link" href="#1-why-sampling" title="Permanent link">&para;</a></h2>
<h3 id="11-integrals-that-cannot-be-computed-analytically">1.1 Integrals that Cannot Be Computed Analytically<a class="header-link" href="#11-integrals-that-cannot-be-computed-analytically" title="Permanent link">&para;</a></h3>
<p>Many machine learning problems require computing integrals of the form:</p>
<p>$$
\mathbb{E}_{p(x)}[f(x)] = \int f(x) p(x) dx
$$</p>
<p>For example:
- <strong>Bayesian inference</strong>: Normalizing constant of the posterior $\int p(x|\theta) p(\theta) d\theta$
- <strong>Reinforcement learning</strong>: Expected reward of a policy $\mathbb{E}_{\pi}[R]$
- <strong>VAE</strong>: Expected value in ELBO $\mathbb{E}_{q(z|x)}[\log p(x|z)]$</p>
<p>These integrals are intractable analytically in high dimensions.</p>
<h3 id="12-monte-carlo-estimation">1.2 Monte Carlo Estimation<a class="header-link" href="#12-monte-carlo-estimation" title="Permanent link">&para;</a></h3>
<p><strong>Monte Carlo principle</strong>: If we draw $N$ samples $x_1, \ldots, x_N$ from distribution $p(x)$:</p>
<p>$$
\mathbb{E}_{p(x)}[f(x)] \approx \frac{1}{N} \sum_{i=1}^{N} f(x_i)
$$</p>
<p>This is guaranteed by the <strong>Law of Large Numbers</strong>.</p>
<h3 id="13-convergence-rate">1.3 Convergence Rate<a class="header-link" href="#13-convergence-rate" title="Permanent link">&para;</a></h3>
<p>The standard error of Monte Carlo estimation is:</p>
<p>$$
\text{SE} = \frac{\sigma}{\sqrt{N}}
$$</p>
<p>where $\sigma^2 = \text{Var}_{p(x)}[f(x)]$.</p>
<p><strong>Key observation</strong>:
- Error decreases as $O(1/\sqrt{N})$ â†’ to increase accuracy by 10x, need 100x more samples
- <strong>Dimension-independent</strong>: applicable to high-dimensional integrals (grid methods suffer from the curse of dimensionality)</p>
<div class="highlight"><pre><span></span><code><span class="kn">import</span><span class="w"> </span><span class="nn">numpy</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">np</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">matplotlib.pyplot</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">plt</span>

<span class="c1"># ëª¬í…Œì¹´ë¥¼ë¡œë¡œ ì›ì£¼ìœ¨ Ï€ ì¶”ì •</span>
<span class="k">def</span><span class="w"> </span><span class="nf">estimate_pi</span><span class="p">(</span><span class="n">n_samples</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;ë‹¨ìœ„ ì‚¬ê°í˜• ë‚´ ëœë¤ ì ì„ ìƒì„±í•˜ê³  ë‹¨ìœ„ ì› ë‚´ë¶€ ë¹„ìœ¨ë¡œ Ï€ ì¶”ì •&quot;&quot;&quot;</span>
    <span class="n">x</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">uniform</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="n">n_samples</span><span class="p">)</span>
    <span class="n">y</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">uniform</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="n">n_samples</span><span class="p">)</span>
    <span class="n">inside_circle</span> <span class="o">=</span> <span class="p">(</span><span class="n">x</span><span class="o">**</span><span class="mi">2</span> <span class="o">+</span> <span class="n">y</span><span class="o">**</span><span class="mi">2</span><span class="p">)</span> <span class="o">&lt;=</span> <span class="mi">1</span>
    <span class="n">pi_estimate</span> <span class="o">=</span> <span class="mi">4</span> <span class="o">*</span> <span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">inside_circle</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">pi_estimate</span>

<span class="c1"># ìƒ˜í”Œ ìˆ˜ì— ë”°ë¥¸ ìˆ˜ë ´</span>
<span class="n">sample_sizes</span> <span class="o">=</span> <span class="p">[</span><span class="mi">10</span><span class="p">,</span> <span class="mi">100</span><span class="p">,</span> <span class="mi">1000</span><span class="p">,</span> <span class="mi">10000</span><span class="p">,</span> <span class="mi">100000</span><span class="p">]</span>
<span class="n">estimates</span> <span class="o">=</span> <span class="p">[</span><span class="n">estimate_pi</span><span class="p">(</span><span class="n">n</span><span class="p">)</span> <span class="k">for</span> <span class="n">n</span> <span class="ow">in</span> <span class="n">sample_sizes</span><span class="p">]</span>

<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;ëª¬í…Œì¹´ë¥¼ë¡œ Ï€ ì¶”ì •:&quot;</span><span class="p">)</span>
<span class="k">for</span> <span class="n">n</span><span class="p">,</span> <span class="n">est</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">(</span><span class="n">sample_sizes</span><span class="p">,</span> <span class="n">estimates</span><span class="p">):</span>
    <span class="n">error</span> <span class="o">=</span> <span class="nb">abs</span><span class="p">(</span><span class="n">est</span> <span class="o">-</span> <span class="n">np</span><span class="o">.</span><span class="n">pi</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;N=</span><span class="si">{</span><span class="n">n</span><span class="si">:</span><span class="s2">6d</span><span class="si">}</span><span class="s2">: Ï€ â‰ˆ </span><span class="si">{</span><span class="n">est</span><span class="si">:</span><span class="s2">.6f</span><span class="si">}</span><span class="s2">, ì˜¤ì°¨ = </span><span class="si">{</span><span class="n">error</span><span class="si">:</span><span class="s2">.6f</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>

<span class="c1"># ìˆ˜ë ´ ì‹œê°í™”</span>
<span class="n">n_trials</span> <span class="o">=</span> <span class="mi">100</span>
<span class="n">sample_range</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">logspace</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">5</span><span class="p">,</span> <span class="mi">50</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="nb">int</span><span class="p">)</span>
<span class="n">errors</span> <span class="o">=</span> <span class="p">[]</span>

<span class="k">for</span> <span class="n">n</span> <span class="ow">in</span> <span class="n">sample_range</span><span class="p">:</span>
    <span class="n">trial_estimates</span> <span class="o">=</span> <span class="p">[</span><span class="n">estimate_pi</span><span class="p">(</span><span class="n">n</span><span class="p">)</span> <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">n_trials</span><span class="p">)]</span>
    <span class="n">errors</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">std</span><span class="p">(</span><span class="n">trial_estimates</span><span class="p">))</span>

<span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">10</span><span class="p">,</span> <span class="mi">6</span><span class="p">))</span>
<span class="n">plt</span><span class="o">.</span><span class="n">loglog</span><span class="p">(</span><span class="n">sample_range</span><span class="p">,</span> <span class="n">errors</span><span class="p">,</span> <span class="s1">&#39;b-&#39;</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s1">&#39;ì‹¤ì œ í‘œì¤€ ì˜¤ì°¨&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">loglog</span><span class="p">(</span><span class="n">sample_range</span><span class="p">,</span> <span class="mi">1</span><span class="o">/</span><span class="n">np</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="n">sample_range</span><span class="p">),</span> <span class="s1">&#39;r--&#39;</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="sa">r</span><span class="s1">&#39;$O(1/\sqrt</span><span class="si">{N}</span><span class="s1">)$&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s1">&#39;ìƒ˜í”Œ ìˆ˜ (N)&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s1">&#39;í‘œì¤€ ì˜¤ì°¨&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s1">&#39;ëª¬í…Œì¹´ë¥¼ë¡œ ì¶”ì •ì˜ ìˆ˜ë ´ ì†ë„&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">legend</span><span class="p">()</span>
<span class="n">plt</span><span class="o">.</span><span class="n">grid</span><span class="p">(</span><span class="kc">True</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.3</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</code></pre></div>

<hr />
<h2 id="2-basic-sampling-methods">2. Basic Sampling Methods<a class="header-link" href="#2-basic-sampling-methods" title="Permanent link">&para;</a></h2>
<h3 id="21-inverse-transform-sampling">2.1 Inverse Transform Sampling<a class="header-link" href="#21-inverse-transform-sampling" title="Permanent link">&para;</a></h3>
<p><strong>Theorem</strong>: If $U \sim \text{Uniform}(0, 1)$ and $F$ is a cumulative distribution function (CDF), then $X = F^{-1}(U)$ follows CDF $F$.</p>
<p><strong>Algorithm</strong>:
1. Generate $u \sim \text{Uniform}(0, 1)$
2. Compute $x = F^{-1}(u)$</p>
<div class="highlight"><pre><span></span><code><span class="c1"># ì—­ë³€í™˜ ìƒ˜í”Œë§ ì˜ˆì œ: ì§€ìˆ˜ë¶„í¬</span>
<span class="k">def</span><span class="w"> </span><span class="nf">inverse_transform_exponential</span><span class="p">(</span><span class="n">n_samples</span><span class="p">,</span> <span class="n">lambda_param</span><span class="o">=</span><span class="mf">1.0</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;ì§€ìˆ˜ë¶„í¬ Exp(Î»)ì—ì„œ ìƒ˜í”Œë§&quot;&quot;&quot;</span>
    <span class="n">u</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">uniform</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="n">n_samples</span><span class="p">)</span>
    <span class="c1"># F^(-1)(u) = -ln(1-u)/Î»</span>
    <span class="n">x</span> <span class="o">=</span> <span class="o">-</span><span class="n">np</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="mi">1</span> <span class="o">-</span> <span class="n">u</span><span class="p">)</span> <span class="o">/</span> <span class="n">lambda_param</span>
    <span class="k">return</span> <span class="n">x</span>

<span class="c1"># ê²€ì¦</span>
<span class="n">samples</span> <span class="o">=</span> <span class="n">inverse_transform_exponential</span><span class="p">(</span><span class="mi">10000</span><span class="p">,</span> <span class="n">lambda_param</span><span class="o">=</span><span class="mf">2.0</span><span class="p">)</span>

<span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">12</span><span class="p">,</span> <span class="mi">5</span><span class="p">))</span>
<span class="n">plt</span><span class="o">.</span><span class="n">subplot</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">hist</span><span class="p">(</span><span class="n">samples</span><span class="p">,</span> <span class="n">bins</span><span class="o">=</span><span class="mi">50</span><span class="p">,</span> <span class="n">density</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.7</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s1">&#39;ìƒ˜í”Œë§ ê²°ê³¼&#39;</span><span class="p">)</span>
<span class="n">x_range</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">5</span><span class="p">,</span> <span class="mi">100</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">x_range</span><span class="p">,</span> <span class="mi">2</span> <span class="o">*</span> <span class="n">np</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="o">-</span><span class="mi">2</span> <span class="o">*</span> <span class="n">x_range</span><span class="p">),</span> <span class="s1">&#39;r-&#39;</span><span class="p">,</span> <span class="n">linewidth</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s1">&#39;ì´ë¡  PDF&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s1">&#39;x&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s1">&#39;ë°€ë„&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s1">&#39;ì—­ë³€í™˜ ìƒ˜í”Œë§: ì§€ìˆ˜ë¶„í¬&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">legend</span><span class="p">()</span>

<span class="n">plt</span><span class="o">.</span><span class="n">subplot</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>
<span class="c1"># Q-Q plot</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">scipy</span><span class="w"> </span><span class="kn">import</span> <span class="n">stats</span>
<span class="n">stats</span><span class="o">.</span><span class="n">probplot</span><span class="p">(</span><span class="n">samples</span><span class="p">,</span> <span class="n">dist</span><span class="o">=</span><span class="n">stats</span><span class="o">.</span><span class="n">expon</span><span class="p">(</span><span class="n">scale</span><span class="o">=</span><span class="mf">0.5</span><span class="p">),</span> <span class="n">plot</span><span class="o">=</span><span class="n">plt</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s1">&#39;Q-Q Plot&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">tight_layout</span><span class="p">()</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</code></pre></div>

<p><strong>Limitation</strong>: Computing $F^{-1}$ is often difficult.</p>
<h3 id="22-rejection-sampling">2.2 Rejection Sampling<a class="header-link" href="#22-rejection-sampling" title="Permanent link">&para;</a></h3>
<p>When direct sampling from the target distribution $p(x)$ is difficult, use a proposal distribution $q(x)$.</p>
<p><strong>Algorithm</strong>:
1. Choose constant $M$ such that $M q(x) \geq p(x)$ (for all $x$)
2. Generate $x \sim q(x)$
3. Generate $u \sim \text{Uniform}(0, 1)$
4. Accept $x$ if $u \leq \frac{p(x)}{M q(x)}$, otherwise reject</p>
<p><strong>Acceptance rate</strong>: $\frac{1}{M}$</p>
<div class="highlight"><pre><span></span><code><span class="c1"># ê¸°ê° ìƒ˜í”Œë§ ì˜ˆì œ: ë² íƒ€ë¶„í¬ Beta(2, 5)</span>
<span class="k">def</span><span class="w"> </span><span class="nf">target_pdf</span><span class="p">(</span><span class="n">x</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;ëª©í‘œ ë¶„í¬: Beta(2, 5)&quot;&quot;&quot;</span>
    <span class="k">if</span> <span class="mi">0</span> <span class="o">&lt;=</span> <span class="n">x</span> <span class="o">&lt;=</span> <span class="mi">1</span><span class="p">:</span>
        <span class="k">return</span> <span class="mi">30</span> <span class="o">*</span> <span class="n">x</span> <span class="o">*</span> <span class="p">(</span><span class="mi">1</span><span class="o">-</span><span class="n">x</span><span class="p">)</span><span class="o">**</span><span class="mi">4</span>  <span class="c1"># ì •ê·œí™”ëœ Beta(2,5)</span>
    <span class="k">return</span> <span class="mi">0</span>

<span class="k">def</span><span class="w"> </span><span class="nf">proposal_pdf</span><span class="p">(</span><span class="n">x</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;ì œì•ˆ ë¶„í¬: Uniform(0, 1)&quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="mf">1.0</span> <span class="k">if</span> <span class="mi">0</span> <span class="o">&lt;=</span> <span class="n">x</span> <span class="o">&lt;=</span> <span class="mi">1</span> <span class="k">else</span> <span class="mi">0</span>

<span class="c1"># M ì°¾ê¸° (ìµœëŒ€ê°’)</span>
<span class="n">x_grid</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1000</span><span class="p">)</span>
<span class="n">M</span> <span class="o">=</span> <span class="nb">max</span><span class="p">(</span><span class="n">target_pdf</span><span class="p">(</span><span class="n">x</span><span class="p">)</span> <span class="o">/</span> <span class="n">proposal_pdf</span><span class="p">(</span><span class="n">x</span><span class="p">)</span> <span class="k">for</span> <span class="n">x</span> <span class="ow">in</span> <span class="n">x_grid</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;M = </span><span class="si">{</span><span class="n">M</span><span class="si">:</span><span class="s2">.2f</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>

<span class="k">def</span><span class="w"> </span><span class="nf">rejection_sampling</span><span class="p">(</span><span class="n">n_samples</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;ê¸°ê° ìƒ˜í”Œë§ êµ¬í˜„&quot;&quot;&quot;</span>
    <span class="n">samples</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="n">n_rejected</span> <span class="o">=</span> <span class="mi">0</span>

    <span class="k">while</span> <span class="nb">len</span><span class="p">(</span><span class="n">samples</span><span class="p">)</span> <span class="o">&lt;</span> <span class="n">n_samples</span><span class="p">:</span>
        <span class="n">x</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">uniform</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>  <span class="c1"># ì œì•ˆ ë¶„í¬ì—ì„œ ìƒ˜í”Œ</span>
        <span class="n">u</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">uniform</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>

        <span class="k">if</span> <span class="n">u</span> <span class="o">&lt;=</span> <span class="n">target_pdf</span><span class="p">(</span><span class="n">x</span><span class="p">)</span> <span class="o">/</span> <span class="p">(</span><span class="n">M</span> <span class="o">*</span> <span class="n">proposal_pdf</span><span class="p">(</span><span class="n">x</span><span class="p">)):</span>
            <span class="n">samples</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">n_rejected</span> <span class="o">+=</span> <span class="mi">1</span>

    <span class="n">acceptance_rate</span> <span class="o">=</span> <span class="n">n_samples</span> <span class="o">/</span> <span class="p">(</span><span class="n">n_samples</span> <span class="o">+</span> <span class="n">n_rejected</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;ìˆ˜ë½ë¥ : </span><span class="si">{</span><span class="n">acceptance_rate</span><span class="si">:</span><span class="s2">.2%</span><span class="si">}</span><span class="s2"> (ì´ë¡ ê°’: </span><span class="si">{</span><span class="mi">1</span><span class="o">/</span><span class="n">M</span><span class="si">:</span><span class="s2">.2%</span><span class="si">}</span><span class="s2">)&quot;</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">samples</span><span class="p">)</span>

<span class="n">samples</span> <span class="o">=</span> <span class="n">rejection_sampling</span><span class="p">(</span><span class="mi">10000</span><span class="p">)</span>

<span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">10</span><span class="p">,</span> <span class="mi">6</span><span class="p">))</span>
<span class="n">plt</span><span class="o">.</span><span class="n">hist</span><span class="p">(</span><span class="n">samples</span><span class="p">,</span> <span class="n">bins</span><span class="o">=</span><span class="mi">50</span><span class="p">,</span> <span class="n">density</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.7</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s1">&#39;ìƒ˜í”Œë§ ê²°ê³¼&#39;</span><span class="p">)</span>
<span class="n">x_range</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">100</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">x_range</span><span class="p">,</span> <span class="p">[</span><span class="n">target_pdf</span><span class="p">(</span><span class="n">x</span><span class="p">)</span> <span class="k">for</span> <span class="n">x</span> <span class="ow">in</span> <span class="n">x_range</span><span class="p">],</span> <span class="s1">&#39;r-&#39;</span><span class="p">,</span> <span class="n">linewidth</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s1">&#39;ëª©í‘œ PDF&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s1">&#39;x&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s1">&#39;ë°€ë„&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s1">&#39;ê¸°ê° ìƒ˜í”Œë§: Beta(2, 5)&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">legend</span><span class="p">()</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</code></pre></div>

<h3 id="23-limitations-in-high-dimensions">2.3 Limitations in High Dimensions<a class="header-link" href="#23-limitations-in-high-dimensions" title="Permanent link">&para;</a></h3>
<p>The acceptance rate of rejection sampling is $1/M$. In high dimensions, $M$ grows exponentially, making it inefficient.</p>
<p><strong>Example</strong>: For a 10-dimensional Gaussian, $M \sim e^{10}$ â†’ acceptance rate $< 0.01\%$</p>
<hr />
<h2 id="3-importance-sampling">3. Importance Sampling<a class="header-link" href="#3-importance-sampling" title="Permanent link">&para;</a></h2>
<h3 id="31-basic-principle">3.1 Basic Principle<a class="header-link" href="#31-basic-principle" title="Permanent link">&para;</a></h3>
<p>Goal: Compute $\mathbb{E}_{p(x)}[f(x)]$</p>
<p><strong>Importance sampling identity</strong>:</p>
<p>$$
\mathbb{E}_{p(x)}[f(x)] = \int f(x) p(x) dx = \int f(x) \frac{p(x)}{q(x)} q(x) dx = \mathbb{E}_{q(x)}\left[f(x) \frac{p(x)}{q(x)}\right]
$$</p>
<p>$q(x)$ is called the <strong>proposal distribution</strong>, and $w(x) = \frac{p(x)}{q(x)}$ is the <strong>importance weight</strong>.</p>
<p><strong>Monte Carlo estimate</strong>:
$$
\mathbb{E}_{p(x)}[f(x)] \approx \frac{1}{N} \sum_{i=1}^{N} f(x_i) w(x_i), \quad x_i \sim q(x)
$$</p>
<h3 id="32-choosing-the-proposal-distribution">3.2 Choosing the Proposal Distribution<a class="header-link" href="#32-choosing-the-proposal-distribution" title="Permanent link">&para;</a></h3>
<p><strong>Good proposal distribution</strong>:
- If $q(x)$ is proportional to $|f(x)| p(x)$, variance is minimized
- In practice, $q(x)$ should cover heavy tails of $p(x)$</p>
<p><strong>Bad choice</strong>: If $q(x)$ has lighter tails than $p(x)$ â†’ weights explode, leading to high variance</p>
<h3 id="33-self-normalized-importance-sampling">3.3 Self-Normalized Importance Sampling<a class="header-link" href="#33-self-normalized-importance-sampling" title="Permanent link">&para;</a></h3>
<p>When $p(x)$ is known only up to a normalizing constant ($p(x) = \tilde{p}(x)/Z$):</p>
<p>$$
\mathbb{E}_{p(x)}[f(x)] \approx \frac{\sum_{i=1}^{N} f(x_i) \tilde{w}(x_i)}{\sum_{i=1}^{N} \tilde{w}(x_i)}, \quad \tilde{w}(x_i) = \frac{\tilde{p}(x_i)}{q(x_i)}
$$</p>
<div class="highlight"><pre><span></span><code><span class="c1"># ì¤‘ìš”ë„ ìƒ˜í”Œë§ ì˜ˆì œ</span>
<span class="k">def</span><span class="w"> </span><span class="nf">target_unnormalized</span><span class="p">(</span><span class="n">x</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;ì •ê·œí™”ë˜ì§€ ì•Šì€ ëª©í‘œ ë¶„í¬: í˜¼í•© ê°€ìš°ìŠ¤&quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="mf">0.3</span> <span class="o">*</span> <span class="n">np</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="o">-</span><span class="mf">0.5</span> <span class="o">*</span> <span class="p">((</span><span class="n">x</span><span class="o">-</span><span class="mi">2</span><span class="p">)</span><span class="o">/</span><span class="mf">0.5</span><span class="p">)</span><span class="o">**</span><span class="mi">2</span><span class="p">)</span> <span class="o">+</span> \
           <span class="mf">0.7</span> <span class="o">*</span> <span class="n">np</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="o">-</span><span class="mf">0.5</span> <span class="o">*</span> <span class="p">((</span><span class="n">x</span><span class="o">+</span><span class="mi">2</span><span class="p">)</span><span class="o">/</span><span class="mf">1.0</span><span class="p">)</span><span class="o">**</span><span class="mi">2</span><span class="p">)</span>

<span class="k">def</span><span class="w"> </span><span class="nf">proposal</span><span class="p">(</span><span class="n">x</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;ì œì•ˆ ë¶„í¬: N(0, 3)&quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="n">np</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="o">-</span><span class="mf">0.5</span> <span class="o">*</span> <span class="p">(</span><span class="n">x</span><span class="o">/</span><span class="mi">3</span><span class="p">)</span><span class="o">**</span><span class="mi">2</span><span class="p">)</span> <span class="o">/</span> <span class="p">(</span><span class="mi">3</span> <span class="o">*</span> <span class="n">np</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="mi">2</span><span class="o">*</span><span class="n">np</span><span class="o">.</span><span class="n">pi</span><span class="p">))</span>

<span class="k">def</span><span class="w"> </span><span class="nf">f</span><span class="p">(</span><span class="n">x</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;ê³„ì‚°í•˜ë ¤ëŠ” í•¨ìˆ˜: x^2&quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="n">x</span><span class="o">**</span><span class="mi">2</span>

<span class="c1"># ì¼ë°˜ ëª¬í…Œì¹´ë¥¼ë¡œ (ëª©í‘œ ë¶„í¬ì—ì„œ ì§ì ‘ ìƒ˜í”Œë§ì´ ê°€ëŠ¥í•˜ë‹¤ê³  ê°€ì •)</span>
<span class="k">def</span><span class="w"> </span><span class="nf">naive_monte_carlo</span><span class="p">(</span><span class="n">n_samples</span><span class="p">):</span>
    <span class="c1"># ê¸°ê° ìƒ˜í”Œë§ìœ¼ë¡œ ëª©í‘œ ë¶„í¬ì—ì„œ ìƒ˜í”Œ</span>
    <span class="n">samples</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="k">while</span> <span class="nb">len</span><span class="p">(</span><span class="n">samples</span><span class="p">)</span> <span class="o">&lt;</span> <span class="n">n_samples</span><span class="p">:</span>
        <span class="n">x</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">normal</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">3</span><span class="p">)</span>
        <span class="n">u</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">uniform</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">target_unnormalized</span><span class="p">(</span><span class="n">x</span><span class="p">))</span>
        <span class="k">if</span> <span class="n">u</span> <span class="o">&lt;=</span> <span class="n">target_unnormalized</span><span class="p">(</span><span class="n">x</span><span class="p">):</span>
            <span class="n">samples</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
    <span class="n">samples</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">samples</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">f</span><span class="p">(</span><span class="n">samples</span><span class="p">))</span>

<span class="c1"># ì¤‘ìš”ë„ ìƒ˜í”Œë§</span>
<span class="k">def</span><span class="w"> </span><span class="nf">importance_sampling</span><span class="p">(</span><span class="n">n_samples</span><span class="p">):</span>
    <span class="c1"># ì œì•ˆ ë¶„í¬ì—ì„œ ìƒ˜í”Œ</span>
    <span class="n">samples</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">normal</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="n">n_samples</span><span class="p">)</span>
    <span class="n">weights</span> <span class="o">=</span> <span class="n">target_unnormalized</span><span class="p">(</span><span class="n">samples</span><span class="p">)</span> <span class="o">/</span> <span class="n">proposal</span><span class="p">(</span><span class="n">samples</span><span class="p">)</span>
    <span class="n">weights</span> <span class="o">=</span> <span class="n">weights</span> <span class="o">/</span> <span class="n">np</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">weights</span><span class="p">)</span>  <span class="c1"># ìê¸°ì •ê·œí™”</span>
    <span class="k">return</span> <span class="n">np</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">f</span><span class="p">(</span><span class="n">samples</span><span class="p">)</span> <span class="o">*</span> <span class="n">weights</span> <span class="o">*</span> <span class="n">n_samples</span><span class="p">)</span>

<span class="c1"># ë¹„êµ</span>
<span class="n">n_trials</span> <span class="o">=</span> <span class="mi">100</span>
<span class="n">n_samples</span> <span class="o">=</span> <span class="mi">1000</span>

<span class="n">naive_results</span> <span class="o">=</span> <span class="p">[</span><span class="n">naive_monte_carlo</span><span class="p">(</span><span class="n">n_samples</span><span class="p">)</span> <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">n_trials</span><span class="p">)]</span>
<span class="n">is_results</span> <span class="o">=</span> <span class="p">[</span><span class="n">importance_sampling</span><span class="p">(</span><span class="n">n_samples</span><span class="p">)</span> <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">n_trials</span><span class="p">)]</span>

<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;ì¼ë°˜ MC: í‰ê·  = </span><span class="si">{</span><span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">naive_results</span><span class="p">)</span><span class="si">:</span><span class="s2">.4f</span><span class="si">}</span><span class="s2">, í‘œì¤€í¸ì°¨ = </span><span class="si">{</span><span class="n">np</span><span class="o">.</span><span class="n">std</span><span class="p">(</span><span class="n">naive_results</span><span class="p">)</span><span class="si">:</span><span class="s2">.4f</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;ì¤‘ìš”ë„ ìƒ˜í”Œë§: í‰ê·  = </span><span class="si">{</span><span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">is_results</span><span class="p">)</span><span class="si">:</span><span class="s2">.4f</span><span class="si">}</span><span class="s2">, í‘œì¤€í¸ì°¨ = </span><span class="si">{</span><span class="n">np</span><span class="o">.</span><span class="n">std</span><span class="p">(</span><span class="n">is_results</span><span class="p">)</span><span class="si">:</span><span class="s2">.4f</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;ë¶„ì‚° ê°ì†Œìœ¨: </span><span class="si">{</span><span class="n">np</span><span class="o">.</span><span class="n">var</span><span class="p">(</span><span class="n">naive_results</span><span class="p">)</span><span class="w"> </span><span class="o">/</span><span class="w"> </span><span class="n">np</span><span class="o">.</span><span class="n">var</span><span class="p">(</span><span class="n">is_results</span><span class="p">)</span><span class="si">:</span><span class="s2">.2f</span><span class="si">}</span><span class="s2">x&quot;</span><span class="p">)</span>
</code></pre></div>

<h3 id="34-connection-to-reinforcement-learning">3.4 Connection to Reinforcement Learning<a class="header-link" href="#34-connection-to-reinforcement-learning" title="Permanent link">&para;</a></h3>
<p>Importance sampling is central to policy gradient methods:</p>
<p>$$
\nabla_\theta J(\theta) = \mathbb{E}_{\pi_{\theta_{\text{old}}}} \left[ \frac{\pi_\theta(a|s)}{\pi_{\theta_{\text{old}}}(a|s)} \nabla_\theta \log \pi_\theta(a|s) Q(s,a) \right]
$$</p>
<p>This enables <strong>off-policy learning</strong> (PPO, TRPO).</p>
<hr />
<h2 id="4-markov-chain-monte-carlo-mcmc">4. Markov Chain Monte Carlo (MCMC)<a class="header-link" href="#4-markov-chain-monte-carlo-mcmc" title="Permanent link">&para;</a></h2>
<h3 id="41-markov-chain-basics">4.1 Markov Chain Basics<a class="header-link" href="#41-markov-chain-basics" title="Permanent link">&para;</a></h3>
<p><strong>Markov chain</strong>: Stochastic state transitions $P(x_{t+1} | x_t)$</p>
<p><strong>Stationary distribution</strong>: $\pi(x) = \int \pi(x') P(x|x') dx'$</p>
<p><strong>Detailed balance condition</strong>:
$$
\pi(x) P(x'|x) = \pi(x') P(x|x')
$$</p>
<p>If detailed balance is satisfied, $\pi$ is the stationary distribution.</p>
<h3 id="42-metropolis-hastings-algorithm">4.2 Metropolis-Hastings Algorithm<a class="header-link" href="#42-metropolis-hastings-algorithm" title="Permanent link">&para;</a></h3>
<p><strong>Goal</strong>: Sample from target distribution $\pi(x)$ (normalizing constant not needed)</p>
<p><strong>Algorithm</strong>:
1. Start from current state $x_t$
2. Generate candidate $x'$ from proposal distribution $q(x'|x_t)$
3. Compute acceptance probability:
$$
\alpha(x', x_t) = \min\left(1, \frac{\pi(x') q(x_t|x')}{\pi(x_t) q(x'|x_t)}\right)
$$
4. Accept $x_{t+1} = x'$ with probability $\alpha$, otherwise $x_{t+1} = x_t$</p>
<div class="highlight"><pre><span></span><code><span class="c1"># ë©”íŠ¸ë¡œí´ë¦¬ìŠ¤-í—¤ì´ìŠ¤íŒ…ìŠ¤ êµ¬í˜„</span>
<span class="k">def</span><span class="w"> </span><span class="nf">target_distribution</span><span class="p">(</span><span class="n">x</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;ëª©í‘œ ë¶„í¬: ì´ë´‰ ë¶„í¬ (bimodal)&quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="n">np</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="o">-</span><span class="mf">0.5</span> <span class="o">*</span> <span class="p">((</span><span class="n">x</span><span class="o">-</span><span class="mi">3</span><span class="p">)</span><span class="o">/</span><span class="mf">0.8</span><span class="p">)</span><span class="o">**</span><span class="mi">2</span><span class="p">)</span> <span class="o">+</span> <span class="n">np</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="o">-</span><span class="mf">0.5</span> <span class="o">*</span> <span class="p">((</span><span class="n">x</span><span class="o">+</span><span class="mi">3</span><span class="p">)</span><span class="o">/</span><span class="mf">0.8</span><span class="p">)</span><span class="o">**</span><span class="mi">2</span><span class="p">)</span>

<span class="k">def</span><span class="w"> </span><span class="nf">metropolis_hastings</span><span class="p">(</span><span class="n">n_samples</span><span class="p">,</span> <span class="n">proposal_std</span><span class="o">=</span><span class="mf">1.0</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;MH ì•Œê³ ë¦¬ì¦˜ (ëŒ€ì¹­ ì œì•ˆ ë¶„í¬)&quot;&quot;&quot;</span>
    <span class="n">samples</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">n_samples</span><span class="p">)</span>
    <span class="n">x</span> <span class="o">=</span> <span class="mf">0.0</span>  <span class="c1"># ì´ˆê¸° ìƒíƒœ</span>
    <span class="n">n_accepted</span> <span class="o">=</span> <span class="mi">0</span>

    <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">n_samples</span><span class="p">):</span>
        <span class="c1"># ì œì•ˆ ìƒì„± (ëŒ€ì¹­ ê°€ìš°ìŠ¤)</span>
        <span class="n">x_proposed</span> <span class="o">=</span> <span class="n">x</span> <span class="o">+</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">normal</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">proposal_std</span><span class="p">)</span>

        <span class="c1"># ìˆ˜ë½ í™•ë¥  ê³„ì‚° (ëŒ€ì¹­ ì œì•ˆì´ë¯€ë¡œ q í•­ ì†Œê±°)</span>
        <span class="n">acceptance_prob</span> <span class="o">=</span> <span class="nb">min</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">target_distribution</span><span class="p">(</span><span class="n">x_proposed</span><span class="p">)</span> <span class="o">/</span> <span class="n">target_distribution</span><span class="p">(</span><span class="n">x</span><span class="p">))</span>

        <span class="c1"># ìˆ˜ë½/ê±°ë¶€ ê²°ì •</span>
        <span class="k">if</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">uniform</span><span class="p">()</span> <span class="o">&lt;</span> <span class="n">acceptance_prob</span><span class="p">:</span>
            <span class="n">x</span> <span class="o">=</span> <span class="n">x_proposed</span>
            <span class="n">n_accepted</span> <span class="o">+=</span> <span class="mi">1</span>

        <span class="n">samples</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">=</span> <span class="n">x</span>

    <span class="n">acceptance_rate</span> <span class="o">=</span> <span class="n">n_accepted</span> <span class="o">/</span> <span class="n">n_samples</span>
    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;ìˆ˜ë½ë¥ : </span><span class="si">{</span><span class="n">acceptance_rate</span><span class="si">:</span><span class="s2">.2%</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">samples</span>

<span class="c1"># ìƒ˜í”Œë§</span>
<span class="n">samples</span> <span class="o">=</span> <span class="n">metropolis_hastings</span><span class="p">(</span><span class="mi">50000</span><span class="p">,</span> <span class="n">proposal_std</span><span class="o">=</span><span class="mf">2.0</span><span class="p">)</span>

<span class="c1"># ë²ˆì¸(burn-in) ì œê±°</span>
<span class="n">burn_in</span> <span class="o">=</span> <span class="mi">5000</span>
<span class="n">samples_burned</span> <span class="o">=</span> <span class="n">samples</span><span class="p">[</span><span class="n">burn_in</span><span class="p">:]</span>

<span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">15</span><span class="p">,</span> <span class="mi">5</span><span class="p">))</span>

<span class="c1"># ê¶¤ì </span>
<span class="n">plt</span><span class="o">.</span><span class="n">subplot</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">samples</span><span class="p">[:</span><span class="mi">1000</span><span class="p">])</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s1">&#39;ë°˜ë³µ&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s1">&#39;x&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s1">&#39;MCMC ê¶¤ì  (ì²˜ìŒ 1000ê°œ)&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">axvline</span><span class="p">(</span><span class="n">burn_in</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s1">&#39;r&#39;</span><span class="p">,</span> <span class="n">linestyle</span><span class="o">=</span><span class="s1">&#39;--&#39;</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s1">&#39;ë²ˆì¸ ì¢…ë£Œ&#39;</span><span class="p">)</span>

<span class="c1"># íˆìŠ¤í† ê·¸ë¨</span>
<span class="n">plt</span><span class="o">.</span><span class="n">subplot</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">hist</span><span class="p">(</span><span class="n">samples_burned</span><span class="p">,</span> <span class="n">bins</span><span class="o">=</span><span class="mi">50</span><span class="p">,</span> <span class="n">density</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.7</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s1">&#39;MCMC ìƒ˜í”Œ&#39;</span><span class="p">)</span>
<span class="n">x_range</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="o">-</span><span class="mi">6</span><span class="p">,</span> <span class="mi">6</span><span class="p">,</span> <span class="mi">200</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">x_range</span><span class="p">,</span> <span class="n">target_distribution</span><span class="p">(</span><span class="n">x_range</span><span class="p">)</span> <span class="o">/</span>
         <span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="mi">2</span><span class="o">*</span><span class="n">np</span><span class="o">.</span><span class="n">pi</span><span class="o">*</span><span class="mf">0.8</span><span class="o">**</span><span class="mi">2</span><span class="p">)</span> <span class="o">*</span> <span class="mi">2</span><span class="p">),</span> <span class="s1">&#39;r-&#39;</span><span class="p">,</span> <span class="n">linewidth</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s1">&#39;ëª©í‘œ ë¶„í¬&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s1">&#39;x&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s1">&#39;ë°€ë„&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s1">&#39;ìƒ˜í”Œ ë¶„í¬&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">legend</span><span class="p">()</span>

<span class="c1"># ìê¸°ìƒê´€</span>
<span class="n">plt</span><span class="o">.</span><span class="n">subplot</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">3</span><span class="p">)</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">statsmodels.graphics.tsaplots</span><span class="w"> </span><span class="kn">import</span> <span class="n">plot_acf</span>
<span class="n">plot_acf</span><span class="p">(</span><span class="n">samples_burned</span><span class="p">[::</span><span class="mi">10</span><span class="p">],</span> <span class="n">lags</span><span class="o">=</span><span class="mi">50</span><span class="p">,</span> <span class="n">ax</span><span class="o">=</span><span class="n">plt</span><span class="o">.</span><span class="n">gca</span><span class="p">())</span>
<span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s1">&#39;ìê¸°ìƒê´€ í•¨ìˆ˜ (ACF)&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">tight_layout</span><span class="p">()</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</code></pre></div>

<h3 id="43-gibbs-sampling">4.3 Gibbs Sampling<a class="header-link" href="#43-gibbs-sampling" title="Permanent link">&para;</a></h3>
<p>Used when conditional distributions $p(x_i | x_{-i})$ are known for multivariate distribution $p(x_1, \ldots, x_d)$.</p>
<p><strong>Algorithm</strong>:
1. Iterate over each variable
2. Sample $x_i^{(t+1)} \sim p(x_i | x_1^{(t+1)}, \ldots, x_{i-1}^{(t+1)}, x_{i+1}^{(t)}, \ldots, x_d^{(t)})$</p>
<div class="highlight"><pre><span></span><code><span class="c1"># ê¹ìŠ¤ ìƒ˜í”Œë§: ì´ë³€ëŸ‰ ê°€ìš°ìŠ¤</span>
<span class="k">def</span><span class="w"> </span><span class="nf">gibbs_sampling_bivariate_gaussian</span><span class="p">(</span><span class="n">n_samples</span><span class="p">,</span> <span class="n">rho</span><span class="o">=</span><span class="mf">0.9</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;ì´ë³€ëŸ‰ ê°€ìš°ìŠ¤ N([0,0], [[1,Ï],[Ï,1]])ì—ì„œ ìƒ˜í”Œë§&quot;&quot;&quot;</span>
    <span class="n">samples</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">((</span><span class="n">n_samples</span><span class="p">,</span> <span class="mi">2</span><span class="p">))</span>
    <span class="n">x</span><span class="p">,</span> <span class="n">y</span> <span class="o">=</span> <span class="mf">0.0</span><span class="p">,</span> <span class="mf">0.0</span>  <span class="c1"># ì´ˆê¸°ê°’</span>

    <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">n_samples</span><span class="p">):</span>
        <span class="c1"># x | y ~ N(Ïy, 1-ÏÂ²)</span>
        <span class="n">x</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">normal</span><span class="p">(</span><span class="n">rho</span> <span class="o">*</span> <span class="n">y</span><span class="p">,</span> <span class="n">np</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="mi">1</span> <span class="o">-</span> <span class="n">rho</span><span class="o">**</span><span class="mi">2</span><span class="p">))</span>
        <span class="c1"># y | x ~ N(Ïx, 1-ÏÂ²)</span>
        <span class="n">y</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">normal</span><span class="p">(</span><span class="n">rho</span> <span class="o">*</span> <span class="n">x</span><span class="p">,</span> <span class="n">np</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="mi">1</span> <span class="o">-</span> <span class="n">rho</span><span class="o">**</span><span class="mi">2</span><span class="p">))</span>
        <span class="n">samples</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">=</span> <span class="p">[</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">]</span>

    <span class="k">return</span> <span class="n">samples</span>

<span class="n">samples</span> <span class="o">=</span> <span class="n">gibbs_sampling_bivariate_gaussian</span><span class="p">(</span><span class="mi">10000</span><span class="p">,</span> <span class="n">rho</span><span class="o">=</span><span class="mf">0.9</span><span class="p">)</span>
<span class="n">samples</span> <span class="o">=</span> <span class="n">samples</span><span class="p">[</span><span class="mi">1000</span><span class="p">:]</span>  <span class="c1"># ë²ˆì¸ ì œê±°</span>

<span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">10</span><span class="p">,</span> <span class="mi">5</span><span class="p">))</span>
<span class="n">plt</span><span class="o">.</span><span class="n">subplot</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">samples</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">],</span> <span class="n">samples</span><span class="p">[:,</span> <span class="mi">1</span><span class="p">],</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.3</span><span class="p">,</span> <span class="n">s</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s1">&#39;x&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s1">&#39;y&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s1">&#39;ê¹ìŠ¤ ìƒ˜í”Œë§: ì´ë³€ëŸ‰ ê°€ìš°ìŠ¤ (Ï=0.9)&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">axis</span><span class="p">(</span><span class="s1">&#39;equal&#39;</span><span class="p">)</span>

<span class="n">plt</span><span class="o">.</span><span class="n">subplot</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>
<span class="c1"># ì´ë¡ ì  ë“±ê³ ì„ </span>
<span class="kn">from</span><span class="w"> </span><span class="nn">scipy.stats</span><span class="w"> </span><span class="kn">import</span> <span class="n">multivariate_normal</span>
<span class="n">x_grid</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="o">-</span><span class="mi">4</span><span class="p">,</span> <span class="mi">4</span><span class="p">,</span> <span class="mi">100</span><span class="p">)</span>
<span class="n">y_grid</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="o">-</span><span class="mi">4</span><span class="p">,</span> <span class="mi">4</span><span class="p">,</span> <span class="mi">100</span><span class="p">)</span>
<span class="n">X</span><span class="p">,</span> <span class="n">Y</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">meshgrid</span><span class="p">(</span><span class="n">x_grid</span><span class="p">,</span> <span class="n">y_grid</span><span class="p">)</span>
<span class="n">pos</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">dstack</span><span class="p">((</span><span class="n">X</span><span class="p">,</span> <span class="n">Y</span><span class="p">))</span>
<span class="n">rv</span> <span class="o">=</span> <span class="n">multivariate_normal</span><span class="p">([</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">],</span> <span class="p">[[</span><span class="mi">1</span><span class="p">,</span> <span class="mf">0.9</span><span class="p">],</span> <span class="p">[</span><span class="mf">0.9</span><span class="p">,</span> <span class="mi">1</span><span class="p">]])</span>
<span class="n">plt</span><span class="o">.</span><span class="n">contour</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">Y</span><span class="p">,</span> <span class="n">rv</span><span class="o">.</span><span class="n">pdf</span><span class="p">(</span><span class="n">pos</span><span class="p">),</span> <span class="n">levels</span><span class="o">=</span><span class="mi">10</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">samples</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">],</span> <span class="n">samples</span><span class="p">[:,</span> <span class="mi">1</span><span class="p">],</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.1</span><span class="p">,</span> <span class="n">s</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">c</span><span class="o">=</span><span class="s1">&#39;red&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s1">&#39;x&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s1">&#39;y&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s1">&#39;ì´ë¡  ë¶„í¬ì™€ ë¹„êµ&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">axis</span><span class="p">(</span><span class="s1">&#39;equal&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">tight_layout</span><span class="p">()</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</code></pre></div>

<h3 id="44-burn-in-and-autocorrelation">4.4 Burn-in and Autocorrelation<a class="header-link" href="#44-burn-in-and-autocorrelation" title="Permanent link">&para;</a></h3>
<ul>
<li><strong>Burn-in</strong>: Initial samples haven't reached the stationary distribution, so discard them</li>
<li><strong>Autocorrelation</strong>: Consecutive samples are correlated â†’ thin by taking every k-th sample</li>
<li><strong>Effective sample size</strong>: $n_{\text{eff}} = \frac{n}{1 + 2\sum_{k=1}^{\infty} \rho_k}$</li>
</ul>
<hr />
<h2 id="5-reparameterization-trick">5. Reparameterization Trick<a class="header-link" href="#5-reparameterization-trick" title="Permanent link">&para;</a></h2>
<h3 id="51-problem-backpropagating-through-stochastic-nodes">5.1 Problem: Backpropagating Through Stochastic Nodes<a class="header-link" href="#51-problem-backpropagating-through-stochastic-nodes" title="Permanent link">&para;</a></h3>
<p>In VAE, the encoder outputs $q_\phi(z|x)$ and samples $z \sim q_\phi(z|x)$. The loss function:</p>
<p>$$
\mathcal{L}(\phi) = \mathbb{E}_{z \sim q_\phi(z|x)}[f(z)]
$$</p>
<p><strong>Problem</strong>: How to compute $\frac{\partial}{\partial \phi} \mathbb{E}_{z \sim q_\phi}[f(z)]$?</p>
<p>Sampling operations are <strong>not differentiable</strong>.</p>
<h3 id="52-reparameterization-solution">5.2 Reparameterization Solution<a class="header-link" href="#52-reparameterization-solution" title="Permanent link">&para;</a></h3>
<p><strong>Idea</strong>: Separate randomness into noise independent of $\phi$.</p>
<p><strong>Gaussian distribution</strong>: $z \sim \mathcal{N}(\mu_\phi, \sigma_\phi^2)$</p>
<p>Reparameterization: $z = \mu_\phi + \sigma_\phi \cdot \epsilon$, where $\epsilon \sim \mathcal{N}(0, 1)$</p>
<p>Now:
$$
\frac{\partial}{\partial \phi} \mathbb{E}_{\epsilon \sim \mathcal{N}(0,1)}[f(\mu_\phi + \sigma_\phi \epsilon)] = \mathbb{E}_{\epsilon}\left[\frac{\partial f(z)}{\partial z} \frac{\partial z}{\partial \phi}\right]
$$</p>
<p><strong>Gradients can flow through the sampling operation!</strong></p>
<div class="highlight"><pre><span></span><code><span class="kn">import</span><span class="w"> </span><span class="nn">torch</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">torch.nn</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">nn</span>

<span class="c1"># ì¬ë§¤ê°œë³€ìˆ˜í™” íŠ¸ë¦­ êµ¬í˜„</span>
<span class="k">class</span><span class="w"> </span><span class="nc">VAEEncoder</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="k">def</span><span class="w"> </span><span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">input_dim</span><span class="p">,</span> <span class="n">latent_dim</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">fc</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">input_dim</span><span class="p">,</span> <span class="mi">128</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">fc_mu</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="mi">128</span><span class="p">,</span> <span class="n">latent_dim</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">fc_logvar</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="mi">128</span><span class="p">,</span> <span class="n">latent_dim</span><span class="p">)</span>

    <span class="k">def</span><span class="w"> </span><span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
        <span class="n">h</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">relu</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">fc</span><span class="p">(</span><span class="n">x</span><span class="p">))</span>
        <span class="n">mu</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">fc_mu</span><span class="p">(</span><span class="n">h</span><span class="p">)</span>
        <span class="n">logvar</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">fc_logvar</span><span class="p">(</span><span class="n">h</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">mu</span><span class="p">,</span> <span class="n">logvar</span>

    <span class="k">def</span><span class="w"> </span><span class="nf">reparameterize</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">mu</span><span class="p">,</span> <span class="n">logvar</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;ì¬ë§¤ê°œë³€ìˆ˜í™” íŠ¸ë¦­&quot;&quot;&quot;</span>
        <span class="n">std</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="mf">0.5</span> <span class="o">*</span> <span class="n">logvar</span><span class="p">)</span>
        <span class="n">eps</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn_like</span><span class="p">(</span><span class="n">std</span><span class="p">)</span>  <span class="c1"># Îµ ~ N(0, 1)</span>
        <span class="n">z</span> <span class="o">=</span> <span class="n">mu</span> <span class="o">+</span> <span class="n">std</span> <span class="o">*</span> <span class="n">eps</span>           <span class="c1"># z = Î¼ + ÏƒÎµ</span>
        <span class="k">return</span> <span class="n">z</span>

<span class="c1"># ê¸°ìš¸ê¸° íë¦„ í…ŒìŠ¤íŠ¸</span>
<span class="n">encoder</span> <span class="o">=</span> <span class="n">VAEEncoder</span><span class="p">(</span><span class="n">input_dim</span><span class="o">=</span><span class="mi">10</span><span class="p">,</span> <span class="n">latent_dim</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
<span class="n">x</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">32</span><span class="p">,</span> <span class="mi">10</span><span class="p">)</span>

<span class="n">mu</span><span class="p">,</span> <span class="n">logvar</span> <span class="o">=</span> <span class="n">encoder</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
<span class="n">z</span> <span class="o">=</span> <span class="n">encoder</span><span class="o">.</span><span class="n">reparameterize</span><span class="p">(</span><span class="n">mu</span><span class="p">,</span> <span class="n">logvar</span><span class="p">)</span>

<span class="c1"># ì„ì˜ì˜ ì†ì‹¤ í•¨ìˆ˜</span>
<span class="n">loss</span> <span class="o">=</span> <span class="p">(</span><span class="n">z</span> <span class="o">**</span> <span class="mi">2</span><span class="p">)</span><span class="o">.</span><span class="n">sum</span><span class="p">()</span>
<span class="n">loss</span><span class="o">.</span><span class="n">backward</span><span class="p">()</span>

<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;ê¸°ìš¸ê¸°ê°€ ì„±ê³µì ìœ¼ë¡œ ê³„ì‚°ë¨:&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;  muì˜ ê¸°ìš¸ê¸°: </span><span class="si">{</span><span class="n">encoder</span><span class="o">.</span><span class="n">fc_mu</span><span class="o">.</span><span class="n">weight</span><span class="o">.</span><span class="n">grad</span><span class="w"> </span><span class="ow">is</span><span class="w"> </span><span class="ow">not</span><span class="w"> </span><span class="kc">None</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;  logvarì˜ ê¸°ìš¸ê¸°: </span><span class="si">{</span><span class="n">encoder</span><span class="o">.</span><span class="n">fc_logvar</span><span class="o">.</span><span class="n">weight</span><span class="o">.</span><span class="n">grad</span><span class="w"> </span><span class="ow">is</span><span class="w"> </span><span class="ow">not</span><span class="w"> </span><span class="kc">None</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;  ê¸°ìš¸ê¸° norm: </span><span class="si">{</span><span class="n">encoder</span><span class="o">.</span><span class="n">fc_mu</span><span class="o">.</span><span class="n">weight</span><span class="o">.</span><span class="n">grad</span><span class="o">.</span><span class="n">norm</span><span class="p">()</span><span class="si">:</span><span class="s2">.4f</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
</code></pre></div>

<h3 id="53-reparameterization-for-other-distributions">5.3 Reparameterization for Other Distributions<a class="header-link" href="#53-reparameterization-for-other-distributions" title="Permanent link">&para;</a></h3>
<p><strong>Bernoulli</strong>: Gumbel-Softmax trick
$$
z = \text{one-hot}\left(\arg\max_i \left[\log \pi_i + G_i\right]\right)
$$
where $G_i \sim \text{Gumbel}(0, 1)$</p>
<p><strong>Gamma distribution</strong>: Shape augmentation</p>
<p><strong>General principle</strong>: Reparameterizable if expressible as $z = g(\epsilon; \theta)$</p>
<h3 id="54-vae-elbo">5.4 VAE ELBO<a class="header-link" href="#54-vae-elbo" title="Permanent link">&para;</a></h3>
<p>VAE loss function (ELBO):</p>
<p>$$
\mathcal{L} = \mathbb{E}_{q_\phi(z|x)}[\log p_\theta(x|z)] - D_{KL}(q_\phi(z|x) \| p(z))
$$</p>
<p><strong>Applying reparameterization</strong>:
- First term: Compute gradients using reparameterization trick
- Second term: Analytically computable under Gaussian prior assumption</p>
<div class="highlight"><pre><span></span><code><span class="k">def</span><span class="w"> </span><span class="nf">vae_loss</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">x_recon</span><span class="p">,</span> <span class="n">mu</span><span class="p">,</span> <span class="n">logvar</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;VAE ì†ì‹¤ í•¨ìˆ˜ (ELBO)&quot;&quot;&quot;</span>
    <span class="c1"># ì¬êµ¬ì„± ì†ì‹¤ (log p(x|z))</span>
    <span class="n">recon_loss</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">functional</span><span class="o">.</span><span class="n">binary_cross_entropy</span><span class="p">(</span><span class="n">x_recon</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">reduction</span><span class="o">=</span><span class="s1">&#39;sum&#39;</span><span class="p">)</span>

    <span class="c1"># KL ë°œì‚° (ê°€ìš°ìŠ¤ ê°€ì • í•˜ì— í•´ì„ì  ê³„ì‚°)</span>
    <span class="c1"># KL(N(Î¼,ÏƒÂ²) || N(0,1)) = -0.5 * Î£(1 + log(ÏƒÂ²) - Î¼Â² - ÏƒÂ²)</span>
    <span class="n">kl_loss</span> <span class="o">=</span> <span class="o">-</span><span class="mf">0.5</span> <span class="o">*</span> <span class="n">torch</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="mi">1</span> <span class="o">+</span> <span class="n">logvar</span> <span class="o">-</span> <span class="n">mu</span><span class="o">.</span><span class="n">pow</span><span class="p">(</span><span class="mi">2</span><span class="p">)</span> <span class="o">-</span> <span class="n">logvar</span><span class="o">.</span><span class="n">exp</span><span class="p">())</span>

    <span class="k">return</span> <span class="n">recon_loss</span> <span class="o">+</span> <span class="n">kl_loss</span>

<span class="c1"># ì „ì²´ VAE í•™ìŠµ ë£¨í”„ (ê°„ëµí™”)</span>
<span class="k">def</span><span class="w"> </span><span class="nf">train_vae</span><span class="p">(</span><span class="n">encoder</span><span class="p">,</span> <span class="n">decoder</span><span class="p">,</span> <span class="n">data_loader</span><span class="p">,</span> <span class="n">optimizer</span><span class="p">,</span> <span class="n">epochs</span><span class="p">):</span>
    <span class="k">for</span> <span class="n">epoch</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">epochs</span><span class="p">):</span>
        <span class="k">for</span> <span class="n">x</span> <span class="ow">in</span> <span class="n">data_loader</span><span class="p">:</span>
            <span class="c1"># ì¸ì½”ë”</span>
            <span class="n">mu</span><span class="p">,</span> <span class="n">logvar</span> <span class="o">=</span> <span class="n">encoder</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
            <span class="n">z</span> <span class="o">=</span> <span class="n">encoder</span><span class="o">.</span><span class="n">reparameterize</span><span class="p">(</span><span class="n">mu</span><span class="p">,</span> <span class="n">logvar</span><span class="p">)</span>

            <span class="c1"># ë””ì½”ë”</span>
            <span class="n">x_recon</span> <span class="o">=</span> <span class="n">decoder</span><span class="p">(</span><span class="n">z</span><span class="p">)</span>

            <span class="c1"># ì†ì‹¤ ê³„ì‚° ë° ì—­ì „íŒŒ</span>
            <span class="n">loss</span> <span class="o">=</span> <span class="n">vae_loss</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">x_recon</span><span class="p">,</span> <span class="n">mu</span><span class="p">,</span> <span class="n">logvar</span><span class="p">)</span>
            <span class="n">optimizer</span><span class="o">.</span><span class="n">zero_grad</span><span class="p">()</span>
            <span class="n">loss</span><span class="o">.</span><span class="n">backward</span><span class="p">()</span>  <span class="c1"># ì¬ë§¤ê°œë³€ìˆ˜í™” ë•ë¶„ì— ê¸°ìš¸ê¸° ê³„ì‚° ê°€ëŠ¥!</span>
            <span class="n">optimizer</span><span class="o">.</span><span class="n">step</span><span class="p">()</span>
</code></pre></div>

<hr />
<h2 id="6-machine-learning-applications">6. Machine Learning Applications<a class="header-link" href="#6-machine-learning-applications" title="Permanent link">&para;</a></h2>
<h3 id="61-vae-elbo-optimization">6.1 VAE: ELBO Optimization<a class="header-link" href="#61-vae-elbo-optimization" title="Permanent link">&para;</a></h3>
<p>Without the reparameterization trick, VAEs would be impossible. REINFORCE (score function estimation) has too high variance.</p>
<h3 id="62-reinforcement-learning-reinforce-monte-carlo">6.2 Reinforcement Learning: REINFORCE = Monte Carlo<a class="header-link" href="#62-reinforcement-learning-reinforce-monte-carlo" title="Permanent link">&para;</a></h3>
<p>Policy gradient theorem:
$$
\nabla_\theta J(\theta) = \mathbb{E}_{\tau \sim \pi_\theta}\left[\sum_{t=0}^T \nabla_\theta \log \pi_\theta(a_t|s_t) R(\tau)\right]
$$</p>
<p>This is estimated using Monte Carlo sampling.</p>
<p><strong>Variance reduction</strong>: Use baselines, GAE (Generalized Advantage Estimation)</p>
<h3 id="63-bayesian-inference-mcmc-vs-variational-inference">6.3 Bayesian Inference: MCMC vs Variational Inference<a class="header-link" href="#63-bayesian-inference-mcmc-vs-variational-inference" title="Permanent link">&para;</a></h3>
<p><strong>MCMC</strong>:
- Pros: Theoretically exact samples
- Cons: Slow, difficult to diagnose convergence</p>
<p><strong>Variational Inference</strong>:
- Pros: Fast, scalable
- Cons: Approximation quality limited by expressiveness of $q$</p>
<h3 id="64-dropout-bernoulli-sampling">6.4 Dropout = Bernoulli Sampling<a class="header-link" href="#64-dropout-bernoulli-sampling" title="Permanent link">&para;</a></h3>
<p>Dropout is Bernoulli sampling during training:
$$
h_{\text{drop}} = h \odot \text{Bernoulli}(p)
$$</p>
<p><strong>Bayesian interpretation</strong>: Dropout can be viewed as approximate Bayesian inference (Gal &amp; Ghahramani, 2016).</p>
<div class="highlight"><pre><span></span><code><span class="c1"># MC Dropoutìœ¼ë¡œ ë¶ˆí™•ì‹¤ì„± ì¶”ì •</span>
<span class="k">class</span><span class="w"> </span><span class="nc">MCDropoutModel</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="k">def</span><span class="w"> </span><span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">fc1</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="mi">10</span><span class="p">,</span> <span class="mi">50</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">dropout</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Dropout</span><span class="p">(</span><span class="mf">0.5</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">fc2</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="mi">50</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>

    <span class="k">def</span><span class="w"> </span><span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
        <span class="n">x</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">relu</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">fc1</span><span class="p">(</span><span class="n">x</span><span class="p">))</span>
        <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">dropout</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>  <span class="c1"># ì¶”ë¡  ì‹œì—ë„ dropout ì ìš©</span>
        <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">fc2</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">x</span>

<span class="k">def</span><span class="w"> </span><span class="nf">predict_with_uncertainty</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">n_samples</span><span class="o">=</span><span class="mi">100</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;MC Dropoutìœ¼ë¡œ ì˜ˆì¸¡ ë¶ˆí™•ì‹¤ì„± ì¶”ì •&quot;&quot;&quot;</span>
    <span class="n">model</span><span class="o">.</span><span class="n">train</span><span class="p">()</span>  <span class="c1"># Dropout í™œì„±í™”</span>
    <span class="n">predictions</span> <span class="o">=</span> <span class="p">[]</span>

    <span class="k">with</span> <span class="n">torch</span><span class="o">.</span><span class="n">no_grad</span><span class="p">():</span>
        <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">n_samples</span><span class="p">):</span>
            <span class="n">pred</span> <span class="o">=</span> <span class="n">model</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
            <span class="n">predictions</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">pred</span><span class="p">)</span>

    <span class="n">predictions</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">cat</span><span class="p">(</span><span class="n">predictions</span><span class="p">,</span> <span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
    <span class="n">mean</span> <span class="o">=</span> <span class="n">predictions</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
    <span class="n">std</span> <span class="o">=</span> <span class="n">predictions</span><span class="o">.</span><span class="n">std</span><span class="p">(</span><span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">mean</span><span class="p">,</span> <span class="n">std</span>

<span class="c1"># í…ŒìŠ¤íŠ¸</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">MCDropoutModel</span><span class="p">()</span>
<span class="n">x_test</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">10</span><span class="p">,</span> <span class="mi">10</span><span class="p">)</span>
<span class="n">mean</span><span class="p">,</span> <span class="n">std</span> <span class="o">=</span> <span class="n">predict_with_uncertainty</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">x_test</span><span class="p">,</span> <span class="n">n_samples</span><span class="o">=</span><span class="mi">100</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;ì˜ˆì¸¡ í‰ê· : </span><span class="si">{</span><span class="n">mean</span><span class="p">[:</span><span class="mi">5</span><span class="p">]</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;ì˜ˆì¸¡ í‘œì¤€í¸ì°¨ (ë¶ˆí™•ì‹¤ì„±): </span><span class="si">{</span><span class="n">std</span><span class="p">[:</span><span class="mi">5</span><span class="p">]</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
</code></pre></div>

<hr />
<h2 id="practice-problems">Practice Problems<a class="header-link" href="#practice-problems" title="Permanent link">&para;</a></h2>
<h3 id="problem-1-monte-carlo-integration">Problem 1: Monte Carlo Integration<a class="header-link" href="#problem-1-monte-carlo-integration" title="Permanent link">&para;</a></h3>
<p>Estimate the following integral using Monte Carlo method:
$$
I = \int_0^1 e^{-x^2} dx
$$</p>
<p>(a) Estimate using 10, 100, 1000, 10000 samples and calculate errors.
(b) Verify on a log-log plot that error decreases as $O(1/\sqrt{N})$.
(c) Compare with results from <code>scipy.integrate.quad</code>.</p>
<h3 id="problem-2-importance-sampling">Problem 2: Importance Sampling<a class="header-link" href="#problem-2-importance-sampling" title="Permanent link">&para;</a></h3>
<p>You want to compute $\mathbb{E}[x^2]$ from target distribution $p(x) \propto e^{-|x|^3}$.</p>
<p>(a) Implement importance sampling using proposal distribution $q(x) = \mathcal{N}(0, 1)$.
(b) Compare variance with the case using $q(x) = \text{Laplace}(0, 1)$ as proposal.
(c) Which proposal distribution is more efficient? Explain why.</p>
<h3 id="problem-3-mcmc-diagnostics">Problem 3: MCMC Diagnostics<a class="header-link" href="#problem-3-mcmc-diagnostics" title="Permanent link">&para;</a></h3>
<p>Perform Metropolis-Hastings sampling from bimodal distribution $p(x) \propto e^{-(x-3)^2/2} + e^{-(x+3)^2/2}$.</p>
<p>(a) Observe acceptance rates as you vary proposal distribution standard deviation: 0.1, 1.0, 10.0.
(b) Plot the autocorrelation function (ACF) for each case and estimate effective sample size.
(c) What is the optimal proposal distribution standard deviation?</p>
<h3 id="problem-4-reparameterization-trick">Problem 4: Reparameterization Trick<a class="header-link" href="#problem-4-reparameterization-trick" title="Permanent link">&para;</a></h3>
<p>Implement a simple VAE and verify the effect of the reparameterization trick.</p>
<p>(a) Train a small VAE on MNIST dataset (latent_dim=2).
(b) Compare with training using REINFORCE without reparameterization trick.
(c) Visualize the latent space with 2D plots.</p>
<h3 id="problem-5-gibbs-sampling-extension">Problem 5: Gibbs Sampling Extension<a class="header-link" href="#problem-5-gibbs-sampling-extension" title="Permanent link">&para;</a></h3>
<p>Implement Gibbs sampling for a 3-variate Gaussian distribution.</p>
<p>$$
\mathbf{x} \sim \mathcal{N}\left(\mathbf{0}, \begin{bmatrix} 1 & 0.8 & 0.5 \\ 0.8 & 1 & 0.7 \\ 0.5 & 0.7 & 1 \end{bmatrix}\right)
$$</p>
<p>(a) Derive conditional distributions $p(x_i | x_{-i})$ (use Gaussian conditional formula).
(b) Perform 10,000 iterations of Gibbs sampling and compute sample covariance.
(c) Compare with theoretical covariance.</p>
<hr />
<h2 id="references">References<a class="header-link" href="#references" title="Permanent link">&para;</a></h2>
<h3 id="books">Books<a class="header-link" href="#books" title="Permanent link">&para;</a></h3>
<ul>
<li><strong>Pattern Recognition and Machine Learning</strong> (Bishop, 2006) - Chapter 11: Sampling Methods</li>
<li><strong>Monte Carlo Statistical Methods</strong> (Robert &amp; Casella, 2004)</li>
<li><strong>Deep Learning</strong> (Goodfellow et al., 2016) - Chapter 17: Monte Carlo Methods</li>
</ul>
<h3 id="papers">Papers<a class="header-link" href="#papers" title="Permanent link">&para;</a></h3>
<ul>
<li>Kingma &amp; Welling (2013), "Auto-Encoding Variational Bayes" - VAE and reparameterization trick</li>
<li>Metropolis et al. (1953), "Equation of State Calculations by Fast Computing Machines"</li>
<li>Hastings (1970), "Monte Carlo Sampling Methods Using Markov Chains"</li>
<li>Gal &amp; Ghahramani (2016), "Dropout as a Bayesian Approximation"</li>
</ul>
<h3 id="online-resources">Online Resources<a class="header-link" href="#online-resources" title="Permanent link">&para;</a></h3>
<ul>
<li><a href="https://chi-feng.github.io/mcmc-demo/">MCMC Interactive Gallery</a></li>
<li><a href="https://distill.pub">Distill.pub: Visualizing Sampling Methods</a></li>
<li><a href="https://www.pymc.io">PyMC Documentation</a> - Practical Bayesian inference library</li>
</ul>
<h3 id="libraries">Libraries<a class="header-link" href="#libraries" title="Permanent link">&para;</a></h3>
<ul>
<li><strong>PyMC3/PyMC4</strong>: Bayesian inference and MCMC</li>
<li><strong>Stan</strong>: Powerful MCMC inference engine</li>
<li><strong>PyTorch</strong>: VAE implementation</li>
<li><strong>NumPyro</strong>: JAX-based probabilistic programming</li>
</ul>
    </div>

    
<div class="lesson-toolbar lesson-toolbar--bottom">
    <div class="toolbar-nav toolbar-nav--prev">
        
        <a href="/study/en/Math_for_AI/11_Probability_Distributions_Advanced.html" class="nav-prev">
            <span class="nav-label">Previous</span>
            <span class="nav-title">11. Advanced Probability Distributions</span>
        </a>
        
    </div>
    <div class="toolbar-actions">
        <button class="btn btn-copy-link" data-action="copy-link" title="Copy link">
            <span class="icon">ğŸ”—</span>
            <span class="text">Copy link</span>
        </button>
        <a href="/study/en/Math_for_AI/" class="btn btn-topic-link" title="Back to topic list">
            <span class="icon">ğŸ“‹</span>
            <span class="text">Topic list</span>
        </a>
    </div>
    <div class="toolbar-nav toolbar-nav--next">
        
        <a href="/study/en/Math_for_AI/13_Linear_Algebra_for_Deep_Learning.html" class="nav-next">
            <span class="nav-label">Next</span>
            <span class="nav-title">13. Linear Algebra for Deep Learning</span>
        </a>
        
    </div>
</div>


    <div class="toolbar-keyboard-hint">
        <kbd>&larr;</kbd> <kbd>&rarr;</kbd> to navigate between lessons
    </div>
</article>

<!-- Scroll to top floating button -->
<button class="scroll-to-top" id="scroll-to-top" title="Scroll to top" aria-label="Scroll to top">â†‘</button>

            </div>
        </main>
    </div>

    <script src="/study/static/js/app.js"></script>
    <script>
        function switchLanguage(newLang) {
            const path = window.location.pathname;
            const base = '/study';
            const currentLang = 'en';
            const newPath = path.replace(base + '/' + currentLang + '/', base + '/' + newLang + '/');
            window.location.href = newPath + window.location.search;
        }
    </script>
    
<script>
    document.addEventListener('DOMContentLoaded', function() {
        // Copy link - independent feedback per button
        document.querySelectorAll('[data-action="copy-link"]').forEach(function(btn) {
            btn.addEventListener('click', function() {
                navigator.clipboard.writeText(window.location.href);
                var textEl = this.querySelector('.text');
                var originalText = textEl.textContent;
                textEl.textContent = 'Copied!';
                setTimeout(function() { textEl.textContent = originalText; }, 2000);
            });
        });

        // Add copy buttons to code blocks
        document.querySelectorAll('.lesson-content pre').forEach(function(pre) {
            var copyBtn = document.createElement('button');
            copyBtn.className = 'code-copy-btn';
            copyBtn.textContent = 'Copy';
            copyBtn.addEventListener('click', function() {
                var code = pre.querySelector('code');
                var text = code ? code.textContent : pre.textContent;
                navigator.clipboard.writeText(text);
                copyBtn.textContent = 'Copied!';
                setTimeout(function() { copyBtn.textContent = 'Copy'; }, 2000);
            });
            pre.style.position = 'relative';
            pre.appendChild(copyBtn);
        });

        // Scroll to top button
        var scrollBtn = document.getElementById('scroll-to-top');
        window.addEventListener('scroll', function() {
            scrollBtn.classList.toggle('visible', window.scrollY > 300);
        });
        scrollBtn.addEventListener('click', function() {
            window.scrollTo({ top: 0, behavior: 'smooth' });
        });

        // Keyboard shortcuts: left/right arrows for lesson navigation
        document.addEventListener('keydown', function(e) {
            var tag = document.activeElement.tagName;
            if (tag === 'INPUT' || tag === 'TEXTAREA' || tag === 'SELECT') return;

            if (e.key === 'ArrowLeft') {
                var prevLink = document.querySelector('.nav-prev');
                if (prevLink) prevLink.click();
            } else if (e.key === 'ArrowRight') {
                var nextLink = document.querySelector('.nav-next');
                if (nextLink) nextLink.click();
            }
        });
    });
</script>

</body>
</html>
{% endraw %}