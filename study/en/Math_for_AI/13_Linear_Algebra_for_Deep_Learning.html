{% raw %}
<!DOCTYPE html>
<html lang="en" data-theme="light">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>13. Linear Algebra for Deep Learning - Study Materials</title>
    <link rel="stylesheet" href="/study/static/css/style.css">
    <link rel="stylesheet" href="/study/static/css/highlight.css">
    <!-- KaTeX for LaTeX math rendering -->
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.11/dist/katex.min.css">
    <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.11/dist/katex.min.js"></script>
    <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.11/dist/contrib/auto-render.min.js"
        onload="renderMathInElement(document.body, {
            delimiters: [
                {left: '$$', right: '$$', display: true},
                {left: '$', right: '$', display: false}
            ],
            throwOnError: false
        });"></script>
    
</head>
<body>
    <div class="app-container">
        <!-- Sidebar -->
        <aside class="sidebar" id="sidebar">
            <div class="sidebar-header">
                <a href="/study/en/" class="logo">Study Materials</a>
                <button class="sidebar-toggle" id="sidebar-toggle" aria-label="Toggle sidebar">
                    <span></span>
                </button>
            </div>

            <nav class="sidebar-nav">
                <a href="/study/en/" class="nav-item ">
                    <span class="nav-icon">ğŸ </span>
                    <span class="nav-text">Home</span>
                </a>
                <a href="/study/examples/" class="nav-item ">
                    <span class="nav-icon">ğŸ’»</span>
                    <span class="nav-text">Examples</span>
                </a>
            </nav>

            <div class="sidebar-search">
                <form action="/study/en/search.html" method="get" id="search-form">
                    <input type="search" name="q" placeholder="Search..." id="search-sidebar-input">
                </form>
            </div>

            <!-- Language Selector -->
            <div class="sidebar-lang">
                <select id="lang-select" class="lang-selector" onchange="switchLanguage(this.value)">
                    
                    <option value="en" selected>
                        English
                    </option>
                    
                    <option value="ko" >
                        í•œêµ­ì–´
                    </option>
                    
                </select>
            </div>

            <div class="sidebar-footer">
                <button class="theme-toggle" id="theme-toggle" aria-label="Toggle theme">
                    <span class="theme-icon light">â˜€ï¸</span>
                    <span class="theme-icon dark">ğŸŒ™</span>
                </button>
            </div>
        </aside>

        <!-- Main Content -->
        <main class="main-content">
            <header class="main-header">
                <button class="menu-toggle" id="menu-toggle" aria-label="Open menu">
                    <span></span>
                </button>
                
<nav class="breadcrumb">
    <a href="/study/en/">Topics</a>
    <span class="separator">/</span>
    <a href="/study/en/Math_for_AI/">Math for AI</a>
    <span class="separator">/</span>
    <span class="current">13. Linear Algebra for Deep Learning</span>
</nav>

            </header>

            <div class="content">
                
<article class="lesson-article">
    <header class="lesson-header">
        <h1>13. Linear Algebra for Deep Learning</h1>
    </header>

    
<div class="lesson-toolbar lesson-toolbar--top">
    <div class="toolbar-nav toolbar-nav--prev">
        
        <a href="/study/en/Math_for_AI/12_Sampling_and_Monte_Carlo.html" class="nav-prev">
            <span class="nav-label">Previous</span>
            <span class="nav-title">12. Sampling and Monte Carlo Methods</span>
        </a>
        
    </div>
    <div class="toolbar-actions">
        <button class="btn btn-copy-link" data-action="copy-link" title="Copy link">
            <span class="icon">ğŸ”—</span>
            <span class="text">Copy link</span>
        </button>
        <a href="/study/en/Math_for_AI/" class="btn btn-topic-link" title="Back to topic list">
            <span class="icon">ğŸ“‹</span>
            <span class="text">Topic list</span>
        </a>
    </div>
    <div class="toolbar-nav toolbar-nav--next">
        
        <a href="/study/en/Math_for_AI/14_Convexity_and_Duality.html" class="nav-next">
            <span class="nav-label">Next</span>
            <span class="nav-title">14. Convexity and Duality</span>
        </a>
        
    </div>
</div>


    
    <nav class="toc" id="toc">
        <h2>Table of Contents</h2>
        <div class="toc">
<ul>
<li><a href="#learning-objectives">Learning Objectives</a></li>
<li><a href="#1-tensor-operations">1. Tensor Operations</a><ul>
<li><a href="#11-tensor-hierarchy">1.1 Tensor Hierarchy</a></li>
<li><a href="#12-tensor-axes-and-dimensions">1.2 Tensor Axes and Dimensions</a></li>
<li><a href="#13-broadcasting">1.3 Broadcasting</a></li>
<li><a href="#14-tensor-shape-transformations">1.4 Tensor Shape Transformations</a></li>
</ul>
</li>
<li><a href="#2-einstein-notation-einsum">2. Einstein Notation / einsum</a><ul>
<li><a href="#21-einstein-summation-convention">2.1 Einstein Summation Convention</a></li>
<li><a href="#22-basic-einsum-usage">2.2 Basic einsum Usage</a></li>
<li><a href="#23-batch-operations">2.3 Batch Operations</a></li>
<li><a href="#24-attention-mechanism">2.4 Attention Mechanism</a></li>
<li><a href="#25-complex-tensor-operations">2.5 Complex Tensor Operations</a></li>
</ul>
</li>
<li><a href="#3-automatic-differentiation">3. Automatic Differentiation</a><ul>
<li><a href="#31-computational-graph">3.1 Computational Graph</a></li>
<li><a href="#32-forward-mode">3.2 Forward Mode</a></li>
<li><a href="#33-reverse-mode-backpropagation">3.3 Reverse Mode / Backpropagation</a></li>
<li><a href="#34-why-reverse-mode-fits-deep-learning">3.4 Why Reverse Mode Fits Deep Learning</a></li>
</ul>
</li>
<li><a href="#4-numerical-stability">4. Numerical Stability</a><ul>
<li><a href="#41-floating-point-arithmetic-limitations">4.1 Floating-Point Arithmetic Limitations</a></li>
<li><a href="#42-log-sum-exp-trick">4.2 Log-Sum-Exp Trick</a></li>
<li><a href="#43-numerically-stable-softmax">4.3 Numerically Stable Softmax</a></li>
<li><a href="#44-gradient-clipping">4.4 Gradient Clipping</a></li>
</ul>
</li>
<li><a href="#5-weight-initialization-theory">5. Weight Initialization Theory</a><ul>
<li><a href="#51-problem-signal-vanishingexplosion">5.1 Problem: Signal Vanishing/Explosion</a></li>
<li><a href="#52-xavierglorot-initialization">5.2 Xavier/Glorot Initialization</a></li>
<li><a href="#53-he-initialization-for-relu">5.3 He Initialization (for ReLU)</a></li>
<li><a href="#54-pytorch-initialization">5.4 PyTorch Initialization</a></li>
</ul>
</li>
<li><a href="#6-normalization-and-residual-connections">6. Normalization and Residual Connections</a><ul>
<li><a href="#61-batch-normalization">6.1 Batch Normalization</a></li>
<li><a href="#62-layer-normalization">6.2 Layer Normalization</a></li>
<li><a href="#63-gradient-flow-in-residual-connections">6.3 Gradient Flow in Residual Connections</a></li>
<li><a href="#64-vanishingexploding-gradient-analysis">6.4 Vanishing/Exploding Gradient Analysis</a></li>
</ul>
</li>
<li><a href="#practice-problems">Practice Problems</a><ul>
<li><a href="#problem-1-tensor-manipulation">Problem 1: Tensor Manipulation</a></li>
<li><a href="#problem-2-einsum-mastery">Problem 2: einsum Mastery</a></li>
<li><a href="#problem-3-numerical-stability">Problem 3: Numerical Stability</a></li>
<li><a href="#problem-4-initialization-experiments">Problem 4: Initialization Experiments</a></li>
<li><a href="#problem-5-batch-normalization-implementation">Problem 5: Batch Normalization Implementation</a></li>
</ul>
</li>
<li><a href="#references">References</a><ul>
<li><a href="#books">Books</a></li>
<li><a href="#papers">Papers</a></li>
<li><a href="#online-resources">Online Resources</a></li>
<li><a href="#tools">Tools</a></li>
</ul>
</li>
</ul>
</div>

    </nav>
    

    <div class="lesson-content markdown-body">
        <h1 id="13-linear-algebra-for-deep-learning">13. Linear Algebra for Deep Learning<a class="header-link" href="#13-linear-algebra-for-deep-learning" title="Permanent link">&para;</a></h1>
<h2 id="learning-objectives">Learning Objectives<a class="header-link" href="#learning-objectives" title="Permanent link">&para;</a></h2>
<ul>
<li>Understand and utilize tensor concepts, dimensions, and axis operations</li>
<li>Master efficient tensor operations using Einstein notation and einsum</li>
<li>Understand the principles of automatic differentiation (forward/reverse mode) and computational graphs</li>
<li>Recognize numerical stability issues in deep learning and their solutions</li>
<li>Learn the mathematical theory of weight initialization and its practical application</li>
<li>Understand the mathematical background of batch/layer normalization, residual connections, etc.</li>
</ul>
<hr />
<h2 id="1-tensor-operations">1. Tensor Operations<a class="header-link" href="#1-tensor-operations" title="Permanent link">&para;</a></h2>
<h3 id="11-tensor-hierarchy">1.1 Tensor Hierarchy<a class="header-link" href="#11-tensor-hierarchy" title="Permanent link">&para;</a></h3>
<p><strong>Scalar (0-tensor)</strong>: Single number</p>
<div class="highlight"><pre><span></span><code><span class="kn">import</span><span class="w"> </span><span class="nn">numpy</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">np</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">torch</span>

<span class="n">s</span> <span class="o">=</span> <span class="mf">3.14</span>
</code></pre></div>

<p><strong>Vector (1-tensor)</strong>: 1D array</p>
<div class="highlight"><pre><span></span><code><span class="n">v</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">])</span>  <span class="c1"># shape: (3,)</span>
</code></pre></div>

<p><strong>Matrix (2-tensor)</strong>: 2D array</p>
<div class="highlight"><pre><span></span><code><span class="n">M</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">],</span> <span class="p">[</span><span class="mi">3</span><span class="p">,</span> <span class="mi">4</span><span class="p">]])</span>  <span class="c1"># shape: (2, 2)</span>
</code></pre></div>

<p><strong>Tensor (n-tensor)</strong>: n-dimensional array</p>
<div class="highlight"><pre><span></span><code><span class="n">T</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="mi">4</span><span class="p">,</span> <span class="mi">5</span><span class="p">)</span>  <span class="c1"># shape: (3, 4, 5)</span>
</code></pre></div>

<h3 id="12-tensor-axes-and-dimensions">1.2 Tensor Axes and Dimensions<a class="header-link" href="#12-tensor-axes-and-dimensions" title="Permanent link">&para;</a></h3>
<p>Typical tensor shapes in deep learning:
- <strong>Images</strong>: (batch, channels, height, width) or (batch, height, width, channels)
- <strong>Sequences</strong>: (batch, sequence_length, features)
- <strong>Weights</strong>: (output_features, input_features)</p>
<div class="highlight"><pre><span></span><code><span class="c1"># ì¶• ì´í•´í•˜ê¸°</span>
<span class="n">batch_images</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">32</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">224</span><span class="p">,</span> <span class="mi">224</span><span class="p">)</span>  <span class="c1"># NCHW í˜•ì‹</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Shape: </span><span class="si">{</span><span class="n">batch_images</span><span class="o">.</span><span class="n">shape</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;ë°°ì¹˜ í¬ê¸°: </span><span class="si">{</span><span class="n">batch_images</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;ì±„ë„ ìˆ˜: </span><span class="si">{</span><span class="n">batch_images</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;ë†’ì´: </span><span class="si">{</span><span class="n">batch_images</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">2</span><span class="p">]</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;ë„ˆë¹„: </span><span class="si">{</span><span class="n">batch_images</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">3</span><span class="p">]</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>

<span class="c1"># ì¶•ì— ë”°ë¥¸ ì—°ì‚°</span>
<span class="n">batch_mean</span> <span class="o">=</span> <span class="n">batch_images</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>  <span class="c1"># ë°°ì¹˜ í‰ê· : (3, 224, 224)</span>
<span class="n">spatial_mean</span> <span class="o">=</span> <span class="n">batch_images</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">axis</span><span class="o">=</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">))</span>  <span class="c1"># ê³µê°„ í‰ê· : (32, 3)</span>
<span class="n">global_mean</span> <span class="o">=</span> <span class="n">batch_images</span><span class="o">.</span><span class="n">mean</span><span class="p">()</span>  <span class="c1"># ì „ì²´ í‰ê· : ìŠ¤ì¹¼ë¼</span>

<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;ë°°ì¹˜ í‰ê·  shape: </span><span class="si">{</span><span class="n">batch_mean</span><span class="o">.</span><span class="n">shape</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;ê³µê°„ í‰ê·  shape: </span><span class="si">{</span><span class="n">spatial_mean</span><span class="o">.</span><span class="n">shape</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;ì „ì²´ í‰ê· : </span><span class="si">{</span><span class="n">global_mean</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
</code></pre></div>

<h3 id="13-broadcasting">1.3 Broadcasting<a class="header-link" href="#13-broadcasting" title="Permanent link">&para;</a></h3>
<p>NumPy and PyTorch automatically expand arrays of different sizes for operations.</p>
<p><strong>Broadcasting rules</strong>:
1. If shapes have different numbers of dimensions, prepend 1s to the smaller shape
2. Compatible if dimensions are equal or one of them is 1
3. Dimensions of size 1 are stretched to match the other size</p>
<div class="highlight"><pre><span></span><code><span class="c1"># ë¸Œë¡œë“œìºìŠ¤íŒ… ì˜ˆì œ</span>
<span class="n">A</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">32</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">224</span><span class="p">,</span> <span class="mi">224</span><span class="p">)</span>  <span class="c1"># ë°°ì¹˜ ì´ë¯¸ì§€</span>
<span class="n">mean</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="mf">0.485</span><span class="p">,</span> <span class="mf">0.456</span><span class="p">,</span> <span class="mf">0.406</span><span class="p">])</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>  <span class="c1"># ì±„ë„ë³„ í‰ê· </span>
<span class="n">std</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="mf">0.229</span><span class="p">,</span> <span class="mf">0.224</span><span class="p">,</span> <span class="mf">0.225</span><span class="p">])</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>   <span class="c1"># ì±„ë„ë³„ í‘œì¤€í¸ì°¨</span>

<span class="c1"># ì •ê·œí™”: ë¸Œë¡œë“œìºìŠ¤íŒ… ì ìš©</span>
<span class="n">A_normalized</span> <span class="o">=</span> <span class="p">(</span><span class="n">A</span> <span class="o">-</span> <span class="n">mean</span><span class="p">)</span> <span class="o">/</span> <span class="n">std</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;ì •ê·œí™”ëœ shape: </span><span class="si">{</span><span class="n">A_normalized</span><span class="o">.</span><span class="n">shape</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>  <span class="c1"># ì—¬ì „íˆ (32, 3, 224, 224)</span>

<span class="c1"># ë¸Œë¡œë“œìºìŠ¤íŒ… ì‹œê°í™”</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;</span><span class="se">\n</span><span class="s2">ë¸Œë¡œë“œìºìŠ¤íŒ… ê³¼ì •:&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;A: </span><span class="si">{</span><span class="n">A</span><span class="o">.</span><span class="n">shape</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;mean: </span><span class="si">{</span><span class="n">mean</span><span class="o">.</span><span class="n">shape</span><span class="si">}</span><span class="s2"> -&gt; ë¸Œë¡œë“œìºìŠ¤íŠ¸ -&gt; </span><span class="si">{</span><span class="n">A</span><span class="o">.</span><span class="n">shape</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;ê²°ê³¼: </span><span class="si">{</span><span class="n">A_normalized</span><span class="o">.</span><span class="n">shape</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
</code></pre></div>

<h3 id="14-tensor-shape-transformations">1.4 Tensor Shape Transformations<a class="header-link" href="#14-tensor-shape-transformations" title="Permanent link">&para;</a></h3>
<div class="highlight"><pre><span></span><code><span class="kn">import</span><span class="w"> </span><span class="nn">torch</span>

<span class="c1"># ë‹¤ì–‘í•œ í˜•íƒœ ë³€í™˜</span>
<span class="n">x</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">32</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">224</span><span class="p">,</span> <span class="mi">224</span><span class="p">)</span>

<span class="c1"># 1. reshape: ì—°ì† ë©”ëª¨ë¦¬ì—ì„œë§Œ ê°€ëŠ¥</span>
<span class="n">x_flat</span> <span class="o">=</span> <span class="n">x</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="mi">32</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">)</span>  <span class="c1"># (32, 3*224*224)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Flatten: </span><span class="si">{</span><span class="n">x_flat</span><span class="o">.</span><span class="n">shape</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>

<span class="c1"># 2. view vs reshape</span>
<span class="n">x_view</span> <span class="o">=</span> <span class="n">x</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="mi">32</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">)</span>  <span class="c1"># (32, 3, 224*224)</span>
<span class="n">x_reshaped</span> <span class="o">=</span> <span class="n">x</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="mi">32</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">)</span>  <span class="c1"># ë™ì¼</span>

<span class="c1"># 3. transpose: ì¶• êµí™˜</span>
<span class="n">x_transposed</span> <span class="o">=</span> <span class="n">x</span><span class="o">.</span><span class="n">transpose</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>  <span class="c1"># (32, 224, 3, 224)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Transpose: </span><span class="si">{</span><span class="n">x_transposed</span><span class="o">.</span><span class="n">shape</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>

<span class="c1"># 4. permute: ì„ì˜ ìˆœì„œë¡œ ì¶• ì¬ë°°ì¹˜</span>
<span class="n">x_permuted</span> <span class="o">=</span> <span class="n">x</span><span class="o">.</span><span class="n">permute</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>  <span class="c1"># NCHW -&gt; NHWC</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Permute: </span><span class="si">{</span><span class="n">x_permuted</span><span class="o">.</span><span class="n">shape</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>

<span class="c1"># 5. squeeze/unsqueeze: ì°¨ì› ì¶”ê°€/ì œê±°</span>
<span class="n">x_squeezed</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">32</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">224</span><span class="p">,</span> <span class="mi">224</span><span class="p">)</span><span class="o">.</span><span class="n">squeeze</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span>  <span class="c1"># (32, 224, 224)</span>
<span class="n">x_unsqueezed</span> <span class="o">=</span> <span class="n">x_squeezed</span><span class="o">.</span><span class="n">unsqueeze</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span>  <span class="c1"># (32, 1, 224, 224)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Squeeze: </span><span class="si">{</span><span class="n">x_squeezed</span><span class="o">.</span><span class="n">shape</span><span class="si">}</span><span class="s2">, Unsqueeze: </span><span class="si">{</span><span class="n">x_unsqueezed</span><span class="o">.</span><span class="n">shape</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
</code></pre></div>

<hr />
<h2 id="2-einstein-notation-einsum">2. Einstein Notation / einsum<a class="header-link" href="#2-einstein-notation-einsum" title="Permanent link">&para;</a></h2>
<h3 id="21-einstein-summation-convention">2.1 Einstein Summation Convention<a class="header-link" href="#21-einstein-summation-convention" title="Permanent link">&para;</a></h3>
<p><strong>Core idea</strong>: Repeated indices are automatically summed.</p>
<p><strong>Traditional notation</strong>:
$$
C_{ik} = \sum_{j} A_{ij} B_{jk}
$$</p>
<p><strong>Einstein notation</strong>:
$$
C_{ik} = A_{ij} B_{jk}
$$</p>
<p>Since $j$ appears on both sides, it's implicitly summed.</p>
<h3 id="22-basic-einsum-usage">2.2 Basic einsum Usage<a class="header-link" href="#22-basic-einsum-usage" title="Permanent link">&para;</a></h3>
<div class="highlight"><pre><span></span><code><span class="c1"># 1. í–‰ë ¬-ë²¡í„° ê³±: y = Ax</span>
<span class="n">A</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="mi">4</span><span class="p">)</span>
<span class="n">x</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">4</span><span class="p">)</span>
<span class="n">y1</span> <span class="o">=</span> <span class="n">A</span> <span class="o">@</span> <span class="n">x</span>                      <span class="c1"># ì „í†µì  ë°©ë²•</span>
<span class="n">y2</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">einsum</span><span class="p">(</span><span class="s1">&#39;ij,j-&gt;i&#39;</span><span class="p">,</span> <span class="n">A</span><span class="p">,</span> <span class="n">x</span><span class="p">)</span>  <span class="c1"># einsum</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;í–‰ë ¬-ë²¡í„° ê³± ì¼ì¹˜: </span><span class="si">{</span><span class="n">np</span><span class="o">.</span><span class="n">allclose</span><span class="p">(</span><span class="n">y1</span><span class="p">,</span><span class="w"> </span><span class="n">y2</span><span class="p">)</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>

<span class="c1"># 2. í–‰ë ¬ ê³±: C = AB</span>
<span class="n">A</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="mi">4</span><span class="p">)</span>
<span class="n">B</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">4</span><span class="p">,</span> <span class="mi">5</span><span class="p">)</span>
<span class="n">C1</span> <span class="o">=</span> <span class="n">A</span> <span class="o">@</span> <span class="n">B</span>
<span class="n">C2</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">einsum</span><span class="p">(</span><span class="s1">&#39;ij,jk-&gt;ik&#39;</span><span class="p">,</span> <span class="n">A</span><span class="p">,</span> <span class="n">B</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;í–‰ë ¬ ê³± ì¼ì¹˜: </span><span class="si">{</span><span class="n">np</span><span class="o">.</span><span class="n">allclose</span><span class="p">(</span><span class="n">C1</span><span class="p">,</span><span class="w"> </span><span class="n">C2</span><span class="p">)</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>

<span class="c1"># 3. ëŒ€ê°í•©(trace): tr(A) = Î£ A_ii</span>
<span class="n">A</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">4</span><span class="p">,</span> <span class="mi">4</span><span class="p">)</span>
<span class="n">trace1</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">trace</span><span class="p">(</span><span class="n">A</span><span class="p">)</span>
<span class="n">trace2</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">einsum</span><span class="p">(</span><span class="s1">&#39;ii-&gt;&#39;</span><span class="p">,</span> <span class="n">A</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;ëŒ€ê°í•© ì¼ì¹˜: </span><span class="si">{</span><span class="n">np</span><span class="o">.</span><span class="n">allclose</span><span class="p">(</span><span class="n">trace1</span><span class="p">,</span><span class="w"> </span><span class="n">trace2</span><span class="p">)</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>

<span class="c1"># 4. ì™¸ì : C_ij = a_i b_j</span>
<span class="n">a</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">])</span>
<span class="n">b</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="mi">4</span><span class="p">,</span> <span class="mi">5</span><span class="p">])</span>
<span class="n">C1</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">outer</span><span class="p">(</span><span class="n">a</span><span class="p">,</span> <span class="n">b</span><span class="p">)</span>
<span class="n">C2</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">einsum</span><span class="p">(</span><span class="s1">&#39;i,j-&gt;ij&#39;</span><span class="p">,</span> <span class="n">a</span><span class="p">,</span> <span class="n">b</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;ì™¸ì  ì¼ì¹˜: </span><span class="si">{</span><span class="n">np</span><span class="o">.</span><span class="n">allclose</span><span class="p">(</span><span class="n">C1</span><span class="p">,</span><span class="w"> </span><span class="n">C2</span><span class="p">)</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
</code></pre></div>

<h3 id="23-batch-operations">2.3 Batch Operations<a class="header-link" href="#23-batch-operations" title="Permanent link">&para;</a></h3>
<p>Most useful for deep learning.</p>
<div class="highlight"><pre><span></span><code><span class="c1"># ë°°ì¹˜ í–‰ë ¬ ê³± (bmm)</span>
<span class="c1"># A: (batch, n, m), B: (batch, m, p) -&gt; C: (batch, n, p)</span>
<span class="n">batch_size</span><span class="p">,</span> <span class="n">n</span><span class="p">,</span> <span class="n">m</span><span class="p">,</span> <span class="n">p</span> <span class="o">=</span> <span class="mi">32</span><span class="p">,</span> <span class="mi">10</span><span class="p">,</span> <span class="mi">20</span><span class="p">,</span> <span class="mi">15</span>
<span class="n">A</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="n">batch_size</span><span class="p">,</span> <span class="n">n</span><span class="p">,</span> <span class="n">m</span><span class="p">)</span>
<span class="n">B</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="n">batch_size</span><span class="p">,</span> <span class="n">m</span><span class="p">,</span> <span class="n">p</span><span class="p">)</span>

<span class="n">C1</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">einsum</span><span class="p">(</span><span class="s1">&#39;bij,bjk-&gt;bik&#39;</span><span class="p">,</span> <span class="n">A</span><span class="p">,</span> <span class="n">B</span><span class="p">)</span>
<span class="c1"># torch ë¹„êµ</span>
<span class="n">A_torch</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">from_numpy</span><span class="p">(</span><span class="n">A</span><span class="p">)</span>
<span class="n">B_torch</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">from_numpy</span><span class="p">(</span><span class="n">B</span><span class="p">)</span>
<span class="n">C2</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">bmm</span><span class="p">(</span><span class="n">A_torch</span><span class="p">,</span> <span class="n">B_torch</span><span class="p">)</span><span class="o">.</span><span class="n">numpy</span><span class="p">()</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;ë°°ì¹˜ í–‰ë ¬ ê³± ì¼ì¹˜: </span><span class="si">{</span><span class="n">np</span><span class="o">.</span><span class="n">allclose</span><span class="p">(</span><span class="n">C1</span><span class="p">,</span><span class="w"> </span><span class="n">C2</span><span class="p">)</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>

<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;ì…ë ¥ shapes: A=</span><span class="si">{</span><span class="n">A</span><span class="o">.</span><span class="n">shape</span><span class="si">}</span><span class="s2">, B=</span><span class="si">{</span><span class="n">B</span><span class="o">.</span><span class="n">shape</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;ì¶œë ¥ shape: C=</span><span class="si">{</span><span class="n">C1</span><span class="o">.</span><span class="n">shape</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
</code></pre></div>

<h3 id="24-attention-mechanism">2.4 Attention Mechanism<a class="header-link" href="#24-attention-mechanism" title="Permanent link">&para;</a></h3>
<p>Transformer's core operations are very concise with einsum.</p>
<p>$$
\text{Attention}(Q, K, V) = \text{softmax}\left(\frac{QK^T}{\sqrt{d_k}}\right) V
$$</p>
<div class="highlight"><pre><span></span><code><span class="k">def</span><span class="w"> </span><span class="nf">attention_einsum</span><span class="p">(</span><span class="n">Q</span><span class="p">,</span> <span class="n">K</span><span class="p">,</span> <span class="n">V</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Q: (batch, query_len, d_k)</span>
<span class="sd">    K: (batch, key_len, d_k)</span>
<span class="sd">    V: (batch, key_len, d_v)</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">d_k</span> <span class="o">=</span> <span class="n">Q</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span>

    <span class="c1"># QK^T: (batch, query_len, key_len)</span>
    <span class="n">scores</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">einsum</span><span class="p">(</span><span class="s1">&#39;bqd,bkd-&gt;bqk&#39;</span><span class="p">,</span> <span class="n">Q</span><span class="p">,</span> <span class="n">K</span><span class="p">)</span> <span class="o">/</span> <span class="n">np</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="n">d_k</span><span class="p">)</span>

    <span class="c1"># Softmax</span>
    <span class="n">attention_weights</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="n">scores</span> <span class="o">-</span> <span class="n">scores</span><span class="o">.</span><span class="n">max</span><span class="p">(</span><span class="n">axis</span><span class="o">=-</span><span class="mi">1</span><span class="p">,</span> <span class="n">keepdims</span><span class="o">=</span><span class="kc">True</span><span class="p">))</span>
    <span class="n">attention_weights</span> <span class="o">/=</span> <span class="n">attention_weights</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">axis</span><span class="o">=-</span><span class="mi">1</span><span class="p">,</span> <span class="n">keepdims</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>

    <span class="c1"># Attention weights Ã— V: (batch, query_len, d_v)</span>
    <span class="n">output</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">einsum</span><span class="p">(</span><span class="s1">&#39;bqk,bkv-&gt;bqv&#39;</span><span class="p">,</span> <span class="n">attention_weights</span><span class="p">,</span> <span class="n">V</span><span class="p">)</span>

    <span class="k">return</span> <span class="n">output</span><span class="p">,</span> <span class="n">attention_weights</span>

<span class="c1"># í…ŒìŠ¤íŠ¸</span>
<span class="n">batch</span><span class="p">,</span> <span class="n">seq_len</span><span class="p">,</span> <span class="n">d_model</span> <span class="o">=</span> <span class="mi">32</span><span class="p">,</span> <span class="mi">10</span><span class="p">,</span> <span class="mi">64</span>
<span class="n">Q</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="n">batch</span><span class="p">,</span> <span class="n">seq_len</span><span class="p">,</span> <span class="n">d_model</span><span class="p">)</span>
<span class="n">K</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="n">batch</span><span class="p">,</span> <span class="n">seq_len</span><span class="p">,</span> <span class="n">d_model</span><span class="p">)</span>
<span class="n">V</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="n">batch</span><span class="p">,</span> <span class="n">seq_len</span><span class="p">,</span> <span class="n">d_model</span><span class="p">)</span>

<span class="n">output</span><span class="p">,</span> <span class="n">weights</span> <span class="o">=</span> <span class="n">attention_einsum</span><span class="p">(</span><span class="n">Q</span><span class="p">,</span> <span class="n">K</span><span class="p">,</span> <span class="n">V</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Attention output shape: </span><span class="si">{</span><span class="n">output</span><span class="o">.</span><span class="n">shape</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Attention weights shape: </span><span class="si">{</span><span class="n">weights</span><span class="o">.</span><span class="n">shape</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
</code></pre></div>

<h3 id="25-complex-tensor-operations">2.5 Complex Tensor Operations<a class="header-link" href="#25-complex-tensor-operations" title="Permanent link">&para;</a></h3>
<div class="highlight"><pre><span></span><code><span class="c1"># 1. ì´ë³€ëŸ‰ ê³±(Bilinear form): x^T A y</span>
<span class="n">A</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">5</span><span class="p">,</span> <span class="mi">5</span><span class="p">)</span>
<span class="n">x</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">5</span><span class="p">)</span>
<span class="n">y</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">5</span><span class="p">)</span>

<span class="n">result1</span> <span class="o">=</span> <span class="n">x</span> <span class="o">@</span> <span class="n">A</span> <span class="o">@</span> <span class="n">y</span>
<span class="n">result2</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">einsum</span><span class="p">(</span><span class="s1">&#39;i,ij,j-&gt;&#39;</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">A</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;ì´ë³€ëŸ‰ ê³± ì¼ì¹˜: </span><span class="si">{</span><span class="n">np</span><span class="o">.</span><span class="n">allclose</span><span class="p">(</span><span class="n">result1</span><span class="p">,</span><span class="w"> </span><span class="n">result2</span><span class="p">)</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>

<span class="c1"># 2. ë°°ì¹˜ ì´ë³€ëŸ‰ ê³±</span>
<span class="n">batch_size</span> <span class="o">=</span> <span class="mi">32</span>
<span class="n">x_batch</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="n">batch_size</span><span class="p">,</span> <span class="mi">5</span><span class="p">)</span>
<span class="n">y_batch</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="n">batch_size</span><span class="p">,</span> <span class="mi">5</span><span class="p">)</span>
<span class="n">result</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">einsum</span><span class="p">(</span><span class="s1">&#39;bi,ij,bj-&gt;b&#39;</span><span class="p">,</span> <span class="n">x_batch</span><span class="p">,</span> <span class="n">A</span><span class="p">,</span> <span class="n">y_batch</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;ë°°ì¹˜ ì´ë³€ëŸ‰ ê³± shape: </span><span class="si">{</span><span class="n">result</span><span class="o">.</span><span class="n">shape</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>

<span class="c1"># 3. í…ì„œ ì¶•ì•½(contraction)</span>
<span class="c1"># A: (i,j,k), B: (k,l,m) -&gt; C: (i,j,l,m)</span>
<span class="n">A</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="mi">4</span><span class="p">,</span> <span class="mi">5</span><span class="p">)</span>
<span class="n">B</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">5</span><span class="p">,</span> <span class="mi">6</span><span class="p">,</span> <span class="mi">7</span><span class="p">)</span>
<span class="n">C</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">einsum</span><span class="p">(</span><span class="s1">&#39;ijk,klm-&gt;ijlm&#39;</span><span class="p">,</span> <span class="n">A</span><span class="p">,</span> <span class="n">B</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;í…ì„œ ì¶•ì•½ ê²°ê³¼ shape: </span><span class="si">{</span><span class="n">C</span><span class="o">.</span><span class="n">shape</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
</code></pre></div>

<hr />
<h2 id="3-automatic-differentiation">3. Automatic Differentiation<a class="header-link" href="#3-automatic-differentiation" title="Permanent link">&para;</a></h2>
<h3 id="31-computational-graph">3.1 Computational Graph<a class="header-link" href="#31-computational-graph" title="Permanent link">&para;</a></h3>
<p>All computations are represented as directed acyclic graphs (DAGs).</p>
<div class="highlight"><pre><span></span><code><span class="kn">import</span><span class="w"> </span><span class="nn">torch</span>

<span class="c1"># ê³„ì‚° ê·¸ë˜í”„ ì˜ˆì œ</span>
<span class="n">x</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="mf">2.0</span><span class="p">,</span> <span class="n">requires_grad</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="n">y</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="mf">3.0</span><span class="p">,</span> <span class="n">requires_grad</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>

<span class="c1"># ìˆœë°©í–¥ ê³„ì‚°</span>
<span class="n">z</span> <span class="o">=</span> <span class="n">x</span><span class="o">**</span><span class="mi">2</span> <span class="o">+</span> <span class="n">x</span><span class="o">*</span><span class="n">y</span> <span class="o">+</span> <span class="n">y</span><span class="o">**</span><span class="mi">2</span>
<span class="n">z</span><span class="o">.</span><span class="n">backward</span><span class="p">()</span>  <span class="c1"># ì—­ì „íŒŒ</span>

<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;z = </span><span class="si">{</span><span class="n">z</span><span class="o">.</span><span class="n">item</span><span class="p">()</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;âˆ‚z/âˆ‚x = </span><span class="si">{</span><span class="n">x</span><span class="o">.</span><span class="n">grad</span><span class="o">.</span><span class="n">item</span><span class="p">()</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>  <span class="c1"># 2x + y = 2*2 + 3 = 7</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;âˆ‚z/âˆ‚y = </span><span class="si">{</span><span class="n">y</span><span class="o">.</span><span class="n">grad</span><span class="o">.</span><span class="n">item</span><span class="p">()</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>  <span class="c1"># x + 2y = 2 + 2*3 = 8</span>

<span class="c1"># ê³„ì‚° ê·¸ë˜í”„ ì‹œê°í™” (ê°œë…ì )</span>
<span class="sd">&quot;&quot;&quot;</span>
<span class="sd">      x=2         y=3</span>
<span class="sd">       â†“           â†“</span>
<span class="sd">    xÂ²=4    â†’  x*y=6  â†  yÂ²=9</span>
<span class="sd">       â†“           â†“        â†“</span>
<span class="sd">       â””â”€â”€â”€â”€â”€â†’  z = 4+6+9 = 19</span>
<span class="sd">&quot;&quot;&quot;</span>
</code></pre></div>

<h3 id="32-forward-mode">3.2 Forward Mode<a class="header-link" href="#32-forward-mode" title="Permanent link">&para;</a></h3>
<p><strong>Direction</strong>: Input â†’ Output (Jacobian-vector product, JVP)</p>
<p><strong>Computes</strong>: $\frac{\partial y}{\partial x} \cdot v$ (directional derivative in direction $v$)</p>
<p><strong>Advantage</strong>: Efficient when inputs are few ($n_{\text{in}} \ll n_{\text{out}}$)</p>
<p><strong>Example</strong>: $f: \mathbb{R} \to \mathbb{R}^{1000}$</p>
<div class="highlight"><pre><span></span><code><span class="c1"># í¬ì›Œë“œ ëª¨ë“œ ê°œë… (PyTorchëŠ” ê¸°ë³¸ì ìœ¼ë¡œ ë¦¬ë²„ìŠ¤ ëª¨ë“œ ì‚¬ìš©)</span>
<span class="k">def</span><span class="w"> </span><span class="nf">forward_mode_example</span><span class="p">():</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;í¬ì›Œë“œ ëª¨ë“œì˜ ê°œë…ì  êµ¬í˜„&quot;&quot;&quot;</span>
    <span class="c1"># f(x) = x^2 + 2x + 1</span>
    <span class="c1"># df/dx = 2x + 2</span>

    <span class="n">x</span> <span class="o">=</span> <span class="mf">3.0</span>
    <span class="n">dx</span> <span class="o">=</span> <span class="mf">1.0</span>  <span class="c1"># ë°©í–¥ ë²¡í„° (ë³´í†µ 1)</span>

    <span class="c1"># ë™ì‹œì— ê°’ê³¼ ë„í•¨ìˆ˜ ê³„ì‚°</span>
    <span class="n">y</span> <span class="o">=</span> <span class="n">x</span><span class="o">**</span><span class="mi">2</span> <span class="o">+</span> <span class="mi">2</span><span class="o">*</span><span class="n">x</span> <span class="o">+</span> <span class="mi">1</span>      <span class="c1"># y = 16</span>
    <span class="n">dy</span> <span class="o">=</span> <span class="mi">2</span><span class="o">*</span><span class="n">x</span><span class="o">*</span><span class="n">dx</span> <span class="o">+</span> <span class="mi">2</span><span class="o">*</span><span class="n">dx</span>      <span class="c1"># dy/dx = 8</span>

    <span class="k">return</span> <span class="n">y</span><span class="p">,</span> <span class="n">dy</span>

<span class="n">y</span><span class="p">,</span> <span class="n">dy</span> <span class="o">=</span> <span class="n">forward_mode_example</span><span class="p">()</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;f(3) = </span><span class="si">{</span><span class="n">y</span><span class="si">}</span><span class="s2">, f&#39;(3) = </span><span class="si">{</span><span class="n">dy</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
</code></pre></div>

<h3 id="33-reverse-mode-backpropagation">3.3 Reverse Mode / Backpropagation<a class="header-link" href="#33-reverse-mode-backpropagation" title="Permanent link">&para;</a></h3>
<p><strong>Direction</strong>: Output â†’ Input (vector-Jacobian product, VJP)</p>
<p><strong>Computes</strong>: $v^T \cdot \frac{\partial y}{\partial x}$ (gradient with respect to output)</p>
<p><strong>Advantage</strong>: Efficient when outputs are few ($n_{\text{out}} \ll n_{\text{in}}$)</p>
<p><strong>Deep Learning</strong>: Loss function is scalar ($n_{\text{out}} = 1$) â†’ reverse mode is optimal</p>
<div class="highlight"><pre><span></span><code><span class="c1"># ë¦¬ë²„ìŠ¤ ëª¨ë“œ (ì—­ì „íŒŒ)</span>
<span class="k">def</span><span class="w"> </span><span class="nf">reverse_mode_demo</span><span class="p">():</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;ë¦¬ë²„ìŠ¤ ëª¨ë“œ ìë™ ë¯¸ë¶„ ì‹œì—°&quot;&quot;&quot;</span>
    <span class="n">x</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([</span><span class="mf">1.0</span><span class="p">,</span> <span class="mf">2.0</span><span class="p">,</span> <span class="mf">3.0</span><span class="p">],</span> <span class="n">requires_grad</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
    <span class="n">w</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([[</span><span class="mf">1.0</span><span class="p">,</span> <span class="mf">2.0</span><span class="p">],</span> <span class="p">[</span><span class="mf">3.0</span><span class="p">,</span> <span class="mf">4.0</span><span class="p">],</span> <span class="p">[</span><span class="mf">5.0</span><span class="p">,</span> <span class="mf">6.0</span><span class="p">]],</span> <span class="n">requires_grad</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
    <span class="n">b</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([</span><span class="mf">1.0</span><span class="p">,</span> <span class="mf">1.0</span><span class="p">],</span> <span class="n">requires_grad</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>

    <span class="c1"># ìˆœë°©í–¥: y = w^T x + b</span>
    <span class="n">y</span> <span class="o">=</span> <span class="n">x</span> <span class="o">@</span> <span class="n">w</span> <span class="o">+</span> <span class="n">b</span>  <span class="c1"># (2,)</span>

    <span class="c1"># ì†ì‹¤: L = ||y||^2</span>
    <span class="n">loss</span> <span class="o">=</span> <span class="p">(</span><span class="n">y</span> <span class="o">**</span> <span class="mi">2</span><span class="p">)</span><span class="o">.</span><span class="n">sum</span><span class="p">()</span>

    <span class="c1"># ì—­ì „íŒŒ</span>
    <span class="n">loss</span><span class="o">.</span><span class="n">backward</span><span class="p">()</span>

    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;x.grad shape: </span><span class="si">{</span><span class="n">x</span><span class="o">.</span><span class="n">grad</span><span class="o">.</span><span class="n">shape</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>  <span class="c1"># (3,)</span>
    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;w.grad shape: </span><span class="si">{</span><span class="n">w</span><span class="o">.</span><span class="n">grad</span><span class="o">.</span><span class="n">shape</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>  <span class="c1"># (3, 2)</span>
    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;b.grad shape: </span><span class="si">{</span><span class="n">b</span><span class="o">.</span><span class="n">grad</span><span class="o">.</span><span class="n">shape</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>  <span class="c1"># (2,)</span>

    <span class="k">return</span> <span class="n">loss</span><span class="o">.</span><span class="n">item</span><span class="p">()</span>

<span class="n">loss</span> <span class="o">=</span> <span class="n">reverse_mode_demo</span><span class="p">()</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Loss: </span><span class="si">{</span><span class="n">loss</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
</code></pre></div>

<h3 id="34-why-reverse-mode-fits-deep-learning">3.4 Why Reverse Mode Fits Deep Learning<a class="header-link" href="#34-why-reverse-mode-fits-deep-learning" title="Permanent link">&para;</a></h3>
<p><strong>Neural networks</strong>: $f: \mathbb{R}^n \to \mathbb{R}$ (parameter space â†’ loss)</p>
<ul>
<li><strong>Input dimension</strong>: $n \sim 10^6$ ~ $10^9$ (number of parameters)</li>
<li><strong>Output dimension</strong>: 1 (loss function)</li>
</ul>
<p><strong>Cost comparison</strong>:
- Forward mode: $O(n)$ passes needed (for each input)
- Reverse mode: $O(1)$ pass (one backpropagation)</p>
<p><strong>Conclusion</strong>: Reverse mode is $O(n)$ times more efficient!</p>
<div class="highlight"><pre><span></span><code><span class="c1"># íš¨ìœ¨ì„± ë¹„êµ ì‹œë®¬ë ˆì´ì…˜</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">time</span>

<span class="k">def</span><span class="w"> </span><span class="nf">forward_pass</span><span class="p">(</span><span class="n">params</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;ê°„ë‹¨í•œ ì‹ ê²½ë§ ìˆœë°©í–¥ íŒ¨ìŠ¤&quot;&quot;&quot;</span>
    <span class="n">x</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">1000</span><span class="p">,</span> <span class="mi">1000</span><span class="p">)</span>
    <span class="k">for</span> <span class="n">p</span> <span class="ow">in</span> <span class="n">params</span><span class="p">:</span>
        <span class="n">x</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">relu</span><span class="p">(</span><span class="n">x</span> <span class="o">@</span> <span class="n">p</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">x</span><span class="o">.</span><span class="n">sum</span><span class="p">()</span>

<span class="c1"># í° ì‹ ê²½ë§ ì‹œë®¬ë ˆì´ì…˜</span>
<span class="n">n_params</span> <span class="o">=</span> <span class="mi">10</span>
<span class="n">param_list</span> <span class="o">=</span> <span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">1000</span><span class="p">,</span> <span class="mi">1000</span><span class="p">,</span> <span class="n">requires_grad</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span> <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">n_params</span><span class="p">)]</span>

<span class="c1"># ë¦¬ë²„ìŠ¤ ëª¨ë“œ (í‘œì¤€ ì—­ì „íŒŒ)</span>
<span class="n">start</span> <span class="o">=</span> <span class="n">time</span><span class="o">.</span><span class="n">time</span><span class="p">()</span>
<span class="n">loss</span> <span class="o">=</span> <span class="n">forward_pass</span><span class="p">(</span><span class="n">param_list</span><span class="p">)</span>
<span class="n">loss</span><span class="o">.</span><span class="n">backward</span><span class="p">()</span>
<span class="n">reverse_time</span> <span class="o">=</span> <span class="n">time</span><span class="o">.</span><span class="n">time</span><span class="p">()</span> <span class="o">-</span> <span class="n">start</span>

<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;ë¦¬ë²„ìŠ¤ ëª¨ë“œ ì‹œê°„: </span><span class="si">{</span><span class="n">reverse_time</span><span class="si">:</span><span class="s2">.4f</span><span class="si">}</span><span class="s2">ì´ˆ&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;ì´ íŒŒë¼ë¯¸í„° ìˆ˜: </span><span class="si">{</span><span class="nb">sum</span><span class="p">(</span><span class="n">p</span><span class="o">.</span><span class="n">numel</span><span class="p">()</span><span class="w"> </span><span class="k">for</span><span class="w"> </span><span class="n">p</span><span class="w"> </span><span class="ow">in</span><span class="w"> </span><span class="n">param_list</span><span class="p">)</span><span class="si">:</span><span class="s2">,</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;í•œ ë²ˆì˜ ì—­ì „íŒŒë¡œ ëª¨ë“  ê·¸ë˜ë””ì–¸íŠ¸ ê³„ì‚° ì™„ë£Œ&quot;</span><span class="p">)</span>
</code></pre></div>

<hr />
<h2 id="4-numerical-stability">4. Numerical Stability<a class="header-link" href="#4-numerical-stability" title="Permanent link">&para;</a></h2>
<h3 id="41-floating-point-arithmetic-limitations">4.1 Floating-Point Arithmetic Limitations<a class="header-link" href="#41-floating-point-arithmetic-limitations" title="Permanent link">&para;</a></h3>
<p>IEEE 754 standard:
- <strong>Float32</strong>: Approximately $10^{-38}$ ~ $10^{38}$, precision $\sim 10^{-7}$
- <strong>Underflow</strong>: Numbers too small â†’ 0
- <strong>Overflow</strong>: Numbers too large â†’ inf</p>
<div class="highlight"><pre><span></span><code><span class="c1"># ë¶€ë™ì†Œìˆ˜ì  í•œê³„ ì‹œì—°</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Float32 ìµœì†Œê°’: </span><span class="si">{</span><span class="n">np</span><span class="o">.</span><span class="n">finfo</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span><span class="o">.</span><span class="n">min</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Float32 ìµœëŒ€ê°’: </span><span class="si">{</span><span class="n">np</span><span class="o">.</span><span class="n">finfo</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span><span class="o">.</span><span class="n">max</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Float32 ì—¡ì‹¤ë¡ : </span><span class="si">{</span><span class="n">np</span><span class="o">.</span><span class="n">finfo</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span><span class="o">.</span><span class="n">eps</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>

<span class="c1"># ì–¸ë”í”Œë¡œìš°</span>
<span class="n">x</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">float32</span><span class="p">(</span><span class="mf">1e-40</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;</span><span class="se">\n</span><span class="s2">1e-40 (float32): </span><span class="si">{</span><span class="n">x</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>  <span class="c1"># 0.0</span>

<span class="c1"># ì˜¤ë²„í”Œë¡œìš°</span>
<span class="n">x</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">float32</span><span class="p">(</span><span class="mf">1e40</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;1e40 (float32): </span><span class="si">{</span><span class="n">x</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>  <span class="c1"># inf</span>

<span class="c1"># ì •ë°€ë„ ì†ì‹¤</span>
<span class="n">x</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">float32</span><span class="p">(</span><span class="mf">1.0</span> <span class="o">+</span> <span class="mf">1e-8</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;1.0 + 1e-8 (float32): </span><span class="si">{</span><span class="n">x</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>  <span class="c1"># 1.0 (ì†ì‹¤ë¨)</span>
</code></pre></div>

<h3 id="42-log-sum-exp-trick">4.2 Log-Sum-Exp Trick<a class="header-link" href="#42-log-sum-exp-trick" title="Permanent link">&para;</a></h3>
<p>Problem: Computing $\log \sum_{i=1}^{n} e^{x_i}$ causes overflow</p>
<p><strong>Naive computation</strong>:</p>
<div class="highlight"><pre><span></span><code><span class="n">x</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="mi">1000</span><span class="p">,</span> <span class="mi">1001</span><span class="p">,</span> <span class="mi">1002</span><span class="p">],</span> <span class="n">dtype</span><span class="o">=</span><span class="n">np</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>
<span class="n">result_naive</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="n">x</span><span class="p">)))</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;ì§ì ‘ ê³„ì‚°: </span><span class="si">{</span><span class="n">result_naive</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>  <span class="c1"># inf (ì˜¤ë²„í”Œë¡œìš°!)</span>
</code></pre></div>

<p><strong>Log-Sum-Exp trick</strong>:
$$
\log \sum_{i} e^{x_i} = a + \log \sum_{i} e^{x_i - a}
$$
where $a = \max_i x_i$</p>
<div class="highlight"><pre><span></span><code><span class="k">def</span><span class="w"> </span><span class="nf">log_sum_exp</span><span class="p">(</span><span class="n">x</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;ìˆ˜ì¹˜ì ìœ¼ë¡œ ì•ˆì •í•œ log-sum-exp&quot;&quot;&quot;</span>
    <span class="n">a</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">max</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">a</span> <span class="o">+</span> <span class="n">np</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="n">x</span> <span class="o">-</span> <span class="n">a</span><span class="p">)))</span>

<span class="n">result_stable</span> <span class="o">=</span> <span class="n">log_sum_exp</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;LSE íŠ¸ë¦­: </span><span class="si">{</span><span class="n">result_stable</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>  <span class="c1"># 1002.407 (ì •í™•í•¨)</span>

<span class="c1"># scipy ë¹„êµ</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">scipy.special</span><span class="w"> </span><span class="kn">import</span> <span class="n">logsumexp</span>
<span class="n">result_scipy</span> <span class="o">=</span> <span class="n">logsumexp</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Scipy: </span><span class="si">{</span><span class="n">result_scipy</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;ì¼ì¹˜: </span><span class="si">{</span><span class="n">np</span><span class="o">.</span><span class="n">isclose</span><span class="p">(</span><span class="n">result_stable</span><span class="p">,</span><span class="w"> </span><span class="n">result_scipy</span><span class="p">)</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
</code></pre></div>

<h3 id="43-numerically-stable-softmax">4.3 Numerically Stable Softmax<a class="header-link" href="#43-numerically-stable-softmax" title="Permanent link">&para;</a></h3>
<p><strong>Standard softmax</strong>:
$$
\text{softmax}(x)_i = \frac{e^{x_i}}{\sum_j e^{x_j}}
$$</p>
<p><strong>Problem</strong>: Overflow with large $x_i$</p>
<p><strong>Stable version</strong>:
$$
\text{softmax}(x)_i = \frac{e^{x_i - \max_j x_j}}{\sum_j e^{x_j - \max_j x_j}}
$$</p>
<div class="highlight"><pre><span></span><code><span class="k">def</span><span class="w"> </span><span class="nf">softmax_naive</span><span class="p">(</span><span class="n">x</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;ë¶ˆì•ˆì •í•œ ì†Œí”„íŠ¸ë§¥ìŠ¤&quot;&quot;&quot;</span>
    <span class="n">exp_x</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">exp_x</span> <span class="o">/</span> <span class="n">np</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">exp_x</span><span class="p">)</span>

<span class="k">def</span><span class="w"> </span><span class="nf">softmax_stable</span><span class="p">(</span><span class="n">x</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;ìˆ˜ì¹˜ì ìœ¼ë¡œ ì•ˆì •í•œ ì†Œí”„íŠ¸ë§¥ìŠ¤&quot;&quot;&quot;</span>
    <span class="n">x_max</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">max</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">axis</span><span class="o">=-</span><span class="mi">1</span><span class="p">,</span> <span class="n">keepdims</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
    <span class="n">exp_x</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="n">x</span> <span class="o">-</span> <span class="n">x_max</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">exp_x</span> <span class="o">/</span> <span class="n">np</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">exp_x</span><span class="p">,</span> <span class="n">axis</span><span class="o">=-</span><span class="mi">1</span><span class="p">,</span> <span class="n">keepdims</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>

<span class="c1"># í…ŒìŠ¤íŠ¸</span>
<span class="n">x</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="mi">1000</span><span class="p">,</span> <span class="mi">1001</span><span class="p">,</span> <span class="mi">1002</span><span class="p">],</span> <span class="n">dtype</span><span class="o">=</span><span class="n">np</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>

<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;ë¶ˆì•ˆì • ë²„ì „:&quot;</span><span class="p">)</span>
<span class="n">result_naive</span> <span class="o">=</span> <span class="n">softmax_naive</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;  ê²°ê³¼: </span><span class="si">{</span><span class="n">result_naive</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;  í•©: </span><span class="si">{</span><span class="n">result_naive</span><span class="o">.</span><span class="n">sum</span><span class="p">()</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>

<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;</span><span class="se">\n</span><span class="s2">ì•ˆì • ë²„ì „:&quot;</span><span class="p">)</span>
<span class="n">result_stable</span> <span class="o">=</span> <span class="n">softmax_stable</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;  ê²°ê³¼: </span><span class="si">{</span><span class="n">result_stable</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;  í•©: </span><span class="si">{</span><span class="n">result_stable</span><span class="o">.</span><span class="n">sum</span><span class="p">()</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>

<span class="c1"># ë°°ì¹˜ ì²˜ë¦¬</span>
<span class="n">batch_logits</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">32</span><span class="p">,</span> <span class="mi">1000</span><span class="p">)</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>
<span class="n">probs</span> <span class="o">=</span> <span class="n">softmax_stable</span><span class="p">(</span><span class="n">batch_logits</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;</span><span class="se">\n</span><span class="s2">ë°°ì¹˜ ì†Œí”„íŠ¸ë§¥ìŠ¤: </span><span class="si">{</span><span class="n">probs</span><span class="o">.</span><span class="n">shape</span><span class="si">}</span><span class="s2">, í•©: </span><span class="si">{</span><span class="n">probs</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">)[:</span><span class="mi">5</span><span class="p">]</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
</code></pre></div>

<h3 id="44-gradient-clipping">4.4 Gradient Clipping<a class="header-link" href="#44-gradient-clipping" title="Permanent link">&para;</a></h3>
<p>Prevents exploding gradients.</p>
<div class="highlight"><pre><span></span><code><span class="k">def</span><span class="w"> </span><span class="nf">clip_gradients</span><span class="p">(</span><span class="n">gradients</span><span class="p">,</span> <span class="n">max_norm</span><span class="o">=</span><span class="mf">1.0</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;ê·¸ë˜ë””ì–¸íŠ¸ ë…¸ë¦„ í´ë¦¬í•‘&quot;&quot;&quot;</span>
    <span class="n">total_norm</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="nb">sum</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">g</span><span class="o">**</span><span class="mi">2</span><span class="p">)</span> <span class="k">for</span> <span class="n">g</span> <span class="ow">in</span> <span class="n">gradients</span><span class="p">))</span>
    <span class="n">clip_coef</span> <span class="o">=</span> <span class="n">max_norm</span> <span class="o">/</span> <span class="p">(</span><span class="n">total_norm</span> <span class="o">+</span> <span class="mf">1e-6</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">clip_coef</span> <span class="o">&lt;</span> <span class="mi">1</span><span class="p">:</span>
        <span class="k">for</span> <span class="n">g</span> <span class="ow">in</span> <span class="n">gradients</span><span class="p">:</span>
            <span class="n">g</span> <span class="o">*=</span> <span class="n">clip_coef</span>
    <span class="k">return</span> <span class="n">gradients</span><span class="p">,</span> <span class="n">total_norm</span>

<span class="c1"># PyTorch ì˜ˆì œ</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="mi">100</span><span class="p">,</span> <span class="mi">10</span><span class="p">)</span>
<span class="n">optimizer</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">optim</span><span class="o">.</span><span class="n">SGD</span><span class="p">(</span><span class="n">model</span><span class="o">.</span><span class="n">parameters</span><span class="p">(),</span> <span class="n">lr</span><span class="o">=</span><span class="mf">0.01</span><span class="p">)</span>

<span class="c1"># í•™ìŠµ ë£¨í”„ ë‚´ì—ì„œ</span>
<span class="k">for</span> <span class="n">batch</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">10</span><span class="p">):</span>
    <span class="n">loss</span> <span class="o">=</span> <span class="p">(</span><span class="n">model</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">32</span><span class="p">,</span> <span class="mi">100</span><span class="p">))</span> <span class="o">**</span> <span class="mi">2</span><span class="p">)</span><span class="o">.</span><span class="n">sum</span><span class="p">()</span>
    <span class="n">loss</span><span class="o">.</span><span class="n">backward</span><span class="p">()</span>

    <span class="c1"># ê·¸ë˜ë””ì–¸íŠ¸ í´ë¦¬í•‘</span>
    <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">utils</span><span class="o">.</span><span class="n">clip_grad_norm_</span><span class="p">(</span><span class="n">model</span><span class="o">.</span><span class="n">parameters</span><span class="p">(),</span> <span class="n">max_norm</span><span class="o">=</span><span class="mf">1.0</span><span class="p">)</span>

    <span class="n">optimizer</span><span class="o">.</span><span class="n">step</span><span class="p">()</span>
    <span class="n">optimizer</span><span class="o">.</span><span class="n">zero_grad</span><span class="p">()</span>
</code></pre></div>

<hr />
<h2 id="5-weight-initialization-theory">5. Weight Initialization Theory<a class="header-link" href="#5-weight-initialization-theory" title="Permanent link">&para;</a></h2>
<h3 id="51-problem-signal-vanishingexplosion">5.1 Problem: Signal Vanishing/Explosion<a class="header-link" href="#51-problem-signal-vanishingexplosion" title="Permanent link">&para;</a></h3>
<p>Without proper initialization:
- <strong>Signal vanishing</strong>: Activations converge to 0 â†’ gradient vanishing
- <strong>Signal explosion</strong>: Activations diverge to infinity â†’ gradient explosion</p>
<p><strong>Goal</strong>: Maintain variance of activations and gradients across layers</p>
<h3 id="52-xavierglorot-initialization">5.2 Xavier/Glorot Initialization<a class="header-link" href="#52-xavierglorot-initialization" title="Permanent link">&para;</a></h3>
<p><strong>Setting</strong>: Linear layer $y = Wx + b$, no activation</p>
<p><strong>Assumptions</strong>:
- $x_i$ has mean 0, variance $\text{Var}(x)$
- $W_{ij}$ are independent, mean 0</p>
<p><strong>Variance propagation</strong>:
$$
\text{Var}(y_i) = n_{\text{in}} \cdot \text{Var}(W) \cdot \text{Var}(x)
$$</p>
<p><strong>Forward</strong>: To maintain $\text{Var}(y) = \text{Var}(x)$
$$
\text{Var}(W) = \frac{1}{n_{\text{in}}}
$$</p>
<p><strong>Backward</strong>: To maintain gradient variance
$$
\text{Var}(W) = \frac{1}{n_{\text{out}}}
$$</p>
<p><strong>Xavier initialization</strong>: Compromise
$$
\text{Var}(W) = \frac{2}{n_{\text{in}} + n_{\text{out}}}
$$</p>
<div class="highlight"><pre><span></span><code><span class="k">def</span><span class="w"> </span><span class="nf">xavier_uniform</span><span class="p">(</span><span class="n">n_in</span><span class="p">,</span> <span class="n">n_out</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Xavier ê· ë“± ì´ˆê¸°í™”&quot;&quot;&quot;</span>
    <span class="n">limit</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="mi">6</span> <span class="o">/</span> <span class="p">(</span><span class="n">n_in</span> <span class="o">+</span> <span class="n">n_out</span><span class="p">))</span>
    <span class="k">return</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">uniform</span><span class="p">(</span><span class="o">-</span><span class="n">limit</span><span class="p">,</span> <span class="n">limit</span><span class="p">,</span> <span class="p">(</span><span class="n">n_in</span><span class="p">,</span> <span class="n">n_out</span><span class="p">))</span>

<span class="k">def</span><span class="w"> </span><span class="nf">xavier_normal</span><span class="p">(</span><span class="n">n_in</span><span class="p">,</span> <span class="n">n_out</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Xavier ì •ê·œ ì´ˆê¸°í™”&quot;&quot;&quot;</span>
    <span class="n">std</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="mi">2</span> <span class="o">/</span> <span class="p">(</span><span class="n">n_in</span> <span class="o">+</span> <span class="n">n_out</span><span class="p">))</span>
    <span class="k">return</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">normal</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">std</span><span class="p">,</span> <span class="p">(</span><span class="n">n_in</span><span class="p">,</span> <span class="n">n_out</span><span class="p">))</span>

<span class="c1"># ë¶„ì‚° ì „íŒŒ ê²€ì¦</span>
<span class="n">n_layers</span> <span class="o">=</span> <span class="mi">10</span>
<span class="n">layer_sizes</span> <span class="o">=</span> <span class="p">[</span><span class="mi">100</span><span class="p">]</span> <span class="o">*</span> <span class="p">(</span><span class="n">n_layers</span> <span class="o">+</span> <span class="mi">1</span><span class="p">)</span>
<span class="n">x</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">1000</span><span class="p">,</span> <span class="n">layer_sizes</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span>

<span class="n">activations</span> <span class="o">=</span> <span class="p">[</span><span class="n">x</span><span class="p">]</span>
<span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">n_layers</span><span class="p">):</span>
    <span class="n">W</span> <span class="o">=</span> <span class="n">xavier_uniform</span><span class="p">(</span><span class="n">layer_sizes</span><span class="p">[</span><span class="n">i</span><span class="p">],</span> <span class="n">layer_sizes</span><span class="p">[</span><span class="n">i</span><span class="o">+</span><span class="mi">1</span><span class="p">])</span>
    <span class="n">x</span> <span class="o">=</span> <span class="n">x</span> <span class="o">@</span> <span class="n">W</span>  <span class="c1"># ì„ í˜• ë³€í™˜ (í™œì„±í™” ì—†ìŒ)</span>
    <span class="n">activations</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>

<span class="c1"># ê° ë ˆì´ì–´ ë¶„ì‚° í™•ì¸</span>
<span class="n">variances</span> <span class="o">=</span> <span class="p">[</span><span class="n">np</span><span class="o">.</span><span class="n">var</span><span class="p">(</span><span class="n">a</span><span class="p">)</span> <span class="k">for</span> <span class="n">a</span> <span class="ow">in</span> <span class="n">activations</span><span class="p">]</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Xavier ì´ˆê¸°í™” - ë ˆì´ì–´ë³„ ë¶„ì‚°:&quot;</span><span class="p">)</span>
<span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">var</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">variances</span><span class="p">):</span>
    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;  ë ˆì´ì–´ </span><span class="si">{</span><span class="n">i</span><span class="si">}</span><span class="s2">: </span><span class="si">{</span><span class="n">var</span><span class="si">:</span><span class="s2">.4f</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
</code></pre></div>

<h3 id="53-he-initialization-for-relu">5.3 He Initialization (for ReLU)<a class="header-link" href="#53-he-initialization-for-relu" title="Permanent link">&para;</a></h3>
<p>ReLU zeroes out negative values, reducing variance by half.</p>
<p><strong>He initialization</strong>:
$$
\text{Var}(W) = \frac{2}{n_{\text{in}}}
$$</p>
<div class="highlight"><pre><span></span><code><span class="k">def</span><span class="w"> </span><span class="nf">he_normal</span><span class="p">(</span><span class="n">n_in</span><span class="p">,</span> <span class="n">n_out</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;He ì •ê·œ ì´ˆê¸°í™” (ReLUìš©)&quot;&quot;&quot;</span>
    <span class="n">std</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="mi">2</span> <span class="o">/</span> <span class="n">n_in</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">normal</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">std</span><span class="p">,</span> <span class="p">(</span><span class="n">n_in</span><span class="p">,</span> <span class="n">n_out</span><span class="p">))</span>

<span class="c1"># ReLU ë„¤íŠ¸ì›Œí¬ ì‹œë®¬ë ˆì´ì…˜</span>
<span class="n">x</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">1000</span><span class="p">,</span> <span class="mi">100</span><span class="p">)</span>
<span class="n">activations_he</span> <span class="o">=</span> <span class="p">[</span><span class="n">x</span><span class="p">]</span>

<span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">10</span><span class="p">):</span>
    <span class="n">W</span> <span class="o">=</span> <span class="n">he_normal</span><span class="p">(</span><span class="mi">100</span><span class="p">,</span> <span class="mi">100</span><span class="p">)</span>
    <span class="n">x</span> <span class="o">=</span> <span class="n">x</span> <span class="o">@</span> <span class="n">W</span>
    <span class="n">x</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">maximum</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">x</span><span class="p">)</span>  <span class="c1"># ReLU</span>
    <span class="n">activations_he</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>

<span class="n">variances_he</span> <span class="o">=</span> <span class="p">[</span><span class="n">np</span><span class="o">.</span><span class="n">var</span><span class="p">(</span><span class="n">a</span><span class="p">)</span> <span class="k">for</span> <span class="n">a</span> <span class="ow">in</span> <span class="n">activations_he</span><span class="p">]</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;</span><span class="se">\n</span><span class="s2">He ì´ˆê¸°í™” + ReLU - ë ˆì´ì–´ë³„ ë¶„ì‚°:&quot;</span><span class="p">)</span>
<span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">var</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">variances_he</span><span class="p">):</span>
    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;  ë ˆì´ì–´ </span><span class="si">{</span><span class="n">i</span><span class="si">}</span><span class="s2">: </span><span class="si">{</span><span class="n">var</span><span class="si">:</span><span class="s2">.4f</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>

<span class="c1"># Xavier vs He ë¹„êµ</span>
<span class="n">x_xavier</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">1000</span><span class="p">,</span> <span class="mi">100</span><span class="p">)</span>
<span class="n">activations_xavier</span> <span class="o">=</span> <span class="p">[</span><span class="n">x_xavier</span><span class="p">]</span>
<span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">10</span><span class="p">):</span>
    <span class="n">W</span> <span class="o">=</span> <span class="n">xavier_uniform</span><span class="p">(</span><span class="mi">100</span><span class="p">,</span> <span class="mi">100</span><span class="p">)</span>
    <span class="n">x_xavier</span> <span class="o">=</span> <span class="n">x_xavier</span> <span class="o">@</span> <span class="n">W</span>
    <span class="n">x_xavier</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">maximum</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">x_xavier</span><span class="p">)</span>
    <span class="n">activations_xavier</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">x_xavier</span><span class="p">)</span>

<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;</span><span class="se">\n</span><span class="s2">Xavier ì´ˆê¸°í™” + ReLU (ë¶€ì ì ˆ):&quot;</span><span class="p">)</span>
<span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">a</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">activations_xavier</span><span class="p">):</span>
    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;  ë ˆì´ì–´ </span><span class="si">{</span><span class="n">i</span><span class="si">}</span><span class="s2">: ë¶„ì‚° </span><span class="si">{</span><span class="n">np</span><span class="o">.</span><span class="n">var</span><span class="p">(</span><span class="n">a</span><span class="p">)</span><span class="si">:</span><span class="s2">.4f</span><span class="si">}</span><span class="s2">, 0 ë¹„ìœ¨ </span><span class="si">{</span><span class="p">(</span><span class="n">a</span><span class="o">==</span><span class="mi">0</span><span class="p">)</span><span class="o">.</span><span class="n">mean</span><span class="p">()</span><span class="si">:</span><span class="s2">.2%</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
</code></pre></div>

<h3 id="54-pytorch-initialization">5.4 PyTorch Initialization<a class="header-link" href="#54-pytorch-initialization" title="Permanent link">&para;</a></h3>
<div class="highlight"><pre><span></span><code><span class="kn">import</span><span class="w"> </span><span class="nn">torch.nn</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">nn</span>

<span class="c1"># ê¸°ë³¸ ì´ˆê¸°í™”</span>
<span class="n">layer</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="mi">100</span><span class="p">,</span> <span class="mi">50</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;ê¸°ë³¸ ì´ˆê¸°í™”: í‰ê·  </span><span class="si">{</span><span class="n">layer</span><span class="o">.</span><span class="n">weight</span><span class="o">.</span><span class="n">mean</span><span class="p">()</span><span class="si">:</span><span class="s2">.4f</span><span class="si">}</span><span class="s2">, í‘œì¤€í¸ì°¨ </span><span class="si">{</span><span class="n">layer</span><span class="o">.</span><span class="n">weight</span><span class="o">.</span><span class="n">std</span><span class="p">()</span><span class="si">:</span><span class="s2">.4f</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>

<span class="c1"># Xavier ì´ˆê¸°í™”</span>
<span class="n">nn</span><span class="o">.</span><span class="n">init</span><span class="o">.</span><span class="n">xavier_uniform_</span><span class="p">(</span><span class="n">layer</span><span class="o">.</span><span class="n">weight</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Xavier: í‰ê·  </span><span class="si">{</span><span class="n">layer</span><span class="o">.</span><span class="n">weight</span><span class="o">.</span><span class="n">mean</span><span class="p">()</span><span class="si">:</span><span class="s2">.4f</span><span class="si">}</span><span class="s2">, í‘œì¤€í¸ì°¨ </span><span class="si">{</span><span class="n">layer</span><span class="o">.</span><span class="n">weight</span><span class="o">.</span><span class="n">std</span><span class="p">()</span><span class="si">:</span><span class="s2">.4f</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>

<span class="c1"># He ì´ˆê¸°í™”</span>
<span class="n">nn</span><span class="o">.</span><span class="n">init</span><span class="o">.</span><span class="n">kaiming_normal_</span><span class="p">(</span><span class="n">layer</span><span class="o">.</span><span class="n">weight</span><span class="p">,</span> <span class="n">mode</span><span class="o">=</span><span class="s1">&#39;fan_in&#39;</span><span class="p">,</span> <span class="n">nonlinearity</span><span class="o">=</span><span class="s1">&#39;relu&#39;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;He: í‰ê·  </span><span class="si">{</span><span class="n">layer</span><span class="o">.</span><span class="n">weight</span><span class="o">.</span><span class="n">mean</span><span class="p">()</span><span class="si">:</span><span class="s2">.4f</span><span class="si">}</span><span class="s2">, í‘œì¤€í¸ì°¨ </span><span class="si">{</span><span class="n">layer</span><span class="o">.</span><span class="n">weight</span><span class="o">.</span><span class="n">std</span><span class="p">()</span><span class="si">:</span><span class="s2">.4f</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>

<span class="c1"># ì „ì²´ ëª¨ë¸ ì´ˆê¸°í™”</span>
<span class="k">def</span><span class="w"> </span><span class="nf">init_weights</span><span class="p">(</span><span class="n">m</span><span class="p">):</span>
    <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">m</span><span class="p">,</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">):</span>
        <span class="n">nn</span><span class="o">.</span><span class="n">init</span><span class="o">.</span><span class="n">kaiming_normal_</span><span class="p">(</span><span class="n">m</span><span class="o">.</span><span class="n">weight</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">m</span><span class="o">.</span><span class="n">bias</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">nn</span><span class="o">.</span><span class="n">init</span><span class="o">.</span><span class="n">constant_</span><span class="p">(</span><span class="n">m</span><span class="o">.</span><span class="n">bias</span><span class="p">,</span> <span class="mi">0</span><span class="p">)</span>

<span class="n">model</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Sequential</span><span class="p">(</span>
    <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="mi">100</span><span class="p">,</span> <span class="mi">50</span><span class="p">),</span>
    <span class="n">nn</span><span class="o">.</span><span class="n">ReLU</span><span class="p">(),</span>
    <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="mi">50</span><span class="p">,</span> <span class="mi">10</span><span class="p">)</span>
<span class="p">)</span>
<span class="n">model</span><span class="o">.</span><span class="n">apply</span><span class="p">(</span><span class="n">init_weights</span><span class="p">)</span>
</code></pre></div>

<hr />
<h2 id="6-normalization-and-residual-connections">6. Normalization and Residual Connections<a class="header-link" href="#6-normalization-and-residual-connections" title="Permanent link">&para;</a></h2>
<h3 id="61-batch-normalization">6.1 Batch Normalization<a class="header-link" href="#61-batch-normalization" title="Permanent link">&para;</a></h3>
<p><strong>Formula</strong>:
$$
\hat{x} = \frac{x - \mathbb{E}[x]}{\sqrt{\text{Var}[x] + \epsilon}}
$$</p>
<p>$$
y = \gamma \hat{x} + \beta
$$</p>
<p>where $\gamma, \beta$ are learnable parameters.</p>
<div class="highlight"><pre><span></span><code><span class="k">class</span><span class="w"> </span><span class="nc">BatchNorm1d</span><span class="p">:</span>
    <span class="k">def</span><span class="w"> </span><span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">num_features</span><span class="p">,</span> <span class="n">eps</span><span class="o">=</span><span class="mf">1e-5</span><span class="p">,</span> <span class="n">momentum</span><span class="o">=</span><span class="mf">0.1</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">eps</span> <span class="o">=</span> <span class="n">eps</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">momentum</span> <span class="o">=</span> <span class="n">momentum</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">gamma</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">ones</span><span class="p">(</span><span class="n">num_features</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">beta</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">num_features</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">running_mean</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">num_features</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">running_var</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">ones</span><span class="p">(</span><span class="n">num_features</span><span class="p">)</span>

    <span class="k">def</span><span class="w"> </span><span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">training</span><span class="o">=</span><span class="kc">True</span><span class="p">):</span>
        <span class="k">if</span> <span class="n">training</span><span class="p">:</span>
            <span class="c1"># ë°°ì¹˜ í†µê³„ ê³„ì‚°</span>
            <span class="n">batch_mean</span> <span class="o">=</span> <span class="n">x</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
            <span class="n">batch_var</span> <span class="o">=</span> <span class="n">x</span><span class="o">.</span><span class="n">var</span><span class="p">(</span><span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>

            <span class="c1"># ëŸ¬ë‹ í†µê³„ ì—…ë°ì´íŠ¸ (ì§€ìˆ˜ ì´ë™ í‰ê· )</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">running_mean</span> <span class="o">=</span> <span class="p">(</span><span class="mi">1</span> <span class="o">-</span> <span class="bp">self</span><span class="o">.</span><span class="n">momentum</span><span class="p">)</span> <span class="o">*</span> <span class="bp">self</span><span class="o">.</span><span class="n">running_mean</span> <span class="o">+</span> \
                                <span class="bp">self</span><span class="o">.</span><span class="n">momentum</span> <span class="o">*</span> <span class="n">batch_mean</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">running_var</span> <span class="o">=</span> <span class="p">(</span><span class="mi">1</span> <span class="o">-</span> <span class="bp">self</span><span class="o">.</span><span class="n">momentum</span><span class="p">)</span> <span class="o">*</span> <span class="bp">self</span><span class="o">.</span><span class="n">running_var</span> <span class="o">+</span> \
                               <span class="bp">self</span><span class="o">.</span><span class="n">momentum</span> <span class="o">*</span> <span class="n">batch_var</span>

            <span class="c1"># ì •ê·œí™”</span>
            <span class="n">x_norm</span> <span class="o">=</span> <span class="p">(</span><span class="n">x</span> <span class="o">-</span> <span class="n">batch_mean</span><span class="p">)</span> <span class="o">/</span> <span class="n">np</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="n">batch_var</span> <span class="o">+</span> <span class="bp">self</span><span class="o">.</span><span class="n">eps</span><span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="c1"># ì¶”ë¡  ì‹œ: ëŸ¬ë‹ í†µê³„ ì‚¬ìš©</span>
            <span class="n">x_norm</span> <span class="o">=</span> <span class="p">(</span><span class="n">x</span> <span class="o">-</span> <span class="bp">self</span><span class="o">.</span><span class="n">running_mean</span><span class="p">)</span> <span class="o">/</span> <span class="n">np</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">running_var</span> <span class="o">+</span> <span class="bp">self</span><span class="o">.</span><span class="n">eps</span><span class="p">)</span>

        <span class="c1"># ìŠ¤ì¼€ì¼/ì‹œí”„íŠ¸</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">gamma</span> <span class="o">*</span> <span class="n">x_norm</span> <span class="o">+</span> <span class="bp">self</span><span class="o">.</span><span class="n">beta</span>

<span class="c1"># í…ŒìŠ¤íŠ¸</span>
<span class="n">bn</span> <span class="o">=</span> <span class="n">BatchNorm1d</span><span class="p">(</span><span class="mi">10</span><span class="p">)</span>
<span class="n">x_train</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">32</span><span class="p">,</span> <span class="mi">10</span><span class="p">)</span> <span class="o">*</span> <span class="mi">10</span> <span class="o">+</span> <span class="mi">5</span>  <span class="c1"># í‰ê·  5, í‘œì¤€í¸ì°¨ 10</span>
<span class="n">x_normed</span> <span class="o">=</span> <span class="n">bn</span><span class="o">.</span><span class="n">forward</span><span class="p">(</span><span class="n">x_train</span><span class="p">,</span> <span class="n">training</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>

<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;ì…ë ¥: í‰ê·  </span><span class="si">{</span><span class="n">x_train</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span><span class="o">.</span><span class="n">mean</span><span class="p">()</span><span class="si">:</span><span class="s2">.2f</span><span class="si">}</span><span class="s2">, í‘œì¤€í¸ì°¨ </span><span class="si">{</span><span class="n">x_train</span><span class="o">.</span><span class="n">std</span><span class="p">(</span><span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span><span class="o">.</span><span class="n">mean</span><span class="p">()</span><span class="si">:</span><span class="s2">.2f</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;ì •ê·œí™” í›„: í‰ê·  </span><span class="si">{</span><span class="n">x_normed</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span><span class="o">.</span><span class="n">mean</span><span class="p">()</span><span class="si">:</span><span class="s2">.4f</span><span class="si">}</span><span class="s2">, í‘œì¤€í¸ì°¨ </span><span class="si">{</span><span class="n">x_normed</span><span class="o">.</span><span class="n">std</span><span class="p">(</span><span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span><span class="o">.</span><span class="n">mean</span><span class="p">()</span><span class="si">:</span><span class="s2">.4f</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
</code></pre></div>

<h3 id="62-layer-normalization">6.2 Layer Normalization<a class="header-link" href="#62-layer-normalization" title="Permanent link">&para;</a></h3>
<p>Normalizes along feature dimension instead of batch dimension (used in Transformers).</p>
<div class="highlight"><pre><span></span><code><span class="k">def</span><span class="w"> </span><span class="nf">layer_norm</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">gamma</span><span class="p">,</span> <span class="n">beta</span><span class="p">,</span> <span class="n">eps</span><span class="o">=</span><span class="mf">1e-5</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;ë ˆì´ì–´ ì •ê·œí™”&quot;&quot;&quot;</span>
    <span class="c1"># x: (batch, features)</span>
    <span class="n">mean</span> <span class="o">=</span> <span class="n">x</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">keepdims</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
    <span class="n">var</span> <span class="o">=</span> <span class="n">x</span><span class="o">.</span><span class="n">var</span><span class="p">(</span><span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">keepdims</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
    <span class="n">x_norm</span> <span class="o">=</span> <span class="p">(</span><span class="n">x</span> <span class="o">-</span> <span class="n">mean</span><span class="p">)</span> <span class="o">/</span> <span class="n">np</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="n">var</span> <span class="o">+</span> <span class="n">eps</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">gamma</span> <span class="o">*</span> <span class="n">x_norm</span> <span class="o">+</span> <span class="n">beta</span>

<span class="n">x</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">32</span><span class="p">,</span> <span class="mi">128</span><span class="p">)</span> <span class="o">*</span> <span class="mi">5</span> <span class="o">+</span> <span class="mi">10</span>
<span class="n">gamma</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">ones</span><span class="p">(</span><span class="mi">128</span><span class="p">)</span>
<span class="n">beta</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="mi">128</span><span class="p">)</span>
<span class="n">x_ln</span> <span class="o">=</span> <span class="n">layer_norm</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">gamma</span><span class="p">,</span> <span class="n">beta</span><span class="p">)</span>

<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;ë ˆì´ì–´ ì •ê·œí™”: ìƒ˜í”Œë³„ í‰ê·  </span><span class="si">{</span><span class="n">x_ln</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">)[:</span><span class="mi">5</span><span class="p">]</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
</code></pre></div>

<h3 id="63-gradient-flow-in-residual-connections">6.3 Gradient Flow in Residual Connections<a class="header-link" href="#63-gradient-flow-in-residual-connections" title="Permanent link">&para;</a></h3>
<p>ResNet's key insight: $y = F(x) + x$</p>
<p><strong>Gradient</strong>:
$$
\frac{\partial \mathcal{L}}{\partial x} = \frac{\partial \mathcal{L}}{\partial y} \left(1 + \frac{\partial F}{\partial x}\right)
$$</p>
<p>The identity path ($+1$) directly propagates gradients.</p>
<div class="highlight"><pre><span></span><code><span class="c1"># ì”ì°¨ ì—°ê²° íš¨ê³¼ ì‹œë®¬ë ˆì´ì…˜</span>
<span class="k">def</span><span class="w"> </span><span class="nf">plain_network</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">depth</span><span class="o">=</span><span class="mi">50</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;ì¼ë°˜ ë„¤íŠ¸ì›Œí¬&quot;&quot;&quot;</span>
    <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">depth</span><span class="p">):</span>
        <span class="n">W</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">100</span><span class="p">,</span> <span class="mi">100</span><span class="p">)</span> <span class="o">*</span> <span class="mf">0.01</span>
        <span class="n">x</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">tanh</span><span class="p">(</span><span class="n">x</span> <span class="o">@</span> <span class="n">W</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">x</span>

<span class="k">def</span><span class="w"> </span><span class="nf">residual_network</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">depth</span><span class="o">=</span><span class="mi">50</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;ì”ì°¨ ë„¤íŠ¸ì›Œí¬&quot;&quot;&quot;</span>
    <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">depth</span><span class="p">):</span>
        <span class="n">W</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">100</span><span class="p">,</span> <span class="mi">100</span><span class="p">)</span> <span class="o">*</span> <span class="mf">0.01</span>
        <span class="n">F_x</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">tanh</span><span class="p">(</span><span class="n">x</span> <span class="o">@</span> <span class="n">W</span><span class="p">)</span>
        <span class="n">x</span> <span class="o">=</span> <span class="n">F_x</span> <span class="o">+</span> <span class="n">x</span>  <span class="c1"># ì”ì°¨ ì—°ê²°</span>
    <span class="k">return</span> <span class="n">x</span>

<span class="n">x</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">10</span><span class="p">,</span> <span class="mi">100</span><span class="p">)</span>

<span class="n">y_plain</span> <span class="o">=</span> <span class="n">plain_network</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">depth</span><span class="o">=</span><span class="mi">50</span><span class="p">)</span>
<span class="n">y_residual</span> <span class="o">=</span> <span class="n">residual_network</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">depth</span><span class="o">=</span><span class="mi">50</span><span class="p">)</span>

<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;ì¼ë°˜ ë„¤íŠ¸ì›Œí¬: í‰ê·  </span><span class="si">{</span><span class="n">y_plain</span><span class="o">.</span><span class="n">mean</span><span class="p">()</span><span class="si">:</span><span class="s2">.6f</span><span class="si">}</span><span class="s2">, í‘œì¤€í¸ì°¨ </span><span class="si">{</span><span class="n">y_plain</span><span class="o">.</span><span class="n">std</span><span class="p">()</span><span class="si">:</span><span class="s2">.6f</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;ì”ì°¨ ë„¤íŠ¸ì›Œí¬: í‰ê·  </span><span class="si">{</span><span class="n">y_residual</span><span class="o">.</span><span class="n">mean</span><span class="p">()</span><span class="si">:</span><span class="s2">.6f</span><span class="si">}</span><span class="s2">, í‘œì¤€í¸ì°¨ </span><span class="si">{</span><span class="n">y_residual</span><span class="o">.</span><span class="n">std</span><span class="p">()</span><span class="si">:</span><span class="s2">.6f</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;</span><span class="se">\n</span><span class="s2">ì¼ë°˜ ë„¤íŠ¸ì›Œí¬ëŠ” ì‹ í˜¸ê°€ ì†Œì‹¤ë˜ì—ˆìŠµë‹ˆë‹¤!&quot;</span><span class="p">)</span>
</code></pre></div>

<h3 id="64-vanishingexploding-gradient-analysis">6.4 Vanishing/Exploding Gradient Analysis<a class="header-link" href="#64-vanishingexploding-gradient-analysis" title="Permanent link">&para;</a></h3>
<p><strong>Chain rule</strong>:
$$
\frac{\partial \mathcal{L}}{\partial W_1} = \frac{\partial \mathcal{L}}{\partial y_L} \prod_{i=2}^{L} \frac{\partial y_i}{\partial y_{i-1}}
$$</p>
<p><strong>Problem</strong>: Product of Jacobians
- $\|\frac{\partial y_i}{\partial y_{i-1}}\| < 1$ â†’ gradient vanishing
- $\|\frac{\partial y_i}{\partial y_{i-1}}\| > 1$ â†’ gradient explosion</p>
<p><strong>Solutions</strong>:
1. Proper initialization (Xavier, He)
2. Batch normalization
3. Residual connections
4. Gradient clipping</p>
<hr />
<h2 id="practice-problems">Practice Problems<a class="header-link" href="#practice-problems" title="Permanent link">&para;</a></h2>
<h3 id="problem-1-tensor-manipulation">Problem 1: Tensor Manipulation<a class="header-link" href="#problem-1-tensor-manipulation" title="Permanent link">&para;</a></h3>
<p>Implement the following in PyTorch:
(a) Transform (32, 3, 64, 64) image batch to (32, 64, 64, 3)
(b) Compute spatial mean for each image to create (32, 3) tensor
(c) Calculate per-channel standard deviation and normalize</p>
<h3 id="problem-2-einsum-mastery">Problem 2: einsum Mastery<a class="header-link" href="#problem-2-einsum-mastery" title="Permanent link">&para;</a></h3>
<p>Implement the following using einsum:
(a) Diagonal sum of 3D batch matrices: (batch, n, n) â†’ (batch,)
(b) Core operation of multi-head attention (Q, K multiplication)
(c) 4D tensor contraction: (a,b,c,d) Ã— (c,d,e,f) â†’ (a,b,e,f)</p>
<h3 id="problem-3-numerical-stability">Problem 3: Numerical Stability<a class="header-link" href="#problem-3-numerical-stability" title="Permanent link">&para;</a></h3>
<p>(a) Implement stable log-softmax function (using log-sum-exp)
(b) Test with very large logit values [1000, 2000, 3000]
(c) Compare results with PyTorch's F.log_softmax</p>
<h3 id="problem-4-initialization-experiments">Problem 4: Initialization Experiments<a class="header-link" href="#problem-4-initialization-experiments" title="Permanent link">&para;</a></h3>
<p>(a) Create 10-layer neural network with Xavier, He, and random(0.01) initialization
(b) Run forward pass with ReLU activation
(c) Plot activation variance at each layer and compare</p>
<h3 id="problem-5-batch-normalization-implementation">Problem 5: Batch Normalization Implementation<a class="header-link" href="#problem-5-batch-normalization-implementation" title="Permanent link">&para;</a></h3>
<p>(a) Implement 2D batch normalization from scratch (forward + backward)
(b) Compare results with PyTorch's nn.BatchNorm2d
(c) Verify difference between training/inference modes</p>
<hr />
<h2 id="references">References<a class="header-link" href="#references" title="Permanent link">&para;</a></h2>
<h3 id="books">Books<a class="header-link" href="#books" title="Permanent link">&para;</a></h3>
<ul>
<li><strong>Deep Learning</strong> (Goodfellow et al., 2016) - Chapter 6 (Deep Networks), Chapter 8 (Optimization)</li>
<li><strong>Dive into Deep Learning</strong> (Zhang et al.) - Interactive book with code</li>
</ul>
<h3 id="papers">Papers<a class="header-link" href="#papers" title="Permanent link">&para;</a></h3>
<ul>
<li>Glorot &amp; Bengio (2010), "Understanding the difficulty of training deep feedforward neural networks" - Xavier initialization</li>
<li>He et al. (2015), "Delving Deep into Rectifiers" - He initialization</li>
<li>Ioffe &amp; Szegedy (2015), "Batch Normalization"</li>
<li>He et al. (2016), "Deep Residual Learning for Image Recognition" - ResNet</li>
</ul>
<h3 id="online-resources">Online Resources<a class="header-link" href="#online-resources" title="Permanent link">&para;</a></h3>
<ul>
<li><a href="https://pytorch.org/docs/stable/generated/torch.einsum.html">PyTorch einsum tutorial</a></li>
<li><a href="https://rockt.github.io/2018/04/30/einsum">Efficient Attention with einsum</a></li>
<li><a href="https://towardsdatascience.com">Numerical Stability in Deep Learning</a></li>
<li><a href="https://pytorch.org/docs/stable/nn.init.html">Weight Initialization Guide</a></li>
</ul>
<h3 id="tools">Tools<a class="header-link" href="#tools" title="Permanent link">&para;</a></h3>
<ul>
<li><strong>PyTorch</strong>: Automatic differentiation and tensor operations</li>
<li><strong>NumPy</strong>: Numerical computation</li>
<li><strong>TensorBoard</strong>: Activation/gradient visualization</li>
</ul>
    </div>

    
<div class="lesson-toolbar lesson-toolbar--bottom">
    <div class="toolbar-nav toolbar-nav--prev">
        
        <a href="/study/en/Math_for_AI/12_Sampling_and_Monte_Carlo.html" class="nav-prev">
            <span class="nav-label">Previous</span>
            <span class="nav-title">12. Sampling and Monte Carlo Methods</span>
        </a>
        
    </div>
    <div class="toolbar-actions">
        <button class="btn btn-copy-link" data-action="copy-link" title="Copy link">
            <span class="icon">ğŸ”—</span>
            <span class="text">Copy link</span>
        </button>
        <a href="/study/en/Math_for_AI/" class="btn btn-topic-link" title="Back to topic list">
            <span class="icon">ğŸ“‹</span>
            <span class="text">Topic list</span>
        </a>
    </div>
    <div class="toolbar-nav toolbar-nav--next">
        
        <a href="/study/en/Math_for_AI/14_Convexity_and_Duality.html" class="nav-next">
            <span class="nav-label">Next</span>
            <span class="nav-title">14. Convexity and Duality</span>
        </a>
        
    </div>
</div>


    <div class="toolbar-keyboard-hint">
        <kbd>&larr;</kbd> <kbd>&rarr;</kbd> to navigate between lessons
    </div>
</article>

<!-- Scroll to top floating button -->
<button class="scroll-to-top" id="scroll-to-top" title="Scroll to top" aria-label="Scroll to top">â†‘</button>

            </div>
        </main>
    </div>

    <script src="/study/static/js/app.js"></script>
    <script>
        function switchLanguage(newLang) {
            const path = window.location.pathname;
            const base = '/study';
            const currentLang = 'en';
            const newPath = path.replace(base + '/' + currentLang + '/', base + '/' + newLang + '/');
            window.location.href = newPath + window.location.search;
        }
    </script>
    
<script>
    document.addEventListener('DOMContentLoaded', function() {
        // Copy link - independent feedback per button
        document.querySelectorAll('[data-action="copy-link"]').forEach(function(btn) {
            btn.addEventListener('click', function() {
                navigator.clipboard.writeText(window.location.href);
                var textEl = this.querySelector('.text');
                var originalText = textEl.textContent;
                textEl.textContent = 'Copied!';
                setTimeout(function() { textEl.textContent = originalText; }, 2000);
            });
        });

        // Add copy buttons to code blocks
        document.querySelectorAll('.lesson-content pre').forEach(function(pre) {
            var copyBtn = document.createElement('button');
            copyBtn.className = 'code-copy-btn';
            copyBtn.textContent = 'Copy';
            copyBtn.addEventListener('click', function() {
                var code = pre.querySelector('code');
                var text = code ? code.textContent : pre.textContent;
                navigator.clipboard.writeText(text);
                copyBtn.textContent = 'Copied!';
                setTimeout(function() { copyBtn.textContent = 'Copy'; }, 2000);
            });
            pre.style.position = 'relative';
            pre.appendChild(copyBtn);
        });

        // Scroll to top button
        var scrollBtn = document.getElementById('scroll-to-top');
        window.addEventListener('scroll', function() {
            scrollBtn.classList.toggle('visible', window.scrollY > 300);
        });
        scrollBtn.addEventListener('click', function() {
            window.scrollTo({ top: 0, behavior: 'smooth' });
        });

        // Keyboard shortcuts: left/right arrows for lesson navigation
        document.addEventListener('keydown', function(e) {
            var tag = document.activeElement.tagName;
            if (tag === 'INPUT' || tag === 'TEXTAREA' || tag === 'SELECT') return;

            if (e.key === 'ArrowLeft') {
                var prevLink = document.querySelector('.nav-prev');
                if (prevLink) prevLink.click();
            } else if (e.key === 'ArrowRight') {
                var nextLink = document.querySelector('.nav-next');
                if (nextLink) nextLink.click();
            }
        });
    });
</script>

</body>
</html>
{% endraw %}