{% raw %}
<!DOCTYPE html>
<html lang="en" data-theme="light">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>18. Mathematics of Generative Models - Study Materials</title>
    <link rel="stylesheet" href="/study/static/css/style.css">
    <link rel="stylesheet" href="/study/static/css/highlight.css">
    <!-- KaTeX for LaTeX math rendering -->
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.11/dist/katex.min.css">
    <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.11/dist/katex.min.js"></script>
    <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.11/dist/contrib/auto-render.min.js"
        onload="renderMathInElement(document.body, {
            delimiters: [
                {left: '$$', right: '$$', display: true},
                {left: '$', right: '$', display: false}
            ],
            throwOnError: false
        });"></script>
    
</head>
<body>
    <div class="app-container">
        <!-- Sidebar -->
        <aside class="sidebar" id="sidebar">
            <div class="sidebar-header">
                <a href="/study/en/" class="logo">Study Materials</a>
                <button class="sidebar-toggle" id="sidebar-toggle" aria-label="Toggle sidebar">
                    <span></span>
                </button>
            </div>

            <nav class="sidebar-nav">
                <a href="/study/en/" class="nav-item ">
                    <span class="nav-icon">üè†</span>
                    <span class="nav-text">Home</span>
                </a>
                <a href="/study/examples/" class="nav-item ">
                    <span class="nav-icon">üíª</span>
                    <span class="nav-text">Examples</span>
                </a>
            </nav>

            <div class="sidebar-search">
                <form action="/study/en/search.html" method="get" id="search-form">
                    <input type="search" name="q" placeholder="Search..." id="search-sidebar-input">
                </form>
            </div>

            <!-- Language Selector -->
            <div class="sidebar-lang">
                <select id="lang-select" class="lang-selector" onchange="switchLanguage(this.value)">
                    
                    <option value="en" selected>
                        English
                    </option>
                    
                    <option value="ko" >
                        ÌïúÍµ≠Ïñ¥
                    </option>
                    
                </select>
            </div>

            <div class="sidebar-footer">
                <button class="theme-toggle" id="theme-toggle" aria-label="Toggle theme">
                    <span class="theme-icon light">‚òÄÔ∏è</span>
                    <span class="theme-icon dark">üåô</span>
                </button>
            </div>
        </aside>

        <!-- Main Content -->
        <main class="main-content">
            <header class="main-header">
                <button class="menu-toggle" id="menu-toggle" aria-label="Open menu">
                    <span></span>
                </button>
                
<nav class="breadcrumb">
    <a href="/study/en/">Topics</a>
    <span class="separator">/</span>
    <a href="/study/en/Math_for_AI/">Math for AI</a>
    <span class="separator">/</span>
    <span class="current">18. Mathematics of Generative Models</span>
</nav>

            </header>

            <div class="content">
                
<article class="lesson-article">
    <header class="lesson-header">
        <h1>18. Mathematics of Generative Models</h1>
    </header>

    
<div class="lesson-toolbar lesson-toolbar--top">
    <div class="toolbar-nav toolbar-nav--prev">
        
        <a href="/study/en/Math_for_AI/17_Math_of_Attention_and_Transformers.html" class="nav-prev">
            <span class="nav-label">Previous</span>
            <span class="nav-title">17. Mathematics of Attention and Transformers</span>
        </a>
        
    </div>
    <div class="toolbar-actions">
        <button class="btn btn-copy-link" data-action="copy-link" title="Copy link">
            <span class="icon">üîó</span>
            <span class="text">Copy link</span>
        </button>
        <a href="/study/en/Math_for_AI/" class="btn btn-topic-link" title="Back to topic list">
            <span class="icon">üìã</span>
            <span class="text">Topic list</span>
        </a>
    </div>
    <div class="toolbar-nav toolbar-nav--next">
        
    </div>
</div>


    
    <nav class="toc" id="toc">
        <h2>Table of Contents</h2>
        <div class="toc">
<ul>
<li><a href="#learning-objectives">Learning Objectives</a></li>
<li><a href="#1-goals-of-generative-models">1. Goals of Generative Models</a><ul>
<li><a href="#11-learning-data-distributions">1.1 Learning Data Distributions</a></li>
<li><a href="#12-explicit-vs-implicit-density">1.2 Explicit vs Implicit Density</a></li>
<li><a href="#13-training-paradigms">1.3 Training Paradigms</a></li>
</ul>
</li>
<li><a href="#2-mathematics-of-vae-complete-elbo-derivation">2. Mathematics of VAE: Complete ELBO Derivation</a><ul>
<li><a href="#21-latent-variable-model">2.1 Latent Variable Model</a></li>
<li><a href="#22-elbo-derivation-method-1-jensens-inequality">2.2 ELBO Derivation (Method 1: Jensen's Inequality)</a></li>
<li><a href="#23-elbo-derivation-method-2-kl-decomposition">2.3 ELBO Derivation (Method 2: KL Decomposition)</a></li>
<li><a href="#24-interpretation-of-the-two-elbo-terms">2.4 Interpretation of the Two ELBO Terms</a></li>
<li><a href="#25-reparameterization-trick">2.5 Reparameterization Trick</a></li>
<li><a href="#26-closed-form-kl-divergence-for-gaussians">2.6 Closed-Form KL Divergence for Gaussians</a></li>
</ul>
</li>
<li><a href="#3-mathematics-of-gans">3. Mathematics of GANs</a><ul>
<li><a href="#31-minimax-game">3.1 Minimax Game</a></li>
<li><a href="#32-optimal-discriminator">3.2 Optimal Discriminator</a></li>
<li><a href="#33-relationship-to-js-divergence">3.3 Relationship to JS Divergence</a></li>
<li><a href="#34-mathematical-causes-of-training-instability">3.4 Mathematical Causes of Training Instability</a></li>
</ul>
</li>
<li><a href="#4-wasserstein-distance-and-wgan">4. Wasserstein Distance and WGAN</a><ul>
<li><a href="#41-optimal-transport">4.1 Optimal Transport</a></li>
<li><a href="#42-kantorovich-rubinstein-duality">4.2 Kantorovich-Rubinstein Duality</a></li>
<li><a href="#43-wgan-objective-function">4.3 WGAN Objective Function</a></li>
<li><a href="#44-lipschitz-constraint-gradient-penalty">4.4 Lipschitz Constraint: Gradient Penalty</a></li>
</ul>
</li>
<li><a href="#5-mathematics-of-diffusion-models">5. Mathematics of Diffusion Models</a><ul>
<li><a href="#51-forward-process">5.1 Forward Process</a></li>
<li><a href="#52-reverse-process">5.2 Reverse Process</a></li>
<li><a href="#53-ddpm-loss-derivation">5.3 DDPM Loss Derivation</a></li>
<li><a href="#54-relationship-to-score-matching">5.4 Relationship to Score Matching</a></li>
<li><a href="#55-langevin-dynamics">5.5 Langevin Dynamics</a></li>
</ul>
</li>
<li><a href="#6-recent-advances-flow-matching-and-cnf">6. Recent Advances: Flow Matching and CNF</a><ul>
<li><a href="#61-continuous-normalizing-flow-cnf">6.1 Continuous Normalizing Flow (CNF)</a></li>
<li><a href="#62-flow-matching">6.2 Flow Matching</a></li>
<li><a href="#63-consistency-models">6.3 Consistency Models</a></li>
</ul>
</li>
<li><a href="#practice-problems">Practice Problems</a><ul>
<li><a href="#problem-1-two-methods-of-elbo-derivation">Problem 1: Two Methods of ELBO Derivation</a></li>
<li><a href="#problem-2-vae-implementation-and-experiments">Problem 2: VAE Implementation and Experiments</a></li>
<li><a href="#problem-3-proof-of-optimal-discriminator-for-gan">Problem 3: Proof of Optimal Discriminator for GAN</a></li>
<li><a href="#problem-4-forward-process-of-diffusion-models">Problem 4: Forward Process of Diffusion Models</a></li>
<li><a href="#problem-5-flow-matching-vs-diffusion-models">Problem 5: Flow Matching vs Diffusion Models</a></li>
</ul>
</li>
<li><a href="#references">References</a><ul>
<li><a href="#papers">Papers</a></li>
<li><a href="#online-resources">Online Resources</a></li>
<li><a href="#libraries">Libraries</a></li>
</ul>
</li>
</ul>
</div>

    </nav>
    

    <div class="lesson-content markdown-body">
        <h1 id="18-mathematics-of-generative-models">18. Mathematics of Generative Models<a class="header-link" href="#18-mathematics-of-generative-models" title="Permanent link">&para;</a></h1>
<h2 id="learning-objectives">Learning Objectives<a class="header-link" href="#learning-objectives" title="Permanent link">&para;</a></h2>
<ul>
<li>Understand the goals of generative models and the differences between explicit and implicit density models</li>
<li>Fully derive the ELBO of VAE and explain the meaning of reconstruction and KL terms</li>
<li>Understand the minimax game theory of GANs and the relationship to JS divergence</li>
<li>Understand the basics of Wasserstein distance and optimal transport theory</li>
<li>Understand the mathematics of the forward/reverse process of diffusion models and score matching</li>
<li>Understand recent advances in Flow Matching and Continuous Normalizing Flows (CNF)</li>
</ul>
<hr />
<h2 id="1-goals-of-generative-models">1. Goals of Generative Models<a class="header-link" href="#1-goals-of-generative-models" title="Permanent link">&para;</a></h2>
<h3 id="11-learning-data-distributions">1.1 Learning Data Distributions<a class="header-link" href="#11-learning-data-distributions" title="Permanent link">&para;</a></h3>
<p><strong>Goal</strong>: Learn the true data distribution $p_{\text{data}}$ from observed data $\{\mathbf{x}^{(1)}, \ldots, \mathbf{x}^{(n)}\} \sim p_{\text{data}}(\mathbf{x})$</p>
<p><strong>Applications</strong>:
- <strong>Sampling</strong>: Generate new data
- <strong>Density estimation</strong>: Compute $p(\mathbf{x})$ (anomaly detection)
- <strong>Conditional generation</strong>: $p(\mathbf{y}|\mathbf{x})$ (image-text, style transfer)</p>
<h3 id="12-explicit-vs-implicit-density">1.2 Explicit vs Implicit Density<a class="header-link" href="#12-explicit-vs-implicit-density" title="Permanent link">&para;</a></h3>
<p><strong>Explicit density models</strong>:
- Directly define probability density $p_\theta(\mathbf{x})$
- Maximize likelihood $\log p_\theta(\mathbf{x})$
- Examples: VAE, autoregressive models, normalizing flows</p>
<p><strong>Implicit density models</strong>:
- Define only sampling process without specifying density
- $\mathbf{x} = G_\theta(\mathbf{z})$, $\mathbf{z} \sim p(\mathbf{z})$
- Example: GAN</p>
<h3 id="13-training-paradigms">1.3 Training Paradigms<a class="header-link" href="#13-training-paradigms" title="Permanent link">&para;</a></h3>
<table>
<thead>
<tr>
<th>Method</th>
<th>Objective</th>
<th>Sampling</th>
<th>Density Evaluation</th>
</tr>
</thead>
<tbody>
<tr>
<td><strong>VAE</strong></td>
<td>Maximize ELBO</td>
<td>Fast</td>
<td>Approximate</td>
</tr>
<tr>
<td><strong>GAN</strong></td>
<td>Adversarial game</td>
<td>Fast</td>
<td>Not possible</td>
</tr>
<tr>
<td><strong>Normalizing Flow</strong></td>
<td>Exact likelihood</td>
<td>Fast</td>
<td>Exact</td>
</tr>
<tr>
<td><strong>Diffusion Models</strong></td>
<td>Noise prediction</td>
<td>Slow (iterative)</td>
<td>Implicit</td>
</tr>
</tbody>
</table>
<div class="highlight"><pre><span></span><code><span class="kn">import</span><span class="w"> </span><span class="nn">numpy</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">np</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">matplotlib.pyplot</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">plt</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">scipy.stats</span><span class="w"> </span><span class="kn">import</span> <span class="n">norm</span>

<span class="c1"># Í∞ÑÎã®Ìïú 1D Îç∞Ïù¥ÌÑ∞ Î∂ÑÌè¨ (ÌòºÌï© Í∞ÄÏö∞ÏãúÏïà)</span>
<span class="k">def</span><span class="w"> </span><span class="nf">generate_data</span><span class="p">(</span><span class="n">n_samples</span><span class="o">=</span><span class="mi">1000</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;ÏßÑÏßú Îç∞Ïù¥ÌÑ∞ Î∂ÑÌè¨: Îëê Í∞úÏùò Í∞ÄÏö∞ÏãúÏïà ÌòºÌï©&quot;&quot;&quot;</span>
    <span class="n">modes</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">choice</span><span class="p">([</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">],</span> <span class="n">size</span><span class="o">=</span><span class="n">n_samples</span><span class="p">,</span> <span class="n">p</span><span class="o">=</span><span class="p">[</span><span class="mf">0.3</span><span class="p">,</span> <span class="mf">0.7</span><span class="p">])</span>
    <span class="n">data</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">where</span><span class="p">(</span><span class="n">modes</span> <span class="o">==</span> <span class="mi">0</span><span class="p">,</span>
                    <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="n">n_samples</span><span class="p">)</span> <span class="o">*</span> <span class="mf">0.5</span> <span class="o">-</span> <span class="mi">2</span><span class="p">,</span>
                    <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="n">n_samples</span><span class="p">)</span> <span class="o">*</span> <span class="mf">0.7</span> <span class="o">+</span> <span class="mi">2</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">data</span>

<span class="n">data</span> <span class="o">=</span> <span class="n">generate_data</span><span class="p">(</span><span class="n">n_samples</span><span class="o">=</span><span class="mi">5000</span><span class="p">)</span>

<span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">10</span><span class="p">,</span> <span class="mi">4</span><span class="p">))</span>
<span class="n">plt</span><span class="o">.</span><span class="n">hist</span><span class="p">(</span><span class="n">data</span><span class="p">,</span> <span class="n">bins</span><span class="o">=</span><span class="mi">50</span><span class="p">,</span> <span class="n">density</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.7</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s1">&#39;True data distribution&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s1">&#39;x&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s1">&#39;Density&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s1">&#39;Example data distribution (mixture of Gaussians)&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">legend</span><span class="p">()</span>
<span class="n">plt</span><span class="o">.</span><span class="n">grid</span><span class="p">(</span><span class="kc">True</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.3</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">tight_layout</span><span class="p">()</span>
<span class="n">plt</span><span class="o">.</span><span class="n">savefig</span><span class="p">(</span><span class="s1">&#39;data_distribution.png&#39;</span><span class="p">,</span> <span class="n">dpi</span><span class="o">=</span><span class="mi">150</span><span class="p">,</span> <span class="n">bbox_inches</span><span class="o">=</span><span class="s1">&#39;tight&#39;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Îç∞Ïù¥ÌÑ∞ Î∂ÑÌè¨ ÏãúÍ∞ÅÌôî Ï†ÄÏû• ÏôÑÎ£å&quot;</span><span class="p">)</span>
</code></pre></div>

<h2 id="2-mathematics-of-vae-complete-elbo-derivation">2. Mathematics of VAE: Complete ELBO Derivation<a class="header-link" href="#2-mathematics-of-vae-complete-elbo-derivation" title="Permanent link">&para;</a></h2>
<h3 id="21-latent-variable-model">2.1 Latent Variable Model<a class="header-link" href="#21-latent-variable-model" title="Permanent link">&para;</a></h3>
<p><strong>Generative process</strong>:
1. Sample latent variable: $\mathbf{z} \sim p(\mathbf{z})$ (usually $\mathcal{N}(\mathbf{0}, I)$)
2. Generate data: $\mathbf{x} \sim p_\theta(\mathbf{x}|\mathbf{z})$</p>
<p><strong>Goal</strong>: Maximize marginal likelihood</p>
<p>$$\log p_\theta(\mathbf{x}) = \log \int p_\theta(\mathbf{x}|\mathbf{z}) p(\mathbf{z}) d\mathbf{z}$$</p>
<p><strong>Problem</strong>: Integral is intractable</p>
<h3 id="22-elbo-derivation-method-1-jensens-inequality">2.2 ELBO Derivation (Method 1: Jensen's Inequality)<a class="header-link" href="#22-elbo-derivation-method-1-jensens-inequality" title="Permanent link">&para;</a></h3>
<p>Using concavity of log and Jensen's inequality:</p>
<p>$$\log p_\theta(\mathbf{x}) = \log \mathbb{E}_{q_\phi(\mathbf{z}|\mathbf{x})} \left[ \frac{p_\theta(\mathbf{x}, \mathbf{z})}{q_\phi(\mathbf{z}|\mathbf{x})} \right]$$</p>
<p>$$\geq \mathbb{E}_{q_\phi(\mathbf{z}|\mathbf{x})} \left[ \log \frac{p_\theta(\mathbf{x}, \mathbf{z})}{q_\phi(\mathbf{z}|\mathbf{x})} \right]$$</p>
<p>$$= \mathbb{E}_{q_\phi(\mathbf{z}|\mathbf{x})} \left[ \log p_\theta(\mathbf{x}|\mathbf{z}) + \log p(\mathbf{z}) - \log q_\phi(\mathbf{z}|\mathbf{x}) \right]$$</p>
<p>$$= \mathbb{E}_{q_\phi(\mathbf{z}|\mathbf{x})} \left[ \log p_\theta(\mathbf{x}|\mathbf{z}) \right] - D_{\text{KL}}(q_\phi(\mathbf{z}|\mathbf{x}) \| p(\mathbf{z}))$$</p>
<p>This is the <strong>ELBO</strong> (Evidence Lower BOund):</p>
<p>$$\mathcal{L}(\theta, \phi; \mathbf{x}) = \mathbb{E}_{q_\phi(\mathbf{z}|\mathbf{x})} \left[ \log p_\theta(\mathbf{x}|\mathbf{z}) \right] - D_{\text{KL}}(q_\phi(\mathbf{z}|\mathbf{x}) \| p(\mathbf{z}))$$</p>
<h3 id="23-elbo-derivation-method-2-kl-decomposition">2.3 ELBO Derivation (Method 2: KL Decomposition)<a class="header-link" href="#23-elbo-derivation-method-2-kl-decomposition" title="Permanent link">&para;</a></h3>
<p>KL divergence between posterior $p_\theta(\mathbf{z}|\mathbf{x})$ and approximation $q_\phi(\mathbf{z}|\mathbf{x})$:</p>
<p>$$D_{\text{KL}}(q_\phi(\mathbf{z}|\mathbf{x}) \| p_\theta(\mathbf{z}|\mathbf{x})) = \mathbb{E}_{q_\phi} \left[ \log \frac{q_\phi(\mathbf{z}|\mathbf{x})}{p_\theta(\mathbf{z}|\mathbf{x})} \right]$$</p>
<p>Bayes' theorem: $p_\theta(\mathbf{z}|\mathbf{x}) = \frac{p_\theta(\mathbf{x}|\mathbf{z})p(\mathbf{z})}{p_\theta(\mathbf{x})}$</p>
<p>$$D_{\text{KL}}(q_\phi \| p_\theta) = \mathbb{E}_{q_\phi} \left[ \log q_\phi(\mathbf{z}|\mathbf{x}) - \log p_\theta(\mathbf{x}|\mathbf{z}) - \log p(\mathbf{z}) + \log p_\theta(\mathbf{x}) \right]$$</p>
<p>Rearranging:</p>
<p>$$\log p_\theta(\mathbf{x}) = D_{\text{KL}}(q_\phi \| p_\theta) + \mathcal{L}(\theta, \phi; \mathbf{x})$$</p>
<p>Since $D_{\text{KL}} \geq 0$, $\mathcal{L}$ is a lower bound for $\log p_\theta(\mathbf{x})$.</p>
<h3 id="24-interpretation-of-the-two-elbo-terms">2.4 Interpretation of the Two ELBO Terms<a class="header-link" href="#24-interpretation-of-the-two-elbo-terms" title="Permanent link">&para;</a></h3>
<p>$$\mathcal{L} = \underbrace{\mathbb{E}_{q_\phi(\mathbf{z}|\mathbf{x})} \left[ \log p_\theta(\mathbf{x}|\mathbf{z}) \right]}_{\text{Reconstruction term}} - \underbrace{D_{\text{KL}}(q_\phi(\mathbf{z}|\mathbf{x}) \| p(\mathbf{z}))}_{\text{Regularization term}}$$</p>
<p><strong>Reconstruction term</strong>:
- How well does it reconstruct $\mathbf{x}$ from latent representation $\mathbf{z}$
- Quality of decoder $p_\theta(\mathbf{x}|\mathbf{z})$</p>
<p><strong>Regularization term</strong>:
- Keeps encoder distribution close to prior
- Structures the latent space
- Prevents overfitting</p>
<h3 id="25-reparameterization-trick">2.5 Reparameterization Trick<a class="header-link" href="#25-reparameterization-trick" title="Permanent link">&para;</a></h3>
<p><strong>Problem</strong>: How to compute $\nabla_\phi \mathbb{E}_{q_\phi(\mathbf{z}|\mathbf{x})} [f(\mathbf{z})]$?</p>
<p><strong>Solution</strong>: Express $\mathbf{z}$ as deterministic function + noise</p>
<p>For Gaussian case: $q_\phi(\mathbf{z}|\mathbf{x}) = \mathcal{N}(\boldsymbol{\mu}_\phi(\mathbf{x}), \boldsymbol{\sigma}_\phi^2(\mathbf{x}))$</p>
<p>$$\mathbf{z} = \boldsymbol{\mu}_\phi(\mathbf{x}) + \boldsymbol{\sigma}_\phi(\mathbf{x}) \odot \boldsymbol{\epsilon}, \quad \boldsymbol{\epsilon} \sim \mathcal{N}(\mathbf{0}, I)$$</p>
<p>Now $\mathbf{z}$ is differentiable with respect to $\phi$.</p>
<div class="highlight"><pre><span></span><code><span class="kn">import</span><span class="w"> </span><span class="nn">torch</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">torch.nn</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">nn</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">torch.nn.functional</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">F</span>

<span class="k">class</span><span class="w"> </span><span class="nc">VAE</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="k">def</span><span class="w"> </span><span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">input_dim</span><span class="o">=</span><span class="mi">784</span><span class="p">,</span> <span class="n">latent_dim</span><span class="o">=</span><span class="mi">20</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>

        <span class="c1"># Ïù∏ÏΩîÎçî</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">fc1</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">input_dim</span><span class="p">,</span> <span class="mi">400</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">fc_mu</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="mi">400</span><span class="p">,</span> <span class="n">latent_dim</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">fc_logvar</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="mi">400</span><span class="p">,</span> <span class="n">latent_dim</span><span class="p">)</span>

        <span class="c1"># ÎîîÏΩîÎçî</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">fc3</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">latent_dim</span><span class="p">,</span> <span class="mi">400</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">fc4</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="mi">400</span><span class="p">,</span> <span class="n">input_dim</span><span class="p">)</span>

    <span class="k">def</span><span class="w"> </span><span class="nf">encode</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Ïù∏ÏΩîÎçî: x -&gt; (mu, logvar)&quot;&quot;&quot;</span>
        <span class="n">h</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">relu</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">fc1</span><span class="p">(</span><span class="n">x</span><span class="p">))</span>
        <span class="n">mu</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">fc_mu</span><span class="p">(</span><span class="n">h</span><span class="p">)</span>
        <span class="n">logvar</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">fc_logvar</span><span class="p">(</span><span class="n">h</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">mu</span><span class="p">,</span> <span class="n">logvar</span>

    <span class="k">def</span><span class="w"> </span><span class="nf">reparameterize</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">mu</span><span class="p">,</span> <span class="n">logvar</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Ïû¨Îß§Í∞úÎ≥ÄÏàòÌôî: z = mu + sigma * epsilon&quot;&quot;&quot;</span>
        <span class="n">std</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="mf">0.5</span> <span class="o">*</span> <span class="n">logvar</span><span class="p">)</span>
        <span class="n">eps</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn_like</span><span class="p">(</span><span class="n">std</span><span class="p">)</span>
        <span class="n">z</span> <span class="o">=</span> <span class="n">mu</span> <span class="o">+</span> <span class="n">std</span> <span class="o">*</span> <span class="n">eps</span>
        <span class="k">return</span> <span class="n">z</span>

    <span class="k">def</span><span class="w"> </span><span class="nf">decode</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">z</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;ÎîîÏΩîÎçî: z -&gt; x_recon&quot;&quot;&quot;</span>
        <span class="n">h</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">relu</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">fc3</span><span class="p">(</span><span class="n">z</span><span class="p">))</span>
        <span class="n">x_recon</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">sigmoid</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">fc4</span><span class="p">(</span><span class="n">h</span><span class="p">))</span>
        <span class="k">return</span> <span class="n">x_recon</span>

    <span class="k">def</span><span class="w"> </span><span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
        <span class="n">mu</span><span class="p">,</span> <span class="n">logvar</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">encode</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="n">z</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">reparameterize</span><span class="p">(</span><span class="n">mu</span><span class="p">,</span> <span class="n">logvar</span><span class="p">)</span>
        <span class="n">x_recon</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">decode</span><span class="p">(</span><span class="n">z</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">x_recon</span><span class="p">,</span> <span class="n">mu</span><span class="p">,</span> <span class="n">logvar</span>

<span class="k">def</span><span class="w"> </span><span class="nf">vae_loss</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">x_recon</span><span class="p">,</span> <span class="n">mu</span><span class="p">,</span> <span class="n">logvar</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    VAE ÏÜêÏã§ Ìï®Ïàò</span>

<span class="sd">    Parameters:</span>
<span class="sd">    -----------</span>
<span class="sd">    x : Tensor</span>
<span class="sd">        ÏõêÎ≥∏ ÏûÖÎ†•</span>
<span class="sd">    x_recon : Tensor</span>
<span class="sd">        Ïû¨Íµ¨ÏÑ±Îêú Ï∂úÎ†•</span>
<span class="sd">    mu, logvar : Tensor</span>
<span class="sd">        Ïû†Ïû¨ Î∂ÑÌè¨Ïùò ÌèâÍ∑†Í≥º Î°úÍ∑∏ Î∂ÑÏÇ∞</span>

<span class="sd">    Returns:</span>
<span class="sd">    --------</span>
<span class="sd">    loss : Tensor</span>
<span class="sd">        Ï¥ù ÏÜêÏã§ (Ïû¨Íµ¨ÏÑ± + KL)</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="c1"># Ïû¨Íµ¨ÏÑ± ÏÜêÏã§ (Bernoulli Î∂ÑÌè¨ Í∞ÄÏ†ï)</span>
    <span class="n">recon_loss</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">binary_cross_entropy</span><span class="p">(</span><span class="n">x_recon</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">reduction</span><span class="o">=</span><span class="s1">&#39;sum&#39;</span><span class="p">)</span>

    <span class="c1"># KL Î∞úÏÇ∞ (Í∞ÄÏö∞ÏãúÏïà Í∞ÑÏùò Îã´Ìûå ÌòïÌÉú)</span>
    <span class="c1"># KL(N(mu, sigma^2) || N(0, 1)) = 0.5 * sum(1 + log(sigma^2) - mu^2 - sigma^2)</span>
    <span class="n">kl_loss</span> <span class="o">=</span> <span class="o">-</span><span class="mf">0.5</span> <span class="o">*</span> <span class="n">torch</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="mi">1</span> <span class="o">+</span> <span class="n">logvar</span> <span class="o">-</span> <span class="n">mu</span><span class="o">.</span><span class="n">pow</span><span class="p">(</span><span class="mi">2</span><span class="p">)</span> <span class="o">-</span> <span class="n">logvar</span><span class="o">.</span><span class="n">exp</span><span class="p">())</span>

    <span class="k">return</span> <span class="n">recon_loss</span> <span class="o">+</span> <span class="n">kl_loss</span><span class="p">,</span> <span class="n">recon_loss</span><span class="p">,</span> <span class="n">kl_loss</span>

<span class="c1"># ÏòàÏ†ú: Í∞ÑÎã®Ìïú 2D Ïû†Ïû¨ Í≥µÍ∞Ñ</span>
<span class="n">vae_model</span> <span class="o">=</span> <span class="n">VAE</span><span class="p">(</span><span class="n">input_dim</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">latent_dim</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
<span class="n">x_sample</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">10</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>

<span class="n">x_recon</span><span class="p">,</span> <span class="n">mu</span><span class="p">,</span> <span class="n">logvar</span> <span class="o">=</span> <span class="n">vae_model</span><span class="p">(</span><span class="n">x_sample</span><span class="p">)</span>
<span class="n">total_loss</span><span class="p">,</span> <span class="n">recon</span><span class="p">,</span> <span class="n">kl</span> <span class="o">=</span> <span class="n">vae_loss</span><span class="p">(</span><span class="n">x_sample</span><span class="p">,</span> <span class="n">x_recon</span><span class="p">,</span> <span class="n">mu</span><span class="p">,</span> <span class="n">logvar</span><span class="p">)</span>

<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Ï¥ù ÏÜêÏã§: </span><span class="si">{</span><span class="n">total_loss</span><span class="o">.</span><span class="n">item</span><span class="p">()</span><span class="si">:</span><span class="s2">.4f</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Ïû¨Íµ¨ÏÑ± ÏÜêÏã§: </span><span class="si">{</span><span class="n">recon</span><span class="o">.</span><span class="n">item</span><span class="p">()</span><span class="si">:</span><span class="s2">.4f</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;KL ÏÜêÏã§: </span><span class="si">{</span><span class="n">kl</span><span class="o">.</span><span class="n">item</span><span class="p">()</span><span class="si">:</span><span class="s2">.4f</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
</code></pre></div>

<h3 id="26-closed-form-kl-divergence-for-gaussians">2.6 Closed-Form KL Divergence for Gaussians<a class="header-link" href="#26-closed-form-kl-divergence-for-gaussians" title="Permanent link">&para;</a></h3>
<p>When $q = \mathcal{N}(\boldsymbol{\mu}, \text{diag}(\boldsymbol{\sigma}^2))$, $p = \mathcal{N}(\mathbf{0}, I)$:</p>
<p>$$D_{\text{KL}}(q \| p) = \frac{1}{2} \sum_{i=1}^{d} \left( \sigma_i^2 + \mu_i^2 - 1 - \log \sigma_i^2 \right)$$</p>
<p><strong>Derivation</strong>: Using definition of KL divergence and entropy formula for Gaussians</p>
<h2 id="3-mathematics-of-gans">3. Mathematics of GANs<a class="header-link" href="#3-mathematics-of-gans" title="Permanent link">&para;</a></h2>
<h3 id="31-minimax-game">3.1 Minimax Game<a class="header-link" href="#31-minimax-game" title="Permanent link">&para;</a></h3>
<p><strong>Generator</strong> (G) $G$: $\mathbf{z} \sim p(\mathbf{z}) \mapsto G(\mathbf{z}) \approx p_{\text{data}}$</p>
<p><strong>Discriminator</strong> (D) $D$: $\mathbf{x} \mapsto D(\mathbf{x}) \in [0, 1]$ (probability of being real)</p>
<p><strong>Objective function</strong>:</p>
<p>$$\min_G \max_D \mathbb{E}_{\mathbf{x} \sim p_{\text{data}}} [\log D(\mathbf{x})] + \mathbb{E}_{\mathbf{z} \sim p(\mathbf{z})} [\log(1 - D(G(\mathbf{z})))]$$</p>
<p><strong>Interpretation</strong>:
- $D$ tries to assign high probability to real data and low to fake
- $G$ tries to fool $D$ (make fake look real)</p>
<h3 id="32-optimal-discriminator">3.2 Optimal Discriminator<a class="header-link" href="#32-optimal-discriminator" title="Permanent link">&para;</a></h3>
<p>When $G$ is fixed, the optimal discriminator is:</p>
<p>$$D^*(x) = \frac{p_{\text{data}}(\mathbf{x})}{p_{\text{data}}(\mathbf{x}) + p_g(\mathbf{x})}$$</p>
<p>where $p_g$ is the distribution induced by the generator.</p>
<p><strong>Proof</strong>: Differentiate objective with respect to $D(\mathbf{x})$ and set to zero</p>
<p>$$\frac{\partial}{\partial D(\mathbf{x})} \left[ p_{\text{data}}(\mathbf{x}) \log D(\mathbf{x}) + p_g(\mathbf{x}) \log(1 - D(\mathbf{x})) \right] = 0$$</p>
<p>$$\frac{p_{\text{data}}(\mathbf{x})}{D(\mathbf{x})} - \frac{p_g(\mathbf{x})}{1 - D(\mathbf{x})} = 0$$</p>
<h3 id="33-relationship-to-js-divergence">3.3 Relationship to JS Divergence<a class="header-link" href="#33-relationship-to-js-divergence" title="Permanent link">&para;</a></h3>
<p>Substituting optimal discriminator $D^*$, the generator's objective becomes:</p>
<p>$$C(G) = -\log 4 + 2 \cdot \text{JS}(p_{\text{data}} \| p_g)$$</p>
<p>where <strong>Jensen-Shannon divergence</strong>:</p>
<p>$$\text{JS}(p \| q) = \frac{1}{2} D_{\text{KL}}\left(p \middle\| \frac{p+q}{2}\right) + \frac{1}{2} D_{\text{KL}}\left(q \middle\| \frac{p+q}{2}\right)$$</p>
<p><strong>Conclusion</strong>: Training a GAN is equivalent to minimizing JS divergence.</p>
<h3 id="34-mathematical-causes-of-training-instability">3.4 Mathematical Causes of Training Instability<a class="header-link" href="#34-mathematical-causes-of-training-instability" title="Permanent link">&para;</a></h3>
<p><strong>Gradient vanishing</strong>:
- When $p_g$ and $p_{\text{data}}$ don't overlap, $D$ becomes perfect
- $\nabla_\theta \log(1 - D(G(\mathbf{z}))) \approx 0$</p>
<p><strong>Mode collapse</strong>:
- Generator produces only some modes ($p_g$ covers only part of $p_{\text{data}}$)
- JS divergence can still be small</p>
<div class="highlight"><pre><span></span><code><span class="c1"># Í∞ÑÎã®Ìïú GAN ÏòàÏ†ú (1D)</span>
<span class="k">class</span><span class="w"> </span><span class="nc">Generator</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="k">def</span><span class="w"> </span><span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">latent_dim</span><span class="o">=</span><span class="mi">10</span><span class="p">,</span> <span class="n">output_dim</span><span class="o">=</span><span class="mi">1</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">net</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Sequential</span><span class="p">(</span>
            <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">latent_dim</span><span class="p">,</span> <span class="mi">50</span><span class="p">),</span>
            <span class="n">nn</span><span class="o">.</span><span class="n">ReLU</span><span class="p">(),</span>
            <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="mi">50</span><span class="p">,</span> <span class="n">output_dim</span><span class="p">)</span>
        <span class="p">)</span>

    <span class="k">def</span><span class="w"> </span><span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">z</span><span class="p">):</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">net</span><span class="p">(</span><span class="n">z</span><span class="p">)</span>

<span class="k">class</span><span class="w"> </span><span class="nc">Discriminator</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="k">def</span><span class="w"> </span><span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">input_dim</span><span class="o">=</span><span class="mi">1</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">net</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Sequential</span><span class="p">(</span>
            <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">input_dim</span><span class="p">,</span> <span class="mi">50</span><span class="p">),</span>
            <span class="n">nn</span><span class="o">.</span><span class="n">LeakyReLU</span><span class="p">(</span><span class="mf">0.2</span><span class="p">),</span>
            <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="mi">50</span><span class="p">,</span> <span class="mi">1</span><span class="p">),</span>
            <span class="n">nn</span><span class="o">.</span><span class="n">Sigmoid</span><span class="p">()</span>
        <span class="p">)</span>

    <span class="k">def</span><span class="w"> </span><span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">net</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>

<span class="c1"># Î™®Îç∏ Ï¥àÍ∏∞Ìôî</span>
<span class="n">latent_dim</span> <span class="o">=</span> <span class="mi">10</span>
<span class="n">G</span> <span class="o">=</span> <span class="n">Generator</span><span class="p">(</span><span class="n">latent_dim</span><span class="o">=</span><span class="n">latent_dim</span><span class="p">,</span> <span class="n">output_dim</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
<span class="n">D</span> <span class="o">=</span> <span class="n">Discriminator</span><span class="p">(</span><span class="n">input_dim</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>

<span class="c1"># ÏÜêÏã§ Ìï®Ïàò</span>
<span class="n">criterion</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">BCELoss</span><span class="p">()</span>

<span class="c1"># ÏòµÌã∞ÎßàÏù¥Ï†Ä</span>
<span class="n">optimizer_G</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">optim</span><span class="o">.</span><span class="n">Adam</span><span class="p">(</span><span class="n">G</span><span class="o">.</span><span class="n">parameters</span><span class="p">(),</span> <span class="n">lr</span><span class="o">=</span><span class="mf">2e-4</span><span class="p">)</span>
<span class="n">optimizer_D</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">optim</span><span class="o">.</span><span class="n">Adam</span><span class="p">(</span><span class="n">D</span><span class="o">.</span><span class="n">parameters</span><span class="p">(),</span> <span class="n">lr</span><span class="o">=</span><span class="mf">2e-4</span><span class="p">)</span>

<span class="c1"># Í∞ÑÎã®Ìïú ÌïôÏäµ Î£®ÌîÑ</span>
<span class="k">def</span><span class="w"> </span><span class="nf">train_gan_step</span><span class="p">(</span><span class="n">real_data</span><span class="p">,</span> <span class="n">G</span><span class="p">,</span> <span class="n">D</span><span class="p">,</span> <span class="n">optimizer_G</span><span class="p">,</span> <span class="n">optimizer_D</span><span class="p">,</span> <span class="n">latent_dim</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;GAN 1 Ïä§ÌÖù ÌïôÏäµ&quot;&quot;&quot;</span>
    <span class="n">batch_size</span> <span class="o">=</span> <span class="n">real_data</span><span class="o">.</span><span class="n">size</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>

    <span class="c1"># Î†àÏù¥Î∏î</span>
    <span class="n">real_labels</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">ones</span><span class="p">(</span><span class="n">batch_size</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
    <span class="n">fake_labels</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">batch_size</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>

    <span class="c1"># ÌåêÎ≥ÑÏûê ÌïôÏäµ</span>
    <span class="n">optimizer_D</span><span class="o">.</span><span class="n">zero_grad</span><span class="p">()</span>

    <span class="c1"># ÏßÑÏßú Îç∞Ïù¥ÌÑ∞</span>
    <span class="n">D_real</span> <span class="o">=</span> <span class="n">D</span><span class="p">(</span><span class="n">real_data</span><span class="p">)</span>
    <span class="n">loss_D_real</span> <span class="o">=</span> <span class="n">criterion</span><span class="p">(</span><span class="n">D_real</span><span class="p">,</span> <span class="n">real_labels</span><span class="p">)</span>

    <span class="c1"># Í∞ÄÏßú Îç∞Ïù¥ÌÑ∞</span>
    <span class="n">z</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="n">batch_size</span><span class="p">,</span> <span class="n">latent_dim</span><span class="p">)</span>
    <span class="n">fake_data</span> <span class="o">=</span> <span class="n">G</span><span class="p">(</span><span class="n">z</span><span class="p">)</span>
    <span class="n">D_fake</span> <span class="o">=</span> <span class="n">D</span><span class="p">(</span><span class="n">fake_data</span><span class="o">.</span><span class="n">detach</span><span class="p">())</span>
    <span class="n">loss_D_fake</span> <span class="o">=</span> <span class="n">criterion</span><span class="p">(</span><span class="n">D_fake</span><span class="p">,</span> <span class="n">fake_labels</span><span class="p">)</span>

    <span class="n">loss_D</span> <span class="o">=</span> <span class="n">loss_D_real</span> <span class="o">+</span> <span class="n">loss_D_fake</span>
    <span class="n">loss_D</span><span class="o">.</span><span class="n">backward</span><span class="p">()</span>
    <span class="n">optimizer_D</span><span class="o">.</span><span class="n">step</span><span class="p">()</span>

    <span class="c1"># ÏÉùÏÑ±Ïûê ÌïôÏäµ</span>
    <span class="n">optimizer_G</span><span class="o">.</span><span class="n">zero_grad</span><span class="p">()</span>

    <span class="n">z</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="n">batch_size</span><span class="p">,</span> <span class="n">latent_dim</span><span class="p">)</span>
    <span class="n">fake_data</span> <span class="o">=</span> <span class="n">G</span><span class="p">(</span><span class="n">z</span><span class="p">)</span>
    <span class="n">D_fake</span> <span class="o">=</span> <span class="n">D</span><span class="p">(</span><span class="n">fake_data</span><span class="p">)</span>
    <span class="n">loss_G</span> <span class="o">=</span> <span class="n">criterion</span><span class="p">(</span><span class="n">D_fake</span><span class="p">,</span> <span class="n">real_labels</span><span class="p">)</span>  <span class="c1"># ÏßÑÏßúÎ°ú Î∂ÑÎ•òÎêòÍ∏∏ ÏõêÌï®</span>

    <span class="n">loss_G</span><span class="o">.</span><span class="n">backward</span><span class="p">()</span>
    <span class="n">optimizer_G</span><span class="o">.</span><span class="n">step</span><span class="p">()</span>

    <span class="k">return</span> <span class="n">loss_D</span><span class="o">.</span><span class="n">item</span><span class="p">(),</span> <span class="n">loss_G</span><span class="o">.</span><span class="n">item</span><span class="p">()</span>

<span class="c1"># ÏòàÏ†ú Îç∞Ïù¥ÌÑ∞</span>
<span class="n">real_samples</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">FloatTensor</span><span class="p">(</span><span class="n">generate_data</span><span class="p">(</span><span class="mi">100</span><span class="p">))</span><span class="o">.</span><span class="n">unsqueeze</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span>
<span class="n">loss_d</span><span class="p">,</span> <span class="n">loss_g</span> <span class="o">=</span> <span class="n">train_gan_step</span><span class="p">(</span><span class="n">real_samples</span><span class="p">,</span> <span class="n">G</span><span class="p">,</span> <span class="n">D</span><span class="p">,</span> <span class="n">optimizer_G</span><span class="p">,</span> <span class="n">optimizer_D</span><span class="p">,</span> <span class="n">latent_dim</span><span class="p">)</span>

<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;ÌåêÎ≥ÑÏûê ÏÜêÏã§: </span><span class="si">{</span><span class="n">loss_d</span><span class="si">:</span><span class="s2">.4f</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;ÏÉùÏÑ±Ïûê ÏÜêÏã§: </span><span class="si">{</span><span class="n">loss_g</span><span class="si">:</span><span class="s2">.4f</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
</code></pre></div>

<h2 id="4-wasserstein-distance-and-wgan">4. Wasserstein Distance and WGAN<a class="header-link" href="#4-wasserstein-distance-and-wgan" title="Permanent link">&para;</a></h2>
<h3 id="41-optimal-transport">4.1 Optimal Transport<a class="header-link" href="#41-optimal-transport" title="Permanent link">&para;</a></h3>
<p><strong>Wasserstein-1 distance</strong> (Earth Mover's Distance) between two probability distributions $P, Q$:</p>
<p>$$W_1(P, Q) = \inf_{\gamma \in \Pi(P, Q)} \mathbb{E}_{(\mathbf{x}, \mathbf{y}) \sim \gamma} \left[ \|\mathbf{x} - \mathbf{y}\| \right]$$</p>
<p>where $\Pi(P, Q)$ is the set of all joint distributions with marginals $P$ and $Q$.</p>
<p><strong>Intuition</strong>: Minimum cost to "transport" $P$ to $Q$</p>
<h3 id="42-kantorovich-rubinstein-duality">4.2 Kantorovich-Rubinstein Duality<a class="header-link" href="#42-kantorovich-rubinstein-duality" title="Permanent link">&para;</a></h3>
<p><strong>Dual problem</strong>:</p>
<p>$$W_1(P, Q) = \sup_{f: \|f\|_L \leq 1} \mathbb{E}_{\mathbf{x} \sim P}[f(\mathbf{x})] - \mathbb{E}_{\mathbf{y} \sim Q}[f(\mathbf{y})]$$</p>
<p>where $\|f\|_L \leq 1$ is the <strong>1-Lipschitz function</strong> constraint:</p>
<p>$$|f(\mathbf{x}_1) - f(\mathbf{x}_2)| \leq \|\mathbf{x}_1 - \mathbf{x}_2\|$$</p>
<h3 id="43-wgan-objective-function">4.3 WGAN Objective Function<a class="header-link" href="#43-wgan-objective-function" title="Permanent link">&para;</a></h3>
<p><strong>Replace discriminator with critic $f_w$</strong> (output is not a probability):</p>
<p>$$\min_G \max_{w: f_w \text{ is 1-Lipschitz}} \mathbb{E}_{\mathbf{x} \sim p_{\text{data}}}[f_w(\mathbf{x})] - \mathbb{E}_{\mathbf{z} \sim p(\mathbf{z})}[f_w(G(\mathbf{z}))]$$</p>
<p><strong>Advantages</strong>:
- Improved training stability
- Meaningful loss values (approximate distance)
- Reduced mode collapse</p>
<h3 id="44-lipschitz-constraint-gradient-penalty">4.4 Lipschitz Constraint: Gradient Penalty<a class="header-link" href="#44-lipschitz-constraint-gradient-penalty" title="Permanent link">&para;</a></h3>
<p><strong>Original WGAN</strong>: Weight clipping ‚Üí limited</p>
<p><strong>WGAN-GP</strong>: Gradient penalty</p>
<p>$$\mathcal{L} = \mathbb{E}_{\tilde{\mathbf{x}}} [f(\tilde{\mathbf{x}})] - \mathbb{E}_{\mathbf{x}} [f(\mathbf{x})] + \lambda \mathbb{E}_{\hat{\mathbf{x}}} \left[ (\|\nabla_{\hat{\mathbf{x}}} f(\hat{\mathbf{x}})\| - 1)^2 \right]$$</p>
<p>where $\hat{\mathbf{x}} = \epsilon \mathbf{x} + (1 - \epsilon) \tilde{\mathbf{x}}$ is interpolation between real and fake.</p>
<div class="highlight"><pre><span></span><code><span class="k">def</span><span class="w"> </span><span class="nf">gradient_penalty</span><span class="p">(</span><span class="n">D</span><span class="p">,</span> <span class="n">real_data</span><span class="p">,</span> <span class="n">fake_data</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="s1">&#39;cpu&#39;</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Í∑∏ÎûòÎîîÏñ∏Ìä∏ Ìå®ÎÑêÌã∞ Í≥ÑÏÇ∞ (WGAN-GP)</span>

<span class="sd">    Parameters:</span>
<span class="sd">    -----------</span>
<span class="sd">    D : nn.Module</span>
<span class="sd">        ÌåêÎ≥ÑÏûê (critic)</span>
<span class="sd">    real_data : Tensor</span>
<span class="sd">        ÏßÑÏßú Îç∞Ïù¥ÌÑ∞</span>
<span class="sd">    fake_data : Tensor</span>
<span class="sd">        Í∞ÄÏßú Îç∞Ïù¥ÌÑ∞</span>
<span class="sd">    device : str</span>
<span class="sd">        ÎîîÎ∞îÏù¥Ïä§</span>

<span class="sd">    Returns:</span>
<span class="sd">    --------</span>
<span class="sd">    gp : Tensor</span>
<span class="sd">        Í∑∏ÎûòÎîîÏñ∏Ìä∏ Ìå®ÎÑêÌã∞</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">batch_size</span> <span class="o">=</span> <span class="n">real_data</span><span class="o">.</span><span class="n">size</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>

    <span class="c1"># ÎûúÎç§ Î≥¥Í∞Ñ</span>
    <span class="n">epsilon</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">rand</span><span class="p">(</span><span class="n">batch_size</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>
    <span class="n">interpolates</span> <span class="o">=</span> <span class="n">epsilon</span> <span class="o">*</span> <span class="n">real_data</span> <span class="o">+</span> <span class="p">(</span><span class="mi">1</span> <span class="o">-</span> <span class="n">epsilon</span><span class="p">)</span> <span class="o">*</span> <span class="n">fake_data</span>
    <span class="n">interpolates</span><span class="o">.</span><span class="n">requires_grad_</span><span class="p">(</span><span class="kc">True</span><span class="p">)</span>

    <span class="c1"># ÌåêÎ≥ÑÏûê Ï∂úÎ†•</span>
    <span class="n">D_interpolates</span> <span class="o">=</span> <span class="n">D</span><span class="p">(</span><span class="n">interpolates</span><span class="p">)</span>

    <span class="c1"># Í∑∏ÎûòÎîîÏñ∏Ìä∏ Í≥ÑÏÇ∞</span>
    <span class="n">gradients</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">autograd</span><span class="o">.</span><span class="n">grad</span><span class="p">(</span>
        <span class="n">outputs</span><span class="o">=</span><span class="n">D_interpolates</span><span class="p">,</span>
        <span class="n">inputs</span><span class="o">=</span><span class="n">interpolates</span><span class="p">,</span>
        <span class="n">grad_outputs</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">ones_like</span><span class="p">(</span><span class="n">D_interpolates</span><span class="p">),</span>
        <span class="n">create_graph</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
        <span class="n">retain_graph</span><span class="o">=</span><span class="kc">True</span>
    <span class="p">)[</span><span class="mi">0</span><span class="p">]</span>

    <span class="c1"># Í∑∏ÎûòÎîîÏñ∏Ìä∏ norm</span>
    <span class="n">gradients</span> <span class="o">=</span> <span class="n">gradients</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="n">batch_size</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">)</span>
    <span class="n">gradient_norm</span> <span class="o">=</span> <span class="n">gradients</span><span class="o">.</span><span class="n">norm</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>

    <span class="c1"># Ìå®ÎÑêÌã∞</span>
    <span class="n">gp</span> <span class="o">=</span> <span class="p">((</span><span class="n">gradient_norm</span> <span class="o">-</span> <span class="mi">1</span><span class="p">)</span> <span class="o">**</span> <span class="mi">2</span><span class="p">)</span><span class="o">.</span><span class="n">mean</span><span class="p">()</span>

    <span class="k">return</span> <span class="n">gp</span>

<span class="c1"># ÏòàÏ†ú</span>
<span class="n">real</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">32</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="n">requires_grad</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
<span class="n">fake</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">32</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="n">requires_grad</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>

<span class="n">D_critic</span> <span class="o">=</span> <span class="n">Discriminator</span><span class="p">(</span><span class="n">input_dim</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
<span class="n">gp</span> <span class="o">=</span> <span class="n">gradient_penalty</span><span class="p">(</span><span class="n">D_critic</span><span class="p">,</span> <span class="n">real</span><span class="p">,</span> <span class="n">fake</span><span class="p">)</span>

<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Í∑∏ÎûòÎîîÏñ∏Ìä∏ Ìå®ÎÑêÌã∞: </span><span class="si">{</span><span class="n">gp</span><span class="o">.</span><span class="n">item</span><span class="p">()</span><span class="si">:</span><span class="s2">.4f</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
</code></pre></div>

<h2 id="5-mathematics-of-diffusion-models">5. Mathematics of Diffusion Models<a class="header-link" href="#5-mathematics-of-diffusion-models" title="Permanent link">&para;</a></h2>
<h3 id="51-forward-process">5.1 Forward Process<a class="header-link" href="#51-forward-process" title="Permanent link">&para;</a></h3>
<p>Progressively add Gaussian noise to data $\mathbf{x}_0 \sim q(\mathbf{x}_0)$:</p>
<p>$$q(\mathbf{x}_t | \mathbf{x}_{t-1}) = \mathcal{N}(\mathbf{x}_t; \sqrt{1 - \beta_t} \mathbf{x}_{t-1}, \beta_t I)$$</p>
<p><strong>Noise schedule</strong>: $\beta_1, \ldots, \beta_T$ (typically $10^{-4} \to 0.02$)</p>
<p><strong>Closed form</strong>: $\alpha_t = 1 - \beta_t$, $\bar{\alpha}_t = \prod_{s=1}^t \alpha_s$</p>
<p>$$q(\mathbf{x}_t | \mathbf{x}_0) = \mathcal{N}(\mathbf{x}_t; \sqrt{\bar{\alpha}_t} \mathbf{x}_0, (1 - \bar{\alpha}_t) I)$$</p>
<p>That is, in one step $\mathbf{x}_t = \sqrt{\bar{\alpha}_t} \mathbf{x}_0 + \sqrt{1 - \bar{\alpha}_t} \boldsymbol{\epsilon}$, $\boldsymbol{\epsilon} \sim \mathcal{N}(\mathbf{0}, I)$</p>
<h3 id="52-reverse-process">5.2 Reverse Process<a class="header-link" href="#52-reverse-process" title="Permanent link">&para;</a></h3>
<p><strong>Goal</strong>: Recover $\mathbf{x}_0$ from $\mathbf{x}_T \sim \mathcal{N}(\mathbf{0}, I)$</p>
<p>$$p_\theta(\mathbf{x}_{t-1} | \mathbf{x}_t) = \mathcal{N}(\mathbf{x}_{t-1}; \boldsymbol{\mu}_\theta(\mathbf{x}_t, t), \Sigma_\theta(\mathbf{x}_t, t))$$</p>
<p><strong>Theory</strong>: If $\beta_t$ is sufficiently small, the reverse is also Gaussian.</p>
<h3 id="53-ddpm-loss-derivation">5.3 DDPM Loss Derivation<a class="header-link" href="#53-ddpm-loss-derivation" title="Permanent link">&para;</a></h3>
<p>Deriving the variational lower bound (ELBO, similar to VAE):</p>
<p>$$\mathcal{L} = \mathbb{E}_q \left[ D_{\text{KL}}(q(\mathbf{x}_T|\mathbf{x}_0) \| p(\mathbf{x}_T)) + \sum_{t=2}^T D_{\text{KL}}(q(\mathbf{x}_{t-1}|\mathbf{x}_t, \mathbf{x}_0) \| p_\theta(\mathbf{x}_{t-1}|\mathbf{x}_t)) - \log p_\theta(\mathbf{x}_0|\mathbf{x}_1) \right]$$</p>
<p><strong>Key</strong>: Closed-form exists for posterior $q(\mathbf{x}_{t-1}|\mathbf{x}_t, \mathbf{x}_0)$</p>
<p>$$q(\mathbf{x}_{t-1}|\mathbf{x}_t, \mathbf{x}_0) = \mathcal{N}(\mathbf{x}_{t-1}; \tilde{\boldsymbol{\mu}}_t(\mathbf{x}_t, \mathbf{x}_0), \tilde{\beta}_t I)$$</p>
<p>where:</p>
<p>$$\tilde{\boldsymbol{\mu}}_t(\mathbf{x}_t, \mathbf{x}_0) = \frac{\sqrt{\bar{\alpha}_{t-1}} \beta_t}{1 - \bar{\alpha}_t} \mathbf{x}_0 + \frac{\sqrt{\alpha_t}(1 - \bar{\alpha}_{t-1})}{1 - \bar{\alpha}_t} \mathbf{x}_t$$</p>
<p><strong>Reparameterization with noise prediction</strong>:</p>
<p>Substituting $\mathbf{x}_0 = \frac{1}{\sqrt{\bar{\alpha}_t}}(\mathbf{x}_t - \sqrt{1 - \bar{\alpha}_t} \boldsymbol{\epsilon})$,</p>
<p><strong>Simplified loss</strong>:</p>
<p>$$\mathcal{L}_{\text{simple}} = \mathbb{E}_{t, \mathbf{x}_0, \boldsymbol{\epsilon}} \left[ \|\boldsymbol{\epsilon} - \boldsymbol{\epsilon}_\theta(\mathbf{x}_t, t)\|^2 \right]$$</p>
<p>That is, it reduces to a <strong>noise prediction</strong> problem!</p>
<div class="highlight"><pre><span></span><code><span class="k">def</span><span class="w"> </span><span class="nf">linear_beta_schedule</span><span class="p">(</span><span class="n">timesteps</span><span class="p">,</span> <span class="n">beta_start</span><span class="o">=</span><span class="mf">1e-4</span><span class="p">,</span> <span class="n">beta_end</span><span class="o">=</span><span class="mf">0.02</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;ÏÑ†Ìòï ÎÖ∏Ïù¥Ï¶à Ïä§ÏºÄÏ§Ñ&quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="n">np</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="n">beta_start</span><span class="p">,</span> <span class="n">beta_end</span><span class="p">,</span> <span class="n">timesteps</span><span class="p">)</span>

<span class="k">def</span><span class="w"> </span><span class="nf">get_diffusion_parameters</span><span class="p">(</span><span class="n">timesteps</span><span class="o">=</span><span class="mi">1000</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;ÌôïÏÇ∞ Î™®Îç∏ ÌååÎùºÎØ∏ÌÑ∞ Í≥ÑÏÇ∞&quot;&quot;&quot;</span>
    <span class="n">betas</span> <span class="o">=</span> <span class="n">linear_beta_schedule</span><span class="p">(</span><span class="n">timesteps</span><span class="p">)</span>
    <span class="n">alphas</span> <span class="o">=</span> <span class="mf">1.0</span> <span class="o">-</span> <span class="n">betas</span>
    <span class="n">alphas_cumprod</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">cumprod</span><span class="p">(</span><span class="n">alphas</span><span class="p">)</span>
    <span class="n">alphas_cumprod_prev</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">concatenate</span><span class="p">([[</span><span class="mf">1.0</span><span class="p">],</span> <span class="n">alphas_cumprod</span><span class="p">[:</span><span class="o">-</span><span class="mi">1</span><span class="p">]])</span>

    <span class="k">return</span> <span class="p">{</span>
        <span class="s1">&#39;betas&#39;</span><span class="p">:</span> <span class="n">betas</span><span class="p">,</span>
        <span class="s1">&#39;alphas&#39;</span><span class="p">:</span> <span class="n">alphas</span><span class="p">,</span>
        <span class="s1">&#39;alphas_cumprod&#39;</span><span class="p">:</span> <span class="n">alphas_cumprod</span><span class="p">,</span>
        <span class="s1">&#39;alphas_cumprod_prev&#39;</span><span class="p">:</span> <span class="n">alphas_cumprod_prev</span><span class="p">,</span>
        <span class="s1">&#39;sqrt_alphas_cumprod&#39;</span><span class="p">:</span> <span class="n">np</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="n">alphas_cumprod</span><span class="p">),</span>
        <span class="s1">&#39;sqrt_one_minus_alphas_cumprod&#39;</span><span class="p">:</span> <span class="n">np</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="mf">1.0</span> <span class="o">-</span> <span class="n">alphas_cumprod</span><span class="p">)</span>
    <span class="p">}</span>

<span class="n">params</span> <span class="o">=</span> <span class="n">get_diffusion_parameters</span><span class="p">(</span><span class="n">timesteps</span><span class="o">=</span><span class="mi">1000</span><span class="p">)</span>

<span class="c1"># ÎÖ∏Ïù¥Ï¶à Ï∂îÍ∞Ä ÏòàÏ†ú</span>
<span class="k">def</span><span class="w"> </span><span class="nf">q_sample</span><span class="p">(</span><span class="n">x_0</span><span class="p">,</span> <span class="n">t</span><span class="p">,</span> <span class="n">params</span><span class="p">,</span> <span class="n">noise</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Ï†ïÎ∞©Ìñ• Í≥ºÏ†ï: x_0ÏóêÏÑú x_t ÏÉòÌîåÎßÅ</span>

<span class="sd">    Parameters:</span>
<span class="sd">    -----------</span>
<span class="sd">    x_0 : Tensor</span>
<span class="sd">        ÏõêÎ≥∏ Îç∞Ïù¥ÌÑ∞</span>
<span class="sd">    t : int</span>
<span class="sd">        ÌÉÄÏûÑÏä§ÌÖù</span>
<span class="sd">    params : dict</span>
<span class="sd">        ÌôïÏÇ∞ ÌååÎùºÎØ∏ÌÑ∞</span>
<span class="sd">    noise : Tensor, optional</span>
<span class="sd">        ÎÖ∏Ïù¥Ï¶à (NoneÏù¥Î©¥ ÏÉòÌîåÎßÅ)</span>

<span class="sd">    Returns:</span>
<span class="sd">    --------</span>
<span class="sd">    x_t : Tensor</span>
<span class="sd">        ÎÖ∏Ïù¥Ï¶àÍ∞Ä Ï∂îÍ∞ÄÎêú Îç∞Ïù¥ÌÑ∞</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">if</span> <span class="n">noise</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">noise</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn_like</span><span class="p">(</span><span class="n">x_0</span><span class="p">)</span>

    <span class="n">sqrt_alpha_cumprod_t</span> <span class="o">=</span> <span class="n">params</span><span class="p">[</span><span class="s1">&#39;sqrt_alphas_cumprod&#39;</span><span class="p">][</span><span class="n">t</span><span class="p">]</span>
    <span class="n">sqrt_one_minus_alpha_cumprod_t</span> <span class="o">=</span> <span class="n">params</span><span class="p">[</span><span class="s1">&#39;sqrt_one_minus_alphas_cumprod&#39;</span><span class="p">][</span><span class="n">t</span><span class="p">]</span>

    <span class="n">x_t</span> <span class="o">=</span> <span class="n">sqrt_alpha_cumprod_t</span> <span class="o">*</span> <span class="n">x_0</span> <span class="o">+</span> <span class="n">sqrt_one_minus_alpha_cumprod_t</span> <span class="o">*</span> <span class="n">noise</span>

    <span class="k">return</span> <span class="n">x_t</span>

<span class="c1"># ÏòàÏ†ú: 1D Îç∞Ïù¥ÌÑ∞Ïóê ÎÖ∏Ïù¥Ï¶à Ï∂îÍ∞Ä</span>
<span class="n">x_0</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">FloatTensor</span><span class="p">([[</span><span class="mf">1.0</span><span class="p">],</span> <span class="p">[</span><span class="mf">2.0</span><span class="p">],</span> <span class="p">[</span><span class="mf">3.0</span><span class="p">]])</span>
<span class="n">timesteps_to_visualize</span> <span class="o">=</span> <span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">250</span><span class="p">,</span> <span class="mi">500</span><span class="p">,</span> <span class="mi">750</span><span class="p">,</span> <span class="mi">999</span><span class="p">]</span>

<span class="n">fig</span><span class="p">,</span> <span class="n">axes</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="nb">len</span><span class="p">(</span><span class="n">timesteps_to_visualize</span><span class="p">),</span> <span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">15</span><span class="p">,</span> <span class="mi">3</span><span class="p">))</span>

<span class="k">for</span> <span class="n">idx</span><span class="p">,</span> <span class="n">t</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">timesteps_to_visualize</span><span class="p">):</span>
    <span class="n">x_t</span> <span class="o">=</span> <span class="n">q_sample</span><span class="p">(</span><span class="n">x_0</span><span class="p">,</span> <span class="n">t</span><span class="p">,</span> <span class="n">params</span><span class="p">)</span>
    <span class="n">axes</span><span class="p">[</span><span class="n">idx</span><span class="p">]</span><span class="o">.</span><span class="n">hist</span><span class="p">(</span><span class="n">x_t</span><span class="o">.</span><span class="n">numpy</span><span class="p">(),</span> <span class="n">bins</span><span class="o">=</span><span class="mi">20</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.7</span><span class="p">)</span>
    <span class="n">axes</span><span class="p">[</span><span class="n">idx</span><span class="p">]</span><span class="o">.</span><span class="n">set_title</span><span class="p">(</span><span class="sa">f</span><span class="s1">&#39;t=</span><span class="si">{</span><span class="n">t</span><span class="si">}</span><span class="s1">&#39;</span><span class="p">)</span>
    <span class="n">axes</span><span class="p">[</span><span class="n">idx</span><span class="p">]</span><span class="o">.</span><span class="n">set_xlabel</span><span class="p">(</span><span class="s1">&#39;Value&#39;</span><span class="p">)</span>
    <span class="n">axes</span><span class="p">[</span><span class="n">idx</span><span class="p">]</span><span class="o">.</span><span class="n">set_xlim</span><span class="p">(</span><span class="o">-</span><span class="mi">4</span><span class="p">,</span> <span class="mi">4</span><span class="p">)</span>

<span class="n">plt</span><span class="o">.</span><span class="n">tight_layout</span><span class="p">()</span>
<span class="n">plt</span><span class="o">.</span><span class="n">savefig</span><span class="p">(</span><span class="s1">&#39;diffusion_forward_process.png&#39;</span><span class="p">,</span> <span class="n">dpi</span><span class="o">=</span><span class="mi">150</span><span class="p">,</span> <span class="n">bbox_inches</span><span class="o">=</span><span class="s1">&#39;tight&#39;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;ÌôïÏÇ∞ Ï†ïÎ∞©Ìñ• Í≥ºÏ†ï ÏãúÍ∞ÅÌôî Ï†ÄÏû• ÏôÑÎ£å&quot;</span><span class="p">)</span>
</code></pre></div>

<h3 id="54-relationship-to-score-matching">5.4 Relationship to Score Matching<a class="header-link" href="#54-relationship-to-score-matching" title="Permanent link">&para;</a></h3>
<p><strong>Score function</strong>: $\nabla_{\mathbf{x}} \log p(\mathbf{x})$</p>
<p><strong>Score matching</strong>: Approximate score function with neural network</p>
<p>$$\mathcal{L}_{\text{score}} = \mathbb{E}_{p(\mathbf{x})} \left[ \left\| \nabla_{\mathbf{x}} \log p(\mathbf{x}) - s_\theta(\mathbf{x}) \right\|^2 \right]$$</p>
<p><strong>Connection</strong>: Relationship between noise prediction $\boldsymbol{\epsilon}_\theta$ and score estimation</p>
<p>$$\nabla_{\mathbf{x}_t} \log q(\mathbf{x}_t|\mathbf{x}_0) = -\frac{\boldsymbol{\epsilon}}{\sqrt{1 - \bar{\alpha}_t}}$$</p>
<p>Therefore, learning $\boldsymbol{\epsilon}_\theta$ is equivalent to learning the score function.</p>
<h3 id="55-langevin-dynamics">5.5 Langevin Dynamics<a class="header-link" href="#55-langevin-dynamics" title="Permanent link">&para;</a></h3>
<p><strong>Sampling method</strong>: Markov Chain Monte Carlo using score function</p>
<p>$$\mathbf{x}_{t+1} = \mathbf{x}_t + \epsilon \nabla_{\mathbf{x}} \log p(\mathbf{x}_t) + \sqrt{2\epsilon} \mathbf{z}_t$$</p>
<p>where $\mathbf{z}_t \sim \mathcal{N}(\mathbf{0}, I)$</p>
<p>The reverse process of diffusion models can be viewed as <strong>discretized Langevin dynamics</strong>.</p>
<h2 id="6-recent-advances-flow-matching-and-cnf">6. Recent Advances: Flow Matching and CNF<a class="header-link" href="#6-recent-advances-flow-matching-and-cnf" title="Permanent link">&para;</a></h2>
<h3 id="61-continuous-normalizing-flow-cnf">6.1 Continuous Normalizing Flow (CNF)<a class="header-link" href="#61-continuous-normalizing-flow-cnf" title="Permanent link">&para;</a></h3>
<p><strong>Idea</strong>: Transform data distribution via ODE</p>
<p>$$\frac{d\mathbf{x}(t)}{dt} = f_\theta(\mathbf{x}(t), t)$$</p>
<p><strong>Time $t=0$</strong>: $\mathbf{x}(0) \sim p_0$ (noise)
<strong>Time $t=1$</strong>: $\mathbf{x}(1) \sim p_1$ (data)</p>
<p><strong>Neural ODE</strong>: Parameterize $f_\theta$ with neural network</p>
<h3 id="62-flow-matching">6.2 Flow Matching<a class="header-link" href="#62-flow-matching" title="Permanent link">&para;</a></h3>
<p><strong>Goal</strong>: Directly learn vector field $u_t(\mathbf{x})$</p>
<p><strong>Conditional Flow Matching</strong> (CFM):</p>
<p>$$\mathcal{L}_{\text{CFM}} = \mathbb{E}_{t, \mathbf{x}_0, \mathbf{x}_1} \left[ \left\| u_t(\mathbf{x}_t) - \frac{d\mathbf{x}_t}{dt} \right\|^2 \right]$$</p>
<p>where $\mathbf{x}_t = (1-t)\mathbf{x}_0 + t\mathbf{x}_1$ is linear interpolation.</p>
<p><strong>Advantages</strong>:
- Simulation-free training
- Faster sampling than diffusion models
- Stable training</p>
<h3 id="63-consistency-models">6.3 Consistency Models<a class="header-link" href="#63-consistency-models" title="Permanent link">&para;</a></h3>
<p><strong>Idea</strong>: Map all points on ODE trajectory to the same point</p>
<p>$$f_\theta(\mathbf{x}_t, t) = f_\theta(\mathbf{x}_{t'}, t') \quad \forall t, t'$$</p>
<p><strong>Consistency loss</strong>:</p>
<p>$$\mathcal{L} = \mathbb{E} \left[ d(f_\theta(\mathbf{x}_{t_{n+1}}, t_{n+1}), f_{\theta^-}(\mathbf{x}_{t_n}, t_n)) \right]$$</p>
<p>where $\theta^-$ is EMA (exponential moving average) parameter.</p>
<p><strong>Advantage</strong>: 1-step generation possible (diffusion models need hundreds of steps)</p>
<div class="highlight"><pre><span></span><code><span class="c1"># Í∞ÑÎã®Ìïú Flow Matching ÏòàÏ†ú</span>
<span class="k">class</span><span class="w"> </span><span class="nc">FlowMatchingModel</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="k">def</span><span class="w"> </span><span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">dim</span><span class="o">=</span><span class="mi">2</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">net</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Sequential</span><span class="p">(</span>
            <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">dim</span> <span class="o">+</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">64</span><span class="p">),</span>  <span class="c1"># x + t</span>
            <span class="n">nn</span><span class="o">.</span><span class="n">ReLU</span><span class="p">(),</span>
            <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="mi">64</span><span class="p">,</span> <span class="mi">64</span><span class="p">),</span>
            <span class="n">nn</span><span class="o">.</span><span class="n">ReLU</span><span class="p">(),</span>
            <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="mi">64</span><span class="p">,</span> <span class="n">dim</span><span class="p">)</span>
        <span class="p">)</span>

    <span class="k">def</span><span class="w"> </span><span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">t</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Î≤°ÌÑ∞ ÌïÑÎìú ÏòàÏ∏°&quot;&quot;&quot;</span>
        <span class="n">t_expanded</span> <span class="o">=</span> <span class="n">t</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span><span class="o">.</span><span class="n">expand</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="n">x</span><span class="o">.</span><span class="n">size</span><span class="p">(</span><span class="mi">1</span><span class="p">))</span>
        <span class="n">xt</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">cat</span><span class="p">([</span><span class="n">x</span><span class="p">,</span> <span class="n">t_expanded</span><span class="p">[:,</span> <span class="p">:</span><span class="mi">1</span><span class="p">]],</span> <span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">net</span><span class="p">(</span><span class="n">xt</span><span class="p">)</span>

<span class="k">def</span><span class="w"> </span><span class="nf">flow_matching_loss</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">x0</span><span class="p">,</span> <span class="n">x1</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Flow Matching ÏÜêÏã§</span>

<span class="sd">    Parameters:</span>
<span class="sd">    -----------</span>
<span class="sd">    model : nn.Module</span>
<span class="sd">        Î≤°ÌÑ∞ ÌïÑÎìú Î™®Îç∏</span>
<span class="sd">    x0 : Tensor</span>
<span class="sd">        ÏãúÏûë Î∂ÑÌè¨ ÏÉòÌîå (ÎÖ∏Ïù¥Ï¶à)</span>
<span class="sd">    x1 : Tensor</span>
<span class="sd">        Î™©Ìëú Î∂ÑÌè¨ ÏÉòÌîå (Îç∞Ïù¥ÌÑ∞)</span>

<span class="sd">    Returns:</span>
<span class="sd">    --------</span>
<span class="sd">    loss : Tensor</span>
<span class="sd">        Flow Matching ÏÜêÏã§</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">batch_size</span> <span class="o">=</span> <span class="n">x0</span><span class="o">.</span><span class="n">size</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>

    <span class="c1"># ÎûúÎç§ ÏãúÍ∞Ñ</span>
    <span class="n">t</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">rand</span><span class="p">(</span><span class="n">batch_size</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>

    <span class="c1"># ÏÑ†Ìòï Î≥¥Í∞Ñ</span>
    <span class="n">x_t</span> <span class="o">=</span> <span class="p">(</span><span class="mi">1</span> <span class="o">-</span> <span class="n">t</span><span class="p">)</span> <span class="o">*</span> <span class="n">x0</span> <span class="o">+</span> <span class="n">t</span> <span class="o">*</span> <span class="n">x1</span>

    <span class="c1"># ÏßÑÏßú Î≤°ÌÑ∞ ÌïÑÎìú (ÏÑ†Ìòï Î≥¥Í∞ÑÏùò ÎèÑÌï®Ïàò)</span>
    <span class="n">true_velocity</span> <span class="o">=</span> <span class="n">x1</span> <span class="o">-</span> <span class="n">x0</span>

    <span class="c1"># ÏòàÏ∏° Î≤°ÌÑ∞ ÌïÑÎìú</span>
    <span class="n">pred_velocity</span> <span class="o">=</span> <span class="n">model</span><span class="p">(</span><span class="n">x_t</span><span class="p">,</span> <span class="n">t</span><span class="o">.</span><span class="n">squeeze</span><span class="p">())</span>

    <span class="c1"># ÏÜêÏã§</span>
    <span class="n">loss</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">mse_loss</span><span class="p">(</span><span class="n">pred_velocity</span><span class="p">,</span> <span class="n">true_velocity</span><span class="p">)</span>

    <span class="k">return</span> <span class="n">loss</span>

<span class="c1"># ÏòàÏ†ú</span>
<span class="n">fm_model</span> <span class="o">=</span> <span class="n">FlowMatchingModel</span><span class="p">(</span><span class="n">dim</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
<span class="n">x0_samples</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">32</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>
<span class="n">x1_samples</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">32</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span> <span class="o">+</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([</span><span class="mf">2.0</span><span class="p">,</span> <span class="mf">0.0</span><span class="p">])</span>

<span class="n">loss</span> <span class="o">=</span> <span class="n">flow_matching_loss</span><span class="p">(</span><span class="n">fm_model</span><span class="p">,</span> <span class="n">x0_samples</span><span class="p">,</span> <span class="n">x1_samples</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Flow Matching ÏÜêÏã§: </span><span class="si">{</span><span class="n">loss</span><span class="o">.</span><span class="n">item</span><span class="p">()</span><span class="si">:</span><span class="s2">.4f</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
</code></pre></div>

<h2 id="practice-problems">Practice Problems<a class="header-link" href="#practice-problems" title="Permanent link">&para;</a></h2>
<h3 id="problem-1-two-methods-of-elbo-derivation">Problem 1: Two Methods of ELBO Derivation<a class="header-link" href="#problem-1-two-methods-of-elbo-derivation" title="Permanent link">&para;</a></h3>
<ol>
<li>Write step-by-step derivation of ELBO using Jensen's inequality.</li>
<li>Write step-by-step derivation using KL decomposition.</li>
<li>Explain why both methods give the same result.</li>
</ol>
<h3 id="problem-2-vae-implementation-and-experiments">Problem 2: VAE Implementation and Experiments<a class="header-link" href="#problem-2-vae-implementation-and-experiments" title="Permanent link">&para;</a></h3>
<p>For 2D Gaussian mixture data:
1. Implement a VAE with 2D latent space.
2. Plot training curves of reconstruction loss and KL loss.
3. Visualize latent space and decoder output via grid sampling.
4. Implement $\beta$-VAE (weight $\beta$ on KL term) and analyze the effect of $\beta$.</p>
<h3 id="problem-3-proof-of-optimal-discriminator-for-gan">Problem 3: Proof of Optimal Discriminator for GAN<a class="header-link" href="#problem-3-proof-of-optimal-discriminator-for-gan" title="Permanent link">&para;</a></h3>
<p>When $G$ is fixed, prove that the discriminator's objective:</p>
<p>$$\max_D \mathbb{E}_{\mathbf{x} \sim p_{\text{data}}} [\log D(\mathbf{x})] + \mathbb{E}_{\mathbf{x} \sim p_g} [\log(1 - D(\mathbf{x}))]$$</p>
<p>is maximized at $D^*(\mathbf{x}) = \frac{p_{\text{data}}(\mathbf{x})}{p_{\text{data}}(\mathbf{x}) + p_g(\mathbf{x})}$.</p>
<h3 id="problem-4-forward-process-of-diffusion-models">Problem 4: Forward Process of Diffusion Models<a class="header-link" href="#problem-4-forward-process-of-diffusion-models" title="Permanent link">&para;</a></h3>
<ol>
<li>Derive $q(\mathbf{x}_t|\mathbf{x}_0) = \mathcal{N}(\sqrt{\bar{\alpha}_t}\mathbf{x}_0, (1-\bar{\alpha}_t)I)$ recursively.</li>
<li>Visualize the distribution of $\mathbf{x}_t$ for 1D data at various $t$.</li>
<li>Show that $\mathbf{x}_T \approx \mathcal{N}(\mathbf{0}, I)$ as $T \to \infty$.</li>
</ol>
<h3 id="problem-5-flow-matching-vs-diffusion-models">Problem 5: Flow Matching vs Diffusion Models<a class="header-link" href="#problem-5-flow-matching-vs-diffusion-models" title="Permanent link">&para;</a></h3>
<ol>
<li>Implement both a simple Flow Matching model and DDPM.</li>
<li>Train both models on the same 2D data.</li>
<li>Compare sampling speed (number of steps).</li>
<li>Compare generation quality (FID, Inception Score, etc.).</li>
</ol>
<h2 id="references">References<a class="header-link" href="#references" title="Permanent link">&para;</a></h2>
<h3 id="papers">Papers<a class="header-link" href="#papers" title="Permanent link">&para;</a></h3>
<ul>
<li><strong>VAE</strong>: Kingma, D. P., &amp; Welling, M. (2014). "Auto-Encoding Variational Bayes." <em>ICLR</em>.</li>
<li><strong>GAN</strong>: Goodfellow, I., et al. (2014). "Generative Adversarial Nets." <em>NeurIPS</em>.</li>
<li><strong>WGAN</strong>: Arjovsky, M., et al. (2017). "Wasserstein Generative Adversarial Networks." <em>ICML</em>.</li>
<li><strong>WGAN-GP</strong>: Gulrajani, I., et al. (2017). "Improved Training of Wasserstein GANs." <em>NeurIPS</em>.</li>
<li><strong>DDPM</strong>: Ho, J., et al. (2020). "Denoising Diffusion Probabilistic Models." <em>NeurIPS</em>.</li>
<li><strong>Score-Based Models</strong>: Song, Y., &amp; Ermon, S. (2019). "Generative Modeling by Estimating Gradients of the Data Distribution." <em>NeurIPS</em>.</li>
<li><strong>Flow Matching</strong>: Lipman, Y., et al. (2023). "Flow Matching for Generative Modeling." <em>ICLR</em>.</li>
<li><strong>Consistency Models</strong>: Song, Y., et al. (2023). "Consistency Models." <em>ICML</em>.</li>
</ul>
<h3 id="online-resources">Online Resources<a class="header-link" href="#online-resources" title="Permanent link">&para;</a></h3>
<ul>
<li><a href="https://lilianweng.github.io/posts/2018-08-12-vae/">Lil'Log: From Autoencoder to Beta-VAE (Lilian Weng)</a></li>
<li><a href="https://lilianweng.github.io/posts/2021-07-11-diffusion-models/">Lil'Log: What are Diffusion Models? (Lilian Weng)</a></li>
<li><a href="https://arxiv.org/abs/2208.11970">Understanding Diffusion Models (Calvin Luo)</a></li>
<li><a href="https://huggingface.co/blog/annotated-diffusion">The Annotated Diffusion Model</a></li>
</ul>
<h3 id="libraries">Libraries<a class="header-link" href="#libraries" title="Permanent link">&para;</a></h3>
<ul>
<li><code>torch</code>: PyTorch implementation</li>
<li><code>diffusers</code> (Hugging Face): Diffusion model library</li>
<li><code>stable-diffusion</code>: Stable Diffusion implementation</li>
</ul>
    </div>

    
<div class="lesson-toolbar lesson-toolbar--bottom">
    <div class="toolbar-nav toolbar-nav--prev">
        
        <a href="/study/en/Math_for_AI/17_Math_of_Attention_and_Transformers.html" class="nav-prev">
            <span class="nav-label">Previous</span>
            <span class="nav-title">17. Mathematics of Attention and Transformers</span>
        </a>
        
    </div>
    <div class="toolbar-actions">
        <button class="btn btn-copy-link" data-action="copy-link" title="Copy link">
            <span class="icon">üîó</span>
            <span class="text">Copy link</span>
        </button>
        <a href="/study/en/Math_for_AI/" class="btn btn-topic-link" title="Back to topic list">
            <span class="icon">üìã</span>
            <span class="text">Topic list</span>
        </a>
    </div>
    <div class="toolbar-nav toolbar-nav--next">
        
    </div>
</div>


    <div class="toolbar-keyboard-hint">
        <kbd>&larr;</kbd> <kbd>&rarr;</kbd> to navigate between lessons
    </div>
</article>

<!-- Scroll to top floating button -->
<button class="scroll-to-top" id="scroll-to-top" title="Scroll to top" aria-label="Scroll to top">‚Üë</button>

            </div>
        </main>
    </div>

    <script src="/study/static/js/app.js"></script>
    <script>
        function switchLanguage(newLang) {
            const path = window.location.pathname;
            const base = '/study';
            const currentLang = 'en';
            const newPath = path.replace(base + '/' + currentLang + '/', base + '/' + newLang + '/');
            window.location.href = newPath + window.location.search;
        }
    </script>
    
<script>
    document.addEventListener('DOMContentLoaded', function() {
        // Copy link - independent feedback per button
        document.querySelectorAll('[data-action="copy-link"]').forEach(function(btn) {
            btn.addEventListener('click', function() {
                navigator.clipboard.writeText(window.location.href);
                var textEl = this.querySelector('.text');
                var originalText = textEl.textContent;
                textEl.textContent = 'Copied!';
                setTimeout(function() { textEl.textContent = originalText; }, 2000);
            });
        });

        // Add copy buttons to code blocks
        document.querySelectorAll('.lesson-content pre').forEach(function(pre) {
            var copyBtn = document.createElement('button');
            copyBtn.className = 'code-copy-btn';
            copyBtn.textContent = 'Copy';
            copyBtn.addEventListener('click', function() {
                var code = pre.querySelector('code');
                var text = code ? code.textContent : pre.textContent;
                navigator.clipboard.writeText(text);
                copyBtn.textContent = 'Copied!';
                setTimeout(function() { copyBtn.textContent = 'Copy'; }, 2000);
            });
            pre.style.position = 'relative';
            pre.appendChild(copyBtn);
        });

        // Scroll to top button
        var scrollBtn = document.getElementById('scroll-to-top');
        window.addEventListener('scroll', function() {
            scrollBtn.classList.toggle('visible', window.scrollY > 300);
        });
        scrollBtn.addEventListener('click', function() {
            window.scrollTo({ top: 0, behavior: 'smooth' });
        });

        // Keyboard shortcuts: left/right arrows for lesson navigation
        document.addEventListener('keydown', function(e) {
            var tag = document.activeElement.tagName;
            if (tag === 'INPUT' || tag === 'TEXTAREA' || tag === 'SELECT') return;

            if (e.key === 'ArrowLeft') {
                var prevLink = document.querySelector('.nav-prev');
                if (prevLink) prevLink.click();
            } else if (e.key === 'ArrowRight') {
                var nextLink = document.querySelector('.nav-next');
                if (nextLink) nextLink.click();
            }
        });
    });
</script>

</body>
</html>
{% endraw %}