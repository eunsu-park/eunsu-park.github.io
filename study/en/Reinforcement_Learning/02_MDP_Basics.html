{% raw %}
<!DOCTYPE html>
<html lang="en" data-theme="light">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>02. Markov Decision Process (MDP) - Study Materials</title>
    <link rel="stylesheet" href="/study/static/css/style.css">
    <link rel="stylesheet" href="/study/static/css/highlight.css">
    <!-- KaTeX for LaTeX math rendering -->
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.11/dist/katex.min.css">
    <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.11/dist/katex.min.js"></script>
    <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.11/dist/contrib/auto-render.min.js"
        onload="renderMathInElement(document.body, {
            delimiters: [
                {left: '$$', right: '$$', display: true},
                {left: '$', right: '$', display: false}
            ],
            throwOnError: false
        });"></script>
    
</head>
<body>
    <div class="app-container">
        <!-- Sidebar -->
        <aside class="sidebar" id="sidebar">
            <div class="sidebar-header">
                <a href="/study/en/" class="logo">Study Materials</a>
                <button class="sidebar-toggle" id="sidebar-toggle" aria-label="Toggle sidebar">
                    <span></span>
                </button>
            </div>

            <nav class="sidebar-nav">
                <a href="/study/en/" class="nav-item ">
                    <span class="nav-icon">ğŸ </span>
                    <span class="nav-text">Home</span>
                </a>
                <a href="/study/examples/" class="nav-item ">
                    <span class="nav-icon">ğŸ’»</span>
                    <span class="nav-text">Examples</span>
                </a>
            </nav>

            <div class="sidebar-search">
                <form action="/study/en/search.html" method="get" id="search-form">
                    <input type="search" name="q" placeholder="Search..." id="search-sidebar-input">
                </form>
            </div>

            <!-- Language Selector -->
            <div class="sidebar-lang">
                <select id="lang-select" class="lang-selector" onchange="switchLanguage(this.value)">
                    
                    <option value="en" selected>
                        English
                    </option>
                    
                    <option value="ko" >
                        í•œêµ­ì–´
                    </option>
                    
                </select>
            </div>

            <div class="sidebar-footer">
                <button class="theme-toggle" id="theme-toggle" aria-label="Toggle theme">
                    <span class="theme-icon light">â˜€ï¸</span>
                    <span class="theme-icon dark">ğŸŒ™</span>
                </button>
            </div>
        </aside>

        <!-- Main Content -->
        <main class="main-content">
            <header class="main-header">
                <button class="menu-toggle" id="menu-toggle" aria-label="Open menu">
                    <span></span>
                </button>
                
<nav class="breadcrumb">
    <a href="/study/en/">Topics</a>
    <span class="separator">/</span>
    <a href="/study/en/Reinforcement_Learning/">Reinforcement Learning</a>
    <span class="separator">/</span>
    <span class="current">02. Markov Decision Process (MDP)</span>
</nav>

            </header>

            <div class="content">
                
<article class="lesson-article">
    <header class="lesson-header">
        <h1>02. Markov Decision Process (MDP)</h1>
    </header>

    
<div class="lesson-toolbar lesson-toolbar--top">
    <div class="toolbar-nav toolbar-nav--prev">
        
        <a href="/study/en/Reinforcement_Learning/01_RL_Introduction.html" class="nav-prev">
            <span class="nav-label">Previous</span>
            <span class="nav-title">01. Introduction to Reinforcement Learning</span>
        </a>
        
    </div>
    <div class="toolbar-actions">
        <button class="btn btn-copy-link" data-action="copy-link" title="Copy link">
            <span class="icon">ğŸ”—</span>
            <span class="text">Copy link</span>
        </button>
        <a href="/study/en/Reinforcement_Learning/" class="btn btn-topic-link" title="Back to topic list">
            <span class="icon">ğŸ“‹</span>
            <span class="text">Topic list</span>
        </a>
    </div>
    <div class="toolbar-nav toolbar-nav--next">
        
        <a href="/study/en/Reinforcement_Learning/03_Dynamic_Programming.html" class="nav-next">
            <span class="nav-label">Next</span>
            <span class="nav-title">03. Dynamic Programming</span>
        </a>
        
    </div>
</div>


    
    <nav class="toc" id="toc">
        <h2>Table of Contents</h2>
        <div class="toc">
<ul>
<li><a href="#learning-objectives">Learning Objectives</a></li>
<li><a href="#1-markov-property">1. Markov Property</a><ul>
<li><a href="#11-definition">1.1 Definition</a></li>
<li><a href="#12-importance-of-state-representation">1.2 Importance of State Representation</a></li>
</ul>
</li>
<li><a href="#2-markov-decision-process-mdp">2. Markov Decision Process (MDP)</a><ul>
<li><a href="#21-five-elements-of-mdp">2.1 Five Elements of MDP</a></li>
<li><a href="#22-mdp-diagram">2.2 MDP Diagram</a></li>
<li><a href="#23-defining-mdp-in-python">2.3 Defining MDP in Python</a></li>
</ul>
</li>
<li><a href="#3-policy">3. Policy</a><ul>
<li><a href="#31-definition-of-policy">3.1 Definition of Policy</a></li>
</ul>
</li>
<li><a href="#4-value-functions">4. Value Functions</a><ul>
<li><a href="#41-state-value-function-vs">4.1 State Value Function V(s)</a></li>
<li><a href="#42-action-value-function-qs-a">4.2 Action Value Function Q(s, a)</a></li>
<li><a href="#43-relationship-between-v-and-q">4.3 Relationship between V and Q</a></li>
</ul>
</li>
<li><a href="#5-bellman-equations">5. Bellman Equations</a><ul>
<li><a href="#51-bellman-expectation-equation">5.1 Bellman Expectation Equation</a></li>
<li><a href="#52-intuitive-understanding-of-bellman-equation">5.2 Intuitive Understanding of Bellman Equation</a></li>
<li><a href="#53-bellman-equation-implementation">5.3 Bellman Equation Implementation</a></li>
</ul>
</li>
<li><a href="#6-optimal-value-functions-and-optimal-policy">6. Optimal Value Functions and Optimal Policy</a><ul>
<li><a href="#61-optimal-value-functions">6.1 Optimal Value Functions</a></li>
<li><a href="#62-bellman-optimality-equation">6.2 Bellman Optimality Equation</a></li>
<li><a href="#63-optimal-policy">6.3 Optimal Policy</a></li>
</ul>
</li>
<li><a href="#7-types-of-mdps">7. Types of MDPs</a><ul>
<li><a href="#71-finite-mdp-vs-infinite-mdp">7.1 Finite MDP vs Infinite MDP</a></li>
<li><a href="#72-deterministic-vs-stochastic-mdp">7.2 Deterministic vs Stochastic MDP</a></li>
<li><a href="#73-partially-observable-mdp-pomdp">7.3 Partially Observable MDP (POMDP)</a></li>
</ul>
</li>
<li><a href="#8-gridworld-example">8. GridWorld Example</a><ul>
<li><a href="#81-environment-definition">8.1 Environment Definition</a></li>
<li><a href="#82-value-function-visualization">8.2 Value Function Visualization</a></li>
</ul>
</li>
<li><a href="#9-summary">9. Summary</a><ul>
<li><a href="#key-concepts">Key Concepts</a></li>
<li><a href="#bellman-equation-summary">Bellman Equation Summary</a></li>
</ul>
</li>
<li><a href="#10-practice-problems">10. Practice Problems</a></li>
<li><a href="#next-steps">Next Steps</a></li>
<li><a href="#references">References</a></li>
</ul>
</div>

    </nav>
    

    <div class="lesson-content markdown-body">
        <h1 id="02-markov-decision-process-mdp">02. Markov Decision Process (MDP)<a class="header-link" href="#02-markov-decision-process-mdp" title="Permanent link">&para;</a></h1>
<p><strong>Difficulty: â­â­ (Basics)</strong></p>
<h2 id="learning-objectives">Learning Objectives<a class="header-link" href="#learning-objectives" title="Permanent link">&para;</a></h2>
<ul>
<li>Understand the Markov property and definition of MDP</li>
<li>Learn state value function V(s) and action value function Q(s,a)</li>
<li>Derive and interpret Bellman expectation and optimality equations</li>
<li>Grasp the existence and properties of optimal policies</li>
</ul>
<hr />
<h2 id="1-markov-property">1. Markov Property<a class="header-link" href="#1-markov-property" title="Permanent link">&para;</a></h2>
<h3 id="11-definition">1.1 Definition<a class="header-link" href="#11-definition" title="Permanent link">&para;</a></h3>
<p><strong>Markov Property</strong>: The future depends only on the current state and is independent of past history.</p>
<p>$$P(S_{t+1} | S_t) = P(S_{t+1} | S_1, S_2, \ldots, S_t)$$</p>
<div class="highlight"><pre><span></span><code><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">Markov Property Examples:</span>

<span class="sd">âœ“ Chess game: Knowing only the current board state is sufficient for optimal moves</span>
<span class="sd">  - Past moves don&#39;t matter</span>

<span class="sd">âœ— Poker game: Opponent&#39;s past betting patterns are important</span>
<span class="sd">  - Cannot make optimal decisions with just current cards</span>
<span class="sd">  â†’ Can be made Markovian by including past information in the state</span>
<span class="sd">&quot;&quot;&quot;</span>
</code></pre></div>

<h3 id="12-importance-of-state-representation">1.2 Importance of State Representation<a class="header-link" href="#12-importance-of-state-representation" title="Permanent link">&para;</a></h3>
<p>To satisfy the Markov property, <strong>states must contain sufficient information</strong>.</p>
<div class="highlight"><pre><span></span><code><span class="c1"># Example: Non-Markov â†’ Markov transformation</span>
<span class="k">class</span><span class="w"> </span><span class="nc">StateRepresentation</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Original state: Ball&#39;s current position (x)</span>
<span class="sd">    Problem: Cannot predict where ball will move (no velocity info)</span>

<span class="sd">    Markov state: (position, velocity) = (x, v)</span>
<span class="sd">    Solution: Next position predictable (x&#39; = x + v)</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="k">def</span><span class="w"> </span><span class="nf">non_markov_state</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">ball</span><span class="p">):</span>
        <span class="k">return</span> <span class="n">ball</span><span class="o">.</span><span class="n">position</span>  <span class="c1"># Insufficient information</span>

    <span class="k">def</span><span class="w"> </span><span class="nf">markov_state</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">ball</span><span class="p">):</span>
        <span class="k">return</span> <span class="p">(</span><span class="n">ball</span><span class="o">.</span><span class="n">position</span><span class="p">,</span> <span class="n">ball</span><span class="o">.</span><span class="n">velocity</span><span class="p">)</span>  <span class="c1"># Sufficient information</span>
</code></pre></div>

<hr />
<h2 id="2-markov-decision-process-mdp">2. Markov Decision Process (MDP)<a class="header-link" href="#2-markov-decision-process-mdp" title="Permanent link">&para;</a></h2>
<h3 id="21-five-elements-of-mdp">2.1 Five Elements of MDP<a class="header-link" href="#21-five-elements-of-mdp" title="Permanent link">&para;</a></h3>
<p>An MDP is defined by the tuple $(S, A, P, R, \gamma)$.</p>
<table>
<thead>
<tr>
<th>Element</th>
<th>Symbol</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr>
<td>State Space</td>
<td>$S$</td>
<td>Set of all possible states</td>
</tr>
<tr>
<td>Action Space</td>
<td>$A$</td>
<td>Set of all possible actions</td>
</tr>
<tr>
<td>Transition Probability</td>
<td>$P$</td>
<td>$P(s'|s, a)$ - State transition probability</td>
</tr>
<tr>
<td>Reward Function</td>
<td>$R$</td>
<td>$R(s, a, s')$ - Immediate reward</td>
</tr>
<tr>
<td>Discount Factor</td>
<td>$\gamma$</td>
<td>Discount for future rewards (0 â‰¤ Î³ â‰¤ 1)</td>
</tr>
</tbody>
</table>
<h3 id="22-mdp-diagram">2.2 MDP Diagram<a class="header-link" href="#22-mdp-diagram" title="Permanent link">&para;</a></h3>
<div class="highlight"><pre><span></span><code>           aâ‚                    aâ‚‚
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚              â”‚      â”‚              â”‚
    â–¼     r=+1     â”‚      â–¼     r=-1     â”‚
   sâ‚ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€sâ‚‚â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â–º sâ‚ƒ
    â”‚    p=0.7     â”‚      â”‚    p=0.3     â”‚      â”‚
    â”‚              â”‚      â”‚              â”‚      â”‚
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜      â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜      â–¼
                                              Terminal
</code></pre></div>

<h3 id="23-defining-mdp-in-python">2.3 Defining MDP in Python<a class="header-link" href="#23-defining-mdp-in-python" title="Permanent link">&para;</a></h3>
<div class="highlight"><pre><span></span><code><span class="kn">import</span><span class="w"> </span><span class="nn">numpy</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">np</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">typing</span><span class="w"> </span><span class="kn">import</span> <span class="n">Dict</span><span class="p">,</span> <span class="n">Tuple</span><span class="p">,</span> <span class="n">List</span>

<span class="k">class</span><span class="w"> </span><span class="nc">MDP</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Markov Decision Process class&quot;&quot;&quot;</span>

    <span class="k">def</span><span class="w"> </span><span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="c1"># State space</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">states</span> <span class="o">=</span> <span class="p">[</span><span class="s1">&#39;s0&#39;</span><span class="p">,</span> <span class="s1">&#39;s1&#39;</span><span class="p">,</span> <span class="s1">&#39;s2&#39;</span><span class="p">,</span> <span class="s1">&#39;terminal&#39;</span><span class="p">]</span>

        <span class="c1"># Action space</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">actions</span> <span class="o">=</span> <span class="p">[</span><span class="s1">&#39;left&#39;</span><span class="p">,</span> <span class="s1">&#39;right&#39;</span><span class="p">]</span>

        <span class="c1"># Discount factor</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">gamma</span> <span class="o">=</span> <span class="mf">0.9</span>

        <span class="c1"># Transition probabilities: P[s][a] = [(prob, next_state, reward, done), ...]</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">P</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_build_transitions</span><span class="p">()</span>

    <span class="k">def</span><span class="w"> </span><span class="nf">_build_transitions</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Dict</span><span class="p">:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Define transition probabilities&quot;&quot;&quot;</span>
        <span class="n">P</span> <span class="o">=</span> <span class="p">{}</span>

        <span class="c1"># Transitions from state s0</span>
        <span class="n">P</span><span class="p">[</span><span class="s1">&#39;s0&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="p">{</span>
            <span class="s1">&#39;left&#39;</span><span class="p">:</span> <span class="p">[(</span><span class="mf">1.0</span><span class="p">,</span> <span class="s1">&#39;s0&#39;</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="kc">False</span><span class="p">)],</span>      <span class="c1"># Hit wall</span>
            <span class="s1">&#39;right&#39;</span><span class="p">:</span> <span class="p">[(</span><span class="mf">1.0</span><span class="p">,</span> <span class="s1">&#39;s1&#39;</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="kc">False</span><span class="p">)]</span>       <span class="c1"># Move to s1</span>
        <span class="p">}</span>

        <span class="c1"># Transitions from state s1</span>
        <span class="n">P</span><span class="p">[</span><span class="s1">&#39;s1&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="p">{</span>
            <span class="s1">&#39;left&#39;</span><span class="p">:</span> <span class="p">[(</span><span class="mf">1.0</span><span class="p">,</span> <span class="s1">&#39;s0&#39;</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="kc">False</span><span class="p">)],</span>       <span class="c1"># Move to s0</span>
            <span class="s1">&#39;right&#39;</span><span class="p">:</span> <span class="p">[(</span><span class="mf">0.8</span><span class="p">,</span> <span class="s1">&#39;s2&#39;</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="kc">False</span><span class="p">),</span>       <span class="c1"># 80%: move to s2</span>
                      <span class="p">(</span><span class="mf">0.2</span><span class="p">,</span> <span class="s1">&#39;s1&#39;</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="kc">False</span><span class="p">)]</span>       <span class="c1"># 20%: stay</span>
        <span class="p">}</span>

        <span class="c1"># Transitions from state s2</span>
        <span class="n">P</span><span class="p">[</span><span class="s1">&#39;s2&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="p">{</span>
            <span class="s1">&#39;left&#39;</span><span class="p">:</span> <span class="p">[(</span><span class="mf">1.0</span><span class="p">,</span> <span class="s1">&#39;s1&#39;</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="kc">False</span><span class="p">)],</span>       <span class="c1"># Move to s1</span>
            <span class="s1">&#39;right&#39;</span><span class="p">:</span> <span class="p">[(</span><span class="mf">1.0</span><span class="p">,</span> <span class="s1">&#39;terminal&#39;</span><span class="p">,</span> <span class="o">+</span><span class="mi">10</span><span class="p">,</span> <span class="kc">True</span><span class="p">)]</span> <span class="c1"># Reach goal!</span>
        <span class="p">}</span>

        <span class="c1"># Terminal state</span>
        <span class="n">P</span><span class="p">[</span><span class="s1">&#39;terminal&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="p">{</span>
            <span class="s1">&#39;left&#39;</span><span class="p">:</span> <span class="p">[(</span><span class="mf">1.0</span><span class="p">,</span> <span class="s1">&#39;terminal&#39;</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="kc">True</span><span class="p">)],</span>
            <span class="s1">&#39;right&#39;</span><span class="p">:</span> <span class="p">[(</span><span class="mf">1.0</span><span class="p">,</span> <span class="s1">&#39;terminal&#39;</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="kc">True</span><span class="p">)]</span>
        <span class="p">}</span>

        <span class="k">return</span> <span class="n">P</span>

    <span class="k">def</span><span class="w"> </span><span class="nf">get_transitions</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">state</span><span class="p">:</span> <span class="nb">str</span><span class="p">,</span> <span class="n">action</span><span class="p">:</span> <span class="nb">str</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">List</span><span class="p">[</span><span class="n">Tuple</span><span class="p">]:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Return transition information for given state and action&quot;&quot;&quot;</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">P</span><span class="p">[</span><span class="n">state</span><span class="p">][</span><span class="n">action</span><span class="p">]</span>

    <span class="k">def</span><span class="w"> </span><span class="nf">step</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">state</span><span class="p">:</span> <span class="nb">str</span><span class="p">,</span> <span class="n">action</span><span class="p">:</span> <span class="nb">str</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tuple</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="nb">float</span><span class="p">,</span> <span class="nb">bool</span><span class="p">]:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Execute one step in environment (stochastic transition)&quot;&quot;&quot;</span>
        <span class="n">transitions</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">P</span><span class="p">[</span><span class="n">state</span><span class="p">][</span><span class="n">action</span><span class="p">]</span>
        <span class="n">probs</span> <span class="o">=</span> <span class="p">[</span><span class="n">t</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="k">for</span> <span class="n">t</span> <span class="ow">in</span> <span class="n">transitions</span><span class="p">]</span>
        <span class="n">idx</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">choice</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">transitions</span><span class="p">),</span> <span class="n">p</span><span class="o">=</span><span class="n">probs</span><span class="p">)</span>
        <span class="n">prob</span><span class="p">,</span> <span class="n">next_state</span><span class="p">,</span> <span class="n">reward</span><span class="p">,</span> <span class="n">done</span> <span class="o">=</span> <span class="n">transitions</span><span class="p">[</span><span class="n">idx</span><span class="p">]</span>
        <span class="k">return</span> <span class="n">next_state</span><span class="p">,</span> <span class="n">reward</span><span class="p">,</span> <span class="n">done</span>


<span class="c1"># MDP usage example</span>
<span class="n">mdp</span> <span class="o">=</span> <span class="n">MDP</span><span class="p">()</span>

<span class="c1"># Run episode</span>
<span class="n">state</span> <span class="o">=</span> <span class="s1">&#39;s0&#39;</span>
<span class="n">total_reward</span> <span class="o">=</span> <span class="mi">0</span>

<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;=== MDP Simulation ===&quot;</span><span class="p">)</span>
<span class="k">while</span> <span class="kc">True</span><span class="p">:</span>
    <span class="n">action</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">choice</span><span class="p">(</span><span class="n">mdp</span><span class="o">.</span><span class="n">actions</span><span class="p">)</span>  <span class="c1"># Random policy</span>
    <span class="n">next_state</span><span class="p">,</span> <span class="n">reward</span><span class="p">,</span> <span class="n">done</span> <span class="o">=</span> <span class="n">mdp</span><span class="o">.</span><span class="n">step</span><span class="p">(</span><span class="n">state</span><span class="p">,</span> <span class="n">action</span><span class="p">)</span>

    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="n">state</span><span class="si">}</span><span class="s2"> --[</span><span class="si">{</span><span class="n">action</span><span class="si">}</span><span class="s2">]--&gt; </span><span class="si">{</span><span class="n">next_state</span><span class="si">}</span><span class="s2">, reward=</span><span class="si">{</span><span class="n">reward</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>

    <span class="n">total_reward</span> <span class="o">+=</span> <span class="n">reward</span>
    <span class="n">state</span> <span class="o">=</span> <span class="n">next_state</span>

    <span class="k">if</span> <span class="n">done</span><span class="p">:</span>
        <span class="k">break</span>

<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;</span><span class="se">\n</span><span class="s2">Total reward: </span><span class="si">{</span><span class="n">total_reward</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
</code></pre></div>

<hr />
<h2 id="3-policy">3. Policy<a class="header-link" href="#3-policy" title="Permanent link">&para;</a></h2>
<h3 id="31-definition-of-policy">3.1 Definition of Policy<a class="header-link" href="#31-definition-of-policy" title="Permanent link">&para;</a></h3>
<p><strong>Policy Ï€</strong> is a rule for selecting actions in states.</p>
<ul>
<li><strong>Deterministic policy</strong>: $\pi(s) = a$</li>
<li><strong>Stochastic policy</strong>: $\pi(a|s) = P(A_t = a | S_t = s)$</li>
</ul>
<div class="highlight"><pre><span></span><code><span class="k">class</span><span class="w"> </span><span class="nc">Policy</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Policy class&quot;&quot;&quot;</span>

    <span class="k">def</span><span class="w"> </span><span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">states</span><span class="p">,</span> <span class="n">actions</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">states</span> <span class="o">=</span> <span class="n">states</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">actions</span> <span class="o">=</span> <span class="n">actions</span>
        <span class="c1"># Stochastic policy: action probability distribution for each state</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">policy</span> <span class="o">=</span> <span class="p">{</span><span class="n">s</span><span class="p">:</span> <span class="p">{</span><span class="n">a</span><span class="p">:</span> <span class="mi">1</span><span class="o">/</span><span class="nb">len</span><span class="p">(</span><span class="n">actions</span><span class="p">)</span> <span class="k">for</span> <span class="n">a</span> <span class="ow">in</span> <span class="n">actions</span><span class="p">}</span>
                       <span class="k">for</span> <span class="n">s</span> <span class="ow">in</span> <span class="n">states</span><span class="p">}</span>

    <span class="k">def</span><span class="w"> </span><span class="nf">get_action_prob</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">state</span><span class="p">:</span> <span class="nb">str</span><span class="p">,</span> <span class="n">action</span><span class="p">:</span> <span class="nb">str</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">float</span><span class="p">:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Return Ï€(a|s)&quot;&quot;&quot;</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">policy</span><span class="p">[</span><span class="n">state</span><span class="p">][</span><span class="n">action</span><span class="p">]</span>

    <span class="k">def</span><span class="w"> </span><span class="nf">sample_action</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">state</span><span class="p">:</span> <span class="nb">str</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">str</span><span class="p">:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Sample action according to policy&quot;&quot;&quot;</span>
        <span class="n">actions</span> <span class="o">=</span> <span class="nb">list</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">policy</span><span class="p">[</span><span class="n">state</span><span class="p">]</span><span class="o">.</span><span class="n">keys</span><span class="p">())</span>
        <span class="n">probs</span> <span class="o">=</span> <span class="nb">list</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">policy</span><span class="p">[</span><span class="n">state</span><span class="p">]</span><span class="o">.</span><span class="n">values</span><span class="p">())</span>
        <span class="k">return</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">choice</span><span class="p">(</span><span class="n">actions</span><span class="p">,</span> <span class="n">p</span><span class="o">=</span><span class="n">probs</span><span class="p">)</span>

    <span class="k">def</span><span class="w"> </span><span class="nf">set_deterministic</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">state</span><span class="p">:</span> <span class="nb">str</span><span class="p">,</span> <span class="n">action</span><span class="p">:</span> <span class="nb">str</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Set as deterministic policy&quot;&quot;&quot;</span>
        <span class="k">for</span> <span class="n">a</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">actions</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">policy</span><span class="p">[</span><span class="n">state</span><span class="p">][</span><span class="n">a</span><span class="p">]</span> <span class="o">=</span> <span class="mf">1.0</span> <span class="k">if</span> <span class="n">a</span> <span class="o">==</span> <span class="n">action</span> <span class="k">else</span> <span class="mf">0.0</span>


<span class="c1"># Example: Policy that always moves right</span>
<span class="n">policy</span> <span class="o">=</span> <span class="n">Policy</span><span class="p">([</span><span class="s1">&#39;s0&#39;</span><span class="p">,</span> <span class="s1">&#39;s1&#39;</span><span class="p">,</span> <span class="s1">&#39;s2&#39;</span><span class="p">],</span> <span class="p">[</span><span class="s1">&#39;left&#39;</span><span class="p">,</span> <span class="s1">&#39;right&#39;</span><span class="p">])</span>
<span class="n">policy</span><span class="o">.</span><span class="n">set_deterministic</span><span class="p">(</span><span class="s1">&#39;s0&#39;</span><span class="p">,</span> <span class="s1">&#39;right&#39;</span><span class="p">)</span>
<span class="n">policy</span><span class="o">.</span><span class="n">set_deterministic</span><span class="p">(</span><span class="s1">&#39;s1&#39;</span><span class="p">,</span> <span class="s1">&#39;right&#39;</span><span class="p">)</span>
<span class="n">policy</span><span class="o">.</span><span class="n">set_deterministic</span><span class="p">(</span><span class="s1">&#39;s2&#39;</span><span class="p">,</span> <span class="s1">&#39;right&#39;</span><span class="p">)</span>
</code></pre></div>

<hr />
<h2 id="4-value-functions">4. Value Functions<a class="header-link" href="#4-value-functions" title="Permanent link">&para;</a></h2>
<h3 id="41-state-value-function-vs">4.1 State Value Function V(s)<a class="header-link" href="#41-state-value-function-vs" title="Permanent link">&para;</a></h3>
<p>The expected <strong>cumulative reward</strong> when following policy Ï€ from state s.</p>
<p>$$V^\pi(s) = \mathbb{E}_\pi[G_t | S_t = s] = \mathbb{E}_\pi\left[\sum_{k=0}^{\infty} \gamma^k R_{t+k+1} \bigg| S_t = s\right]$$</p>
<div class="highlight"><pre><span></span><code><span class="k">def</span><span class="w"> </span><span class="nf">compute_state_value</span><span class="p">(</span><span class="n">mdp</span><span class="p">,</span> <span class="n">policy</span><span class="p">,</span> <span class="n">state</span><span class="p">,</span> <span class="n">gamma</span><span class="p">,</span> <span class="n">depth</span><span class="o">=</span><span class="mi">100</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Estimate state value using Monte Carlo approach</span>
<span class="sd">    (average return over multiple episodes)</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">returns</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="n">n_episodes</span> <span class="o">=</span> <span class="mi">1000</span>

    <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">n_episodes</span><span class="p">):</span>
        <span class="n">s</span> <span class="o">=</span> <span class="n">state</span>
        <span class="n">episode_return</span> <span class="o">=</span> <span class="mi">0</span>
        <span class="n">discount</span> <span class="o">=</span> <span class="mf">1.0</span>

        <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">depth</span><span class="p">):</span>
            <span class="n">action</span> <span class="o">=</span> <span class="n">policy</span><span class="o">.</span><span class="n">sample_action</span><span class="p">(</span><span class="n">s</span><span class="p">)</span>
            <span class="n">next_s</span><span class="p">,</span> <span class="n">reward</span><span class="p">,</span> <span class="n">done</span> <span class="o">=</span> <span class="n">mdp</span><span class="o">.</span><span class="n">step</span><span class="p">(</span><span class="n">s</span><span class="p">,</span> <span class="n">action</span><span class="p">)</span>

            <span class="n">episode_return</span> <span class="o">+=</span> <span class="n">discount</span> <span class="o">*</span> <span class="n">reward</span>
            <span class="n">discount</span> <span class="o">*=</span> <span class="n">gamma</span>
            <span class="n">s</span> <span class="o">=</span> <span class="n">next_s</span>

            <span class="k">if</span> <span class="n">done</span><span class="p">:</span>
                <span class="k">break</span>

        <span class="n">returns</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">episode_return</span><span class="p">)</span>

    <span class="k">return</span> <span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">returns</span><span class="p">)</span>
</code></pre></div>

<h3 id="42-action-value-function-qs-a">4.2 Action Value Function Q(s, a)<a class="header-link" href="#42-action-value-function-qs-a" title="Permanent link">&para;</a></h3>
<p>Expected <strong>cumulative reward</strong> when taking action a in state s, then following policy Ï€.</p>
<p>$$Q^\pi(s, a) = \mathbb{E}_\pi[G_t | S_t = s, A_t = a]$$</p>
<div class="highlight"><pre><span></span><code><span class="k">def</span><span class="w"> </span><span class="nf">compute_action_value</span><span class="p">(</span><span class="n">mdp</span><span class="p">,</span> <span class="n">policy</span><span class="p">,</span> <span class="n">state</span><span class="p">,</span> <span class="n">action</span><span class="p">,</span> <span class="n">gamma</span><span class="p">,</span> <span class="n">depth</span><span class="o">=</span><span class="mi">100</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Estimate action value&quot;&quot;&quot;</span>
    <span class="n">returns</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="n">n_episodes</span> <span class="o">=</span> <span class="mi">1000</span>

    <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">n_episodes</span><span class="p">):</span>
        <span class="c1"># First action is given</span>
        <span class="n">s</span> <span class="o">=</span> <span class="n">state</span>
        <span class="n">next_s</span><span class="p">,</span> <span class="n">reward</span><span class="p">,</span> <span class="n">done</span> <span class="o">=</span> <span class="n">mdp</span><span class="o">.</span><span class="n">step</span><span class="p">(</span><span class="n">s</span><span class="p">,</span> <span class="n">action</span><span class="p">)</span>

        <span class="n">episode_return</span> <span class="o">=</span> <span class="n">reward</span>
        <span class="n">discount</span> <span class="o">=</span> <span class="n">gamma</span>
        <span class="n">s</span> <span class="o">=</span> <span class="n">next_s</span>

        <span class="c1"># Then follow policy</span>
        <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">depth</span> <span class="o">-</span> <span class="mi">1</span><span class="p">):</span>
            <span class="k">if</span> <span class="n">done</span><span class="p">:</span>
                <span class="k">break</span>

            <span class="n">a</span> <span class="o">=</span> <span class="n">policy</span><span class="o">.</span><span class="n">sample_action</span><span class="p">(</span><span class="n">s</span><span class="p">)</span>
            <span class="n">next_s</span><span class="p">,</span> <span class="n">reward</span><span class="p">,</span> <span class="n">done</span> <span class="o">=</span> <span class="n">mdp</span><span class="o">.</span><span class="n">step</span><span class="p">(</span><span class="n">s</span><span class="p">,</span> <span class="n">a</span><span class="p">)</span>

            <span class="n">episode_return</span> <span class="o">+=</span> <span class="n">discount</span> <span class="o">*</span> <span class="n">reward</span>
            <span class="n">discount</span> <span class="o">*=</span> <span class="n">gamma</span>
            <span class="n">s</span> <span class="o">=</span> <span class="n">next_s</span>

        <span class="n">returns</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">episode_return</span><span class="p">)</span>

    <span class="k">return</span> <span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">returns</span><span class="p">)</span>
</code></pre></div>

<h3 id="43-relationship-between-v-and-q">4.3 Relationship between V and Q<a class="header-link" href="#43-relationship-between-v-and-q" title="Permanent link">&para;</a></h3>
<p>$$V^\pi(s) = \sum_{a \in A} \pi(a|s) \cdot Q^\pi(s, a)$$</p>
<div class="highlight"><pre><span></span><code><span class="k">def</span><span class="w"> </span><span class="nf">v_from_q</span><span class="p">(</span><span class="n">policy</span><span class="p">,</span> <span class="n">q_values</span><span class="p">,</span> <span class="n">state</span><span class="p">,</span> <span class="n">actions</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Calculate V value from Q values&quot;&quot;&quot;</span>
    <span class="n">v</span> <span class="o">=</span> <span class="mi">0</span>
    <span class="k">for</span> <span class="n">a</span> <span class="ow">in</span> <span class="n">actions</span><span class="p">:</span>
        <span class="n">v</span> <span class="o">+=</span> <span class="n">policy</span><span class="o">.</span><span class="n">get_action_prob</span><span class="p">(</span><span class="n">state</span><span class="p">,</span> <span class="n">a</span><span class="p">)</span> <span class="o">*</span> <span class="n">q_values</span><span class="p">[(</span><span class="n">state</span><span class="p">,</span> <span class="n">a</span><span class="p">)]</span>
    <span class="k">return</span> <span class="n">v</span>
</code></pre></div>

<hr />
<h2 id="5-bellman-equations">5. Bellman Equations<a class="header-link" href="#5-bellman-equations" title="Permanent link">&para;</a></h2>
<h3 id="51-bellman-expectation-equation">5.1 Bellman Expectation Equation<a class="header-link" href="#51-bellman-expectation-equation" title="Permanent link">&para;</a></h3>
<p>Expresses the value of the current state <strong>recursively</strong> in terms of the value of the next state.</p>
<p><strong>State value function:</strong></p>
<p>$$V^\pi(s) = \sum_{a} \pi(a|s) \sum_{s', r} P(s', r | s, a) \left[ r + \gamma V^\pi(s') \right]$$</p>
<p><strong>Action value function:</strong></p>
<p>$$Q^\pi(s, a) = \sum_{s', r} P(s', r | s, a) \left[ r + \gamma \sum_{a'} \pi(a'|s') Q^\pi(s', a') \right]$$</p>
<h3 id="52-intuitive-understanding-of-bellman-equation">5.2 Intuitive Understanding of Bellman Equation<a class="header-link" href="#52-intuitive-understanding-of-bellman-equation" title="Permanent link">&para;</a></h3>
<div class="highlight"><pre><span></span><code>V(s) = Immediate reward + Discounted future value

     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
     â”‚   V(s) = r + Î³ * V(s&#39;)                  â”‚
     â”‚                                          â”‚
     â”‚   Current value = Immediate reward + Î³ Ã— Next state valueâ”‚
     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
</code></pre></div>

<h3 id="53-bellman-equation-implementation">5.3 Bellman Equation Implementation<a class="header-link" href="#53-bellman-equation-implementation" title="Permanent link">&para;</a></h3>
<div class="highlight"><pre><span></span><code><span class="k">def</span><span class="w"> </span><span class="nf">bellman_expectation_v</span><span class="p">(</span><span class="n">mdp</span><span class="p">,</span> <span class="n">policy</span><span class="p">,</span> <span class="n">V</span><span class="p">,</span> <span class="n">state</span><span class="p">,</span> <span class="n">gamma</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Calculate V(s) using Bellman expectation equation</span>

<span class="sd">    V(s) = Î£_a Ï€(a|s) * Î£_{s&#39;,r} P(s&#39;,r|s,a) * [r + Î³V(s&#39;)]</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">if</span> <span class="n">state</span> <span class="o">==</span> <span class="s1">&#39;terminal&#39;</span><span class="p">:</span>
        <span class="k">return</span> <span class="mi">0</span>

    <span class="n">value</span> <span class="o">=</span> <span class="mi">0</span>

    <span class="k">for</span> <span class="n">action</span> <span class="ow">in</span> <span class="n">mdp</span><span class="o">.</span><span class="n">actions</span><span class="p">:</span>
        <span class="n">action_prob</span> <span class="o">=</span> <span class="n">policy</span><span class="o">.</span><span class="n">get_action_prob</span><span class="p">(</span><span class="n">state</span><span class="p">,</span> <span class="n">action</span><span class="p">)</span>

        <span class="k">for</span> <span class="n">prob</span><span class="p">,</span> <span class="n">next_state</span><span class="p">,</span> <span class="n">reward</span><span class="p">,</span> <span class="n">done</span> <span class="ow">in</span> <span class="n">mdp</span><span class="o">.</span><span class="n">get_transitions</span><span class="p">(</span><span class="n">state</span><span class="p">,</span> <span class="n">action</span><span class="p">):</span>
            <span class="k">if</span> <span class="n">done</span><span class="p">:</span>
                <span class="n">value</span> <span class="o">+=</span> <span class="n">action_prob</span> <span class="o">*</span> <span class="n">prob</span> <span class="o">*</span> <span class="n">reward</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="n">value</span> <span class="o">+=</span> <span class="n">action_prob</span> <span class="o">*</span> <span class="n">prob</span> <span class="o">*</span> <span class="p">(</span><span class="n">reward</span> <span class="o">+</span> <span class="n">gamma</span> <span class="o">*</span> <span class="n">V</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="n">next_state</span><span class="p">,</span> <span class="mi">0</span><span class="p">))</span>

    <span class="k">return</span> <span class="n">value</span>


<span class="k">def</span><span class="w"> </span><span class="nf">bellman_expectation_q</span><span class="p">(</span><span class="n">mdp</span><span class="p">,</span> <span class="n">policy</span><span class="p">,</span> <span class="n">Q</span><span class="p">,</span> <span class="n">state</span><span class="p">,</span> <span class="n">action</span><span class="p">,</span> <span class="n">gamma</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Calculate Q(s,a) using Bellman expectation equation</span>

<span class="sd">    Q(s,a) = Î£_{s&#39;,r} P(s&#39;,r|s,a) * [r + Î³ * Î£_a&#39; Ï€(a&#39;|s&#39;) * Q(s&#39;,a&#39;)]</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">value</span> <span class="o">=</span> <span class="mi">0</span>

    <span class="k">for</span> <span class="n">prob</span><span class="p">,</span> <span class="n">next_state</span><span class="p">,</span> <span class="n">reward</span><span class="p">,</span> <span class="n">done</span> <span class="ow">in</span> <span class="n">mdp</span><span class="o">.</span><span class="n">get_transitions</span><span class="p">(</span><span class="n">state</span><span class="p">,</span> <span class="n">action</span><span class="p">):</span>
        <span class="k">if</span> <span class="n">done</span><span class="p">:</span>
            <span class="n">value</span> <span class="o">+=</span> <span class="n">prob</span> <span class="o">*</span> <span class="n">reward</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="c1"># Expected value at next state</span>
            <span class="n">next_value</span> <span class="o">=</span> <span class="nb">sum</span><span class="p">(</span>
                <span class="n">policy</span><span class="o">.</span><span class="n">get_action_prob</span><span class="p">(</span><span class="n">next_state</span><span class="p">,</span> <span class="n">a</span><span class="p">)</span> <span class="o">*</span> <span class="n">Q</span><span class="o">.</span><span class="n">get</span><span class="p">((</span><span class="n">next_state</span><span class="p">,</span> <span class="n">a</span><span class="p">),</span> <span class="mi">0</span><span class="p">)</span>
                <span class="k">for</span> <span class="n">a</span> <span class="ow">in</span> <span class="n">mdp</span><span class="o">.</span><span class="n">actions</span>
            <span class="p">)</span>
            <span class="n">value</span> <span class="o">+=</span> <span class="n">prob</span> <span class="o">*</span> <span class="p">(</span><span class="n">reward</span> <span class="o">+</span> <span class="n">gamma</span> <span class="o">*</span> <span class="n">next_value</span><span class="p">)</span>

    <span class="k">return</span> <span class="n">value</span>
</code></pre></div>

<hr />
<h2 id="6-optimal-value-functions-and-optimal-policy">6. Optimal Value Functions and Optimal Policy<a class="header-link" href="#6-optimal-value-functions-and-optimal-policy" title="Permanent link">&para;</a></h2>
<h3 id="61-optimal-value-functions">6.1 Optimal Value Functions<a class="header-link" href="#61-optimal-value-functions" title="Permanent link">&para;</a></h3>
<p><strong>Optimal state value function</strong> $V^*(s)$: Maximum value across all policies</p>
<p>$$V^*(s) = \max_\pi V^\pi(s)$$</p>
<p><strong>Optimal action value function</strong> $Q^*(s, a)$:</p>
<p>$$Q^*(s, a) = \max_\pi Q^\pi(s, a)$$</p>
<h3 id="62-bellman-optimality-equation">6.2 Bellman Optimality Equation<a class="header-link" href="#62-bellman-optimality-equation" title="Permanent link">&para;</a></h3>
<p>$$V^*(s) = \max_{a} \sum_{s', r} P(s', r | s, a) \left[ r + \gamma V^*(s') \right]$$</p>
<p>$$Q^*(s, a) = \sum_{s', r} P(s', r | s, a) \left[ r + \gamma \max_{a'} Q^*(s', a') \right]$$</p>
<div class="highlight"><pre><span></span><code><span class="k">def</span><span class="w"> </span><span class="nf">bellman_optimality_v</span><span class="p">(</span><span class="n">mdp</span><span class="p">,</span> <span class="n">V</span><span class="p">,</span> <span class="n">state</span><span class="p">,</span> <span class="n">gamma</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Calculate V*(s) using Bellman optimality equation</span>

<span class="sd">    V*(s) = max_a Î£_{s&#39;,r} P(s&#39;,r|s,a) * [r + Î³V*(s&#39;)]</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">if</span> <span class="n">state</span> <span class="o">==</span> <span class="s1">&#39;terminal&#39;</span><span class="p">:</span>
        <span class="k">return</span> <span class="mi">0</span>

    <span class="n">max_value</span> <span class="o">=</span> <span class="nb">float</span><span class="p">(</span><span class="s1">&#39;-inf&#39;</span><span class="p">)</span>

    <span class="k">for</span> <span class="n">action</span> <span class="ow">in</span> <span class="n">mdp</span><span class="o">.</span><span class="n">actions</span><span class="p">:</span>
        <span class="n">action_value</span> <span class="o">=</span> <span class="mi">0</span>

        <span class="k">for</span> <span class="n">prob</span><span class="p">,</span> <span class="n">next_state</span><span class="p">,</span> <span class="n">reward</span><span class="p">,</span> <span class="n">done</span> <span class="ow">in</span> <span class="n">mdp</span><span class="o">.</span><span class="n">get_transitions</span><span class="p">(</span><span class="n">state</span><span class="p">,</span> <span class="n">action</span><span class="p">):</span>
            <span class="k">if</span> <span class="n">done</span><span class="p">:</span>
                <span class="n">action_value</span> <span class="o">+=</span> <span class="n">prob</span> <span class="o">*</span> <span class="n">reward</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="n">action_value</span> <span class="o">+=</span> <span class="n">prob</span> <span class="o">*</span> <span class="p">(</span><span class="n">reward</span> <span class="o">+</span> <span class="n">gamma</span> <span class="o">*</span> <span class="n">V</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="n">next_state</span><span class="p">,</span> <span class="mi">0</span><span class="p">))</span>

        <span class="n">max_value</span> <span class="o">=</span> <span class="nb">max</span><span class="p">(</span><span class="n">max_value</span><span class="p">,</span> <span class="n">action_value</span><span class="p">)</span>

    <span class="k">return</span> <span class="n">max_value</span>


<span class="k">def</span><span class="w"> </span><span class="nf">bellman_optimality_q</span><span class="p">(</span><span class="n">mdp</span><span class="p">,</span> <span class="n">Q</span><span class="p">,</span> <span class="n">state</span><span class="p">,</span> <span class="n">action</span><span class="p">,</span> <span class="n">gamma</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Calculate Q*(s,a) using Bellman optimality equation</span>

<span class="sd">    Q*(s,a) = Î£_{s&#39;,r} P(s&#39;,r|s,a) * [r + Î³ * max_a&#39; Q*(s&#39;,a&#39;)]</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">value</span> <span class="o">=</span> <span class="mi">0</span>

    <span class="k">for</span> <span class="n">prob</span><span class="p">,</span> <span class="n">next_state</span><span class="p">,</span> <span class="n">reward</span><span class="p">,</span> <span class="n">done</span> <span class="ow">in</span> <span class="n">mdp</span><span class="o">.</span><span class="n">get_transitions</span><span class="p">(</span><span class="n">state</span><span class="p">,</span> <span class="n">action</span><span class="p">):</span>
        <span class="k">if</span> <span class="n">done</span><span class="p">:</span>
            <span class="n">value</span> <span class="o">+=</span> <span class="n">prob</span> <span class="o">*</span> <span class="n">reward</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">max_next_q</span> <span class="o">=</span> <span class="nb">max</span><span class="p">(</span><span class="n">Q</span><span class="o">.</span><span class="n">get</span><span class="p">((</span><span class="n">next_state</span><span class="p">,</span> <span class="n">a</span><span class="p">),</span> <span class="mi">0</span><span class="p">)</span> <span class="k">for</span> <span class="n">a</span> <span class="ow">in</span> <span class="n">mdp</span><span class="o">.</span><span class="n">actions</span><span class="p">)</span>
            <span class="n">value</span> <span class="o">+=</span> <span class="n">prob</span> <span class="o">*</span> <span class="p">(</span><span class="n">reward</span> <span class="o">+</span> <span class="n">gamma</span> <span class="o">*</span> <span class="n">max_next_q</span><span class="p">)</span>

    <span class="k">return</span> <span class="n">value</span>
</code></pre></div>

<h3 id="63-optimal-policy">6.3 Optimal Policy<a class="header-link" href="#63-optimal-policy" title="Permanent link">&para;</a></h3>
<p>Optimal policy $\pi^*$ can be easily derived if $Q^*$ is known:</p>
<p>$$\pi^*(s) = \arg\max_{a} Q^*(s, a)$$</p>
<div class="highlight"><pre><span></span><code><span class="k">def</span><span class="w"> </span><span class="nf">get_optimal_policy</span><span class="p">(</span><span class="n">mdp</span><span class="p">,</span> <span class="n">Q_star</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Derive optimal policy from Q*&quot;&quot;&quot;</span>
    <span class="n">optimal_policy</span> <span class="o">=</span> <span class="p">{}</span>

    <span class="k">for</span> <span class="n">state</span> <span class="ow">in</span> <span class="n">mdp</span><span class="o">.</span><span class="n">states</span><span class="p">:</span>
        <span class="k">if</span> <span class="n">state</span> <span class="o">==</span> <span class="s1">&#39;terminal&#39;</span><span class="p">:</span>
            <span class="k">continue</span>

        <span class="c1"># Calculate Q* value for each action</span>
        <span class="n">q_values</span> <span class="o">=</span> <span class="p">{</span><span class="n">a</span><span class="p">:</span> <span class="n">Q_star</span><span class="o">.</span><span class="n">get</span><span class="p">((</span><span class="n">state</span><span class="p">,</span> <span class="n">a</span><span class="p">),</span> <span class="mi">0</span><span class="p">)</span> <span class="k">for</span> <span class="n">a</span> <span class="ow">in</span> <span class="n">mdp</span><span class="o">.</span><span class="n">actions</span><span class="p">}</span>

        <span class="c1"># Select action with maximum Q* value</span>
        <span class="n">optimal_action</span> <span class="o">=</span> <span class="nb">max</span><span class="p">(</span><span class="n">q_values</span><span class="p">,</span> <span class="n">key</span><span class="o">=</span><span class="n">q_values</span><span class="o">.</span><span class="n">get</span><span class="p">)</span>
        <span class="n">optimal_policy</span><span class="p">[</span><span class="n">state</span><span class="p">]</span> <span class="o">=</span> <span class="n">optimal_action</span>

    <span class="k">return</span> <span class="n">optimal_policy</span>
</code></pre></div>

<hr />
<h2 id="7-types-of-mdps">7. Types of MDPs<a class="header-link" href="#7-types-of-mdps" title="Permanent link">&para;</a></h2>
<h3 id="71-finite-mdp-vs-infinite-mdp">7.1 Finite MDP vs Infinite MDP<a class="header-link" href="#71-finite-mdp-vs-infinite-mdp" title="Permanent link">&para;</a></h3>
<table>
<thead>
<tr>
<th>Characteristic</th>
<th>Finite MDP</th>
<th>Infinite MDP</th>
</tr>
</thead>
<tbody>
<tr>
<td>State space</td>
<td>Finite</td>
<td>Infinite (continuous)</td>
</tr>
<tr>
<td>Action space</td>
<td>Finite</td>
<td>Infinite (continuous)</td>
</tr>
<tr>
<td>Representation</td>
<td>Table</td>
<td>Function approximation required</td>
</tr>
<tr>
<td>Example</td>
<td>Grid world</td>
<td>Robot control</td>
</tr>
</tbody>
</table>
<h3 id="72-deterministic-vs-stochastic-mdp">7.2 Deterministic vs Stochastic MDP<a class="header-link" href="#72-deterministic-vs-stochastic-mdp" title="Permanent link">&para;</a></h3>
<div class="highlight"><pre><span></span><code><span class="c1"># Deterministic MDP: P(s&#39;|s,a) = 1 for one s&#39;</span>
<span class="n">deterministic_transitions</span> <span class="o">=</span> <span class="p">{</span>
    <span class="s1">&#39;s0&#39;</span><span class="p">:</span> <span class="p">{</span><span class="s1">&#39;right&#39;</span><span class="p">:</span> <span class="p">[(</span><span class="mf">1.0</span><span class="p">,</span> <span class="s1">&#39;s1&#39;</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="kc">False</span><span class="p">)]}</span>  <span class="c1"># 100% move to s1</span>
<span class="p">}</span>

<span class="c1"># Stochastic MDP: P(s&#39;|s,a) can be &lt; 1</span>
<span class="n">stochastic_transitions</span> <span class="o">=</span> <span class="p">{</span>
    <span class="s1">&#39;s0&#39;</span><span class="p">:</span> <span class="p">{</span><span class="s1">&#39;right&#39;</span><span class="p">:</span> <span class="p">[(</span><span class="mf">0.8</span><span class="p">,</span> <span class="s1">&#39;s1&#39;</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="kc">False</span><span class="p">),</span>   <span class="c1"># 80% move to s1</span>
                     <span class="p">(</span><span class="mf">0.2</span><span class="p">,</span> <span class="s1">&#39;s0&#39;</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="kc">False</span><span class="p">)]}</span>  <span class="c1"># 20% stay</span>
<span class="p">}</span>
</code></pre></div>

<h3 id="73-partially-observable-mdp-pomdp">7.3 Partially Observable MDP (POMDP)<a class="header-link" href="#73-partially-observable-mdp-pomdp" title="Permanent link">&para;</a></h3>
<p>When the state cannot be directly observed, only <strong>observations</strong> are available</p>
<div class="highlight"><pre><span></span><code><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">POMDP example: Poker game</span>
<span class="sd">- True state: All players&#39; cards + deck cards</span>
<span class="sd">- Observation: Only own cards + public cards</span>
<span class="sd">- Belief state: Maintain probability distribution over true states</span>

<span class="sd">Solutions:</span>
<span class="sd">1. Use belief state as state (belief MDP)</span>
<span class="sd">2. Use history of past observations</span>
<span class="sd">3. Learn implicit state using RNN/LSTM</span>
<span class="sd">&quot;&quot;&quot;</span>
</code></pre></div>

<hr />
<h2 id="8-gridworld-example">8. GridWorld Example<a class="header-link" href="#8-gridworld-example" title="Permanent link">&para;</a></h2>
<h3 id="81-environment-definition">8.1 Environment Definition<a class="header-link" href="#81-environment-definition" title="Permanent link">&para;</a></h3>
<div class="highlight"><pre><span></span><code><span class="kn">import</span><span class="w"> </span><span class="nn">numpy</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">np</span>

<span class="k">class</span><span class="w"> </span><span class="nc">GridWorld</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    4x4 Gridworld</span>

<span class="sd">    [S][ ][ ][ ]</span>
<span class="sd">    [ ][X][ ][ ]</span>
<span class="sd">    [ ][ ][ ][ ]</span>
<span class="sd">    [ ][ ][ ][G]</span>

<span class="sd">    S: Start, G: Goal(+1), X: Obstacle(-1)</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="k">def</span><span class="w"> </span><span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">size</span> <span class="o">=</span> <span class="mi">4</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">start</span> <span class="o">=</span> <span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">goal</span> <span class="o">=</span> <span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="mi">3</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">obstacle</span> <span class="o">=</span> <span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">actions</span> <span class="o">=</span> <span class="p">[</span><span class="s1">&#39;up&#39;</span><span class="p">,</span> <span class="s1">&#39;down&#39;</span><span class="p">,</span> <span class="s1">&#39;left&#39;</span><span class="p">,</span> <span class="s1">&#39;right&#39;</span><span class="p">]</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">action_deltas</span> <span class="o">=</span> <span class="p">{</span>
            <span class="s1">&#39;up&#39;</span><span class="p">:</span> <span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">),</span>
            <span class="s1">&#39;down&#39;</span><span class="p">:</span> <span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">),</span>
            <span class="s1">&#39;left&#39;</span><span class="p">:</span> <span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">),</span>
            <span class="s1">&#39;right&#39;</span><span class="p">:</span> <span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
        <span class="p">}</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">gamma</span> <span class="o">=</span> <span class="mf">0.9</span>

    <span class="k">def</span><span class="w"> </span><span class="nf">get_states</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Return all states&quot;&quot;&quot;</span>
        <span class="n">states</span> <span class="o">=</span> <span class="p">[]</span>
        <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">size</span><span class="p">):</span>
            <span class="k">for</span> <span class="n">j</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">size</span><span class="p">):</span>
                <span class="n">states</span><span class="o">.</span><span class="n">append</span><span class="p">((</span><span class="n">i</span><span class="p">,</span> <span class="n">j</span><span class="p">))</span>
        <span class="k">return</span> <span class="n">states</span>

    <span class="k">def</span><span class="w"> </span><span class="nf">is_terminal</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">state</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Check if terminal state&quot;&quot;&quot;</span>
        <span class="k">return</span> <span class="n">state</span> <span class="o">==</span> <span class="bp">self</span><span class="o">.</span><span class="n">goal</span>

    <span class="k">def</span><span class="w"> </span><span class="nf">get_reward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">state</span><span class="p">,</span> <span class="n">action</span><span class="p">,</span> <span class="n">next_state</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Reward function&quot;&quot;&quot;</span>
        <span class="k">if</span> <span class="n">next_state</span> <span class="o">==</span> <span class="bp">self</span><span class="o">.</span><span class="n">goal</span><span class="p">:</span>
            <span class="k">return</span> <span class="mf">1.0</span>
        <span class="k">elif</span> <span class="n">next_state</span> <span class="o">==</span> <span class="bp">self</span><span class="o">.</span><span class="n">obstacle</span><span class="p">:</span>
            <span class="k">return</span> <span class="o">-</span><span class="mf">1.0</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="k">return</span> <span class="o">-</span><span class="mf">0.01</span>  <span class="c1"># Step penalty</span>

    <span class="k">def</span><span class="w"> </span><span class="nf">get_transitions</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">state</span><span class="p">,</span> <span class="n">action</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Return transition probabilities</span>
<span class="sd">        80% intended direction, 10% each for slipping left/right</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">is_terminal</span><span class="p">(</span><span class="n">state</span><span class="p">):</span>
            <span class="k">return</span> <span class="p">[(</span><span class="mf">1.0</span><span class="p">,</span> <span class="n">state</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="kc">True</span><span class="p">)]</span>

        <span class="n">transitions</span> <span class="o">=</span> <span class="p">[]</span>

        <span class="c1"># Intended direction</span>
        <span class="n">intended_delta</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">action_deltas</span><span class="p">[</span><span class="n">action</span><span class="p">]</span>

        <span class="c1"># Slip directions</span>
        <span class="k">if</span> <span class="n">action</span> <span class="ow">in</span> <span class="p">[</span><span class="s1">&#39;up&#39;</span><span class="p">,</span> <span class="s1">&#39;down&#39;</span><span class="p">]:</span>
            <span class="n">slip_actions</span> <span class="o">=</span> <span class="p">[</span><span class="s1">&#39;left&#39;</span><span class="p">,</span> <span class="s1">&#39;right&#39;</span><span class="p">]</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">slip_actions</span> <span class="o">=</span> <span class="p">[</span><span class="s1">&#39;up&#39;</span><span class="p">,</span> <span class="s1">&#39;down&#39;</span><span class="p">]</span>

        <span class="c1"># Add transitions for each direction</span>
        <span class="n">directions</span> <span class="o">=</span> <span class="p">[(</span><span class="mf">0.8</span><span class="p">,</span> <span class="n">action</span><span class="p">),</span> <span class="p">(</span><span class="mf">0.1</span><span class="p">,</span> <span class="n">slip_actions</span><span class="p">[</span><span class="mi">0</span><span class="p">]),</span> <span class="p">(</span><span class="mf">0.1</span><span class="p">,</span> <span class="n">slip_actions</span><span class="p">[</span><span class="mi">1</span><span class="p">])]</span>

        <span class="k">for</span> <span class="n">prob</span><span class="p">,</span> <span class="n">dir_action</span> <span class="ow">in</span> <span class="n">directions</span><span class="p">:</span>
            <span class="n">delta</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">action_deltas</span><span class="p">[</span><span class="n">dir_action</span><span class="p">]</span>
            <span class="n">next_state</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_move</span><span class="p">(</span><span class="n">state</span><span class="p">,</span> <span class="n">delta</span><span class="p">)</span>
            <span class="n">reward</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">get_reward</span><span class="p">(</span><span class="n">state</span><span class="p">,</span> <span class="n">action</span><span class="p">,</span> <span class="n">next_state</span><span class="p">)</span>
            <span class="n">done</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">is_terminal</span><span class="p">(</span><span class="n">next_state</span><span class="p">)</span>
            <span class="n">transitions</span><span class="o">.</span><span class="n">append</span><span class="p">((</span><span class="n">prob</span><span class="p">,</span> <span class="n">next_state</span><span class="p">,</span> <span class="n">reward</span><span class="p">,</span> <span class="n">done</span><span class="p">))</span>

        <span class="k">return</span> <span class="n">transitions</span>

    <span class="k">def</span><span class="w"> </span><span class="nf">_move</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">state</span><span class="p">,</span> <span class="n">delta</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Handle movement (stay in place on wall collision)&quot;&quot;&quot;</span>
        <span class="n">new_row</span> <span class="o">=</span> <span class="n">state</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">+</span> <span class="n">delta</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
        <span class="n">new_col</span> <span class="o">=</span> <span class="n">state</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span> <span class="o">+</span> <span class="n">delta</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span>

        <span class="c1"># Check grid boundaries</span>
        <span class="k">if</span> <span class="mi">0</span> <span class="o">&lt;=</span> <span class="n">new_row</span> <span class="o">&lt;</span> <span class="bp">self</span><span class="o">.</span><span class="n">size</span> <span class="ow">and</span> <span class="mi">0</span> <span class="o">&lt;=</span> <span class="n">new_col</span> <span class="o">&lt;</span> <span class="bp">self</span><span class="o">.</span><span class="n">size</span><span class="p">:</span>
            <span class="k">return</span> <span class="p">(</span><span class="n">new_row</span><span class="p">,</span> <span class="n">new_col</span><span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="k">return</span> <span class="n">state</span>  <span class="c1"># Hit wall</span>
</code></pre></div>

<h3 id="82-value-function-visualization">8.2 Value Function Visualization<a class="header-link" href="#82-value-function-visualization" title="Permanent link">&para;</a></h3>
<div class="highlight"><pre><span></span><code><span class="k">def</span><span class="w"> </span><span class="nf">visualize_values</span><span class="p">(</span><span class="n">grid</span><span class="p">,</span> <span class="n">V</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Visualize value function&quot;&quot;&quot;</span>
    <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;</span><span class="se">\n</span><span class="s2">=== State Values ===&quot;</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;-&quot;</span> <span class="o">*</span> <span class="mi">40</span><span class="p">)</span>

    <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">grid</span><span class="o">.</span><span class="n">size</span><span class="p">):</span>
        <span class="n">row_str</span> <span class="o">=</span> <span class="s2">&quot;|&quot;</span>
        <span class="k">for</span> <span class="n">j</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">grid</span><span class="o">.</span><span class="n">size</span><span class="p">):</span>
            <span class="n">state</span> <span class="o">=</span> <span class="p">(</span><span class="n">i</span><span class="p">,</span> <span class="n">j</span><span class="p">)</span>
            <span class="n">value</span> <span class="o">=</span> <span class="n">V</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="n">state</span><span class="p">,</span> <span class="mi">0</span><span class="p">)</span>

            <span class="k">if</span> <span class="n">state</span> <span class="o">==</span> <span class="n">grid</span><span class="o">.</span><span class="n">goal</span><span class="p">:</span>
                <span class="n">row_str</span> <span class="o">+=</span> <span class="sa">f</span><span class="s2">&quot;  G   |&quot;</span>
            <span class="k">elif</span> <span class="n">state</span> <span class="o">==</span> <span class="n">grid</span><span class="o">.</span><span class="n">obstacle</span><span class="p">:</span>
                <span class="n">row_str</span> <span class="o">+=</span> <span class="sa">f</span><span class="s2">&quot;  X   |&quot;</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="n">row_str</span> <span class="o">+=</span> <span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="n">value</span><span class="si">:</span><span class="s2">6.2f</span><span class="si">}</span><span class="s2">|&quot;</span>
        <span class="nb">print</span><span class="p">(</span><span class="n">row_str</span><span class="p">)</span>
        <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;-&quot;</span> <span class="o">*</span> <span class="mi">40</span><span class="p">)</span>


<span class="k">def</span><span class="w"> </span><span class="nf">visualize_policy</span><span class="p">(</span><span class="n">grid</span><span class="p">,</span> <span class="n">policy</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Visualize policy&quot;&quot;&quot;</span>
    <span class="n">arrows</span> <span class="o">=</span> <span class="p">{</span><span class="s1">&#39;up&#39;</span><span class="p">:</span> <span class="s1">&#39;^&#39;</span><span class="p">,</span> <span class="s1">&#39;down&#39;</span><span class="p">:</span> <span class="s1">&#39;v&#39;</span><span class="p">,</span> <span class="s1">&#39;left&#39;</span><span class="p">:</span> <span class="s1">&#39;&lt;&#39;</span><span class="p">,</span> <span class="s1">&#39;right&#39;</span><span class="p">:</span> <span class="s1">&#39;&gt;&#39;</span><span class="p">}</span>

    <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;</span><span class="se">\n</span><span class="s2">=== Optimal Policy ===&quot;</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;-&quot;</span> <span class="o">*</span> <span class="mi">25</span><span class="p">)</span>

    <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">grid</span><span class="o">.</span><span class="n">size</span><span class="p">):</span>
        <span class="n">row_str</span> <span class="o">=</span> <span class="s2">&quot;|&quot;</span>
        <span class="k">for</span> <span class="n">j</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">grid</span><span class="o">.</span><span class="n">size</span><span class="p">):</span>
            <span class="n">state</span> <span class="o">=</span> <span class="p">(</span><span class="n">i</span><span class="p">,</span> <span class="n">j</span><span class="p">)</span>

            <span class="k">if</span> <span class="n">state</span> <span class="o">==</span> <span class="n">grid</span><span class="o">.</span><span class="n">goal</span><span class="p">:</span>
                <span class="n">row_str</span> <span class="o">+=</span> <span class="s2">&quot;  G  |&quot;</span>
            <span class="k">elif</span> <span class="n">state</span> <span class="o">==</span> <span class="n">grid</span><span class="o">.</span><span class="n">obstacle</span><span class="p">:</span>
                <span class="n">row_str</span> <span class="o">+=</span> <span class="s2">&quot;  X  |&quot;</span>
            <span class="k">elif</span> <span class="n">state</span> <span class="ow">in</span> <span class="n">policy</span><span class="p">:</span>
                <span class="n">row_str</span> <span class="o">+=</span> <span class="sa">f</span><span class="s2">&quot;  </span><span class="si">{</span><span class="n">arrows</span><span class="p">[</span><span class="n">policy</span><span class="p">[</span><span class="n">state</span><span class="p">]]</span><span class="si">}</span><span class="s2">  |&quot;</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="n">row_str</span> <span class="o">+=</span> <span class="s2">&quot;  ?  |&quot;</span>
        <span class="nb">print</span><span class="p">(</span><span class="n">row_str</span><span class="p">)</span>
        <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;-&quot;</span> <span class="o">*</span> <span class="mi">25</span><span class="p">)</span>
</code></pre></div>

<hr />
<h2 id="9-summary">9. Summary<a class="header-link" href="#9-summary" title="Permanent link">&para;</a></h2>
<h3 id="key-concepts">Key Concepts<a class="header-link" href="#key-concepts" title="Permanent link">&para;</a></h3>
<table>
<thead>
<tr>
<th>Concept</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr>
<td>Markov Property</td>
<td>Future depends only on current state</td>
</tr>
<tr>
<td>MDP</td>
<td>Decision problem defined by (S, A, P, R, Î³)</td>
</tr>
<tr>
<td>Policy Ï€</td>
<td>Strategy for selecting actions in states</td>
</tr>
<tr>
<td>V(s)</td>
<td>Long-term expected value of state</td>
</tr>
<tr>
<td>Q(s,a)</td>
<td>Expected value of state-action pair</td>
</tr>
</tbody>
</table>
<h3 id="bellman-equation-summary">Bellman Equation Summary<a class="header-link" href="#bellman-equation-summary" title="Permanent link">&para;</a></h3>
<table>
<thead>
<tr>
<th>Equation</th>
<th>Formula</th>
</tr>
</thead>
<tbody>
<tr>
<td>Expectation (V)</td>
<td>$V^\pi(s) = \sum_a \pi(a|s) \sum_{s'} P(s'|s,a)[R + \gamma V^\pi(s')]$</td>
</tr>
<tr>
<td>Expectation (Q)</td>
<td>$Q^\pi(s,a) = \sum_{s'} P(s'|s,a)[R + \gamma \sum_{a'} \pi(a'|s') Q^\pi(s',a')]$</td>
</tr>
<tr>
<td>Optimality (V)</td>
<td>$V^*(s) = \max_a \sum_{s'} P(s'|s,a)[R + \gamma V^*(s')]$</td>
</tr>
<tr>
<td>Optimality (Q)</td>
<td>$Q^*(s,a) = \sum_{s'} P(s'|s,a)[R + \gamma \max_{a'} Q^*(s',a')]$</td>
</tr>
</tbody>
</table>
<hr />
<h2 id="10-practice-problems">10. Practice Problems<a class="header-link" href="#10-practice-problems" title="Permanent link">&para;</a></h2>
<ol>
<li><strong>Markov Property</strong>: Which of the following satisfies the Markov property?</li>
<li>(a) Today's stock price determines tomorrow's price</li>
<li>(b) Chess board state</li>
<li>
<p>(c) Determining appropriate response using only current sentence in a conversation</p>
</li>
<li>
<p><strong>MDP Definition</strong>: Define the MDP elements for a 3x3 grid world.</p>
</li>
<li>
<p><strong>Bellman Equation</strong>: Given discount factor Î³=0.9, immediate reward r=1, V(s')=5, what is V(s)?</p>
</li>
<li>
<p><strong>Optimal Policy</strong>: Given Q<em>(s, left)=3, Q</em>(s, right)=5, what is the optimal action?</p>
</li>
</ol>
<hr />
<h2 id="next-steps">Next Steps<a class="header-link" href="#next-steps" title="Permanent link">&para;</a></h2>
<p>In the next lesson <strong>03_Dynamic_Programming.md</strong>, we will learn <strong>dynamic programming</strong> methods (policy iteration, value iteration) for solving MDPs.</p>
<hr />
<h2 id="references">References<a class="header-link" href="#references" title="Permanent link">&para;</a></h2>
<ul>
<li>Sutton &amp; Barto, "Reinforcement Learning: An Introduction", Chapter 3</li>
<li>David Silver's RL Course, Lecture 2: Markov Decision Processes</li>
<li><a href="https://en.wikipedia.org/wiki/Markov_decision_process">MDP Wikipedia</a></li>
</ul>
    </div>

    
<div class="lesson-toolbar lesson-toolbar--bottom">
    <div class="toolbar-nav toolbar-nav--prev">
        
        <a href="/study/en/Reinforcement_Learning/01_RL_Introduction.html" class="nav-prev">
            <span class="nav-label">Previous</span>
            <span class="nav-title">01. Introduction to Reinforcement Learning</span>
        </a>
        
    </div>
    <div class="toolbar-actions">
        <button class="btn btn-copy-link" data-action="copy-link" title="Copy link">
            <span class="icon">ğŸ”—</span>
            <span class="text">Copy link</span>
        </button>
        <a href="/study/en/Reinforcement_Learning/" class="btn btn-topic-link" title="Back to topic list">
            <span class="icon">ğŸ“‹</span>
            <span class="text">Topic list</span>
        </a>
    </div>
    <div class="toolbar-nav toolbar-nav--next">
        
        <a href="/study/en/Reinforcement_Learning/03_Dynamic_Programming.html" class="nav-next">
            <span class="nav-label">Next</span>
            <span class="nav-title">03. Dynamic Programming</span>
        </a>
        
    </div>
</div>


    <div class="toolbar-keyboard-hint">
        <kbd>&larr;</kbd> <kbd>&rarr;</kbd> to navigate between lessons
    </div>
</article>

<!-- Scroll to top floating button -->
<button class="scroll-to-top" id="scroll-to-top" title="Scroll to top" aria-label="Scroll to top">â†‘</button>

            </div>
        </main>
    </div>

    <script src="/study/static/js/app.js"></script>
    <script>
        function switchLanguage(newLang) {
            const path = window.location.pathname;
            const base = '/study';
            const currentLang = 'en';
            const newPath = path.replace(base + '/' + currentLang + '/', base + '/' + newLang + '/');
            window.location.href = newPath + window.location.search;
        }
    </script>
    
<script>
    document.addEventListener('DOMContentLoaded', function() {
        // Copy link - independent feedback per button
        document.querySelectorAll('[data-action="copy-link"]').forEach(function(btn) {
            btn.addEventListener('click', function() {
                navigator.clipboard.writeText(window.location.href);
                var textEl = this.querySelector('.text');
                var originalText = textEl.textContent;
                textEl.textContent = 'Copied!';
                setTimeout(function() { textEl.textContent = originalText; }, 2000);
            });
        });

        // Add copy buttons to code blocks
        document.querySelectorAll('.lesson-content pre').forEach(function(pre) {
            var copyBtn = document.createElement('button');
            copyBtn.className = 'code-copy-btn';
            copyBtn.textContent = 'Copy';
            copyBtn.addEventListener('click', function() {
                var code = pre.querySelector('code');
                var text = code ? code.textContent : pre.textContent;
                navigator.clipboard.writeText(text);
                copyBtn.textContent = 'Copied!';
                setTimeout(function() { copyBtn.textContent = 'Copy'; }, 2000);
            });
            pre.style.position = 'relative';
            pre.appendChild(copyBtn);
        });

        // Scroll to top button
        var scrollBtn = document.getElementById('scroll-to-top');
        window.addEventListener('scroll', function() {
            scrollBtn.classList.toggle('visible', window.scrollY > 300);
        });
        scrollBtn.addEventListener('click', function() {
            window.scrollTo({ top: 0, behavior: 'smooth' });
        });

        // Keyboard shortcuts: left/right arrows for lesson navigation
        document.addEventListener('keydown', function(e) {
            var tag = document.activeElement.tagName;
            if (tag === 'INPUT' || tag === 'TEXTAREA' || tag === 'SELECT') return;

            if (e.key === 'ArrowLeft') {
                var prevLink = document.querySelector('.nav-prev');
                if (prevLink) prevLink.click();
            } else if (e.key === 'ArrowRight') {
                var nextLink = document.querySelector('.nav-next');
                if (nextLink) nextLink.click();
            }
        });
    });
</script>

</body>
</html>
{% endraw %}