{% raw %}
<!DOCTYPE html>
<html lang="en" data-theme="light">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>10. PPO and TRPO - Study Materials</title>
    <link rel="stylesheet" href="/study/static/css/style.css">
    <link rel="stylesheet" href="/study/static/css/highlight.css">
    <!-- KaTeX for LaTeX math rendering -->
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.11/dist/katex.min.css">
    <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.11/dist/katex.min.js"></script>
    <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.11/dist/contrib/auto-render.min.js"
        onload="renderMathInElement(document.body, {
            delimiters: [
                {left: '$$', right: '$$', display: true},
                {left: '$', right: '$', display: false}
            ],
            throwOnError: false
        });"></script>
    
</head>
<body>
    <div class="app-container">
        <!-- Sidebar -->
        <aside class="sidebar" id="sidebar">
            <div class="sidebar-header">
                <a href="/study/en/" class="logo">Study Materials</a>
                <button class="sidebar-toggle" id="sidebar-toggle" aria-label="Toggle sidebar">
                    <span></span>
                </button>
            </div>

            <nav class="sidebar-nav">
                <a href="/study/en/" class="nav-item ">
                    <span class="nav-icon">üè†</span>
                    <span class="nav-text">Home</span>
                </a>
                <a href="/study/examples/" class="nav-item ">
                    <span class="nav-icon">üíª</span>
                    <span class="nav-text">Examples</span>
                </a>
            </nav>

            <div class="sidebar-search">
                <form action="/study/en/search.html" method="get" id="search-form">
                    <input type="search" name="q" placeholder="Search..." id="search-sidebar-input">
                </form>
            </div>

            <!-- Language Selector -->
            <div class="sidebar-lang">
                <select id="lang-select" class="lang-selector" onchange="switchLanguage(this.value)">
                    
                    <option value="en" selected>
                        English
                    </option>
                    
                    <option value="ko" >
                        ÌïúÍµ≠Ïñ¥
                    </option>
                    
                </select>
            </div>

            <div class="sidebar-footer">
                <button class="theme-toggle" id="theme-toggle" aria-label="Toggle theme">
                    <span class="theme-icon light">‚òÄÔ∏è</span>
                    <span class="theme-icon dark">üåô</span>
                </button>
            </div>
        </aside>

        <!-- Main Content -->
        <main class="main-content">
            <header class="main-header">
                <button class="menu-toggle" id="menu-toggle" aria-label="Open menu">
                    <span></span>
                </button>
                
<nav class="breadcrumb">
    <a href="/study/en/">Topics</a>
    <span class="separator">/</span>
    <a href="/study/en/Reinforcement_Learning/">Reinforcement Learning</a>
    <span class="separator">/</span>
    <span class="current">10. PPO and TRPO</span>
</nav>

            </header>

            <div class="content">
                
<article class="lesson-article">
    <header class="lesson-header">
        <h1>10. PPO and TRPO</h1>
    </header>

    
<div class="lesson-toolbar lesson-toolbar--top">
    <div class="toolbar-nav toolbar-nav--prev">
        
        <a href="/study/en/Reinforcement_Learning/09_Actor_Critic.html" class="nav-prev">
            <span class="nav-label">Previous</span>
            <span class="nav-title">09. Actor-Critic Methods</span>
        </a>
        
    </div>
    <div class="toolbar-actions">
        <button class="btn btn-copy-link" data-action="copy-link" title="Copy link">
            <span class="icon">üîó</span>
            <span class="text">Copy link</span>
        </button>
        <a href="/study/en/Reinforcement_Learning/" class="btn btn-topic-link" title="Back to topic list">
            <span class="icon">üìã</span>
            <span class="text">Topic list</span>
        </a>
    </div>
    <div class="toolbar-nav toolbar-nav--next">
        
        <a href="/study/en/Reinforcement_Learning/11_Multi_Agent_RL.html" class="nav-next">
            <span class="nav-label">Next</span>
            <span class="nav-title">11. Multi-Agent Reinforcement Learning (MARL)</span>
        </a>
        
    </div>
</div>


    
    <nav class="toc" id="toc">
        <h2>Table of Contents</h2>
        <div class="toc">
<ul>
<li><a href="#learning-objectives">Learning Objectives</a></li>
<li><a href="#1-problems-in-policy-optimization">1. Problems in Policy Optimization</a><ul>
<li><a href="#11-dangers-of-large-updates">1.1 Dangers of Large Updates</a></li>
<li><a href="#12-solution-approaches">1.2 Solution Approaches</a></li>
</ul>
</li>
<li><a href="#2-trpo-trust-region-policy-optimization">2. TRPO (Trust Region Policy Optimization)</a><ul>
<li><a href="#21-objective-function">2.1 Objective Function</a></li>
<li><a href="#22-kl-divergence-constraint">2.2 KL Divergence Constraint</a></li>
<li><a href="#23-problems-with-trpo">2.3 Problems with TRPO</a></li>
</ul>
</li>
<li><a href="#3-ppo-proximal-policy-optimization">3. PPO (Proximal Policy Optimization)</a><ul>
<li><a href="#31-core-idea">3.1 Core Idea</a></li>
<li><a href="#32-clipped-objective-function">3.2 Clipped Objective Function</a></li>
<li><a href="#33-clipping-intuition">3.3 Clipping Intuition</a></li>
</ul>
</li>
<li><a href="#4-complete-ppo-implementation">4. Complete PPO Implementation</a><ul>
<li><a href="#41-ppo-agent">4.1 PPO Agent</a></li>
<li><a href="#42-ppo-training-loop">4.2 PPO Training Loop</a></li>
</ul>
</li>
<li><a href="#5-ppo-variants">5. PPO Variants</a><ul>
<li><a href="#51-ppo-clip-basic">5.1 PPO-Clip (Basic)</a></li>
<li><a href="#52-ppo-penalty">5.2 PPO-Penalty</a></li>
<li><a href="#53-clipped-value-loss">5.3 Clipped Value Loss</a></li>
</ul>
</li>
<li><a href="#6-continuous-action-space-ppo">6. Continuous Action Space PPO</a></li>
<li><a href="#7-hyperparameter-guide">7. Hyperparameter Guide</a><ul>
<li><a href="#71-general-settings">7.1 General Settings</a></li>
<li><a href="#72-environment-specific-tuning">7.2 Environment-Specific Tuning</a></li>
</ul>
</li>
<li><a href="#8-ppo-vs-other-algorithms">8. PPO vs Other Algorithms</a></li>
<li><a href="#summary">Summary</a></li>
<li><a href="#next-steps">Next Steps</a></li>
</ul>
</div>

    </nav>
    

    <div class="lesson-content markdown-body">
        <h1 id="10-ppo-and-trpo">10. PPO and TRPO<a class="header-link" href="#10-ppo-and-trpo" title="Permanent link">&para;</a></h1>
<p><strong>Difficulty: ‚≠ê‚≠ê‚≠ê‚≠ê (Advanced)</strong></p>
<h2 id="learning-objectives">Learning Objectives<a class="header-link" href="#learning-objectives" title="Permanent link">&para;</a></h2>
<ul>
<li>Understand stability issues in policy updates</li>
<li>Learn TRPO's trust region concept</li>
<li>Understand PPO's clipping mechanism</li>
<li>Implement PPO with PyTorch</li>
</ul>
<hr />
<h2 id="1-problems-in-policy-optimization">1. Problems in Policy Optimization<a class="header-link" href="#1-problems-in-policy-optimization" title="Permanent link">&para;</a></h2>
<h3 id="11-dangers-of-large-updates">1.1 Dangers of Large Updates<a class="header-link" href="#11-dangers-of-large-updates" title="Permanent link">&para;</a></h3>
<p>In policy gradients, overly large updates can drastically degrade performance.</p>
<div class="highlight"><pre><span></span><code>Œ∏_new = Œ∏_old + Œ±‚àáJ(Œ∏)

Problem: Large Œ± causes drastic policy changes, making learning unstable
Solution: Constrain policy changes
</code></pre></div>

<h3 id="12-solution-approaches">1.2 Solution Approaches<a class="header-link" href="#12-solution-approaches" title="Permanent link">&para;</a></h3>
<ul>
<li><strong>TRPO</strong>: Constrain with KL divergence trust region (complex)</li>
<li><strong>PPO</strong>: Simple constraint using clipping</li>
</ul>
<hr />
<h2 id="2-trpo-trust-region-policy-optimization">2. TRPO (Trust Region Policy Optimization)<a class="header-link" href="#2-trpo-trust-region-policy-optimization" title="Permanent link">&para;</a></h2>
<h3 id="21-objective-function">2.1 Objective Function<a class="header-link" href="#21-objective-function" title="Permanent link">&para;</a></h3>
<p>Use ratio of new and old policies:</p>
<p>$$L^{CPI}(\theta) = \mathbb{E}\left[\frac{\pi_\theta(a|s)}{\pi_{\theta_{old}}(a|s)} A^{\pi_{old}}(s, a)\right]$$</p>
<h3 id="22-kl-divergence-constraint">2.2 KL Divergence Constraint<a class="header-link" href="#22-kl-divergence-constraint" title="Permanent link">&para;</a></h3>
<p>$$\text{maximize}_\theta \quad L^{CPI}(\theta)$$
$$\text{subject to} \quad \mathbb{E}[D_{KL}(\pi_{\theta_{old}} || \pi_\theta)] \leq \delta$$</p>
<h3 id="23-problems-with-trpo">2.3 Problems with TRPO<a class="header-link" href="#23-problems-with-trpo" title="Permanent link">&para;</a></h3>
<ul>
<li>Requires second-order derivatives (Hessian)</li>
<li>Needs conjugate gradient algorithm</li>
<li>Complex implementation and high computational cost</li>
</ul>
<hr />
<h2 id="3-ppo-proximal-policy-optimization">3. PPO (Proximal Policy Optimization)<a class="header-link" href="#3-ppo-proximal-policy-optimization" title="Permanent link">&para;</a></h2>
<h3 id="31-core-idea">3.1 Core Idea<a class="header-link" href="#31-core-idea" title="Permanent link">&para;</a></h3>
<p>Use clipping to constrain policy ratio.</p>
<p>$$r_t(\theta) = \frac{\pi_\theta(a_t|s_t)}{\pi_{\theta_{old}}(a_t|s_t)}$$</p>
<h3 id="32-clipped-objective-function">3.2 Clipped Objective Function<a class="header-link" href="#32-clipped-objective-function" title="Permanent link">&para;</a></h3>
<p>$$L^{CLIP}(\theta) = \mathbb{E}\left[\min(r_t(\theta) A_t, \text{clip}(r_t(\theta), 1-\epsilon, 1+\epsilon) A_t)\right]$$</p>
<div class="highlight"><pre><span></span><code><span class="k">def</span><span class="w"> </span><span class="nf">compute_ppo_loss</span><span class="p">(</span><span class="n">ratio</span><span class="p">,</span> <span class="n">advantage</span><span class="p">,</span> <span class="n">clip_epsilon</span><span class="o">=</span><span class="mf">0.2</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;PPO Clipped Loss&quot;&quot;&quot;</span>
    <span class="c1"># Clipped ratio</span>
    <span class="n">clipped_ratio</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">clamp</span><span class="p">(</span><span class="n">ratio</span><span class="p">,</span> <span class="mi">1</span> <span class="o">-</span> <span class="n">clip_epsilon</span><span class="p">,</span> <span class="mi">1</span> <span class="o">+</span> <span class="n">clip_epsilon</span><span class="p">)</span>

    <span class="c1"># Take minimum of two terms</span>
    <span class="n">loss1</span> <span class="o">=</span> <span class="n">ratio</span> <span class="o">*</span> <span class="n">advantage</span>
    <span class="n">loss2</span> <span class="o">=</span> <span class="n">clipped_ratio</span> <span class="o">*</span> <span class="n">advantage</span>

    <span class="k">return</span> <span class="o">-</span><span class="n">torch</span><span class="o">.</span><span class="n">min</span><span class="p">(</span><span class="n">loss1</span><span class="p">,</span> <span class="n">loss2</span><span class="p">)</span><span class="o">.</span><span class="n">mean</span><span class="p">()</span>
</code></pre></div>

<h3 id="33-clipping-intuition">3.3 Clipping Intuition<a class="header-link" href="#33-clipping-intuition" title="Permanent link">&para;</a></h3>
<div class="highlight"><pre><span></span><code><span class="nv">Advantage</span><span class="w"> </span><span class="o">&gt;</span><span class="w"> </span><span class="mi">0</span><span class="w"> </span><span class="ss">(</span><span class="nv">good</span><span class="w"> </span><span class="nv">action</span><span class="ss">)</span>:
<span class="o">-</span><span class="w"> </span><span class="nv">ratio</span><span class="w"> </span><span class="nv">increases</span><span class="w"> </span>‚Üí<span class="w"> </span><span class="nv">probability</span><span class="w"> </span><span class="nv">increases</span>
<span class="o">-</span><span class="w"> </span><span class="nv">But</span><span class="w"> </span><span class="nv">ignore</span><span class="w"> </span><span class="k">if</span><span class="w"> </span><span class="nv">ratio</span><span class="w"> </span><span class="o">&gt;</span><span class="w"> </span><span class="mi">1</span><span class="o">+</span>Œµ<span class="w"> </span><span class="ss">(</span><span class="nv">prevent</span><span class="w"> </span><span class="nv">drastic</span><span class="w"> </span><span class="nv">increase</span><span class="ss">)</span>

<span class="nv">Advantage</span><span class="w"> </span><span class="o">&lt;</span><span class="w"> </span><span class="mi">0</span><span class="w"> </span><span class="ss">(</span><span class="nv">bad</span><span class="w"> </span><span class="nv">action</span><span class="ss">)</span>:
<span class="o">-</span><span class="w"> </span><span class="nv">ratio</span><span class="w"> </span><span class="nv">decreases</span><span class="w"> </span>‚Üí<span class="w"> </span><span class="nv">probability</span><span class="w"> </span><span class="nv">decreases</span>
<span class="o">-</span><span class="w"> </span><span class="nv">But</span><span class="w"> </span><span class="nv">ignore</span><span class="w"> </span><span class="k">if</span><span class="w"> </span><span class="nv">ratio</span><span class="w"> </span><span class="o">&lt;</span><span class="w"> </span><span class="mi">1</span><span class="o">-</span>Œµ<span class="w"> </span><span class="ss">(</span><span class="nv">prevent</span><span class="w"> </span><span class="nv">drastic</span><span class="w"> </span><span class="nv">decrease</span><span class="ss">)</span>
</code></pre></div>

<hr />
<h2 id="4-complete-ppo-implementation">4. Complete PPO Implementation<a class="header-link" href="#4-complete-ppo-implementation" title="Permanent link">&para;</a></h2>
<h3 id="41-ppo-agent">4.1 PPO Agent<a class="header-link" href="#41-ppo-agent" title="Permanent link">&para;</a></h3>
<div class="highlight"><pre><span></span><code><span class="kn">import</span><span class="w"> </span><span class="nn">torch</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">torch.nn</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">nn</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">torch.nn.functional</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">F</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">numpy</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">np</span>

<span class="k">class</span><span class="w"> </span><span class="nc">PPONetwork</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="k">def</span><span class="w"> </span><span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">state_dim</span><span class="p">,</span> <span class="n">action_dim</span><span class="p">,</span> <span class="n">hidden_dim</span><span class="o">=</span><span class="mi">64</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>

        <span class="c1"># Actor</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">actor</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Sequential</span><span class="p">(</span>
            <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">state_dim</span><span class="p">,</span> <span class="n">hidden_dim</span><span class="p">),</span>
            <span class="n">nn</span><span class="o">.</span><span class="n">Tanh</span><span class="p">(),</span>
            <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">hidden_dim</span><span class="p">,</span> <span class="n">hidden_dim</span><span class="p">),</span>
            <span class="n">nn</span><span class="o">.</span><span class="n">Tanh</span><span class="p">(),</span>
            <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">hidden_dim</span><span class="p">,</span> <span class="n">action_dim</span><span class="p">),</span>
            <span class="n">nn</span><span class="o">.</span><span class="n">Softmax</span><span class="p">(</span><span class="n">dim</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span>
        <span class="p">)</span>

        <span class="c1"># Critic</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">critic</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Sequential</span><span class="p">(</span>
            <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">state_dim</span><span class="p">,</span> <span class="n">hidden_dim</span><span class="p">),</span>
            <span class="n">nn</span><span class="o">.</span><span class="n">Tanh</span><span class="p">(),</span>
            <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">hidden_dim</span><span class="p">,</span> <span class="n">hidden_dim</span><span class="p">),</span>
            <span class="n">nn</span><span class="o">.</span><span class="n">Tanh</span><span class="p">(),</span>
            <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">hidden_dim</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
        <span class="p">)</span>

    <span class="k">def</span><span class="w"> </span><span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">state</span><span class="p">):</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">actor</span><span class="p">(</span><span class="n">state</span><span class="p">),</span> <span class="bp">self</span><span class="o">.</span><span class="n">critic</span><span class="p">(</span><span class="n">state</span><span class="p">)</span>

    <span class="k">def</span><span class="w"> </span><span class="nf">get_action</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">state</span><span class="p">,</span> <span class="n">action</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
        <span class="n">probs</span><span class="p">,</span> <span class="n">value</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">forward</span><span class="p">(</span><span class="n">state</span><span class="p">)</span>
        <span class="n">dist</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">distributions</span><span class="o">.</span><span class="n">Categorical</span><span class="p">(</span><span class="n">probs</span><span class="p">)</span>

        <span class="k">if</span> <span class="n">action</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">action</span> <span class="o">=</span> <span class="n">dist</span><span class="o">.</span><span class="n">sample</span><span class="p">()</span>

        <span class="k">return</span> <span class="n">action</span><span class="p">,</span> <span class="n">dist</span><span class="o">.</span><span class="n">log_prob</span><span class="p">(</span><span class="n">action</span><span class="p">),</span> <span class="n">dist</span><span class="o">.</span><span class="n">entropy</span><span class="p">(),</span> <span class="n">value</span>


<span class="k">class</span><span class="w"> </span><span class="nc">PPOAgent</span><span class="p">:</span>
    <span class="k">def</span><span class="w"> </span><span class="fm">__init__</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">state_dim</span><span class="p">,</span>
        <span class="n">action_dim</span><span class="p">,</span>
        <span class="n">lr</span><span class="o">=</span><span class="mf">3e-4</span><span class="p">,</span>
        <span class="n">gamma</span><span class="o">=</span><span class="mf">0.99</span><span class="p">,</span>
        <span class="n">gae_lambda</span><span class="o">=</span><span class="mf">0.95</span><span class="p">,</span>
        <span class="n">clip_epsilon</span><span class="o">=</span><span class="mf">0.2</span><span class="p">,</span>
        <span class="n">value_coef</span><span class="o">=</span><span class="mf">0.5</span><span class="p">,</span>
        <span class="n">entropy_coef</span><span class="o">=</span><span class="mf">0.01</span><span class="p">,</span>
        <span class="n">max_grad_norm</span><span class="o">=</span><span class="mf">0.5</span><span class="p">,</span>
        <span class="n">update_epochs</span><span class="o">=</span><span class="mi">10</span><span class="p">,</span>
        <span class="n">batch_size</span><span class="o">=</span><span class="mi">64</span>
    <span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">network</span> <span class="o">=</span> <span class="n">PPONetwork</span><span class="p">(</span><span class="n">state_dim</span><span class="p">,</span> <span class="n">action_dim</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">optimizer</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">optim</span><span class="o">.</span><span class="n">Adam</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">network</span><span class="o">.</span><span class="n">parameters</span><span class="p">(),</span> <span class="n">lr</span><span class="o">=</span><span class="n">lr</span><span class="p">)</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">gamma</span> <span class="o">=</span> <span class="n">gamma</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">gae_lambda</span> <span class="o">=</span> <span class="n">gae_lambda</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">clip_epsilon</span> <span class="o">=</span> <span class="n">clip_epsilon</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">value_coef</span> <span class="o">=</span> <span class="n">value_coef</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">entropy_coef</span> <span class="o">=</span> <span class="n">entropy_coef</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">max_grad_norm</span> <span class="o">=</span> <span class="n">max_grad_norm</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">update_epochs</span> <span class="o">=</span> <span class="n">update_epochs</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">batch_size</span> <span class="o">=</span> <span class="n">batch_size</span>

    <span class="k">def</span><span class="w"> </span><span class="nf">collect_rollouts</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">env</span><span class="p">,</span> <span class="n">n_steps</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Collect experience&quot;&quot;&quot;</span>
        <span class="n">states</span><span class="p">,</span> <span class="n">actions</span><span class="p">,</span> <span class="n">rewards</span><span class="p">,</span> <span class="n">dones</span> <span class="o">=</span> <span class="p">[],</span> <span class="p">[],</span> <span class="p">[],</span> <span class="p">[]</span>
        <span class="n">values</span><span class="p">,</span> <span class="n">log_probs</span> <span class="o">=</span> <span class="p">[],</span> <span class="p">[]</span>

        <span class="n">state</span><span class="p">,</span> <span class="n">_</span> <span class="o">=</span> <span class="n">env</span><span class="o">.</span><span class="n">reset</span><span class="p">()</span>

        <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">n_steps</span><span class="p">):</span>
            <span class="n">state_tensor</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">FloatTensor</span><span class="p">(</span><span class="n">state</span><span class="p">)</span><span class="o">.</span><span class="n">unsqueeze</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>

            <span class="k">with</span> <span class="n">torch</span><span class="o">.</span><span class="n">no_grad</span><span class="p">():</span>
                <span class="n">action</span><span class="p">,</span> <span class="n">log_prob</span><span class="p">,</span> <span class="n">_</span><span class="p">,</span> <span class="n">value</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">network</span><span class="o">.</span><span class="n">get_action</span><span class="p">(</span><span class="n">state_tensor</span><span class="p">)</span>

            <span class="n">next_state</span><span class="p">,</span> <span class="n">reward</span><span class="p">,</span> <span class="n">terminated</span><span class="p">,</span> <span class="n">truncated</span><span class="p">,</span> <span class="n">_</span> <span class="o">=</span> <span class="n">env</span><span class="o">.</span><span class="n">step</span><span class="p">(</span><span class="n">action</span><span class="o">.</span><span class="n">item</span><span class="p">())</span>
            <span class="n">done</span> <span class="o">=</span> <span class="n">terminated</span> <span class="ow">or</span> <span class="n">truncated</span>

            <span class="n">states</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">state</span><span class="p">)</span>
            <span class="n">actions</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">action</span><span class="o">.</span><span class="n">item</span><span class="p">())</span>
            <span class="n">rewards</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">reward</span><span class="p">)</span>
            <span class="n">dones</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">done</span><span class="p">)</span>
            <span class="n">values</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">value</span><span class="o">.</span><span class="n">item</span><span class="p">())</span>
            <span class="n">log_probs</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">log_prob</span><span class="o">.</span><span class="n">item</span><span class="p">())</span>

            <span class="n">state</span> <span class="o">=</span> <span class="n">next_state</span> <span class="k">if</span> <span class="ow">not</span> <span class="n">done</span> <span class="k">else</span> <span class="n">env</span><span class="o">.</span><span class="n">reset</span><span class="p">()[</span><span class="mi">0</span><span class="p">]</span>

        <span class="c1"># Last state value</span>
        <span class="k">with</span> <span class="n">torch</span><span class="o">.</span><span class="n">no_grad</span><span class="p">():</span>
            <span class="n">_</span><span class="p">,</span> <span class="n">_</span><span class="p">,</span> <span class="n">_</span><span class="p">,</span> <span class="n">last_value</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">network</span><span class="o">.</span><span class="n">get_action</span><span class="p">(</span>
                <span class="n">torch</span><span class="o">.</span><span class="n">FloatTensor</span><span class="p">(</span><span class="n">state</span><span class="p">)</span><span class="o">.</span><span class="n">unsqueeze</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>
            <span class="p">)</span>

        <span class="k">return</span> <span class="p">{</span>
            <span class="s1">&#39;states&#39;</span><span class="p">:</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">states</span><span class="p">),</span>
            <span class="s1">&#39;actions&#39;</span><span class="p">:</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">actions</span><span class="p">),</span>
            <span class="s1">&#39;rewards&#39;</span><span class="p">:</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">rewards</span><span class="p">),</span>
            <span class="s1">&#39;dones&#39;</span><span class="p">:</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">dones</span><span class="p">),</span>
            <span class="s1">&#39;values&#39;</span><span class="p">:</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">values</span><span class="p">),</span>
            <span class="s1">&#39;log_probs&#39;</span><span class="p">:</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">log_probs</span><span class="p">),</span>
            <span class="s1">&#39;last_value&#39;</span><span class="p">:</span> <span class="n">last_value</span><span class="o">.</span><span class="n">item</span><span class="p">()</span>
        <span class="p">}</span>

    <span class="k">def</span><span class="w"> </span><span class="nf">compute_gae</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">rollout</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Compute GAE&quot;&quot;&quot;</span>
        <span class="n">rewards</span> <span class="o">=</span> <span class="n">rollout</span><span class="p">[</span><span class="s1">&#39;rewards&#39;</span><span class="p">]</span>
        <span class="n">values</span> <span class="o">=</span> <span class="n">rollout</span><span class="p">[</span><span class="s1">&#39;values&#39;</span><span class="p">]</span>
        <span class="n">dones</span> <span class="o">=</span> <span class="n">rollout</span><span class="p">[</span><span class="s1">&#39;dones&#39;</span><span class="p">]</span>
        <span class="n">last_value</span> <span class="o">=</span> <span class="n">rollout</span><span class="p">[</span><span class="s1">&#39;last_value&#39;</span><span class="p">]</span>

        <span class="n">advantages</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros_like</span><span class="p">(</span><span class="n">rewards</span><span class="p">)</span>
        <span class="n">last_gae</span> <span class="o">=</span> <span class="mi">0</span>

        <span class="k">for</span> <span class="n">t</span> <span class="ow">in</span> <span class="nb">reversed</span><span class="p">(</span><span class="nb">range</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">rewards</span><span class="p">))):</span>
            <span class="k">if</span> <span class="n">t</span> <span class="o">==</span> <span class="nb">len</span><span class="p">(</span><span class="n">rewards</span><span class="p">)</span> <span class="o">-</span> <span class="mi">1</span><span class="p">:</span>
                <span class="n">next_value</span> <span class="o">=</span> <span class="n">last_value</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="n">next_value</span> <span class="o">=</span> <span class="n">values</span><span class="p">[</span><span class="n">t</span> <span class="o">+</span> <span class="mi">1</span><span class="p">]</span>

            <span class="n">next_non_terminal</span> <span class="o">=</span> <span class="mf">1.0</span> <span class="o">-</span> <span class="n">dones</span><span class="p">[</span><span class="n">t</span><span class="p">]</span>
            <span class="n">delta</span> <span class="o">=</span> <span class="n">rewards</span><span class="p">[</span><span class="n">t</span><span class="p">]</span> <span class="o">+</span> <span class="bp">self</span><span class="o">.</span><span class="n">gamma</span> <span class="o">*</span> <span class="n">next_value</span> <span class="o">*</span> <span class="n">next_non_terminal</span> <span class="o">-</span> <span class="n">values</span><span class="p">[</span><span class="n">t</span><span class="p">]</span>
            <span class="n">advantages</span><span class="p">[</span><span class="n">t</span><span class="p">]</span> <span class="o">=</span> <span class="n">last_gae</span> <span class="o">=</span> <span class="n">delta</span> <span class="o">+</span> <span class="bp">self</span><span class="o">.</span><span class="n">gamma</span> <span class="o">*</span> <span class="bp">self</span><span class="o">.</span><span class="n">gae_lambda</span> <span class="o">*</span> <span class="n">next_non_terminal</span> <span class="o">*</span> <span class="n">last_gae</span>

        <span class="n">returns</span> <span class="o">=</span> <span class="n">advantages</span> <span class="o">+</span> <span class="n">values</span>
        <span class="k">return</span> <span class="n">advantages</span><span class="p">,</span> <span class="n">returns</span>

    <span class="k">def</span><span class="w"> </span><span class="nf">update</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">rollout</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;PPO Update&quot;&quot;&quot;</span>
        <span class="n">advantages</span><span class="p">,</span> <span class="n">returns</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">compute_gae</span><span class="p">(</span><span class="n">rollout</span><span class="p">)</span>

        <span class="c1"># Normalize</span>
        <span class="n">advantages</span> <span class="o">=</span> <span class="p">(</span><span class="n">advantages</span> <span class="o">-</span> <span class="n">advantages</span><span class="o">.</span><span class="n">mean</span><span class="p">())</span> <span class="o">/</span> <span class="p">(</span><span class="n">advantages</span><span class="o">.</span><span class="n">std</span><span class="p">()</span> <span class="o">+</span> <span class="mf">1e-8</span><span class="p">)</span>

        <span class="c1"># Convert to tensors</span>
        <span class="n">states</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">FloatTensor</span><span class="p">(</span><span class="n">rollout</span><span class="p">[</span><span class="s1">&#39;states&#39;</span><span class="p">])</span>
        <span class="n">actions</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">LongTensor</span><span class="p">(</span><span class="n">rollout</span><span class="p">[</span><span class="s1">&#39;actions&#39;</span><span class="p">])</span>
        <span class="n">old_log_probs</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">FloatTensor</span><span class="p">(</span><span class="n">rollout</span><span class="p">[</span><span class="s1">&#39;log_probs&#39;</span><span class="p">])</span>
        <span class="n">returns</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">FloatTensor</span><span class="p">(</span><span class="n">returns</span><span class="p">)</span>
        <span class="n">advantages</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">FloatTensor</span><span class="p">(</span><span class="n">advantages</span><span class="p">)</span>

        <span class="c1"># Multiple epoch updates</span>
        <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">update_epochs</span><span class="p">):</span>
            <span class="c1"># Generate minibatches</span>
            <span class="n">indices</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">permutation</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">states</span><span class="p">))</span>

            <span class="k">for</span> <span class="n">start</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="nb">len</span><span class="p">(</span><span class="n">states</span><span class="p">),</span> <span class="bp">self</span><span class="o">.</span><span class="n">batch_size</span><span class="p">):</span>
                <span class="n">end</span> <span class="o">=</span> <span class="n">start</span> <span class="o">+</span> <span class="bp">self</span><span class="o">.</span><span class="n">batch_size</span>
                <span class="n">batch_indices</span> <span class="o">=</span> <span class="n">indices</span><span class="p">[</span><span class="n">start</span><span class="p">:</span><span class="n">end</span><span class="p">]</span>

                <span class="n">batch_states</span> <span class="o">=</span> <span class="n">states</span><span class="p">[</span><span class="n">batch_indices</span><span class="p">]</span>
                <span class="n">batch_actions</span> <span class="o">=</span> <span class="n">actions</span><span class="p">[</span><span class="n">batch_indices</span><span class="p">]</span>
                <span class="n">batch_old_log_probs</span> <span class="o">=</span> <span class="n">old_log_probs</span><span class="p">[</span><span class="n">batch_indices</span><span class="p">]</span>
                <span class="n">batch_returns</span> <span class="o">=</span> <span class="n">returns</span><span class="p">[</span><span class="n">batch_indices</span><span class="p">]</span>
                <span class="n">batch_advantages</span> <span class="o">=</span> <span class="n">advantages</span><span class="p">[</span><span class="n">batch_indices</span><span class="p">]</span>

                <span class="c1"># Evaluate with current policy</span>
                <span class="n">_</span><span class="p">,</span> <span class="n">new_log_probs</span><span class="p">,</span> <span class="n">entropy</span><span class="p">,</span> <span class="n">values</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">network</span><span class="o">.</span><span class="n">get_action</span><span class="p">(</span>
                    <span class="n">batch_states</span><span class="p">,</span> <span class="n">batch_actions</span>
                <span class="p">)</span>

                <span class="c1"># Compute ratio</span>
                <span class="n">ratio</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="n">new_log_probs</span> <span class="o">-</span> <span class="n">batch_old_log_probs</span><span class="p">)</span>

                <span class="c1"># Clipped loss</span>
                <span class="n">surr1</span> <span class="o">=</span> <span class="n">ratio</span> <span class="o">*</span> <span class="n">batch_advantages</span>
                <span class="n">surr2</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">clamp</span><span class="p">(</span><span class="n">ratio</span><span class="p">,</span> <span class="mi">1</span> <span class="o">-</span> <span class="bp">self</span><span class="o">.</span><span class="n">clip_epsilon</span><span class="p">,</span> <span class="mi">1</span> <span class="o">+</span> <span class="bp">self</span><span class="o">.</span><span class="n">clip_epsilon</span><span class="p">)</span> <span class="o">*</span> <span class="n">batch_advantages</span>
                <span class="n">actor_loss</span> <span class="o">=</span> <span class="o">-</span><span class="n">torch</span><span class="o">.</span><span class="n">min</span><span class="p">(</span><span class="n">surr1</span><span class="p">,</span> <span class="n">surr2</span><span class="p">)</span><span class="o">.</span><span class="n">mean</span><span class="p">()</span>

                <span class="c1"># Value loss</span>
                <span class="n">critic_loss</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">mse_loss</span><span class="p">(</span><span class="n">values</span><span class="o">.</span><span class="n">squeeze</span><span class="p">(),</span> <span class="n">batch_returns</span><span class="p">)</span>

                <span class="c1"># Entropy bonus</span>
                <span class="n">entropy_loss</span> <span class="o">=</span> <span class="o">-</span><span class="n">entropy</span><span class="o">.</span><span class="n">mean</span><span class="p">()</span>

                <span class="c1"># Total loss</span>
                <span class="n">loss</span> <span class="o">=</span> <span class="n">actor_loss</span> <span class="o">+</span> <span class="bp">self</span><span class="o">.</span><span class="n">value_coef</span> <span class="o">*</span> <span class="n">critic_loss</span> <span class="o">+</span> <span class="bp">self</span><span class="o">.</span><span class="n">entropy_coef</span> <span class="o">*</span> <span class="n">entropy_loss</span>

                <span class="c1"># Update</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">optimizer</span><span class="o">.</span><span class="n">zero_grad</span><span class="p">()</span>
                <span class="n">loss</span><span class="o">.</span><span class="n">backward</span><span class="p">()</span>
                <span class="n">nn</span><span class="o">.</span><span class="n">utils</span><span class="o">.</span><span class="n">clip_grad_norm_</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">network</span><span class="o">.</span><span class="n">parameters</span><span class="p">(),</span> <span class="bp">self</span><span class="o">.</span><span class="n">max_grad_norm</span><span class="p">)</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">optimizer</span><span class="o">.</span><span class="n">step</span><span class="p">()</span>

        <span class="k">return</span> <span class="n">actor_loss</span><span class="o">.</span><span class="n">item</span><span class="p">(),</span> <span class="n">critic_loss</span><span class="o">.</span><span class="n">item</span><span class="p">()</span>
</code></pre></div>

<h3 id="42-ppo-training-loop">4.2 PPO Training Loop<a class="header-link" href="#42-ppo-training-loop" title="Permanent link">&para;</a></h3>
<div class="highlight"><pre><span></span><code><span class="kn">import</span><span class="w"> </span><span class="nn">gymnasium</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">gym</span>

<span class="k">def</span><span class="w"> </span><span class="nf">train_ppo</span><span class="p">(</span><span class="n">env_name</span><span class="o">=</span><span class="s1">&#39;CartPole-v1&#39;</span><span class="p">,</span> <span class="n">total_timesteps</span><span class="o">=</span><span class="mi">100000</span><span class="p">,</span> <span class="n">n_steps</span><span class="o">=</span><span class="mi">2048</span><span class="p">):</span>
    <span class="n">env</span> <span class="o">=</span> <span class="n">gym</span><span class="o">.</span><span class="n">make</span><span class="p">(</span><span class="n">env_name</span><span class="p">)</span>
    <span class="n">state_dim</span> <span class="o">=</span> <span class="n">env</span><span class="o">.</span><span class="n">observation_space</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
    <span class="n">action_dim</span> <span class="o">=</span> <span class="n">env</span><span class="o">.</span><span class="n">action_space</span><span class="o">.</span><span class="n">n</span>

    <span class="n">agent</span> <span class="o">=</span> <span class="n">PPOAgent</span><span class="p">(</span><span class="n">state_dim</span><span class="p">,</span> <span class="n">action_dim</span><span class="p">)</span>

    <span class="n">timesteps</span> <span class="o">=</span> <span class="mi">0</span>
    <span class="n">episode_rewards</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="n">current_episode_reward</span> <span class="o">=</span> <span class="mi">0</span>

    <span class="k">while</span> <span class="n">timesteps</span> <span class="o">&lt;</span> <span class="n">total_timesteps</span><span class="p">:</span>
        <span class="c1"># Collect rollouts</span>
        <span class="n">rollout</span> <span class="o">=</span> <span class="n">agent</span><span class="o">.</span><span class="n">collect_rollouts</span><span class="p">(</span><span class="n">env</span><span class="p">,</span> <span class="n">n_steps</span><span class="p">)</span>
        <span class="n">timesteps</span> <span class="o">+=</span> <span class="n">n_steps</span>

        <span class="c1"># Track episode rewards</span>
        <span class="k">for</span> <span class="n">r</span><span class="p">,</span> <span class="n">d</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">(</span><span class="n">rollout</span><span class="p">[</span><span class="s1">&#39;rewards&#39;</span><span class="p">],</span> <span class="n">rollout</span><span class="p">[</span><span class="s1">&#39;dones&#39;</span><span class="p">]):</span>
            <span class="n">current_episode_reward</span> <span class="o">+=</span> <span class="n">r</span>
            <span class="k">if</span> <span class="n">d</span><span class="p">:</span>
                <span class="n">episode_rewards</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">current_episode_reward</span><span class="p">)</span>
                <span class="n">current_episode_reward</span> <span class="o">=</span> <span class="mi">0</span>

        <span class="c1"># PPO update</span>
        <span class="n">actor_loss</span><span class="p">,</span> <span class="n">critic_loss</span> <span class="o">=</span> <span class="n">agent</span><span class="o">.</span><span class="n">update</span><span class="p">(</span><span class="n">rollout</span><span class="p">)</span>

        <span class="c1"># Logging</span>
        <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">episode_rewards</span><span class="p">)</span> <span class="o">&gt;</span> <span class="mi">0</span> <span class="ow">and</span> <span class="n">timesteps</span> <span class="o">%</span> <span class="mi">10000</span> <span class="o">&lt;</span> <span class="n">n_steps</span><span class="p">:</span>
            <span class="n">avg_reward</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">episode_rewards</span><span class="p">[</span><span class="o">-</span><span class="mi">10</span><span class="p">:])</span> <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">episode_rewards</span><span class="p">)</span> <span class="o">&gt;=</span> <span class="mi">10</span> <span class="k">else</span> <span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">episode_rewards</span><span class="p">)</span>
            <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Timesteps: </span><span class="si">{</span><span class="n">timesteps</span><span class="si">}</span><span class="s2">, Avg Reward: </span><span class="si">{</span><span class="n">avg_reward</span><span class="si">:</span><span class="s2">.2f</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>

    <span class="k">return</span> <span class="n">agent</span><span class="p">,</span> <span class="n">episode_rewards</span>
</code></pre></div>

<hr />
<h2 id="5-ppo-variants">5. PPO Variants<a class="header-link" href="#5-ppo-variants" title="Permanent link">&para;</a></h2>
<h3 id="51-ppo-clip-basic">5.1 PPO-Clip (Basic)<a class="header-link" href="#51-ppo-clip-basic" title="Permanent link">&para;</a></h3>
<p>The method implemented above.</p>
<h3 id="52-ppo-penalty">5.2 PPO-Penalty<a class="header-link" href="#52-ppo-penalty" title="Permanent link">&para;</a></h3>
<p>Add KL divergence as a penalty:</p>
<div class="highlight"><pre><span></span><code><span class="k">def</span><span class="w"> </span><span class="nf">ppo_penalty_loss</span><span class="p">(</span><span class="n">ratio</span><span class="p">,</span> <span class="n">advantage</span><span class="p">,</span> <span class="n">old_probs</span><span class="p">,</span> <span class="n">new_probs</span><span class="p">,</span> <span class="n">beta</span><span class="o">=</span><span class="mf">0.01</span><span class="p">):</span>
    <span class="n">policy_loss</span> <span class="o">=</span> <span class="p">(</span><span class="n">ratio</span> <span class="o">*</span> <span class="n">advantage</span><span class="p">)</span><span class="o">.</span><span class="n">mean</span><span class="p">()</span>

    <span class="n">kl_div</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">kl_div</span><span class="p">(</span><span class="n">new_probs</span><span class="o">.</span><span class="n">log</span><span class="p">(),</span> <span class="n">old_probs</span><span class="p">,</span> <span class="n">reduction</span><span class="o">=</span><span class="s1">&#39;batchmean&#39;</span><span class="p">)</span>

    <span class="k">return</span> <span class="o">-</span><span class="n">policy_loss</span> <span class="o">+</span> <span class="n">beta</span> <span class="o">*</span> <span class="n">kl_div</span>
</code></pre></div>

<h3 id="53-clipped-value-loss">5.3 Clipped Value Loss<a class="header-link" href="#53-clipped-value-loss" title="Permanent link">&para;</a></h3>
<p>Apply clipping to value function as well:</p>
<div class="highlight"><pre><span></span><code><span class="k">def</span><span class="w"> </span><span class="nf">clipped_value_loss</span><span class="p">(</span><span class="n">values</span><span class="p">,</span> <span class="n">old_values</span><span class="p">,</span> <span class="n">returns</span><span class="p">,</span> <span class="n">clip_epsilon</span><span class="o">=</span><span class="mf">0.2</span><span class="p">):</span>
    <span class="c1"># Clipped values</span>
    <span class="n">clipped_values</span> <span class="o">=</span> <span class="n">old_values</span> <span class="o">+</span> <span class="n">torch</span><span class="o">.</span><span class="n">clamp</span><span class="p">(</span>
        <span class="n">values</span> <span class="o">-</span> <span class="n">old_values</span><span class="p">,</span> <span class="o">-</span><span class="n">clip_epsilon</span><span class="p">,</span> <span class="n">clip_epsilon</span>
    <span class="p">)</span>

    <span class="c1"># Maximum of two losses</span>
    <span class="n">loss1</span> <span class="o">=</span> <span class="p">(</span><span class="n">values</span> <span class="o">-</span> <span class="n">returns</span><span class="p">)</span> <span class="o">**</span> <span class="mi">2</span>
    <span class="n">loss2</span> <span class="o">=</span> <span class="p">(</span><span class="n">clipped_values</span> <span class="o">-</span> <span class="n">returns</span><span class="p">)</span> <span class="o">**</span> <span class="mi">2</span>

    <span class="k">return</span> <span class="mf">0.5</span> <span class="o">*</span> <span class="n">torch</span><span class="o">.</span><span class="n">max</span><span class="p">(</span><span class="n">loss1</span><span class="p">,</span> <span class="n">loss2</span><span class="p">)</span><span class="o">.</span><span class="n">mean</span><span class="p">()</span>
</code></pre></div>

<hr />
<h2 id="6-continuous-action-space-ppo">6. Continuous Action Space PPO<a class="header-link" href="#6-continuous-action-space-ppo" title="Permanent link">&para;</a></h2>
<div class="highlight"><pre><span></span><code><span class="k">class</span><span class="w"> </span><span class="nc">ContinuousPPONetwork</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="k">def</span><span class="w"> </span><span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">state_dim</span><span class="p">,</span> <span class="n">action_dim</span><span class="p">,</span> <span class="n">hidden_dim</span><span class="o">=</span><span class="mi">64</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>

        <span class="c1"># Actor</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">actor_mean</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Sequential</span><span class="p">(</span>
            <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">state_dim</span><span class="p">,</span> <span class="n">hidden_dim</span><span class="p">),</span>
            <span class="n">nn</span><span class="o">.</span><span class="n">Tanh</span><span class="p">(),</span>
            <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">hidden_dim</span><span class="p">,</span> <span class="n">hidden_dim</span><span class="p">),</span>
            <span class="n">nn</span><span class="o">.</span><span class="n">Tanh</span><span class="p">(),</span>
            <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">hidden_dim</span><span class="p">,</span> <span class="n">action_dim</span><span class="p">)</span>
        <span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">actor_log_std</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Parameter</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">action_dim</span><span class="p">))</span>

        <span class="c1"># Critic</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">critic</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Sequential</span><span class="p">(</span>
            <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">state_dim</span><span class="p">,</span> <span class="n">hidden_dim</span><span class="p">),</span>
            <span class="n">nn</span><span class="o">.</span><span class="n">Tanh</span><span class="p">(),</span>
            <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">hidden_dim</span><span class="p">,</span> <span class="n">hidden_dim</span><span class="p">),</span>
            <span class="n">nn</span><span class="o">.</span><span class="n">Tanh</span><span class="p">(),</span>
            <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">hidden_dim</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
        <span class="p">)</span>

    <span class="k">def</span><span class="w"> </span><span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">state</span><span class="p">):</span>
        <span class="n">mean</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">actor_mean</span><span class="p">(</span><span class="n">state</span><span class="p">)</span>
        <span class="n">std</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">actor_log_std</span><span class="o">.</span><span class="n">exp</span><span class="p">()</span>
        <span class="n">value</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">critic</span><span class="p">(</span><span class="n">state</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">mean</span><span class="p">,</span> <span class="n">std</span><span class="p">,</span> <span class="n">value</span>

    <span class="k">def</span><span class="w"> </span><span class="nf">get_action</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">state</span><span class="p">,</span> <span class="n">action</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
        <span class="n">mean</span><span class="p">,</span> <span class="n">std</span><span class="p">,</span> <span class="n">value</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">forward</span><span class="p">(</span><span class="n">state</span><span class="p">)</span>
        <span class="n">dist</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">distributions</span><span class="o">.</span><span class="n">Normal</span><span class="p">(</span><span class="n">mean</span><span class="p">,</span> <span class="n">std</span><span class="p">)</span>

        <span class="k">if</span> <span class="n">action</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">action</span> <span class="o">=</span> <span class="n">dist</span><span class="o">.</span><span class="n">sample</span><span class="p">()</span>

        <span class="n">log_prob</span> <span class="o">=</span> <span class="n">dist</span><span class="o">.</span><span class="n">log_prob</span><span class="p">(</span><span class="n">action</span><span class="p">)</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">)</span>
        <span class="n">entropy</span> <span class="o">=</span> <span class="n">dist</span><span class="o">.</span><span class="n">entropy</span><span class="p">()</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">)</span>

        <span class="k">return</span> <span class="n">action</span><span class="p">,</span> <span class="n">log_prob</span><span class="p">,</span> <span class="n">entropy</span><span class="p">,</span> <span class="n">value</span>
</code></pre></div>

<hr />
<h2 id="7-hyperparameter-guide">7. Hyperparameter Guide<a class="header-link" href="#7-hyperparameter-guide" title="Permanent link">&para;</a></h2>
<h3 id="71-general-settings">7.1 General Settings<a class="header-link" href="#71-general-settings" title="Permanent link">&para;</a></h3>
<div class="highlight"><pre><span></span><code><span class="n">config</span> <span class="o">=</span> <span class="p">{</span>
    <span class="c1"># Learning</span>
    <span class="s1">&#39;lr&#39;</span><span class="p">:</span> <span class="mf">3e-4</span><span class="p">,</span>                  <span class="c1"># Learning rate</span>
    <span class="s1">&#39;gamma&#39;</span><span class="p">:</span> <span class="mf">0.99</span><span class="p">,</span>               <span class="c1"># Discount factor</span>
    <span class="s1">&#39;gae_lambda&#39;</span><span class="p">:</span> <span class="mf">0.95</span><span class="p">,</span>          <span class="c1"># GAE lambda</span>

    <span class="c1"># PPO specific</span>
    <span class="s1">&#39;clip_epsilon&#39;</span><span class="p">:</span> <span class="mf">0.2</span><span class="p">,</span>         <span class="c1"># Clipping range</span>
    <span class="s1">&#39;update_epochs&#39;</span><span class="p">:</span> <span class="mi">10</span><span class="p">,</span>         <span class="c1"># Update iterations</span>
    <span class="s1">&#39;batch_size&#39;</span><span class="p">:</span> <span class="mi">64</span><span class="p">,</span>            <span class="c1"># Minibatch size</span>

    <span class="c1"># Loss coefficients</span>
    <span class="s1">&#39;value_coef&#39;</span><span class="p">:</span> <span class="mf">0.5</span><span class="p">,</span>           <span class="c1"># Value loss coefficient</span>
    <span class="s1">&#39;entropy_coef&#39;</span><span class="p">:</span> <span class="mf">0.01</span><span class="p">,</span>        <span class="c1"># Entropy coefficient</span>

    <span class="c1"># Rollout</span>
    <span class="s1">&#39;n_steps&#39;</span><span class="p">:</span> <span class="mi">2048</span><span class="p">,</span>             <span class="c1"># Rollout length</span>
    <span class="s1">&#39;n_envs&#39;</span><span class="p">:</span> <span class="mi">8</span><span class="p">,</span>                 <span class="c1"># Parallel environments</span>

    <span class="c1"># Stabilization</span>
    <span class="s1">&#39;max_grad_norm&#39;</span><span class="p">:</span> <span class="mf">0.5</span><span class="p">,</span>        <span class="c1"># Gradient clipping</span>
<span class="p">}</span>
</code></pre></div>

<h3 id="72-environment-specific-tuning">7.2 Environment-Specific Tuning<a class="header-link" href="#72-environment-specific-tuning" title="Permanent link">&para;</a></h3>
<table>
<thead>
<tr>
<th>Environment</th>
<th>lr</th>
<th>n_steps</th>
<th>clip_epsilon</th>
</tr>
</thead>
<tbody>
<tr>
<td>CartPole</td>
<td>3e-4</td>
<td>128</td>
<td>0.2</td>
</tr>
<tr>
<td>LunarLander</td>
<td>3e-4</td>
<td>2048</td>
<td>0.2</td>
</tr>
<tr>
<td>Atari</td>
<td>2.5e-4</td>
<td>128</td>
<td>0.1</td>
</tr>
<tr>
<td>MuJoCo</td>
<td>3e-4</td>
<td>2048</td>
<td>0.2</td>
</tr>
</tbody>
</table>
<hr />
<h2 id="8-ppo-vs-other-algorithms">8. PPO vs Other Algorithms<a class="header-link" href="#8-ppo-vs-other-algorithms" title="Permanent link">&para;</a></h2>
<table>
<thead>
<tr>
<th>Algorithm</th>
<th>Complexity</th>
<th>Sample Efficiency</th>
<th>Stability</th>
</tr>
</thead>
<tbody>
<tr>
<td>REINFORCE</td>
<td>Low</td>
<td>Low</td>
<td>Low</td>
</tr>
<tr>
<td>A2C</td>
<td>Medium</td>
<td>Medium</td>
<td>Medium</td>
</tr>
<tr>
<td>TRPO</td>
<td>High</td>
<td>High</td>
<td>High</td>
</tr>
<tr>
<td><strong>PPO</strong></td>
<td><strong>Medium</strong></td>
<td><strong>High</strong></td>
<td><strong>High</strong></td>
</tr>
<tr>
<td>SAC</td>
<td>Medium</td>
<td>High</td>
<td>High</td>
</tr>
</tbody>
</table>
<p><strong>PPO Advantages:</strong>
- TRPO-level performance, simple implementation
- Stable across various environments
- Low sensitivity to hyperparameters</p>
<hr />
<h2 id="summary">Summary<a class="header-link" href="#summary" title="Permanent link">&para;</a></h2>
<p><strong>PPO Core:</strong></p>
<div class="highlight"><pre><span></span><code>L^{CLIP} = E[min(r(Œ∏)A, clip(r(Œ∏), 1-Œµ, 1+Œµ)A)]

r(Œ∏) = œÄ_Œ∏(a|s) / œÄ_Œ∏_old(a|s)  # Policy ratio
</code></pre></div>

<p><strong>Clipping Effect:</strong>
- Constrains policy changes to [1-Œµ, 1+Œµ] range
- Prevents drastic updates
- Ensures learning stability</p>
<hr />
<h2 id="next-steps">Next Steps<a class="header-link" href="#next-steps" title="Permanent link">&para;</a></h2>
<ul>
<li><a href="./11_Multi_Agent_RL.md">11_Multi_Agent_RL.md</a> - Multi-Agent Reinforcement Learning</li>
</ul>
    </div>

    
<div class="lesson-toolbar lesson-toolbar--bottom">
    <div class="toolbar-nav toolbar-nav--prev">
        
        <a href="/study/en/Reinforcement_Learning/09_Actor_Critic.html" class="nav-prev">
            <span class="nav-label">Previous</span>
            <span class="nav-title">09. Actor-Critic Methods</span>
        </a>
        
    </div>
    <div class="toolbar-actions">
        <button class="btn btn-copy-link" data-action="copy-link" title="Copy link">
            <span class="icon">üîó</span>
            <span class="text">Copy link</span>
        </button>
        <a href="/study/en/Reinforcement_Learning/" class="btn btn-topic-link" title="Back to topic list">
            <span class="icon">üìã</span>
            <span class="text">Topic list</span>
        </a>
    </div>
    <div class="toolbar-nav toolbar-nav--next">
        
        <a href="/study/en/Reinforcement_Learning/11_Multi_Agent_RL.html" class="nav-next">
            <span class="nav-label">Next</span>
            <span class="nav-title">11. Multi-Agent Reinforcement Learning (MARL)</span>
        </a>
        
    </div>
</div>


    <div class="toolbar-keyboard-hint">
        <kbd>&larr;</kbd> <kbd>&rarr;</kbd> to navigate between lessons
    </div>
</article>

<!-- Scroll to top floating button -->
<button class="scroll-to-top" id="scroll-to-top" title="Scroll to top" aria-label="Scroll to top">‚Üë</button>

            </div>
        </main>
    </div>

    <script src="/study/static/js/app.js"></script>
    <script>
        function switchLanguage(newLang) {
            const path = window.location.pathname;
            const base = '/study';
            const currentLang = 'en';
            const newPath = path.replace(base + '/' + currentLang + '/', base + '/' + newLang + '/');
            window.location.href = newPath + window.location.search;
        }
    </script>
    
<script>
    document.addEventListener('DOMContentLoaded', function() {
        // Copy link - independent feedback per button
        document.querySelectorAll('[data-action="copy-link"]').forEach(function(btn) {
            btn.addEventListener('click', function() {
                navigator.clipboard.writeText(window.location.href);
                var textEl = this.querySelector('.text');
                var originalText = textEl.textContent;
                textEl.textContent = 'Copied!';
                setTimeout(function() { textEl.textContent = originalText; }, 2000);
            });
        });

        // Add copy buttons to code blocks
        document.querySelectorAll('.lesson-content pre').forEach(function(pre) {
            var copyBtn = document.createElement('button');
            copyBtn.className = 'code-copy-btn';
            copyBtn.textContent = 'Copy';
            copyBtn.addEventListener('click', function() {
                var code = pre.querySelector('code');
                var text = code ? code.textContent : pre.textContent;
                navigator.clipboard.writeText(text);
                copyBtn.textContent = 'Copied!';
                setTimeout(function() { copyBtn.textContent = 'Copy'; }, 2000);
            });
            pre.style.position = 'relative';
            pre.appendChild(copyBtn);
        });

        // Scroll to top button
        var scrollBtn = document.getElementById('scroll-to-top');
        window.addEventListener('scroll', function() {
            scrollBtn.classList.toggle('visible', window.scrollY > 300);
        });
        scrollBtn.addEventListener('click', function() {
            window.scrollTo({ top: 0, behavior: 'smooth' });
        });

        // Keyboard shortcuts: left/right arrows for lesson navigation
        document.addEventListener('keydown', function(e) {
            var tag = document.activeElement.tagName;
            if (tag === 'INPUT' || tag === 'TEXTAREA' || tag === 'SELECT') return;

            if (e.key === 'ArrowLeft') {
                var prevLink = document.querySelector('.nav-prev');
                if (prevLink) prevLink.click();
            } else if (e.key === 'ArrowRight') {
                var nextLink = document.querySelector('.nav-next');
                if (nextLink) nextLink.click();
            }
        });
    });
</script>

</body>
</html>
{% endraw %}