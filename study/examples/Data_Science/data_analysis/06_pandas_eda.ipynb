{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exploratory Data Analysis with Pandas\n",
    "\n",
    "This notebook demonstrates essential exploratory data analysis (EDA) techniques using Pandas.\n",
    "\n",
    "## Topics Covered:\n",
    "- Dataset overview and inspection\n",
    "- Missing value analysis\n",
    "- Statistical summaries\n",
    "- Groupby operations and aggregations\n",
    "- Correlation analysis\n",
    "- Filtering and sorting\n",
    "- Date/time operations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "np.random.seed(42)\n",
    "\n",
    "# Set display options\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_rows', 100)\n",
    "pd.set_option('display.precision', 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Creating a Sample Dataset\n",
    "\n",
    "We'll create a synthetic sales dataset with multiple columns including dates, categories, and numerical values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate synthetic sales data\n",
    "n_records = 1000\n",
    "\n",
    "# Date range\n",
    "dates = pd.date_range(start='2023-01-01', end='2023-12-31', periods=n_records)\n",
    "\n",
    "# Product categories\n",
    "categories = ['Electronics', 'Clothing', 'Food', 'Books', 'Home & Garden']\n",
    "category_list = np.random.choice(categories, n_records)\n",
    "\n",
    "# Regions\n",
    "regions = ['North', 'South', 'East', 'West']\n",
    "region_list = np.random.choice(regions, n_records)\n",
    "\n",
    "# Sales amounts (with some variation by category)\n",
    "base_sales = np.random.uniform(10, 500, n_records)\n",
    "category_multipliers = {'Electronics': 2.0, 'Clothing': 1.2, 'Food': 0.8, 'Books': 1.0, 'Home & Garden': 1.5}\n",
    "sales = [base_sales[i] * category_multipliers[category_list[i]] for i in range(n_records)]\n",
    "\n",
    "# Quantities\n",
    "quantities = np.random.randint(1, 20, n_records)\n",
    "\n",
    "# Customer IDs\n",
    "customer_ids = np.random.randint(1000, 2000, n_records)\n",
    "\n",
    "# Create DataFrame\n",
    "df = pd.DataFrame({\n",
    "    'date': dates,\n",
    "    'customer_id': customer_ids,\n",
    "    'category': category_list,\n",
    "    'region': region_list,\n",
    "    'sales_amount': sales,\n",
    "    'quantity': quantities\n",
    "})\n",
    "\n",
    "# Calculate unit price\n",
    "df['unit_price'] = df['sales_amount'] / df['quantity']\n",
    "\n",
    "# Introduce some missing values (realistic scenario)\n",
    "missing_indices = np.random.choice(df.index, size=50, replace=False)\n",
    "df.loc[missing_indices, 'sales_amount'] = np.nan\n",
    "\n",
    "missing_indices_2 = np.random.choice(df.index, size=30, replace=False)\n",
    "df.loc[missing_indices_2, 'quantity'] = np.nan\n",
    "\n",
    "print(\"Sample dataset created successfully!\")\n",
    "df.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Dataset Overview\n",
    "\n",
    "First, let's understand the basic structure and characteristics of our dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Shape of the dataset\n",
    "print(f\"Dataset shape: {df.shape}\")\n",
    "print(f\"Number of rows: {df.shape[0]}\")\n",
    "print(f\"Number of columns: {df.shape[1]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Column names and data types\n",
    "print(\"\\nColumn names:\")\n",
    "print(df.columns.tolist())\n",
    "\n",
    "print(\"\\nData types:\")\n",
    "print(df.dtypes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Comprehensive information about the DataFrame\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Missing Value Analysis\n",
    "\n",
    "Identifying and handling missing values is a critical step in EDA."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count missing values per column\n",
    "missing_counts = df.isnull().sum()\n",
    "print(\"Missing values per column:\")\n",
    "print(missing_counts)\n",
    "\n",
    "# Percentage of missing values\n",
    "missing_percentages = (df.isnull().sum() / len(df)) * 100\n",
    "print(\"\\nPercentage of missing values:\")\n",
    "print(missing_percentages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize missing values\n",
    "missing_summary = pd.DataFrame({\n",
    "    'Column': df.columns,\n",
    "    'Missing_Count': missing_counts.values,\n",
    "    'Missing_Percentage': missing_percentages.values\n",
    "})\n",
    "missing_summary = missing_summary[missing_summary['Missing_Count'] > 0]\n",
    "print(\"Summary of columns with missing values:\")\n",
    "print(missing_summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Handle missing values - create a copy for demonstration\n",
    "df_filled = df.copy()\n",
    "\n",
    "# Fill missing sales_amount with median\n",
    "df_filled['sales_amount'].fillna(df_filled['sales_amount'].median(), inplace=True)\n",
    "\n",
    "# Fill missing quantity with mode (most common value)\n",
    "df_filled['quantity'].fillna(df_filled['quantity'].mode()[0], inplace=True)\n",
    "\n",
    "# Recalculate unit_price\n",
    "df_filled['unit_price'] = df_filled['sales_amount'] / df_filled['quantity']\n",
    "\n",
    "print(\"Missing values after filling:\")\n",
    "print(df_filled.isnull().sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Statistical Summaries\n",
    "\n",
    "Understanding the distribution and central tendencies of numerical columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Descriptive statistics for numerical columns\n",
    "df_filled.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Detailed statistics for a specific column\n",
    "print(\"Statistics for sales_amount:\")\n",
    "print(f\"Mean: {df_filled['sales_amount'].mean():.2f}\")\n",
    "print(f\"Median: {df_filled['sales_amount'].median():.2f}\")\n",
    "print(f\"Standard Deviation: {df_filled['sales_amount'].std():.2f}\")\n",
    "print(f\"Min: {df_filled['sales_amount'].min():.2f}\")\n",
    "print(f\"Max: {df_filled['sales_amount'].max():.2f}\")\n",
    "print(f\"25th Percentile: {df_filled['sales_amount'].quantile(0.25):.2f}\")\n",
    "print(f\"75th Percentile: {df_filled['sales_amount'].quantile(0.75):.2f}\")\n",
    "print(f\"IQR: {df_filled['sales_amount'].quantile(0.75) - df_filled['sales_amount'].quantile(0.25):.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Summary statistics for categorical columns\n",
    "print(\"Category distribution:\")\n",
    "print(df_filled['category'].value_counts())\n",
    "\n",
    "print(\"\\nRegion distribution:\")\n",
    "print(df_filled['region'].value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Groupby Operations and Aggregations\n",
    "\n",
    "Analyzing data by groups to identify patterns and trends."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Group by category and calculate aggregates\n",
    "category_summary = df_filled.groupby('category').agg({\n",
    "    'sales_amount': ['sum', 'mean', 'count'],\n",
    "    'quantity': ['sum', 'mean'],\n",
    "    'unit_price': 'mean'\n",
    "}).round(2)\n",
    "\n",
    "print(\"Summary by Category:\")\n",
    "print(category_summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Group by region\n",
    "region_summary = df_filled.groupby('region').agg({\n",
    "    'sales_amount': ['sum', 'mean'],\n",
    "    'customer_id': 'nunique'  # Count unique customers\n",
    "}).round(2)\n",
    "\n",
    "region_summary.columns = ['Total Sales', 'Avg Sales', 'Unique Customers']\n",
    "print(\"\\nSummary by Region:\")\n",
    "print(region_summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Multiple groupby - category and region\n",
    "multi_group = df_filled.groupby(['category', 'region'])['sales_amount'].agg(['sum', 'mean', 'count']).round(2)\n",
    "print(\"\\nSummary by Category and Region:\")\n",
    "print(multi_group.head(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Correlation Analysis\n",
    "\n",
    "Examining relationships between numerical variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select only numerical columns for correlation\n",
    "numerical_cols = df_filled.select_dtypes(include=[np.number]).columns\n",
    "correlation_matrix = df_filled[numerical_cols].corr()\n",
    "\n",
    "print(\"Correlation Matrix:\")\n",
    "print(correlation_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Correlation with a specific column\n",
    "print(\"\\nCorrelation with sales_amount:\")\n",
    "print(correlation_matrix['sales_amount'].sort_values(ascending=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Filtering and Sorting\n",
    "\n",
    "Essential techniques for data exploration and analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter: High-value sales (> 500)\n",
    "high_value_sales = df_filled[df_filled['sales_amount'] > 500]\n",
    "print(f\"Number of high-value sales: {len(high_value_sales)}\")\n",
    "print(high_value_sales.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Multiple conditions: Electronics in North region\n",
    "electronics_north = df_filled[(df_filled['category'] == 'Electronics') & (df_filled['region'] == 'North')]\n",
    "print(f\"\\nElectronics sales in North region: {len(electronics_north)}\")\n",
    "print(electronics_north.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sort by sales_amount (descending)\n",
    "top_sales = df_filled.sort_values('sales_amount', ascending=False).head(10)\n",
    "print(\"\\nTop 10 sales:\")\n",
    "print(top_sales[['date', 'category', 'region', 'sales_amount', 'quantity']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Value counts for categorical analysis\n",
    "print(\"\\nTop categories by number of transactions:\")\n",
    "print(df_filled['category'].value_counts())\n",
    "\n",
    "print(\"\\nCategory proportions:\")\n",
    "print(df_filled['category'].value_counts(normalize=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Date/Time Operations\n",
    "\n",
    "Working with temporal data to identify trends and patterns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract date components\n",
    "df_filled['year'] = df_filled['date'].dt.year\n",
    "df_filled['month'] = df_filled['date'].dt.month\n",
    "df_filled['day'] = df_filled['date'].dt.day\n",
    "df_filled['day_of_week'] = df_filled['date'].dt.day_name()\n",
    "df_filled['quarter'] = df_filled['date'].dt.quarter\n",
    "\n",
    "print(\"Date components added:\")\n",
    "print(df_filled[['date', 'year', 'month', 'day', 'day_of_week', 'quarter']].head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Monthly sales trend\n",
    "monthly_sales = df_filled.groupby('month')['sales_amount'].agg(['sum', 'mean', 'count']).round(2)\n",
    "monthly_sales.columns = ['Total Sales', 'Avg Sales', 'Transactions']\n",
    "print(\"\\nMonthly Sales Summary:\")\n",
    "print(monthly_sales)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sales by day of week\n",
    "day_of_week_sales = df_filled.groupby('day_of_week')['sales_amount'].agg(['sum', 'mean', 'count']).round(2)\n",
    "print(\"\\nSales by Day of Week:\")\n",
    "print(day_of_week_sales)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Quarterly performance\n",
    "quarterly_sales = df_filled.groupby('quarter')['sales_amount'].agg(['sum', 'mean', 'count']).round(2)\n",
    "quarterly_sales.columns = ['Total Sales', 'Avg Sales', 'Transactions']\n",
    "print(\"\\nQuarterly Sales Summary:\")\n",
    "print(quarterly_sales)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Advanced Filtering and Insights\n",
    "\n",
    "Combining techniques to derive actionable insights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find top-performing category-region combinations\n",
    "top_combinations = df_filled.groupby(['category', 'region'])['sales_amount'].sum().sort_values(ascending=False).head(10)\n",
    "print(\"Top 10 Category-Region Combinations by Total Sales:\")\n",
    "print(top_combinations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identify customers with highest total purchases\n",
    "top_customers = df_filled.groupby('customer_id').agg({\n",
    "    'sales_amount': 'sum',\n",
    "    'date': 'count'\n",
    "}).sort_values('sales_amount', ascending=False).head(10)\n",
    "top_customers.columns = ['Total Purchases', 'Number of Transactions']\n",
    "print(\"\\nTop 10 Customers:\")\n",
    "print(top_customers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate category-wise market share\n",
    "category_sales = df_filled.groupby('category')['sales_amount'].sum()\n",
    "category_market_share = (category_sales / category_sales.sum() * 100).round(2)\n",
    "print(\"\\nMarket Share by Category (%):\")\n",
    "print(category_market_share.sort_values(ascending=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "In this notebook, we covered essential EDA techniques:\n",
    "\n",
    "1. **Dataset Overview**: Understanding shape, columns, and data types\n",
    "2. **Missing Value Analysis**: Identifying and handling missing data\n",
    "3. **Statistical Summaries**: Mean, median, standard deviation, quantiles\n",
    "4. **Groupby Operations**: Aggregating data by categories\n",
    "5. **Correlation Analysis**: Examining relationships between variables\n",
    "6. **Filtering and Sorting**: Extracting specific subsets of data\n",
    "7. **Date/Time Operations**: Temporal analysis and trends\n",
    "8. **Advanced Insights**: Combining techniques for business intelligence\n",
    "\n",
    "These techniques form the foundation of data analysis and are essential for understanding your data before modeling."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
