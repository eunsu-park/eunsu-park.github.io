{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b2c3d4e5-0001-4000-8000-000000000001",
   "metadata": {},
   "source": [
    "# Regression Analysis\n",
    "\n",
    "Regression analysis models the relationship between a **response variable** (dependent) and one or more **predictor variables** (independent). It is the workhorse of statistical modelling and machine learning.\n",
    "\n",
    "**Topics covered in this notebook:**\n",
    "1. Simple linear regression — fitting a line, OLS estimation\n",
    "2. Interpreting coefficients — R², p-values, residual diagnostics\n",
    "3. Multiple linear regression — multiple predictors\n",
    "4. Polynomial regression — non-linear patterns, bias-variance tradeoff\n",
    "5. Logistic regression — binary outcomes, sigmoid function\n",
    "6. Regression assumptions — linearity, normality, homoscedasticity\n",
    "\n",
    "**Mathematical foundation — Ordinary Least Squares (OLS):**\n",
    "\n",
    "$$\\hat{\\boldsymbol{\\beta}} = (\\mathbf{X}^\\top \\mathbf{X})^{-1} \\mathbf{X}^\\top \\mathbf{y}$$\n",
    "\n",
    "OLS minimises the **residual sum of squares (RSS)** $\\sum_{i=1}^{n}(y_i - \\hat{y}_i)^2$ and is the **BLUE** (Best Linear Unbiased Estimator) under the Gauss-Markov assumptions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2c3d4e5-0002-4000-8000-000000000002",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import scipy.stats as stats\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.gridspec as gridspec\n",
    "\n",
    "# Optional: scikit-learn (used where noted; gracefully falls back to manual)\n",
    "try:\n",
    "    from sklearn.linear_model import LinearRegression, LogisticRegression\n",
    "    from sklearn.preprocessing import PolynomialFeatures, StandardScaler\n",
    "    from sklearn.pipeline import make_pipeline\n",
    "    from sklearn.metrics import r2_score\n",
    "    SKLEARN = True\n",
    "    print('scikit-learn is available — using sklearn where convenient.')\n",
    "except ImportError:\n",
    "    SKLEARN = False\n",
    "    print('scikit-learn not found — using NumPy/SciPy only.')\n",
    "\n",
    "rng = np.random.default_rng(seed=0)\n",
    "\n",
    "plt.rcParams.update({\n",
    "    'figure.dpi': 100,\n",
    "    'axes.spines.top': False,\n",
    "    'axes.spines.right': False,\n",
    "    'font.size': 11,\n",
    "})\n",
    "\n",
    "print('NumPy:', np.__version__)\n",
    "print('SciPy:', __import__('scipy').__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2c3d4e5-0003-4000-8000-000000000003",
   "metadata": {},
   "source": [
    "## 1. Simple Linear Regression\n",
    "\n",
    "The simple linear model relates a single predictor $x$ to the response $y$:\n",
    "\n",
    "$$y_i = \\beta_0 + \\beta_1 x_i + \\varepsilon_i, \\qquad \\varepsilon_i \\overset{\\text{i.i.d.}}{\\sim} \\mathcal{N}(0, \\sigma^2)$$\n",
    "\n",
    "OLS closed-form estimates:\n",
    "\n",
    "$$\\hat{\\beta}_1 = \\frac{\\sum(x_i - \\bar{x})(y_i - \\bar{y})}{\\sum(x_i - \\bar{x})^2} = \\frac{S_{xy}}{S_{xx}}, \\qquad \\hat{\\beta}_0 = \\bar{y} - \\hat{\\beta}_1 \\bar{x}$$\n",
    "\n",
    "The **95% confidence band** around the regression line reflects uncertainty in the estimated mean response $E[y \\mid x]$. The wider **prediction interval** additionally accounts for individual observation noise $\\sigma^2$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2c3d4e5-0004-4000-8000-000000000004",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scenario: predict house price (€ thousands) from size (m²)\n",
    "n = 80\n",
    "x = rng.uniform(40, 200, size=n)                            # house size (m²)\n",
    "true_beta0, true_beta1 = 30.0, 2.5                         # true intercept, slope\n",
    "noise_std = 25.0\n",
    "y = true_beta0 + true_beta1 * x + rng.normal(0, noise_std, size=n)\n",
    "\n",
    "# OLS via scipy.stats.linregress\n",
    "slope, intercept, r_value, p_value, stderr = stats.linregress(x, y)\n",
    "\n",
    "print(f'Fitted intercept β₀ : {intercept:.4f}  (true: {true_beta0})')\n",
    "print(f'Fitted slope     β₁ : {slope:.4f}  (true: {true_beta1})')\n",
    "print(f'R                   : {r_value:.4f}')\n",
    "print(f'R²                  : {r_value**2:.4f}')\n",
    "print(f'p-value (slope)     : {p_value:.6f}')\n",
    "print(f'Std error (slope)   : {stderr:.4f}')\n",
    "\n",
    "# Predictions + 95% confidence band (analytical formula)\n",
    "x_grid = np.linspace(x.min(), x.max(), 300)\n",
    "y_hat  = intercept + slope * x_grid\n",
    "\n",
    "x_mean = x.mean()\n",
    "n_obs  = len(x)\n",
    "Sxx = ((x - x_mean) ** 2).sum()\n",
    "residuals = y - (intercept + slope * x)\n",
    "s_e = np.sqrt((residuals ** 2).sum() / (n_obs - 2))       # residual std error\n",
    "\n",
    "t_crit = stats.t.ppf(0.975, df=n_obs - 2)\n",
    "se_mean = s_e * np.sqrt(1 / n_obs + (x_grid - x_mean) ** 2 / Sxx)\n",
    "se_pred = s_e * np.sqrt(1 + 1 / n_obs + (x_grid - x_mean) ** 2 / Sxx)\n",
    "\n",
    "ci_lower = y_hat - t_crit * se_mean\n",
    "ci_upper = y_hat + t_crit * se_mean\n",
    "pi_lower = y_hat - t_crit * se_pred\n",
    "pi_upper = y_hat + t_crit * se_pred\n",
    "\n",
    "# Plot\n",
    "fig, ax = plt.subplots(figsize=(8, 5))\n",
    "ax.scatter(x, y, color='steelblue', alpha=0.6, s=30, label='Observations')\n",
    "ax.plot(x_grid, y_hat, 'r-', lw=2, label=f'OLS fit: y = {intercept:.1f} + {slope:.2f}x')\n",
    "ax.fill_between(x_grid, ci_lower, ci_upper, color='red', alpha=0.15,\n",
    "                label='95% confidence band')\n",
    "ax.fill_between(x_grid, pi_lower, pi_upper, color='orange', alpha=0.10,\n",
    "                label='95% prediction interval')\n",
    "ax.set_xlabel('House size (m²)')\n",
    "ax.set_ylabel('Price (€ thousands)')\n",
    "ax.set_title(f'Simple Linear Regression  |  R²={r_value**2:.3f}, p={p_value:.4f}')\n",
    "ax.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2c3d4e5-0005-4000-8000-000000000005",
   "metadata": {},
   "source": [
    "## 2. Interpreting Coefficients: R², Residuals, and Q-Q Plot\n",
    "\n",
    "**R² (coefficient of determination)** measures the proportion of variance in $y$ explained by the model:\n",
    "\n",
    "$$R^2 = 1 - \\frac{\\text{RSS}}{\\text{TSS}} = 1 - \\frac{\\sum(y_i - \\hat{y}_i)^2}{\\sum(y_i - \\bar{y})^2}$$\n",
    "\n",
    "**Residual diagnostics** are essential for validating OLS assumptions:\n",
    "\n",
    "| Plot | What to look for | Violation suggests |\n",
    "|------|------------------|-----------------------|\n",
    "| Residuals vs Fitted | Random scatter around 0 | Non-linearity or heteroscedasticity |\n",
    "| Q-Q plot | Points on diagonal | Non-normality of errors |\n",
    "| Scale-Location | Horizontal band | Heteroscedasticity |\n",
    "| Residuals vs Leverage | No high-leverage outliers | Influential observations |\n",
    "\n",
    "The **p-value of the slope** tests H₀: β₁ = 0 (no linear relationship). Reject when p < α."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2c3d4e5-0006-4000-8000-000000000006",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Re-use the house price data from Cell 4\n",
    "y_fitted   = intercept + slope * x\n",
    "residuals  = y - y_fitted\n",
    "std_resid  = residuals / s_e                               # standardised residuals\n",
    "\n",
    "RSS = (residuals ** 2).sum()\n",
    "TSS = ((y - y.mean()) ** 2).sum()\n",
    "R2  = 1 - RSS / TSS\n",
    "adj_R2 = 1 - (1 - R2) * (n_obs - 1) / (n_obs - 2)\n",
    "\n",
    "print(f'R²          : {R2:.4f}')\n",
    "print(f'Adjusted R² : {adj_R2:.4f}')\n",
    "print(f'RSS         : {RSS:.2f}')\n",
    "print(f'Residual SE : {s_e:.4f}')\n",
    "\n",
    "# Shapiro-Wilk test on residuals\n",
    "sw_stat, sw_p = stats.shapiro(residuals)\n",
    "print(f'\\nShapiro-Wilk test on residuals: W={sw_stat:.4f}, p={sw_p:.4f}')\n",
    "print('Residuals appear normal.' if sw_p > 0.05 else 'Residuals may NOT be normal.')\n",
    "\n",
    "fig = plt.figure(figsize=(11, 8))\n",
    "gs  = gridspec.GridSpec(2, 2, hspace=0.40, wspace=0.35)\n",
    "\n",
    "# 1. Residuals vs Fitted\n",
    "ax1 = fig.add_subplot(gs[0, 0])\n",
    "ax1.scatter(y_fitted, residuals, alpha=0.6, color='steelblue', s=25)\n",
    "ax1.axhline(0, color='red', lw=1.5, ls='--')\n",
    "ax1.set_xlabel('Fitted values'); ax1.set_ylabel('Residuals')\n",
    "ax1.set_title('Residuals vs Fitted')\n",
    "\n",
    "# 2. Q-Q plot\n",
    "ax2 = fig.add_subplot(gs[0, 1])\n",
    "(osm, osr), (slope_qq, intercept_qq, r_qq) = stats.probplot(residuals, dist='norm')\n",
    "ax2.scatter(osm, osr, alpha=0.6, color='steelblue', s=25)\n",
    "ax2.plot(osm, slope_qq * np.array(osm) + intercept_qq, 'r-', lw=1.5)\n",
    "ax2.set_xlabel('Theoretical quantiles'); ax2.set_ylabel('Sample quantiles')\n",
    "ax2.set_title('Normal Q-Q Plot')\n",
    "\n",
    "# 3. Scale-Location (sqrt |standardised residuals| vs fitted)\n",
    "ax3 = fig.add_subplot(gs[1, 0])\n",
    "ax3.scatter(y_fitted, np.sqrt(np.abs(std_resid)), alpha=0.6, color='steelblue', s=25)\n",
    "ax3.set_xlabel('Fitted values'); ax3.set_ylabel('√|Standardised residuals|')\n",
    "ax3.set_title('Scale-Location')\n",
    "\n",
    "# 4. Residual histogram\n",
    "ax4 = fig.add_subplot(gs[1, 1])\n",
    "ax4.hist(residuals, bins=14, color='steelblue', edgecolor='white', alpha=0.8, density=True)\n",
    "xr = np.linspace(residuals.min(), residuals.max(), 200)\n",
    "ax4.plot(xr, stats.norm.pdf(xr, residuals.mean(), residuals.std()), 'r-', lw=2)\n",
    "ax4.set_xlabel('Residual'); ax4.set_ylabel('Density')\n",
    "ax4.set_title('Residual Distribution')\n",
    "\n",
    "fig.suptitle(f'Residual Diagnostics  |  R²={R2:.3f}', fontsize=13, y=1.01)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2c3d4e5-0007-4000-8000-000000000007",
   "metadata": {},
   "source": [
    "## 3. Multiple Linear Regression\n",
    "\n",
    "With $p$ predictors the model becomes:\n",
    "\n",
    "$$\\mathbf{y} = \\mathbf{X}\\boldsymbol{\\beta} + \\boldsymbol{\\varepsilon}, \\qquad \\hat{\\boldsymbol{\\beta}} = (\\mathbf{X}^\\top\\mathbf{X})^{-1}\\mathbf{X}^\\top\\mathbf{y}$$\n",
    "\n",
    "where $\\mathbf{X}$ is the $n \\times (p+1)$ design matrix (first column all ones for the intercept).\n",
    "\n",
    "**Key considerations:**\n",
    "- **Multicollinearity**: when predictors are correlated, coefficient estimates become unstable. Check with Variance Inflation Factor (VIF).\n",
    "- **Adjusted R²**: penalises adding uninformative predictors — always report alongside R².\n",
    "- **F-statistic**: tests whether *all* coefficients (except intercept) are simultaneously zero.\n",
    "\n",
    "$$F = \\frac{(\\text{TSS} - \\text{RSS})/p}{\\text{RSS}/(n - p - 1)}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2c3d4e5-0008-4000-8000-000000000008",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scenario: predict salary (€k) from years of experience, education level (0-3), and performance score\n",
    "n = 120\n",
    "experience  = rng.uniform(0, 30, n)\n",
    "education   = rng.integers(0, 4, n).astype(float)\n",
    "performance = rng.uniform(50, 100, n)\n",
    "\n",
    "# True model: salary = 25 + 1.8*exp + 5*edu + 0.3*perf + noise\n",
    "true_betas = np.array([25.0, 1.8, 5.0, 0.3])\n",
    "noise = rng.normal(0, 8, n)\n",
    "y = true_betas[0] + true_betas[1]*experience + true_betas[2]*education + true_betas[3]*performance + noise\n",
    "\n",
    "# Build design matrix (with intercept column)\n",
    "X_raw = np.column_stack([experience, education, performance])\n",
    "ones  = np.ones((n, 1))\n",
    "X     = np.hstack([ones, X_raw])                           # shape: (n, 4)\n",
    "\n",
    "# OLS via normal equations\n",
    "beta_hat = np.linalg.lstsq(X, y, rcond=None)[0]\n",
    "\n",
    "y_hat    = X @ beta_hat\n",
    "residuals = y - y_hat\n",
    "p_pred   = X.shape[1] - 1                                  # number of predictors (excl. intercept)\n",
    "RSS      = (residuals ** 2).sum()\n",
    "TSS      = ((y - y.mean()) ** 2).sum()\n",
    "R2       = 1 - RSS / TSS\n",
    "adj_R2   = 1 - (1 - R2) * (n - 1) / (n - p_pred - 1)\n",
    "s2       = RSS / (n - p_pred - 1)\n",
    "\n",
    "# Standard errors of coefficients\n",
    "XtX_inv  = np.linalg.inv(X.T @ X)\n",
    "se_betas = np.sqrt(np.diag(XtX_inv) * s2)\n",
    "\n",
    "# t-statistics and p-values\n",
    "t_stats = beta_hat / se_betas\n",
    "df_res  = n - p_pred - 1\n",
    "p_vals  = 2 * stats.t.sf(np.abs(t_stats), df=df_res)\n",
    "\n",
    "# F-statistic for overall model significance\n",
    "F_stat = ((TSS - RSS) / p_pred) / (RSS / df_res)\n",
    "F_pval = stats.f.sf(F_stat, p_pred, df_res)\n",
    "\n",
    "print('Multiple Linear Regression Results')\n",
    "print('=' * 55)\n",
    "print(f'R²: {R2:.4f}   Adjusted R²: {adj_R2:.4f}')\n",
    "print(f'F-statistic: {F_stat:.2f}  (p={F_pval:.2e})')\n",
    "print(f'n={n}, p={p_pred}')\n",
    "print()\n",
    "labels = ['Intercept', 'Experience', 'Education', 'Performance']\n",
    "print(f'{\"Variable\":<14} {\"Estimate\":>10} {\"Std Err\":>10} {\"t-stat\":>10} {\"p-value\":>12}')\n",
    "print('-' * 58)\n",
    "for lbl, b, se, t, p in zip(labels, beta_hat, se_betas, t_stats, p_vals):\n",
    "    sig = '***' if p < 0.001 else ('**' if p < 0.01 else ('*' if p < 0.05 else ''))\n",
    "    print(f'{lbl:<14} {b:>10.4f} {se:>10.4f} {t:>10.4f} {p:>12.4f} {sig}')\n",
    "\n",
    "# Coefficient plot\n",
    "fig, ax = plt.subplots(figsize=(7, 4))\n",
    "ci_mult = stats.t.ppf(0.975, df=df_res)\n",
    "ci_half = ci_mult * se_betas[1:]                           # skip intercept\n",
    "y_pos = np.arange(p_pred)\n",
    "ax.barh(y_pos, beta_hat[1:], xerr=ci_half, color='steelblue', alpha=0.7,\n",
    "        edgecolor='white', capsize=4)\n",
    "ax.axvline(0, color='black', lw=1)\n",
    "ax.set_yticks(y_pos)\n",
    "ax.set_yticklabels(['Experience', 'Education', 'Performance'])\n",
    "ax.set_xlabel('Coefficient estimate (with 95% CI)')\n",
    "ax.set_title('Multiple Regression Coefficients')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2c3d4e5-0009-4000-8000-000000000009",
   "metadata": {},
   "source": [
    "## 4. Polynomial Regression\n",
    "\n",
    "When the relationship between $x$ and $y$ is non-linear, we can extend OLS by including polynomial features:\n",
    "\n",
    "$$y = \\beta_0 + \\beta_1 x + \\beta_2 x^2 + \\cdots + \\beta_d x^d + \\varepsilon$$\n",
    "\n",
    "This is still *linear* in the parameters $\\boldsymbol{\\beta}$ — just a linear model in a transformed feature space.\n",
    "\n",
    "**Bias-variance tradeoff:**\n",
    "- Low-degree polynomial → high bias (underfitting)\n",
    "- High-degree polynomial → high variance (overfitting)\n",
    "- The optimal degree minimises prediction error on unseen data (use cross-validation)\n",
    "\n",
    "The training R² always increases with degree, so use **adjusted R²** or **AIC/BIC** to compare models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2c3d4e5-0010-4000-8000-000000000010",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scenario: model a noisy sinusoidal relationship\n",
    "n = 60\n",
    "x_poly = rng.uniform(-3, 3, size=n)\n",
    "y_poly = np.sin(x_poly) + rng.normal(0, 0.3, size=n)\n",
    "\n",
    "x_grid_poly = np.linspace(-3.2, 3.2, 300)\n",
    "degrees = [1, 3, 7, 15]\n",
    "\n",
    "fig, axes = plt.subplots(2, 2, figsize=(11, 8), sharey=True)\n",
    "axes = axes.ravel()\n",
    "\n",
    "r2_scores = []\n",
    "for ax, deg in zip(axes, degrees):\n",
    "    # Build Vandermonde matrix for training data and grid\n",
    "    X_train = np.vander(x_poly,      N=deg + 1, increasing=True)\n",
    "    X_grid  = np.vander(x_grid_poly, N=deg + 1, increasing=True)\n",
    "\n",
    "    coeffs = np.linalg.lstsq(X_train, y_poly, rcond=None)[0]\n",
    "    y_pred_train = X_train @ coeffs\n",
    "    y_pred_grid  = X_grid  @ coeffs\n",
    "\n",
    "    rss = ((y_poly - y_pred_train) ** 2).sum()\n",
    "    tss = ((y_poly - y_poly.mean()) ** 2).sum()\n",
    "    r2  = 1 - rss / tss\n",
    "    adj_r2 = 1 - (1 - r2) * (n - 1) / (n - deg - 1)\n",
    "    r2_scores.append(r2)\n",
    "\n",
    "    ax.scatter(x_poly, y_poly, color='steelblue', alpha=0.5, s=20, label='Data')\n",
    "    ax.plot(x_grid_poly, np.sin(x_grid_poly), 'g--', lw=1.5, label='True sin(x)')\n",
    "    ax.plot(x_grid_poly, y_pred_grid, 'r-', lw=2,\n",
    "            label=f'Degree {deg}')\n",
    "    ax.set_ylim(-2.5, 2.5)\n",
    "    ax.set_title(f'Degree {deg}  |  R²={r2:.3f}, adj-R²={adj_r2:.3f}')\n",
    "    ax.set_xlabel('x')\n",
    "    ax.set_ylabel('y')\n",
    "    ax.legend(fontsize=9)\n",
    "\n",
    "plt.suptitle('Polynomial Regression: Underfitting → Overfitting', fontsize=13)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print('Training R² by degree:')\n",
    "for d, r2 in zip(degrees, r2_scores):\n",
    "    bar = '#' * int(r2 * 40)\n",
    "    print(f'  Degree {d:>2}: R²={r2:.4f}  |{bar}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2c3d4e5-0011-4000-8000-000000000011",
   "metadata": {},
   "source": [
    "## 5. Logistic Regression (Binary Classification)\n",
    "\n",
    "When the response is binary ($y \\in \\{0, 1\\}$), linear regression is inappropriate. **Logistic regression** models the log-odds:\n",
    "\n",
    "$$\\log\\frac{P(y=1 \\mid x)}{1 - P(y=1 \\mid x)} = \\beta_0 + \\beta_1 x$$\n",
    "\n",
    "Transforming back via the **sigmoid function**:\n",
    "\n",
    "$$P(y=1 \\mid x) = \\sigma(\\beta_0 + \\beta_1 x) = \\frac{1}{1 + e^{-(\\beta_0 + \\beta_1 x)}}$$\n",
    "\n",
    "Parameters are estimated by **maximum likelihood** (not OLS). The decision boundary is where $P = 0.5$, i.e., $\\beta_0 + \\beta_1 x = 0$.\n",
    "\n",
    "**Interpretation of coefficients:** $e^{\\beta_j}$ is the **odds ratio** — the factor by which the odds of $y=1$ multiply for a unit increase in $x_j$, holding other predictors constant."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2c3d4e5-0012-4000-8000-000000000012",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scenario: predict exam pass/fail from hours studied\n",
    "n_logit = 100\n",
    "hours   = rng.uniform(0, 10, n_logit)\n",
    "\n",
    "# True logistic model: log-odds = -4 + 1.2 * hours\n",
    "true_b0, true_b1 = -4.0, 1.2\n",
    "log_odds = true_b0 + true_b1 * hours\n",
    "prob_pass = 1 / (1 + np.exp(-log_odds))\n",
    "passed = rng.binomial(1, prob_pass)                        # 0 = fail, 1 = pass\n",
    "\n",
    "# Fit via scikit-learn (or manual gradient descent fallback)\n",
    "if SKLEARN:\n",
    "    X_logit = hours.reshape(-1, 1)\n",
    "    clf = LogisticRegression(solver='lbfgs', C=1e6)        # large C → minimal regularisation\n",
    "    clf.fit(X_logit, passed)\n",
    "    b0_hat = clf.intercept_[0]\n",
    "    b1_hat = clf.coef_[0, 0]\n",
    "    print(f'Fitted (sklearn):   β₀={b0_hat:.4f}, β₁={b1_hat:.4f}')\n",
    "else:\n",
    "    # Manual gradient descent\n",
    "    def sigmoid(z): return 1 / (1 + np.exp(-z))\n",
    "    b0_hat, b1_hat = 0.0, 0.0\n",
    "    lr = 0.05\n",
    "    for _ in range(3000):\n",
    "        p = sigmoid(b0_hat + b1_hat * hours)\n",
    "        err = p - passed\n",
    "        b0_hat -= lr * err.mean()\n",
    "        b1_hat -= lr * (err * hours).mean()\n",
    "    print(f'Fitted (grad desc): β₀={b0_hat:.4f}, β₁={b1_hat:.4f}')\n",
    "\n",
    "print(f'True coefficients:  β₀={true_b0}, β₁={true_b1}')\n",
    "\n",
    "# Decision boundary: b0 + b1*x = 0  →  x = -b0/b1\n",
    "boundary = -b0_hat / b1_hat\n",
    "print(f'Decision boundary   : {boundary:.2f} hours')\n",
    "print(f'Odds ratio e^β₁     : {np.exp(b1_hat):.4f}')\n",
    "\n",
    "# Visualise\n",
    "x_range = np.linspace(0, 10, 300)\n",
    "p_hat   = 1 / (1 + np.exp(-(b0_hat + b1_hat * x_range)))\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(12, 4))\n",
    "\n",
    "# Left: sigmoid curve\n",
    "ax = axes[0]\n",
    "ax.scatter(hours[passed == 0], passed[passed == 0], color='tomato',    alpha=0.5, s=30,\n",
    "           label='Fail (0)', zorder=3)\n",
    "ax.scatter(hours[passed == 1], passed[passed == 1], color='steelblue', alpha=0.5, s=30,\n",
    "           label='Pass (1)', zorder=3)\n",
    "ax.plot(x_range, p_hat, 'k-', lw=2, label='Fitted P(pass)')\n",
    "ax.axvline(boundary, color='green', ls='--', lw=1.5, label=f'Boundary = {boundary:.1f}h')\n",
    "ax.axhline(0.5, color='gray', ls=':', lw=1)\n",
    "ax.set_xlabel('Hours studied')\n",
    "ax.set_ylabel('P(pass)')\n",
    "ax.set_title('Logistic Regression: Sigmoid Curve')\n",
    "ax.legend()\n",
    "\n",
    "# Right: log-odds plot\n",
    "ax = axes[1]\n",
    "ax.plot(x_range, b0_hat + b1_hat * x_range, 'purple', lw=2, label='Log-odds')\n",
    "ax.axhline(0, color='gray', ls='--')\n",
    "ax.axvline(boundary, color='green', ls='--', lw=1.5, label=f'Boundary ({boundary:.1f}h)')\n",
    "ax.fill_between(x_range, b0_hat + b1_hat * x_range, 0,\n",
    "                where=(b0_hat + b1_hat * x_range > 0), alpha=0.15, color='steelblue', label='Predict pass')\n",
    "ax.fill_between(x_range, b0_hat + b1_hat * x_range, 0,\n",
    "                where=(b0_hat + b1_hat * x_range < 0), alpha=0.15, color='tomato', label='Predict fail')\n",
    "ax.set_xlabel('Hours studied')\n",
    "ax.set_ylabel('Log-odds')\n",
    "ax.set_title('Log-odds and Decision Boundary')\n",
    "ax.legend(fontsize=9)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2c3d4e5-0013-4000-8000-000000000013",
   "metadata": {},
   "source": [
    "## 6. Checking Regression Assumptions\n",
    "\n",
    "OLS inference is valid only when the Gauss-Markov assumptions hold. Violations affect different inferential properties:\n",
    "\n",
    "| Assumption | Consequence if violated | Diagnostic |\n",
    "|------------|------------------------|------------|\n",
    "| **Linearity** | Biased estimates | Residuals vs Fitted — look for curvature |\n",
    "| **Independence** | Underestimated SEs | Durbin-Watson test (time-series) |\n",
    "| **Homoscedasticity** | Inefficient estimates, invalid SEs | Scale-Location plot, Breusch-Pagan |\n",
    "| **Normality of errors** | Invalid t/F intervals (small n) | Q-Q plot, Shapiro-Wilk test |\n",
    "| **No perfect multicollinearity** | Singular (X'X), unreliable estimates | VIF > 10 is problematic |\n",
    "\n",
    "Below we deliberately create two datasets — one that satisfies assumptions and one that violates **homoscedasticity** (heteroscedastic errors) — to compare the diagnostic plots."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2c3d4e5-0014-4000-8000-000000000014",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_diag = 100\n",
    "x_diag = rng.uniform(1, 10, n_diag)\n",
    "\n",
    "# --- Dataset A: well-behaved (homoscedastic, normal errors)\n",
    "y_good = 3 + 2 * x_diag + rng.normal(0, 1.5, n_diag)\n",
    "\n",
    "# --- Dataset B: heteroscedastic (variance grows with x)\n",
    "y_hetero = 3 + 2 * x_diag + rng.normal(0, 0.5 * x_diag, n_diag)\n",
    "\n",
    "def fit_ols_1d(x, y):\n",
    "    slope, intercept, *_ = stats.linregress(x, y)\n",
    "    y_hat = intercept + slope * x\n",
    "    resid = y - y_hat\n",
    "    s = np.sqrt((resid**2).sum() / (len(x) - 2))\n",
    "    return y_hat, resid, s\n",
    "\n",
    "y_hat_good,   resid_good,   s_good   = fit_ols_1d(x_diag, y_good)\n",
    "y_hat_hetero, resid_hetero, s_hetero = fit_ols_1d(x_diag, y_hetero)\n",
    "\n",
    "def add_lowess_line(ax, x, y, frac=0.5, color='red'):\n",
    "    \"\"\"Simple moving average as a smoother (no statsmodels dependency).\"\"\"\n",
    "    idx    = np.argsort(x)\n",
    "    x_s, y_s = x[idx], y[idx]\n",
    "    w = max(1, int(frac * len(x)))\n",
    "    y_smooth = np.convolve(y_s, np.ones(w)/w, mode='same')\n",
    "    ax.plot(x_s, y_smooth, color=color, lw=1.5, ls='--')\n",
    "\n",
    "fig, axes = plt.subplots(2, 3, figsize=(13, 8))\n",
    "titles_row = ['Homoscedastic (well-behaved)', 'Heteroscedastic (variance grows with x)']\n",
    "\n",
    "for row, (x, y, y_hat, resid, s, label) in enumerate([\n",
    "        (x_diag, y_good,   y_hat_good,   resid_good,   s_good,   'A: Well-behaved'),\n",
    "        (x_diag, y_hetero, y_hat_hetero, resid_hetero, s_hetero, 'B: Heteroscedastic')]):\n",
    "\n",
    "    std_resid = resid / s\n",
    "\n",
    "    # (1) Residuals vs Fitted\n",
    "    ax = axes[row, 0]\n",
    "    ax.scatter(y_hat, resid, alpha=0.5, color='steelblue', s=20)\n",
    "    ax.axhline(0, color='red', lw=1.5, ls='--')\n",
    "    add_lowess_line(ax, y_hat, resid)\n",
    "    ax.set_xlabel('Fitted'); ax.set_ylabel('Residuals')\n",
    "    ax.set_title(f'{label}\\nResiduals vs Fitted')\n",
    "\n",
    "    # (2) Q-Q plot\n",
    "    ax = axes[row, 1]\n",
    "    (osm, osr), (sq, iq, rq) = stats.probplot(resid, dist='norm')\n",
    "    ax.scatter(osm, osr, alpha=0.5, color='steelblue', s=20)\n",
    "    ax.plot(osm, sq * np.array(osm) + iq, 'r-', lw=1.5)\n",
    "    ax.set_xlabel('Theoretical quantiles'); ax.set_ylabel('Sample quantiles')\n",
    "    ax.set_title('Normal Q-Q')\n",
    "\n",
    "    # (3) Scale-Location\n",
    "    ax = axes[row, 2]\n",
    "    ax.scatter(y_hat, np.sqrt(np.abs(std_resid)), alpha=0.5, color='steelblue', s=20)\n",
    "    add_lowess_line(ax, y_hat, np.sqrt(np.abs(std_resid)))\n",
    "    ax.set_xlabel('Fitted'); ax.set_ylabel('√|Std residuals|')\n",
    "    ax.set_title('Scale-Location')\n",
    "\n",
    "    # Breusch-Pagan-like test: correlation between |residuals| and fitted values\n",
    "    corr, bp_p = stats.pearsonr(y_hat, np.abs(resid))\n",
    "    print(f'{label}: |residual| ~ fitted  r={corr:.3f}, p={bp_p:.4f}')\n",
    "\n",
    "plt.suptitle('Regression Diagnostic Plots: Well-behaved vs Heteroscedastic', fontsize=13)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2c3d4e5-0015-4000-8000-000000000015",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "### Regression Model Comparison\n",
    "\n",
    "| Model | Response | Key idea | Fitting method | Interpretation |\n",
    "|-------|----------|----------|----------------|----------------|\n",
    "| Simple linear | Continuous | One predictor | OLS | β₁: change in ŷ per unit x |\n",
    "| Multiple linear | Continuous | Multiple predictors | OLS | β_j: partial effect of x_j |\n",
    "| Polynomial | Continuous | Non-linear with OLS | OLS in transformed space | Compare adj-R² across degrees |\n",
    "| Logistic | Binary (0/1) | Log-odds linear in x | MLE (iterative) | e^β_j: odds ratio |\n",
    "\n",
    "### Workflow for Any Regression Problem\n",
    "\n",
    "1. **Explore** — scatter plots, correlation matrix, check for outliers\n",
    "2. **Fit** — choose model family (linear / polynomial / logistic / etc.)\n",
    "3. **Evaluate** — R², adjusted R², AIC/BIC, cross-validation MSE\n",
    "4. **Diagnose** — residual plots, Q-Q plot, scale-location, leverage\n",
    "5. **Iterate** — transform variables, add/remove predictors, consider regularisation (Ridge/Lasso)\n",
    "6. **Report** — coefficient estimates, confidence intervals, effect sizes\n",
    "\n",
    "### Extensions to Explore\n",
    "- **Regularised regression**: Ridge (L2), Lasso (L1), ElasticNet — handle multicollinearity and feature selection\n",
    "- **Generalised Linear Models (GLM)**: Poisson (count data), Negative Binomial, Gamma — when errors are non-normal\n",
    "- **Mixed-effects models**: repeated measures and nested data structures\n",
    "- **Robust regression**: Huber loss — reduces sensitivity to outliers\n",
    "- **Non-parametric regression**: Kernel regression, GAM — flexible functional forms"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
