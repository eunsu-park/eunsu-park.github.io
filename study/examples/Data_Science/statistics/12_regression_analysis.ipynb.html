{% raw %}
<!DOCTYPE html>
<html lang="en" data-theme="light">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>12_regression_analysis.ipynb - Examples</title>
    <link rel="stylesheet" href="/study/static/css/style.css">
    <link rel="stylesheet" href="/study/static/css/highlight.css">
    <!-- KaTeX for LaTeX math rendering -->
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.11/dist/katex.min.css">
    <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.11/dist/katex.min.js"></script>
    <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.11/dist/contrib/auto-render.min.js"
        onload="renderMathInElement(document.body, {
            delimiters: [
                {left: '$$', right: '$$', display: true},
                {left: '$', right: '$', display: false}
            ],
            throwOnError: false
        });"></script>
    
</head>
<body>
    <div class="app-container">
        <!-- Sidebar -->
        <aside class="sidebar" id="sidebar">
            <div class="sidebar-header">
                <a href="/study/en/" class="logo">Study Materials</a>
                <button class="sidebar-toggle" id="sidebar-toggle" aria-label="Toggle sidebar">
                    <span></span>
                </button>
            </div>

            <nav class="sidebar-nav">
                <a href="/study/en/" class="nav-item ">
                    <span class="nav-icon">üè†</span>
                    <span class="nav-text">Home</span>
                </a>
                <a href="/study/examples/" class="nav-item active">
                    <span class="nav-icon">üíª</span>
                    <span class="nav-text">Examples</span>
                </a>
            </nav>

            <div class="sidebar-search">
                <form action="/study/en/search.html" method="get" id="search-form">
                    <input type="search" name="q" placeholder="Search..." id="search-sidebar-input">
                </form>
            </div>

            <!-- Language Selector -->
            <div class="sidebar-lang">
                <select id="lang-select" class="lang-selector" onchange="switchLanguage(this.value)">
                    
                    <option value="en" selected>
                        English
                    </option>
                    
                    <option value="ko" >
                        ÌïúÍµ≠Ïñ¥
                    </option>
                    
                </select>
            </div>

            <div class="sidebar-footer">
                <button class="theme-toggle" id="theme-toggle" aria-label="Toggle theme">
                    <span class="theme-icon light">‚òÄÔ∏è</span>
                    <span class="theme-icon dark">üåô</span>
                </button>
            </div>
        </aside>

        <!-- Main Content -->
        <main class="main-content">
            <header class="main-header">
                <button class="menu-toggle" id="menu-toggle" aria-label="Open menu">
                    <span></span>
                </button>
                
<nav class="breadcrumb">
    <a href="/study/examples/">Examples</a>
    <span class="separator">/</span>
    <a href="/study/examples/Data_Science/">Data Science</a>
    <span class="separator">/</span>
    <span class="current">12_regression_analysis.ipynb</span>
</nav>

            </header>

            <div class="content">
                
<article class="example-article">
    <header class="example-header">
        <h1>12_regression_analysis.ipynb</h1>
        <div class="example-actions">
            <a href="12_regression_analysis.ipynb" download class="btn">Download</a>
            <button class="btn" id="copy-code-btn">Copy code</button>
        </div>
    </header>

    <div class="example-meta">
        <span>json</span>
        <span>645 lines</span>
        <span>28.2 KB</span>
    </div>

    <div class="example-code markdown-body">
        <div class="highlight"><pre><span></span><code><span class="linenos">  1</span><span class="p">{</span>
<span class="linenos">  2</span><span class="w"> </span><span class="nt">&quot;cells&quot;</span><span class="p">:</span><span class="w"> </span><span class="p">[</span>
<span class="linenos">  3</span><span class="w">  </span><span class="p">{</span>
<span class="linenos">  4</span><span class="w">   </span><span class="nt">&quot;cell_type&quot;</span><span class="p">:</span><span class="w"> </span><span class="s2">&quot;markdown&quot;</span><span class="p">,</span>
<span class="linenos">  5</span><span class="w">   </span><span class="nt">&quot;id&quot;</span><span class="p">:</span><span class="w"> </span><span class="s2">&quot;b2c3d4e5-0001-4000-8000-000000000001&quot;</span><span class="p">,</span>
<span class="linenos">  6</span><span class="w">   </span><span class="nt">&quot;metadata&quot;</span><span class="p">:</span><span class="w"> </span><span class="p">{},</span>
<span class="linenos">  7</span><span class="w">   </span><span class="nt">&quot;source&quot;</span><span class="p">:</span><span class="w"> </span><span class="p">[</span>
<span class="linenos">  8</span><span class="w">    </span><span class="s2">&quot;# Regression Analysis\n&quot;</span><span class="p">,</span>
<span class="linenos">  9</span><span class="w">    </span><span class="s2">&quot;\n&quot;</span><span class="p">,</span>
<span class="linenos"> 10</span><span class="w">    </span><span class="s2">&quot;Regression analysis models the relationship between a **response variable** (dependent) and one or more **predictor variables** (independent). It is the workhorse of statistical modelling and machine learning.\n&quot;</span><span class="p">,</span>
<span class="linenos"> 11</span><span class="w">    </span><span class="s2">&quot;\n&quot;</span><span class="p">,</span>
<span class="linenos"> 12</span><span class="w">    </span><span class="s2">&quot;**Topics covered in this notebook:**\n&quot;</span><span class="p">,</span>
<span class="linenos"> 13</span><span class="w">    </span><span class="s2">&quot;1. Simple linear regression ‚Äî fitting a line, OLS estimation\n&quot;</span><span class="p">,</span>
<span class="linenos"> 14</span><span class="w">    </span><span class="s2">&quot;2. Interpreting coefficients ‚Äî R¬≤, p-values, residual diagnostics\n&quot;</span><span class="p">,</span>
<span class="linenos"> 15</span><span class="w">    </span><span class="s2">&quot;3. Multiple linear regression ‚Äî multiple predictors\n&quot;</span><span class="p">,</span>
<span class="linenos"> 16</span><span class="w">    </span><span class="s2">&quot;4. Polynomial regression ‚Äî non-linear patterns, bias-variance tradeoff\n&quot;</span><span class="p">,</span>
<span class="linenos"> 17</span><span class="w">    </span><span class="s2">&quot;5. Logistic regression ‚Äî binary outcomes, sigmoid function\n&quot;</span><span class="p">,</span>
<span class="linenos"> 18</span><span class="w">    </span><span class="s2">&quot;6. Regression assumptions ‚Äî linearity, normality, homoscedasticity\n&quot;</span><span class="p">,</span>
<span class="linenos"> 19</span><span class="w">    </span><span class="s2">&quot;\n&quot;</span><span class="p">,</span>
<span class="linenos"> 20</span><span class="w">    </span><span class="s2">&quot;**Mathematical foundation ‚Äî Ordinary Least Squares (OLS):**\n&quot;</span><span class="p">,</span>
<span class="linenos"> 21</span><span class="w">    </span><span class="s2">&quot;\n&quot;</span><span class="p">,</span>
<span class="linenos"> 22</span><span class="w">    </span><span class="s2">&quot;$$\\hat{\\boldsymbol{\\beta}} = (\\mathbf{X}^\\top \\mathbf{X})^{-1} \\mathbf{X}^\\top \\mathbf{y}$$\n&quot;</span><span class="p">,</span>
<span class="linenos"> 23</span><span class="w">    </span><span class="s2">&quot;\n&quot;</span><span class="p">,</span>
<span class="linenos"> 24</span><span class="w">    </span><span class="s2">&quot;OLS minimises the **residual sum of squares (RSS)** $\\sum_{i=1}^{n}(y_i - \\hat{y}_i)^2$ and is the **BLUE** (Best Linear Unbiased Estimator) under the Gauss-Markov assumptions.&quot;</span>
<span class="linenos"> 25</span><span class="w">   </span><span class="p">]</span>
<span class="linenos"> 26</span><span class="w">  </span><span class="p">},</span>
<span class="linenos"> 27</span><span class="w">  </span><span class="p">{</span>
<span class="linenos"> 28</span><span class="w">   </span><span class="nt">&quot;cell_type&quot;</span><span class="p">:</span><span class="w"> </span><span class="s2">&quot;code&quot;</span><span class="p">,</span>
<span class="linenos"> 29</span><span class="w">   </span><span class="nt">&quot;execution_count&quot;</span><span class="p">:</span><span class="w"> </span><span class="kc">null</span><span class="p">,</span>
<span class="linenos"> 30</span><span class="w">   </span><span class="nt">&quot;id&quot;</span><span class="p">:</span><span class="w"> </span><span class="s2">&quot;b2c3d4e5-0002-4000-8000-000000000002&quot;</span><span class="p">,</span>
<span class="linenos"> 31</span><span class="w">   </span><span class="nt">&quot;metadata&quot;</span><span class="p">:</span><span class="w"> </span><span class="p">{},</span>
<span class="linenos"> 32</span><span class="w">   </span><span class="nt">&quot;outputs&quot;</span><span class="p">:</span><span class="w"> </span><span class="p">[],</span>
<span class="linenos"> 33</span><span class="w">   </span><span class="nt">&quot;source&quot;</span><span class="p">:</span><span class="w"> </span><span class="p">[</span>
<span class="linenos"> 34</span><span class="w">    </span><span class="s2">&quot;import numpy as np\n&quot;</span><span class="p">,</span>
<span class="linenos"> 35</span><span class="w">    </span><span class="s2">&quot;import scipy.stats as stats\n&quot;</span><span class="p">,</span>
<span class="linenos"> 36</span><span class="w">    </span><span class="s2">&quot;import matplotlib.pyplot as plt\n&quot;</span><span class="p">,</span>
<span class="linenos"> 37</span><span class="w">    </span><span class="s2">&quot;import matplotlib.gridspec as gridspec\n&quot;</span><span class="p">,</span>
<span class="linenos"> 38</span><span class="w">    </span><span class="s2">&quot;\n&quot;</span><span class="p">,</span>
<span class="linenos"> 39</span><span class="w">    </span><span class="s2">&quot;# Optional: scikit-learn (used where noted; gracefully falls back to manual)\n&quot;</span><span class="p">,</span>
<span class="linenos"> 40</span><span class="w">    </span><span class="s2">&quot;try:\n&quot;</span><span class="p">,</span>
<span class="linenos"> 41</span><span class="w">    </span><span class="s2">&quot;    from sklearn.linear_model import LinearRegression, LogisticRegression\n&quot;</span><span class="p">,</span>
<span class="linenos"> 42</span><span class="w">    </span><span class="s2">&quot;    from sklearn.preprocessing import PolynomialFeatures, StandardScaler\n&quot;</span><span class="p">,</span>
<span class="linenos"> 43</span><span class="w">    </span><span class="s2">&quot;    from sklearn.pipeline import make_pipeline\n&quot;</span><span class="p">,</span>
<span class="linenos"> 44</span><span class="w">    </span><span class="s2">&quot;    from sklearn.metrics import r2_score\n&quot;</span><span class="p">,</span>
<span class="linenos"> 45</span><span class="w">    </span><span class="s2">&quot;    SKLEARN = True\n&quot;</span><span class="p">,</span>
<span class="linenos"> 46</span><span class="w">    </span><span class="s2">&quot;    print(&#39;scikit-learn is available ‚Äî using sklearn where convenient.&#39;)\n&quot;</span><span class="p">,</span>
<span class="linenos"> 47</span><span class="w">    </span><span class="s2">&quot;except ImportError:\n&quot;</span><span class="p">,</span>
<span class="linenos"> 48</span><span class="w">    </span><span class="s2">&quot;    SKLEARN = False\n&quot;</span><span class="p">,</span>
<span class="linenos"> 49</span><span class="w">    </span><span class="s2">&quot;    print(&#39;scikit-learn not found ‚Äî using NumPy/SciPy only.&#39;)\n&quot;</span><span class="p">,</span>
<span class="linenos"> 50</span><span class="w">    </span><span class="s2">&quot;\n&quot;</span><span class="p">,</span>
<span class="linenos"> 51</span><span class="w">    </span><span class="s2">&quot;rng = np.random.default_rng(seed=0)\n&quot;</span><span class="p">,</span>
<span class="linenos"> 52</span><span class="w">    </span><span class="s2">&quot;\n&quot;</span><span class="p">,</span>
<span class="linenos"> 53</span><span class="w">    </span><span class="s2">&quot;plt.rcParams.update({\n&quot;</span><span class="p">,</span>
<span class="linenos"> 54</span><span class="w">    </span><span class="s2">&quot;    &#39;figure.dpi&#39;: 100,\n&quot;</span><span class="p">,</span>
<span class="linenos"> 55</span><span class="w">    </span><span class="s2">&quot;    &#39;axes.spines.top&#39;: False,\n&quot;</span><span class="p">,</span>
<span class="linenos"> 56</span><span class="w">    </span><span class="s2">&quot;    &#39;axes.spines.right&#39;: False,\n&quot;</span><span class="p">,</span>
<span class="linenos"> 57</span><span class="w">    </span><span class="s2">&quot;    &#39;font.size&#39;: 11,\n&quot;</span><span class="p">,</span>
<span class="linenos"> 58</span><span class="w">    </span><span class="s2">&quot;})\n&quot;</span><span class="p">,</span>
<span class="linenos"> 59</span><span class="w">    </span><span class="s2">&quot;\n&quot;</span><span class="p">,</span>
<span class="linenos"> 60</span><span class="w">    </span><span class="s2">&quot;print(&#39;NumPy:&#39;, np.__version__)\n&quot;</span><span class="p">,</span>
<span class="linenos"> 61</span><span class="w">    </span><span class="s2">&quot;print(&#39;SciPy:&#39;, __import__(&#39;scipy&#39;).__version__)&quot;</span>
<span class="linenos"> 62</span><span class="w">   </span><span class="p">]</span>
<span class="linenos"> 63</span><span class="w">  </span><span class="p">},</span>
<span class="linenos"> 64</span><span class="w">  </span><span class="p">{</span>
<span class="linenos"> 65</span><span class="w">   </span><span class="nt">&quot;cell_type&quot;</span><span class="p">:</span><span class="w"> </span><span class="s2">&quot;markdown&quot;</span><span class="p">,</span>
<span class="linenos"> 66</span><span class="w">   </span><span class="nt">&quot;id&quot;</span><span class="p">:</span><span class="w"> </span><span class="s2">&quot;b2c3d4e5-0003-4000-8000-000000000003&quot;</span><span class="p">,</span>
<span class="linenos"> 67</span><span class="w">   </span><span class="nt">&quot;metadata&quot;</span><span class="p">:</span><span class="w"> </span><span class="p">{},</span>
<span class="linenos"> 68</span><span class="w">   </span><span class="nt">&quot;source&quot;</span><span class="p">:</span><span class="w"> </span><span class="p">[</span>
<span class="linenos"> 69</span><span class="w">    </span><span class="s2">&quot;## 1. Simple Linear Regression\n&quot;</span><span class="p">,</span>
<span class="linenos"> 70</span><span class="w">    </span><span class="s2">&quot;\n&quot;</span><span class="p">,</span>
<span class="linenos"> 71</span><span class="w">    </span><span class="s2">&quot;The simple linear model relates a single predictor $x$ to the response $y$:\n&quot;</span><span class="p">,</span>
<span class="linenos"> 72</span><span class="w">    </span><span class="s2">&quot;\n&quot;</span><span class="p">,</span>
<span class="linenos"> 73</span><span class="w">    </span><span class="s2">&quot;$$y_i = \\beta_0 + \\beta_1 x_i + \\varepsilon_i, \\qquad \\varepsilon_i \\overset{\\text{i.i.d.}}{\\sim} \\mathcal{N}(0, \\sigma^2)$$\n&quot;</span><span class="p">,</span>
<span class="linenos"> 74</span><span class="w">    </span><span class="s2">&quot;\n&quot;</span><span class="p">,</span>
<span class="linenos"> 75</span><span class="w">    </span><span class="s2">&quot;OLS closed-form estimates:\n&quot;</span><span class="p">,</span>
<span class="linenos"> 76</span><span class="w">    </span><span class="s2">&quot;\n&quot;</span><span class="p">,</span>
<span class="linenos"> 77</span><span class="w">    </span><span class="s2">&quot;$$\\hat{\\beta}_1 = \\frac{\\sum(x_i - \\bar{x})(y_i - \\bar{y})}{\\sum(x_i - \\bar{x})^2} = \\frac{S_{xy}}{S_{xx}}, \\qquad \\hat{\\beta}_0 = \\bar{y} - \\hat{\\beta}_1 \\bar{x}$$\n&quot;</span><span class="p">,</span>
<span class="linenos"> 78</span><span class="w">    </span><span class="s2">&quot;\n&quot;</span><span class="p">,</span>
<span class="linenos"> 79</span><span class="w">    </span><span class="s2">&quot;The **95% confidence band** around the regression line reflects uncertainty in the estimated mean response $E[y \\mid x]$. The wider **prediction interval** additionally accounts for individual observation noise $\\sigma^2$.&quot;</span>
<span class="linenos"> 80</span><span class="w">   </span><span class="p">]</span>
<span class="linenos"> 81</span><span class="w">  </span><span class="p">},</span>
<span class="linenos"> 82</span><span class="w">  </span><span class="p">{</span>
<span class="linenos"> 83</span><span class="w">   </span><span class="nt">&quot;cell_type&quot;</span><span class="p">:</span><span class="w"> </span><span class="s2">&quot;code&quot;</span><span class="p">,</span>
<span class="linenos"> 84</span><span class="w">   </span><span class="nt">&quot;execution_count&quot;</span><span class="p">:</span><span class="w"> </span><span class="kc">null</span><span class="p">,</span>
<span class="linenos"> 85</span><span class="w">   </span><span class="nt">&quot;id&quot;</span><span class="p">:</span><span class="w"> </span><span class="s2">&quot;b2c3d4e5-0004-4000-8000-000000000004&quot;</span><span class="p">,</span>
<span class="linenos"> 86</span><span class="w">   </span><span class="nt">&quot;metadata&quot;</span><span class="p">:</span><span class="w"> </span><span class="p">{},</span>
<span class="linenos"> 87</span><span class="w">   </span><span class="nt">&quot;outputs&quot;</span><span class="p">:</span><span class="w"> </span><span class="p">[],</span>
<span class="linenos"> 88</span><span class="w">   </span><span class="nt">&quot;source&quot;</span><span class="p">:</span><span class="w"> </span><span class="p">[</span>
<span class="linenos"> 89</span><span class="w">    </span><span class="s2">&quot;# Scenario: predict house price (‚Ç¨ thousands) from size (m¬≤)\n&quot;</span><span class="p">,</span>
<span class="linenos"> 90</span><span class="w">    </span><span class="s2">&quot;n = 80\n&quot;</span><span class="p">,</span>
<span class="linenos"> 91</span><span class="w">    </span><span class="s2">&quot;x = rng.uniform(40, 200, size=n)                            # house size (m¬≤)\n&quot;</span><span class="p">,</span>
<span class="linenos"> 92</span><span class="w">    </span><span class="s2">&quot;true_beta0, true_beta1 = 30.0, 2.5                         # true intercept, slope\n&quot;</span><span class="p">,</span>
<span class="linenos"> 93</span><span class="w">    </span><span class="s2">&quot;noise_std = 25.0\n&quot;</span><span class="p">,</span>
<span class="linenos"> 94</span><span class="w">    </span><span class="s2">&quot;y = true_beta0 + true_beta1 * x + rng.normal(0, noise_std, size=n)\n&quot;</span><span class="p">,</span>
<span class="linenos"> 95</span><span class="w">    </span><span class="s2">&quot;\n&quot;</span><span class="p">,</span>
<span class="linenos"> 96</span><span class="w">    </span><span class="s2">&quot;# OLS via scipy.stats.linregress\n&quot;</span><span class="p">,</span>
<span class="linenos"> 97</span><span class="w">    </span><span class="s2">&quot;slope, intercept, r_value, p_value, stderr = stats.linregress(x, y)\n&quot;</span><span class="p">,</span>
<span class="linenos"> 98</span><span class="w">    </span><span class="s2">&quot;\n&quot;</span><span class="p">,</span>
<span class="linenos"> 99</span><span class="w">    </span><span class="s2">&quot;print(f&#39;Fitted intercept Œ≤‚ÇÄ : {intercept:.4f}  (true: {true_beta0})&#39;)\n&quot;</span><span class="p">,</span>
<span class="linenos">100</span><span class="w">    </span><span class="s2">&quot;print(f&#39;Fitted slope     Œ≤‚ÇÅ : {slope:.4f}  (true: {true_beta1})&#39;)\n&quot;</span><span class="p">,</span>
<span class="linenos">101</span><span class="w">    </span><span class="s2">&quot;print(f&#39;R                   : {r_value:.4f}&#39;)\n&quot;</span><span class="p">,</span>
<span class="linenos">102</span><span class="w">    </span><span class="s2">&quot;print(f&#39;R¬≤                  : {r_value**2:.4f}&#39;)\n&quot;</span><span class="p">,</span>
<span class="linenos">103</span><span class="w">    </span><span class="s2">&quot;print(f&#39;p-value (slope)     : {p_value:.6f}&#39;)\n&quot;</span><span class="p">,</span>
<span class="linenos">104</span><span class="w">    </span><span class="s2">&quot;print(f&#39;Std error (slope)   : {stderr:.4f}&#39;)\n&quot;</span><span class="p">,</span>
<span class="linenos">105</span><span class="w">    </span><span class="s2">&quot;\n&quot;</span><span class="p">,</span>
<span class="linenos">106</span><span class="w">    </span><span class="s2">&quot;# Predictions + 95% confidence band (analytical formula)\n&quot;</span><span class="p">,</span>
<span class="linenos">107</span><span class="w">    </span><span class="s2">&quot;x_grid = np.linspace(x.min(), x.max(), 300)\n&quot;</span><span class="p">,</span>
<span class="linenos">108</span><span class="w">    </span><span class="s2">&quot;y_hat  = intercept + slope * x_grid\n&quot;</span><span class="p">,</span>
<span class="linenos">109</span><span class="w">    </span><span class="s2">&quot;\n&quot;</span><span class="p">,</span>
<span class="linenos">110</span><span class="w">    </span><span class="s2">&quot;x_mean = x.mean()\n&quot;</span><span class="p">,</span>
<span class="linenos">111</span><span class="w">    </span><span class="s2">&quot;n_obs  = len(x)\n&quot;</span><span class="p">,</span>
<span class="linenos">112</span><span class="w">    </span><span class="s2">&quot;Sxx = ((x - x_mean) ** 2).sum()\n&quot;</span><span class="p">,</span>
<span class="linenos">113</span><span class="w">    </span><span class="s2">&quot;residuals = y - (intercept + slope * x)\n&quot;</span><span class="p">,</span>
<span class="linenos">114</span><span class="w">    </span><span class="s2">&quot;s_e = np.sqrt((residuals ** 2).sum() / (n_obs - 2))       # residual std error\n&quot;</span><span class="p">,</span>
<span class="linenos">115</span><span class="w">    </span><span class="s2">&quot;\n&quot;</span><span class="p">,</span>
<span class="linenos">116</span><span class="w">    </span><span class="s2">&quot;t_crit = stats.t.ppf(0.975, df=n_obs - 2)\n&quot;</span><span class="p">,</span>
<span class="linenos">117</span><span class="w">    </span><span class="s2">&quot;se_mean = s_e * np.sqrt(1 / n_obs + (x_grid - x_mean) ** 2 / Sxx)\n&quot;</span><span class="p">,</span>
<span class="linenos">118</span><span class="w">    </span><span class="s2">&quot;se_pred = s_e * np.sqrt(1 + 1 / n_obs + (x_grid - x_mean) ** 2 / Sxx)\n&quot;</span><span class="p">,</span>
<span class="linenos">119</span><span class="w">    </span><span class="s2">&quot;\n&quot;</span><span class="p">,</span>
<span class="linenos">120</span><span class="w">    </span><span class="s2">&quot;ci_lower = y_hat - t_crit * se_mean\n&quot;</span><span class="p">,</span>
<span class="linenos">121</span><span class="w">    </span><span class="s2">&quot;ci_upper = y_hat + t_crit * se_mean\n&quot;</span><span class="p">,</span>
<span class="linenos">122</span><span class="w">    </span><span class="s2">&quot;pi_lower = y_hat - t_crit * se_pred\n&quot;</span><span class="p">,</span>
<span class="linenos">123</span><span class="w">    </span><span class="s2">&quot;pi_upper = y_hat + t_crit * se_pred\n&quot;</span><span class="p">,</span>
<span class="linenos">124</span><span class="w">    </span><span class="s2">&quot;\n&quot;</span><span class="p">,</span>
<span class="linenos">125</span><span class="w">    </span><span class="s2">&quot;# Plot\n&quot;</span><span class="p">,</span>
<span class="linenos">126</span><span class="w">    </span><span class="s2">&quot;fig, ax = plt.subplots(figsize=(8, 5))\n&quot;</span><span class="p">,</span>
<span class="linenos">127</span><span class="w">    </span><span class="s2">&quot;ax.scatter(x, y, color=&#39;steelblue&#39;, alpha=0.6, s=30, label=&#39;Observations&#39;)\n&quot;</span><span class="p">,</span>
<span class="linenos">128</span><span class="w">    </span><span class="s2">&quot;ax.plot(x_grid, y_hat, &#39;r-&#39;, lw=2, label=f&#39;OLS fit: y = {intercept:.1f} + {slope:.2f}x&#39;)\n&quot;</span><span class="p">,</span>
<span class="linenos">129</span><span class="w">    </span><span class="s2">&quot;ax.fill_between(x_grid, ci_lower, ci_upper, color=&#39;red&#39;, alpha=0.15,\n&quot;</span><span class="p">,</span>
<span class="linenos">130</span><span class="w">    </span><span class="s2">&quot;                label=&#39;95% confidence band&#39;)\n&quot;</span><span class="p">,</span>
<span class="linenos">131</span><span class="w">    </span><span class="s2">&quot;ax.fill_between(x_grid, pi_lower, pi_upper, color=&#39;orange&#39;, alpha=0.10,\n&quot;</span><span class="p">,</span>
<span class="linenos">132</span><span class="w">    </span><span class="s2">&quot;                label=&#39;95% prediction interval&#39;)\n&quot;</span><span class="p">,</span>
<span class="linenos">133</span><span class="w">    </span><span class="s2">&quot;ax.set_xlabel(&#39;House size (m¬≤)&#39;)\n&quot;</span><span class="p">,</span>
<span class="linenos">134</span><span class="w">    </span><span class="s2">&quot;ax.set_ylabel(&#39;Price (‚Ç¨ thousands)&#39;)\n&quot;</span><span class="p">,</span>
<span class="linenos">135</span><span class="w">    </span><span class="s2">&quot;ax.set_title(f&#39;Simple Linear Regression  |  R¬≤={r_value**2:.3f}, p={p_value:.4f}&#39;)\n&quot;</span><span class="p">,</span>
<span class="linenos">136</span><span class="w">    </span><span class="s2">&quot;ax.legend()\n&quot;</span><span class="p">,</span>
<span class="linenos">137</span><span class="w">    </span><span class="s2">&quot;plt.tight_layout()\n&quot;</span><span class="p">,</span>
<span class="linenos">138</span><span class="w">    </span><span class="s2">&quot;plt.show()&quot;</span>
<span class="linenos">139</span><span class="w">   </span><span class="p">]</span>
<span class="linenos">140</span><span class="w">  </span><span class="p">},</span>
<span class="linenos">141</span><span class="w">  </span><span class="p">{</span>
<span class="linenos">142</span><span class="w">   </span><span class="nt">&quot;cell_type&quot;</span><span class="p">:</span><span class="w"> </span><span class="s2">&quot;markdown&quot;</span><span class="p">,</span>
<span class="linenos">143</span><span class="w">   </span><span class="nt">&quot;id&quot;</span><span class="p">:</span><span class="w"> </span><span class="s2">&quot;b2c3d4e5-0005-4000-8000-000000000005&quot;</span><span class="p">,</span>
<span class="linenos">144</span><span class="w">   </span><span class="nt">&quot;metadata&quot;</span><span class="p">:</span><span class="w"> </span><span class="p">{},</span>
<span class="linenos">145</span><span class="w">   </span><span class="nt">&quot;source&quot;</span><span class="p">:</span><span class="w"> </span><span class="p">[</span>
<span class="linenos">146</span><span class="w">    </span><span class="s2">&quot;## 2. Interpreting Coefficients: R¬≤, Residuals, and Q-Q Plot\n&quot;</span><span class="p">,</span>
<span class="linenos">147</span><span class="w">    </span><span class="s2">&quot;\n&quot;</span><span class="p">,</span>
<span class="linenos">148</span><span class="w">    </span><span class="s2">&quot;**R¬≤ (coefficient of determination)** measures the proportion of variance in $y$ explained by the model:\n&quot;</span><span class="p">,</span>
<span class="linenos">149</span><span class="w">    </span><span class="s2">&quot;\n&quot;</span><span class="p">,</span>
<span class="linenos">150</span><span class="w">    </span><span class="s2">&quot;$$R^2 = 1 - \\frac{\\text{RSS}}{\\text{TSS}} = 1 - \\frac{\\sum(y_i - \\hat{y}_i)^2}{\\sum(y_i - \\bar{y})^2}$$\n&quot;</span><span class="p">,</span>
<span class="linenos">151</span><span class="w">    </span><span class="s2">&quot;\n&quot;</span><span class="p">,</span>
<span class="linenos">152</span><span class="w">    </span><span class="s2">&quot;**Residual diagnostics** are essential for validating OLS assumptions:\n&quot;</span><span class="p">,</span>
<span class="linenos">153</span><span class="w">    </span><span class="s2">&quot;\n&quot;</span><span class="p">,</span>
<span class="linenos">154</span><span class="w">    </span><span class="s2">&quot;| Plot | What to look for | Violation suggests |\n&quot;</span><span class="p">,</span>
<span class="linenos">155</span><span class="w">    </span><span class="s2">&quot;|------|------------------|-----------------------|\n&quot;</span><span class="p">,</span>
<span class="linenos">156</span><span class="w">    </span><span class="s2">&quot;| Residuals vs Fitted | Random scatter around 0 | Non-linearity or heteroscedasticity |\n&quot;</span><span class="p">,</span>
<span class="linenos">157</span><span class="w">    </span><span class="s2">&quot;| Q-Q plot | Points on diagonal | Non-normality of errors |\n&quot;</span><span class="p">,</span>
<span class="linenos">158</span><span class="w">    </span><span class="s2">&quot;| Scale-Location | Horizontal band | Heteroscedasticity |\n&quot;</span><span class="p">,</span>
<span class="linenos">159</span><span class="w">    </span><span class="s2">&quot;| Residuals vs Leverage | No high-leverage outliers | Influential observations |\n&quot;</span><span class="p">,</span>
<span class="linenos">160</span><span class="w">    </span><span class="s2">&quot;\n&quot;</span><span class="p">,</span>
<span class="linenos">161</span><span class="w">    </span><span class="s2">&quot;The **p-value of the slope** tests H‚ÇÄ: Œ≤‚ÇÅ = 0 (no linear relationship). Reject when p &lt; Œ±.&quot;</span>
<span class="linenos">162</span><span class="w">   </span><span class="p">]</span>
<span class="linenos">163</span><span class="w">  </span><span class="p">},</span>
<span class="linenos">164</span><span class="w">  </span><span class="p">{</span>
<span class="linenos">165</span><span class="w">   </span><span class="nt">&quot;cell_type&quot;</span><span class="p">:</span><span class="w"> </span><span class="s2">&quot;code&quot;</span><span class="p">,</span>
<span class="linenos">166</span><span class="w">   </span><span class="nt">&quot;execution_count&quot;</span><span class="p">:</span><span class="w"> </span><span class="kc">null</span><span class="p">,</span>
<span class="linenos">167</span><span class="w">   </span><span class="nt">&quot;id&quot;</span><span class="p">:</span><span class="w"> </span><span class="s2">&quot;b2c3d4e5-0006-4000-8000-000000000006&quot;</span><span class="p">,</span>
<span class="linenos">168</span><span class="w">   </span><span class="nt">&quot;metadata&quot;</span><span class="p">:</span><span class="w"> </span><span class="p">{},</span>
<span class="linenos">169</span><span class="w">   </span><span class="nt">&quot;outputs&quot;</span><span class="p">:</span><span class="w"> </span><span class="p">[],</span>
<span class="linenos">170</span><span class="w">   </span><span class="nt">&quot;source&quot;</span><span class="p">:</span><span class="w"> </span><span class="p">[</span>
<span class="linenos">171</span><span class="w">    </span><span class="s2">&quot;# Re-use the house price data from Cell 4\n&quot;</span><span class="p">,</span>
<span class="linenos">172</span><span class="w">    </span><span class="s2">&quot;y_fitted   = intercept + slope * x\n&quot;</span><span class="p">,</span>
<span class="linenos">173</span><span class="w">    </span><span class="s2">&quot;residuals  = y - y_fitted\n&quot;</span><span class="p">,</span>
<span class="linenos">174</span><span class="w">    </span><span class="s2">&quot;std_resid  = residuals / s_e                               # standardised residuals\n&quot;</span><span class="p">,</span>
<span class="linenos">175</span><span class="w">    </span><span class="s2">&quot;\n&quot;</span><span class="p">,</span>
<span class="linenos">176</span><span class="w">    </span><span class="s2">&quot;RSS = (residuals ** 2).sum()\n&quot;</span><span class="p">,</span>
<span class="linenos">177</span><span class="w">    </span><span class="s2">&quot;TSS = ((y - y.mean()) ** 2).sum()\n&quot;</span><span class="p">,</span>
<span class="linenos">178</span><span class="w">    </span><span class="s2">&quot;R2  = 1 - RSS / TSS\n&quot;</span><span class="p">,</span>
<span class="linenos">179</span><span class="w">    </span><span class="s2">&quot;adj_R2 = 1 - (1 - R2) * (n_obs - 1) / (n_obs - 2)\n&quot;</span><span class="p">,</span>
<span class="linenos">180</span><span class="w">    </span><span class="s2">&quot;\n&quot;</span><span class="p">,</span>
<span class="linenos">181</span><span class="w">    </span><span class="s2">&quot;print(f&#39;R¬≤          : {R2:.4f}&#39;)\n&quot;</span><span class="p">,</span>
<span class="linenos">182</span><span class="w">    </span><span class="s2">&quot;print(f&#39;Adjusted R¬≤ : {adj_R2:.4f}&#39;)\n&quot;</span><span class="p">,</span>
<span class="linenos">183</span><span class="w">    </span><span class="s2">&quot;print(f&#39;RSS         : {RSS:.2f}&#39;)\n&quot;</span><span class="p">,</span>
<span class="linenos">184</span><span class="w">    </span><span class="s2">&quot;print(f&#39;Residual SE : {s_e:.4f}&#39;)\n&quot;</span><span class="p">,</span>
<span class="linenos">185</span><span class="w">    </span><span class="s2">&quot;\n&quot;</span><span class="p">,</span>
<span class="linenos">186</span><span class="w">    </span><span class="s2">&quot;# Shapiro-Wilk test on residuals\n&quot;</span><span class="p">,</span>
<span class="linenos">187</span><span class="w">    </span><span class="s2">&quot;sw_stat, sw_p = stats.shapiro(residuals)\n&quot;</span><span class="p">,</span>
<span class="linenos">188</span><span class="w">    </span><span class="s2">&quot;print(f&#39;\\nShapiro-Wilk test on residuals: W={sw_stat:.4f}, p={sw_p:.4f}&#39;)\n&quot;</span><span class="p">,</span>
<span class="linenos">189</span><span class="w">    </span><span class="s2">&quot;print(&#39;Residuals appear normal.&#39; if sw_p &gt; 0.05 else &#39;Residuals may NOT be normal.&#39;)\n&quot;</span><span class="p">,</span>
<span class="linenos">190</span><span class="w">    </span><span class="s2">&quot;\n&quot;</span><span class="p">,</span>
<span class="linenos">191</span><span class="w">    </span><span class="s2">&quot;fig = plt.figure(figsize=(11, 8))\n&quot;</span><span class="p">,</span>
<span class="linenos">192</span><span class="w">    </span><span class="s2">&quot;gs  = gridspec.GridSpec(2, 2, hspace=0.40, wspace=0.35)\n&quot;</span><span class="p">,</span>
<span class="linenos">193</span><span class="w">    </span><span class="s2">&quot;\n&quot;</span><span class="p">,</span>
<span class="linenos">194</span><span class="w">    </span><span class="s2">&quot;# 1. Residuals vs Fitted\n&quot;</span><span class="p">,</span>
<span class="linenos">195</span><span class="w">    </span><span class="s2">&quot;ax1 = fig.add_subplot(gs[0, 0])\n&quot;</span><span class="p">,</span>
<span class="linenos">196</span><span class="w">    </span><span class="s2">&quot;ax1.scatter(y_fitted, residuals, alpha=0.6, color=&#39;steelblue&#39;, s=25)\n&quot;</span><span class="p">,</span>
<span class="linenos">197</span><span class="w">    </span><span class="s2">&quot;ax1.axhline(0, color=&#39;red&#39;, lw=1.5, ls=&#39;--&#39;)\n&quot;</span><span class="p">,</span>
<span class="linenos">198</span><span class="w">    </span><span class="s2">&quot;ax1.set_xlabel(&#39;Fitted values&#39;); ax1.set_ylabel(&#39;Residuals&#39;)\n&quot;</span><span class="p">,</span>
<span class="linenos">199</span><span class="w">    </span><span class="s2">&quot;ax1.set_title(&#39;Residuals vs Fitted&#39;)\n&quot;</span><span class="p">,</span>
<span class="linenos">200</span><span class="w">    </span><span class="s2">&quot;\n&quot;</span><span class="p">,</span>
<span class="linenos">201</span><span class="w">    </span><span class="s2">&quot;# 2. Q-Q plot\n&quot;</span><span class="p">,</span>
<span class="linenos">202</span><span class="w">    </span><span class="s2">&quot;ax2 = fig.add_subplot(gs[0, 1])\n&quot;</span><span class="p">,</span>
<span class="linenos">203</span><span class="w">    </span><span class="s2">&quot;(osm, osr), (slope_qq, intercept_qq, r_qq) = stats.probplot(residuals, dist=&#39;norm&#39;)\n&quot;</span><span class="p">,</span>
<span class="linenos">204</span><span class="w">    </span><span class="s2">&quot;ax2.scatter(osm, osr, alpha=0.6, color=&#39;steelblue&#39;, s=25)\n&quot;</span><span class="p">,</span>
<span class="linenos">205</span><span class="w">    </span><span class="s2">&quot;ax2.plot(osm, slope_qq * np.array(osm) + intercept_qq, &#39;r-&#39;, lw=1.5)\n&quot;</span><span class="p">,</span>
<span class="linenos">206</span><span class="w">    </span><span class="s2">&quot;ax2.set_xlabel(&#39;Theoretical quantiles&#39;); ax2.set_ylabel(&#39;Sample quantiles&#39;)\n&quot;</span><span class="p">,</span>
<span class="linenos">207</span><span class="w">    </span><span class="s2">&quot;ax2.set_title(&#39;Normal Q-Q Plot&#39;)\n&quot;</span><span class="p">,</span>
<span class="linenos">208</span><span class="w">    </span><span class="s2">&quot;\n&quot;</span><span class="p">,</span>
<span class="linenos">209</span><span class="w">    </span><span class="s2">&quot;# 3. Scale-Location (sqrt |standardised residuals| vs fitted)\n&quot;</span><span class="p">,</span>
<span class="linenos">210</span><span class="w">    </span><span class="s2">&quot;ax3 = fig.add_subplot(gs[1, 0])\n&quot;</span><span class="p">,</span>
<span class="linenos">211</span><span class="w">    </span><span class="s2">&quot;ax3.scatter(y_fitted, np.sqrt(np.abs(std_resid)), alpha=0.6, color=&#39;steelblue&#39;, s=25)\n&quot;</span><span class="p">,</span>
<span class="linenos">212</span><span class="w">    </span><span class="s2">&quot;ax3.set_xlabel(&#39;Fitted values&#39;); ax3.set_ylabel(&#39;‚àö|Standardised residuals|&#39;)\n&quot;</span><span class="p">,</span>
<span class="linenos">213</span><span class="w">    </span><span class="s2">&quot;ax3.set_title(&#39;Scale-Location&#39;)\n&quot;</span><span class="p">,</span>
<span class="linenos">214</span><span class="w">    </span><span class="s2">&quot;\n&quot;</span><span class="p">,</span>
<span class="linenos">215</span><span class="w">    </span><span class="s2">&quot;# 4. Residual histogram\n&quot;</span><span class="p">,</span>
<span class="linenos">216</span><span class="w">    </span><span class="s2">&quot;ax4 = fig.add_subplot(gs[1, 1])\n&quot;</span><span class="p">,</span>
<span class="linenos">217</span><span class="w">    </span><span class="s2">&quot;ax4.hist(residuals, bins=14, color=&#39;steelblue&#39;, edgecolor=&#39;white&#39;, alpha=0.8, density=True)\n&quot;</span><span class="p">,</span>
<span class="linenos">218</span><span class="w">    </span><span class="s2">&quot;xr = np.linspace(residuals.min(), residuals.max(), 200)\n&quot;</span><span class="p">,</span>
<span class="linenos">219</span><span class="w">    </span><span class="s2">&quot;ax4.plot(xr, stats.norm.pdf(xr, residuals.mean(), residuals.std()), &#39;r-&#39;, lw=2)\n&quot;</span><span class="p">,</span>
<span class="linenos">220</span><span class="w">    </span><span class="s2">&quot;ax4.set_xlabel(&#39;Residual&#39;); ax4.set_ylabel(&#39;Density&#39;)\n&quot;</span><span class="p">,</span>
<span class="linenos">221</span><span class="w">    </span><span class="s2">&quot;ax4.set_title(&#39;Residual Distribution&#39;)\n&quot;</span><span class="p">,</span>
<span class="linenos">222</span><span class="w">    </span><span class="s2">&quot;\n&quot;</span><span class="p">,</span>
<span class="linenos">223</span><span class="w">    </span><span class="s2">&quot;fig.suptitle(f&#39;Residual Diagnostics  |  R¬≤={R2:.3f}&#39;, fontsize=13, y=1.01)\n&quot;</span><span class="p">,</span>
<span class="linenos">224</span><span class="w">    </span><span class="s2">&quot;plt.show()&quot;</span>
<span class="linenos">225</span><span class="w">   </span><span class="p">]</span>
<span class="linenos">226</span><span class="w">  </span><span class="p">},</span>
<span class="linenos">227</span><span class="w">  </span><span class="p">{</span>
<span class="linenos">228</span><span class="w">   </span><span class="nt">&quot;cell_type&quot;</span><span class="p">:</span><span class="w"> </span><span class="s2">&quot;markdown&quot;</span><span class="p">,</span>
<span class="linenos">229</span><span class="w">   </span><span class="nt">&quot;id&quot;</span><span class="p">:</span><span class="w"> </span><span class="s2">&quot;b2c3d4e5-0007-4000-8000-000000000007&quot;</span><span class="p">,</span>
<span class="linenos">230</span><span class="w">   </span><span class="nt">&quot;metadata&quot;</span><span class="p">:</span><span class="w"> </span><span class="p">{},</span>
<span class="linenos">231</span><span class="w">   </span><span class="nt">&quot;source&quot;</span><span class="p">:</span><span class="w"> </span><span class="p">[</span>
<span class="linenos">232</span><span class="w">    </span><span class="s2">&quot;## 3. Multiple Linear Regression\n&quot;</span><span class="p">,</span>
<span class="linenos">233</span><span class="w">    </span><span class="s2">&quot;\n&quot;</span><span class="p">,</span>
<span class="linenos">234</span><span class="w">    </span><span class="s2">&quot;With $p$ predictors the model becomes:\n&quot;</span><span class="p">,</span>
<span class="linenos">235</span><span class="w">    </span><span class="s2">&quot;\n&quot;</span><span class="p">,</span>
<span class="linenos">236</span><span class="w">    </span><span class="s2">&quot;$$\\mathbf{y} = \\mathbf{X}\\boldsymbol{\\beta} + \\boldsymbol{\\varepsilon}, \\qquad \\hat{\\boldsymbol{\\beta}} = (\\mathbf{X}^\\top\\mathbf{X})^{-1}\\mathbf{X}^\\top\\mathbf{y}$$\n&quot;</span><span class="p">,</span>
<span class="linenos">237</span><span class="w">    </span><span class="s2">&quot;\n&quot;</span><span class="p">,</span>
<span class="linenos">238</span><span class="w">    </span><span class="s2">&quot;where $\\mathbf{X}$ is the $n \\times (p+1)$ design matrix (first column all ones for the intercept).\n&quot;</span><span class="p">,</span>
<span class="linenos">239</span><span class="w">    </span><span class="s2">&quot;\n&quot;</span><span class="p">,</span>
<span class="linenos">240</span><span class="w">    </span><span class="s2">&quot;**Key considerations:**\n&quot;</span><span class="p">,</span>
<span class="linenos">241</span><span class="w">    </span><span class="s2">&quot;- **Multicollinearity**: when predictors are correlated, coefficient estimates become unstable. Check with Variance Inflation Factor (VIF).\n&quot;</span><span class="p">,</span>
<span class="linenos">242</span><span class="w">    </span><span class="s2">&quot;- **Adjusted R¬≤**: penalises adding uninformative predictors ‚Äî always report alongside R¬≤.\n&quot;</span><span class="p">,</span>
<span class="linenos">243</span><span class="w">    </span><span class="s2">&quot;- **F-statistic**: tests whether *all* coefficients (except intercept) are simultaneously zero.\n&quot;</span><span class="p">,</span>
<span class="linenos">244</span><span class="w">    </span><span class="s2">&quot;\n&quot;</span><span class="p">,</span>
<span class="linenos">245</span><span class="w">    </span><span class="s2">&quot;$$F = \\frac{(\\text{TSS} - \\text{RSS})/p}{\\text{RSS}/(n - p - 1)}$$&quot;</span>
<span class="linenos">246</span><span class="w">   </span><span class="p">]</span>
<span class="linenos">247</span><span class="w">  </span><span class="p">},</span>
<span class="linenos">248</span><span class="w">  </span><span class="p">{</span>
<span class="linenos">249</span><span class="w">   </span><span class="nt">&quot;cell_type&quot;</span><span class="p">:</span><span class="w"> </span><span class="s2">&quot;code&quot;</span><span class="p">,</span>
<span class="linenos">250</span><span class="w">   </span><span class="nt">&quot;execution_count&quot;</span><span class="p">:</span><span class="w"> </span><span class="kc">null</span><span class="p">,</span>
<span class="linenos">251</span><span class="w">   </span><span class="nt">&quot;id&quot;</span><span class="p">:</span><span class="w"> </span><span class="s2">&quot;b2c3d4e5-0008-4000-8000-000000000008&quot;</span><span class="p">,</span>
<span class="linenos">252</span><span class="w">   </span><span class="nt">&quot;metadata&quot;</span><span class="p">:</span><span class="w"> </span><span class="p">{},</span>
<span class="linenos">253</span><span class="w">   </span><span class="nt">&quot;outputs&quot;</span><span class="p">:</span><span class="w"> </span><span class="p">[],</span>
<span class="linenos">254</span><span class="w">   </span><span class="nt">&quot;source&quot;</span><span class="p">:</span><span class="w"> </span><span class="p">[</span>
<span class="linenos">255</span><span class="w">    </span><span class="s2">&quot;# Scenario: predict salary (‚Ç¨k) from years of experience, education level (0-3), and performance score\n&quot;</span><span class="p">,</span>
<span class="linenos">256</span><span class="w">    </span><span class="s2">&quot;n = 120\n&quot;</span><span class="p">,</span>
<span class="linenos">257</span><span class="w">    </span><span class="s2">&quot;experience  = rng.uniform(0, 30, n)\n&quot;</span><span class="p">,</span>
<span class="linenos">258</span><span class="w">    </span><span class="s2">&quot;education   = rng.integers(0, 4, n).astype(float)\n&quot;</span><span class="p">,</span>
<span class="linenos">259</span><span class="w">    </span><span class="s2">&quot;performance = rng.uniform(50, 100, n)\n&quot;</span><span class="p">,</span>
<span class="linenos">260</span><span class="w">    </span><span class="s2">&quot;\n&quot;</span><span class="p">,</span>
<span class="linenos">261</span><span class="w">    </span><span class="s2">&quot;# True model: salary = 25 + 1.8*exp + 5*edu + 0.3*perf + noise\n&quot;</span><span class="p">,</span>
<span class="linenos">262</span><span class="w">    </span><span class="s2">&quot;true_betas = np.array([25.0, 1.8, 5.0, 0.3])\n&quot;</span><span class="p">,</span>
<span class="linenos">263</span><span class="w">    </span><span class="s2">&quot;noise = rng.normal(0, 8, n)\n&quot;</span><span class="p">,</span>
<span class="linenos">264</span><span class="w">    </span><span class="s2">&quot;y = true_betas[0] + true_betas[1]*experience + true_betas[2]*education + true_betas[3]*performance + noise\n&quot;</span><span class="p">,</span>
<span class="linenos">265</span><span class="w">    </span><span class="s2">&quot;\n&quot;</span><span class="p">,</span>
<span class="linenos">266</span><span class="w">    </span><span class="s2">&quot;# Build design matrix (with intercept column)\n&quot;</span><span class="p">,</span>
<span class="linenos">267</span><span class="w">    </span><span class="s2">&quot;X_raw = np.column_stack([experience, education, performance])\n&quot;</span><span class="p">,</span>
<span class="linenos">268</span><span class="w">    </span><span class="s2">&quot;ones  = np.ones((n, 1))\n&quot;</span><span class="p">,</span>
<span class="linenos">269</span><span class="w">    </span><span class="s2">&quot;X     = np.hstack([ones, X_raw])                           # shape: (n, 4)\n&quot;</span><span class="p">,</span>
<span class="linenos">270</span><span class="w">    </span><span class="s2">&quot;\n&quot;</span><span class="p">,</span>
<span class="linenos">271</span><span class="w">    </span><span class="s2">&quot;# OLS via normal equations\n&quot;</span><span class="p">,</span>
<span class="linenos">272</span><span class="w">    </span><span class="s2">&quot;beta_hat = np.linalg.lstsq(X, y, rcond=None)[0]\n&quot;</span><span class="p">,</span>
<span class="linenos">273</span><span class="w">    </span><span class="s2">&quot;\n&quot;</span><span class="p">,</span>
<span class="linenos">274</span><span class="w">    </span><span class="s2">&quot;y_hat    = X @ beta_hat\n&quot;</span><span class="p">,</span>
<span class="linenos">275</span><span class="w">    </span><span class="s2">&quot;residuals = y - y_hat\n&quot;</span><span class="p">,</span>
<span class="linenos">276</span><span class="w">    </span><span class="s2">&quot;p_pred   = X.shape[1] - 1                                  # number of predictors (excl. intercept)\n&quot;</span><span class="p">,</span>
<span class="linenos">277</span><span class="w">    </span><span class="s2">&quot;RSS      = (residuals ** 2).sum()\n&quot;</span><span class="p">,</span>
<span class="linenos">278</span><span class="w">    </span><span class="s2">&quot;TSS      = ((y - y.mean()) ** 2).sum()\n&quot;</span><span class="p">,</span>
<span class="linenos">279</span><span class="w">    </span><span class="s2">&quot;R2       = 1 - RSS / TSS\n&quot;</span><span class="p">,</span>
<span class="linenos">280</span><span class="w">    </span><span class="s2">&quot;adj_R2   = 1 - (1 - R2) * (n - 1) / (n - p_pred - 1)\n&quot;</span><span class="p">,</span>
<span class="linenos">281</span><span class="w">    </span><span class="s2">&quot;s2       = RSS / (n - p_pred - 1)\n&quot;</span><span class="p">,</span>
<span class="linenos">282</span><span class="w">    </span><span class="s2">&quot;\n&quot;</span><span class="p">,</span>
<span class="linenos">283</span><span class="w">    </span><span class="s2">&quot;# Standard errors of coefficients\n&quot;</span><span class="p">,</span>
<span class="linenos">284</span><span class="w">    </span><span class="s2">&quot;XtX_inv  = np.linalg.inv(X.T @ X)\n&quot;</span><span class="p">,</span>
<span class="linenos">285</span><span class="w">    </span><span class="s2">&quot;se_betas = np.sqrt(np.diag(XtX_inv) * s2)\n&quot;</span><span class="p">,</span>
<span class="linenos">286</span><span class="w">    </span><span class="s2">&quot;\n&quot;</span><span class="p">,</span>
<span class="linenos">287</span><span class="w">    </span><span class="s2">&quot;# t-statistics and p-values\n&quot;</span><span class="p">,</span>
<span class="linenos">288</span><span class="w">    </span><span class="s2">&quot;t_stats = beta_hat / se_betas\n&quot;</span><span class="p">,</span>
<span class="linenos">289</span><span class="w">    </span><span class="s2">&quot;df_res  = n - p_pred - 1\n&quot;</span><span class="p">,</span>
<span class="linenos">290</span><span class="w">    </span><span class="s2">&quot;p_vals  = 2 * stats.t.sf(np.abs(t_stats), df=df_res)\n&quot;</span><span class="p">,</span>
<span class="linenos">291</span><span class="w">    </span><span class="s2">&quot;\n&quot;</span><span class="p">,</span>
<span class="linenos">292</span><span class="w">    </span><span class="s2">&quot;# F-statistic for overall model significance\n&quot;</span><span class="p">,</span>
<span class="linenos">293</span><span class="w">    </span><span class="s2">&quot;F_stat = ((TSS - RSS) / p_pred) / (RSS / df_res)\n&quot;</span><span class="p">,</span>
<span class="linenos">294</span><span class="w">    </span><span class="s2">&quot;F_pval = stats.f.sf(F_stat, p_pred, df_res)\n&quot;</span><span class="p">,</span>
<span class="linenos">295</span><span class="w">    </span><span class="s2">&quot;\n&quot;</span><span class="p">,</span>
<span class="linenos">296</span><span class="w">    </span><span class="s2">&quot;print(&#39;Multiple Linear Regression Results&#39;)\n&quot;</span><span class="p">,</span>
<span class="linenos">297</span><span class="w">    </span><span class="s2">&quot;print(&#39;=&#39; * 55)\n&quot;</span><span class="p">,</span>
<span class="linenos">298</span><span class="w">    </span><span class="s2">&quot;print(f&#39;R¬≤: {R2:.4f}   Adjusted R¬≤: {adj_R2:.4f}&#39;)\n&quot;</span><span class="p">,</span>
<span class="linenos">299</span><span class="w">    </span><span class="s2">&quot;print(f&#39;F-statistic: {F_stat:.2f}  (p={F_pval:.2e})&#39;)\n&quot;</span><span class="p">,</span>
<span class="linenos">300</span><span class="w">    </span><span class="s2">&quot;print(f&#39;n={n}, p={p_pred}&#39;)\n&quot;</span><span class="p">,</span>
<span class="linenos">301</span><span class="w">    </span><span class="s2">&quot;print()\n&quot;</span><span class="p">,</span>
<span class="linenos">302</span><span class="w">    </span><span class="s2">&quot;labels = [&#39;Intercept&#39;, &#39;Experience&#39;, &#39;Education&#39;, &#39;Performance&#39;]\n&quot;</span><span class="p">,</span>
<span class="linenos">303</span><span class="w">    </span><span class="s2">&quot;print(f&#39;{\&quot;Variable\&quot;:&lt;14} {\&quot;Estimate\&quot;:&gt;10} {\&quot;Std Err\&quot;:&gt;10} {\&quot;t-stat\&quot;:&gt;10} {\&quot;p-value\&quot;:&gt;12}&#39;)\n&quot;</span><span class="p">,</span>
<span class="linenos">304</span><span class="w">    </span><span class="s2">&quot;print(&#39;-&#39; * 58)\n&quot;</span><span class="p">,</span>
<span class="linenos">305</span><span class="w">    </span><span class="s2">&quot;for lbl, b, se, t, p in zip(labels, beta_hat, se_betas, t_stats, p_vals):\n&quot;</span><span class="p">,</span>
<span class="linenos">306</span><span class="w">    </span><span class="s2">&quot;    sig = &#39;***&#39; if p &lt; 0.001 else (&#39;**&#39; if p &lt; 0.01 else (&#39;*&#39; if p &lt; 0.05 else &#39;&#39;))\n&quot;</span><span class="p">,</span>
<span class="linenos">307</span><span class="w">    </span><span class="s2">&quot;    print(f&#39;{lbl:&lt;14} {b:&gt;10.4f} {se:&gt;10.4f} {t:&gt;10.4f} {p:&gt;12.4f} {sig}&#39;)\n&quot;</span><span class="p">,</span>
<span class="linenos">308</span><span class="w">    </span><span class="s2">&quot;\n&quot;</span><span class="p">,</span>
<span class="linenos">309</span><span class="w">    </span><span class="s2">&quot;# Coefficient plot\n&quot;</span><span class="p">,</span>
<span class="linenos">310</span><span class="w">    </span><span class="s2">&quot;fig, ax = plt.subplots(figsize=(7, 4))\n&quot;</span><span class="p">,</span>
<span class="linenos">311</span><span class="w">    </span><span class="s2">&quot;ci_mult = stats.t.ppf(0.975, df=df_res)\n&quot;</span><span class="p">,</span>
<span class="linenos">312</span><span class="w">    </span><span class="s2">&quot;ci_half = ci_mult * se_betas[1:]                           # skip intercept\n&quot;</span><span class="p">,</span>
<span class="linenos">313</span><span class="w">    </span><span class="s2">&quot;y_pos = np.arange(p_pred)\n&quot;</span><span class="p">,</span>
<span class="linenos">314</span><span class="w">    </span><span class="s2">&quot;ax.barh(y_pos, beta_hat[1:], xerr=ci_half, color=&#39;steelblue&#39;, alpha=0.7,\n&quot;</span><span class="p">,</span>
<span class="linenos">315</span><span class="w">    </span><span class="s2">&quot;        edgecolor=&#39;white&#39;, capsize=4)\n&quot;</span><span class="p">,</span>
<span class="linenos">316</span><span class="w">    </span><span class="s2">&quot;ax.axvline(0, color=&#39;black&#39;, lw=1)\n&quot;</span><span class="p">,</span>
<span class="linenos">317</span><span class="w">    </span><span class="s2">&quot;ax.set_yticks(y_pos)\n&quot;</span><span class="p">,</span>
<span class="linenos">318</span><span class="w">    </span><span class="s2">&quot;ax.set_yticklabels([&#39;Experience&#39;, &#39;Education&#39;, &#39;Performance&#39;])\n&quot;</span><span class="p">,</span>
<span class="linenos">319</span><span class="w">    </span><span class="s2">&quot;ax.set_xlabel(&#39;Coefficient estimate (with 95% CI)&#39;)\n&quot;</span><span class="p">,</span>
<span class="linenos">320</span><span class="w">    </span><span class="s2">&quot;ax.set_title(&#39;Multiple Regression Coefficients&#39;)\n&quot;</span><span class="p">,</span>
<span class="linenos">321</span><span class="w">    </span><span class="s2">&quot;plt.tight_layout()\n&quot;</span><span class="p">,</span>
<span class="linenos">322</span><span class="w">    </span><span class="s2">&quot;plt.show()&quot;</span>
<span class="linenos">323</span><span class="w">   </span><span class="p">]</span>
<span class="linenos">324</span><span class="w">  </span><span class="p">},</span>
<span class="linenos">325</span><span class="w">  </span><span class="p">{</span>
<span class="linenos">326</span><span class="w">   </span><span class="nt">&quot;cell_type&quot;</span><span class="p">:</span><span class="w"> </span><span class="s2">&quot;markdown&quot;</span><span class="p">,</span>
<span class="linenos">327</span><span class="w">   </span><span class="nt">&quot;id&quot;</span><span class="p">:</span><span class="w"> </span><span class="s2">&quot;b2c3d4e5-0009-4000-8000-000000000009&quot;</span><span class="p">,</span>
<span class="linenos">328</span><span class="w">   </span><span class="nt">&quot;metadata&quot;</span><span class="p">:</span><span class="w"> </span><span class="p">{},</span>
<span class="linenos">329</span><span class="w">   </span><span class="nt">&quot;source&quot;</span><span class="p">:</span><span class="w"> </span><span class="p">[</span>
<span class="linenos">330</span><span class="w">    </span><span class="s2">&quot;## 4. Polynomial Regression\n&quot;</span><span class="p">,</span>
<span class="linenos">331</span><span class="w">    </span><span class="s2">&quot;\n&quot;</span><span class="p">,</span>
<span class="linenos">332</span><span class="w">    </span><span class="s2">&quot;When the relationship between $x$ and $y$ is non-linear, we can extend OLS by including polynomial features:\n&quot;</span><span class="p">,</span>
<span class="linenos">333</span><span class="w">    </span><span class="s2">&quot;\n&quot;</span><span class="p">,</span>
<span class="linenos">334</span><span class="w">    </span><span class="s2">&quot;$$y = \\beta_0 + \\beta_1 x + \\beta_2 x^2 + \\cdots + \\beta_d x^d + \\varepsilon$$\n&quot;</span><span class="p">,</span>
<span class="linenos">335</span><span class="w">    </span><span class="s2">&quot;\n&quot;</span><span class="p">,</span>
<span class="linenos">336</span><span class="w">    </span><span class="s2">&quot;This is still *linear* in the parameters $\\boldsymbol{\\beta}$ ‚Äî just a linear model in a transformed feature space.\n&quot;</span><span class="p">,</span>
<span class="linenos">337</span><span class="w">    </span><span class="s2">&quot;\n&quot;</span><span class="p">,</span>
<span class="linenos">338</span><span class="w">    </span><span class="s2">&quot;**Bias-variance tradeoff:**\n&quot;</span><span class="p">,</span>
<span class="linenos">339</span><span class="w">    </span><span class="s2">&quot;- Low-degree polynomial ‚Üí high bias (underfitting)\n&quot;</span><span class="p">,</span>
<span class="linenos">340</span><span class="w">    </span><span class="s2">&quot;- High-degree polynomial ‚Üí high variance (overfitting)\n&quot;</span><span class="p">,</span>
<span class="linenos">341</span><span class="w">    </span><span class="s2">&quot;- The optimal degree minimises prediction error on unseen data (use cross-validation)\n&quot;</span><span class="p">,</span>
<span class="linenos">342</span><span class="w">    </span><span class="s2">&quot;\n&quot;</span><span class="p">,</span>
<span class="linenos">343</span><span class="w">    </span><span class="s2">&quot;The training R¬≤ always increases with degree, so use **adjusted R¬≤** or **AIC/BIC** to compare models.&quot;</span>
<span class="linenos">344</span><span class="w">   </span><span class="p">]</span>
<span class="linenos">345</span><span class="w">  </span><span class="p">},</span>
<span class="linenos">346</span><span class="w">  </span><span class="p">{</span>
<span class="linenos">347</span><span class="w">   </span><span class="nt">&quot;cell_type&quot;</span><span class="p">:</span><span class="w"> </span><span class="s2">&quot;code&quot;</span><span class="p">,</span>
<span class="linenos">348</span><span class="w">   </span><span class="nt">&quot;execution_count&quot;</span><span class="p">:</span><span class="w"> </span><span class="kc">null</span><span class="p">,</span>
<span class="linenos">349</span><span class="w">   </span><span class="nt">&quot;id&quot;</span><span class="p">:</span><span class="w"> </span><span class="s2">&quot;b2c3d4e5-0010-4000-8000-000000000010&quot;</span><span class="p">,</span>
<span class="linenos">350</span><span class="w">   </span><span class="nt">&quot;metadata&quot;</span><span class="p">:</span><span class="w"> </span><span class="p">{},</span>
<span class="linenos">351</span><span class="w">   </span><span class="nt">&quot;outputs&quot;</span><span class="p">:</span><span class="w"> </span><span class="p">[],</span>
<span class="linenos">352</span><span class="w">   </span><span class="nt">&quot;source&quot;</span><span class="p">:</span><span class="w"> </span><span class="p">[</span>
<span class="linenos">353</span><span class="w">    </span><span class="s2">&quot;# Scenario: model a noisy sinusoidal relationship\n&quot;</span><span class="p">,</span>
<span class="linenos">354</span><span class="w">    </span><span class="s2">&quot;n = 60\n&quot;</span><span class="p">,</span>
<span class="linenos">355</span><span class="w">    </span><span class="s2">&quot;x_poly = rng.uniform(-3, 3, size=n)\n&quot;</span><span class="p">,</span>
<span class="linenos">356</span><span class="w">    </span><span class="s2">&quot;y_poly = np.sin(x_poly) + rng.normal(0, 0.3, size=n)\n&quot;</span><span class="p">,</span>
<span class="linenos">357</span><span class="w">    </span><span class="s2">&quot;\n&quot;</span><span class="p">,</span>
<span class="linenos">358</span><span class="w">    </span><span class="s2">&quot;x_grid_poly = np.linspace(-3.2, 3.2, 300)\n&quot;</span><span class="p">,</span>
<span class="linenos">359</span><span class="w">    </span><span class="s2">&quot;degrees = [1, 3, 7, 15]\n&quot;</span><span class="p">,</span>
<span class="linenos">360</span><span class="w">    </span><span class="s2">&quot;\n&quot;</span><span class="p">,</span>
<span class="linenos">361</span><span class="w">    </span><span class="s2">&quot;fig, axes = plt.subplots(2, 2, figsize=(11, 8), sharey=True)\n&quot;</span><span class="p">,</span>
<span class="linenos">362</span><span class="w">    </span><span class="s2">&quot;axes = axes.ravel()\n&quot;</span><span class="p">,</span>
<span class="linenos">363</span><span class="w">    </span><span class="s2">&quot;\n&quot;</span><span class="p">,</span>
<span class="linenos">364</span><span class="w">    </span><span class="s2">&quot;r2_scores = []\n&quot;</span><span class="p">,</span>
<span class="linenos">365</span><span class="w">    </span><span class="s2">&quot;for ax, deg in zip(axes, degrees):\n&quot;</span><span class="p">,</span>
<span class="linenos">366</span><span class="w">    </span><span class="s2">&quot;    # Build Vandermonde matrix for training data and grid\n&quot;</span><span class="p">,</span>
<span class="linenos">367</span><span class="w">    </span><span class="s2">&quot;    X_train = np.vander(x_poly,      N=deg + 1, increasing=True)\n&quot;</span><span class="p">,</span>
<span class="linenos">368</span><span class="w">    </span><span class="s2">&quot;    X_grid  = np.vander(x_grid_poly, N=deg + 1, increasing=True)\n&quot;</span><span class="p">,</span>
<span class="linenos">369</span><span class="w">    </span><span class="s2">&quot;\n&quot;</span><span class="p">,</span>
<span class="linenos">370</span><span class="w">    </span><span class="s2">&quot;    coeffs = np.linalg.lstsq(X_train, y_poly, rcond=None)[0]\n&quot;</span><span class="p">,</span>
<span class="linenos">371</span><span class="w">    </span><span class="s2">&quot;    y_pred_train = X_train @ coeffs\n&quot;</span><span class="p">,</span>
<span class="linenos">372</span><span class="w">    </span><span class="s2">&quot;    y_pred_grid  = X_grid  @ coeffs\n&quot;</span><span class="p">,</span>
<span class="linenos">373</span><span class="w">    </span><span class="s2">&quot;\n&quot;</span><span class="p">,</span>
<span class="linenos">374</span><span class="w">    </span><span class="s2">&quot;    rss = ((y_poly - y_pred_train) ** 2).sum()\n&quot;</span><span class="p">,</span>
<span class="linenos">375</span><span class="w">    </span><span class="s2">&quot;    tss = ((y_poly - y_poly.mean()) ** 2).sum()\n&quot;</span><span class="p">,</span>
<span class="linenos">376</span><span class="w">    </span><span class="s2">&quot;    r2  = 1 - rss / tss\n&quot;</span><span class="p">,</span>
<span class="linenos">377</span><span class="w">    </span><span class="s2">&quot;    adj_r2 = 1 - (1 - r2) * (n - 1) / (n - deg - 1)\n&quot;</span><span class="p">,</span>
<span class="linenos">378</span><span class="w">    </span><span class="s2">&quot;    r2_scores.append(r2)\n&quot;</span><span class="p">,</span>
<span class="linenos">379</span><span class="w">    </span><span class="s2">&quot;\n&quot;</span><span class="p">,</span>
<span class="linenos">380</span><span class="w">    </span><span class="s2">&quot;    ax.scatter(x_poly, y_poly, color=&#39;steelblue&#39;, alpha=0.5, s=20, label=&#39;Data&#39;)\n&quot;</span><span class="p">,</span>
<span class="linenos">381</span><span class="w">    </span><span class="s2">&quot;    ax.plot(x_grid_poly, np.sin(x_grid_poly), &#39;g--&#39;, lw=1.5, label=&#39;True sin(x)&#39;)\n&quot;</span><span class="p">,</span>
<span class="linenos">382</span><span class="w">    </span><span class="s2">&quot;    ax.plot(x_grid_poly, y_pred_grid, &#39;r-&#39;, lw=2,\n&quot;</span><span class="p">,</span>
<span class="linenos">383</span><span class="w">    </span><span class="s2">&quot;            label=f&#39;Degree {deg}&#39;)\n&quot;</span><span class="p">,</span>
<span class="linenos">384</span><span class="w">    </span><span class="s2">&quot;    ax.set_ylim(-2.5, 2.5)\n&quot;</span><span class="p">,</span>
<span class="linenos">385</span><span class="w">    </span><span class="s2">&quot;    ax.set_title(f&#39;Degree {deg}  |  R¬≤={r2:.3f}, adj-R¬≤={adj_r2:.3f}&#39;)\n&quot;</span><span class="p">,</span>
<span class="linenos">386</span><span class="w">    </span><span class="s2">&quot;    ax.set_xlabel(&#39;x&#39;)\n&quot;</span><span class="p">,</span>
<span class="linenos">387</span><span class="w">    </span><span class="s2">&quot;    ax.set_ylabel(&#39;y&#39;)\n&quot;</span><span class="p">,</span>
<span class="linenos">388</span><span class="w">    </span><span class="s2">&quot;    ax.legend(fontsize=9)\n&quot;</span><span class="p">,</span>
<span class="linenos">389</span><span class="w">    </span><span class="s2">&quot;\n&quot;</span><span class="p">,</span>
<span class="linenos">390</span><span class="w">    </span><span class="s2">&quot;plt.suptitle(&#39;Polynomial Regression: Underfitting ‚Üí Overfitting&#39;, fontsize=13)\n&quot;</span><span class="p">,</span>
<span class="linenos">391</span><span class="w">    </span><span class="s2">&quot;plt.tight_layout()\n&quot;</span><span class="p">,</span>
<span class="linenos">392</span><span class="w">    </span><span class="s2">&quot;plt.show()\n&quot;</span><span class="p">,</span>
<span class="linenos">393</span><span class="w">    </span><span class="s2">&quot;\n&quot;</span><span class="p">,</span>
<span class="linenos">394</span><span class="w">    </span><span class="s2">&quot;print(&#39;Training R¬≤ by degree:&#39;)\n&quot;</span><span class="p">,</span>
<span class="linenos">395</span><span class="w">    </span><span class="s2">&quot;for d, r2 in zip(degrees, r2_scores):\n&quot;</span><span class="p">,</span>
<span class="linenos">396</span><span class="w">    </span><span class="s2">&quot;    bar = &#39;#&#39; * int(r2 * 40)\n&quot;</span><span class="p">,</span>
<span class="linenos">397</span><span class="w">    </span><span class="s2">&quot;    print(f&#39;  Degree {d:&gt;2}: R¬≤={r2:.4f}  |{bar}&#39;)&quot;</span>
<span class="linenos">398</span><span class="w">   </span><span class="p">]</span>
<span class="linenos">399</span><span class="w">  </span><span class="p">},</span>
<span class="linenos">400</span><span class="w">  </span><span class="p">{</span>
<span class="linenos">401</span><span class="w">   </span><span class="nt">&quot;cell_type&quot;</span><span class="p">:</span><span class="w"> </span><span class="s2">&quot;markdown&quot;</span><span class="p">,</span>
<span class="linenos">402</span><span class="w">   </span><span class="nt">&quot;id&quot;</span><span class="p">:</span><span class="w"> </span><span class="s2">&quot;b2c3d4e5-0011-4000-8000-000000000011&quot;</span><span class="p">,</span>
<span class="linenos">403</span><span class="w">   </span><span class="nt">&quot;metadata&quot;</span><span class="p">:</span><span class="w"> </span><span class="p">{},</span>
<span class="linenos">404</span><span class="w">   </span><span class="nt">&quot;source&quot;</span><span class="p">:</span><span class="w"> </span><span class="p">[</span>
<span class="linenos">405</span><span class="w">    </span><span class="s2">&quot;## 5. Logistic Regression (Binary Classification)\n&quot;</span><span class="p">,</span>
<span class="linenos">406</span><span class="w">    </span><span class="s2">&quot;\n&quot;</span><span class="p">,</span>
<span class="linenos">407</span><span class="w">    </span><span class="s2">&quot;When the response is binary ($y \\in \\{0, 1\\}$), linear regression is inappropriate. **Logistic regression** models the log-odds:\n&quot;</span><span class="p">,</span>
<span class="linenos">408</span><span class="w">    </span><span class="s2">&quot;\n&quot;</span><span class="p">,</span>
<span class="linenos">409</span><span class="w">    </span><span class="s2">&quot;$$\\log\\frac{P(y=1 \\mid x)}{1 - P(y=1 \\mid x)} = \\beta_0 + \\beta_1 x$$\n&quot;</span><span class="p">,</span>
<span class="linenos">410</span><span class="w">    </span><span class="s2">&quot;\n&quot;</span><span class="p">,</span>
<span class="linenos">411</span><span class="w">    </span><span class="s2">&quot;Transforming back via the **sigmoid function**:\n&quot;</span><span class="p">,</span>
<span class="linenos">412</span><span class="w">    </span><span class="s2">&quot;\n&quot;</span><span class="p">,</span>
<span class="linenos">413</span><span class="w">    </span><span class="s2">&quot;$$P(y=1 \\mid x) = \\sigma(\\beta_0 + \\beta_1 x) = \\frac{1}{1 + e^{-(\\beta_0 + \\beta_1 x)}}$$\n&quot;</span><span class="p">,</span>
<span class="linenos">414</span><span class="w">    </span><span class="s2">&quot;\n&quot;</span><span class="p">,</span>
<span class="linenos">415</span><span class="w">    </span><span class="s2">&quot;Parameters are estimated by **maximum likelihood** (not OLS). The decision boundary is where $P = 0.5$, i.e., $\\beta_0 + \\beta_1 x = 0$.\n&quot;</span><span class="p">,</span>
<span class="linenos">416</span><span class="w">    </span><span class="s2">&quot;\n&quot;</span><span class="p">,</span>
<span class="linenos">417</span><span class="w">    </span><span class="s2">&quot;**Interpretation of coefficients:** $e^{\\beta_j}$ is the **odds ratio** ‚Äî the factor by which the odds of $y=1$ multiply for a unit increase in $x_j$, holding other predictors constant.&quot;</span>
<span class="linenos">418</span><span class="w">   </span><span class="p">]</span>
<span class="linenos">419</span><span class="w">  </span><span class="p">},</span>
<span class="linenos">420</span><span class="w">  </span><span class="p">{</span>
<span class="linenos">421</span><span class="w">   </span><span class="nt">&quot;cell_type&quot;</span><span class="p">:</span><span class="w"> </span><span class="s2">&quot;code&quot;</span><span class="p">,</span>
<span class="linenos">422</span><span class="w">   </span><span class="nt">&quot;execution_count&quot;</span><span class="p">:</span><span class="w"> </span><span class="kc">null</span><span class="p">,</span>
<span class="linenos">423</span><span class="w">   </span><span class="nt">&quot;id&quot;</span><span class="p">:</span><span class="w"> </span><span class="s2">&quot;b2c3d4e5-0012-4000-8000-000000000012&quot;</span><span class="p">,</span>
<span class="linenos">424</span><span class="w">   </span><span class="nt">&quot;metadata&quot;</span><span class="p">:</span><span class="w"> </span><span class="p">{},</span>
<span class="linenos">425</span><span class="w">   </span><span class="nt">&quot;outputs&quot;</span><span class="p">:</span><span class="w"> </span><span class="p">[],</span>
<span class="linenos">426</span><span class="w">   </span><span class="nt">&quot;source&quot;</span><span class="p">:</span><span class="w"> </span><span class="p">[</span>
<span class="linenos">427</span><span class="w">    </span><span class="s2">&quot;# Scenario: predict exam pass/fail from hours studied\n&quot;</span><span class="p">,</span>
<span class="linenos">428</span><span class="w">    </span><span class="s2">&quot;n_logit = 100\n&quot;</span><span class="p">,</span>
<span class="linenos">429</span><span class="w">    </span><span class="s2">&quot;hours   = rng.uniform(0, 10, n_logit)\n&quot;</span><span class="p">,</span>
<span class="linenos">430</span><span class="w">    </span><span class="s2">&quot;\n&quot;</span><span class="p">,</span>
<span class="linenos">431</span><span class="w">    </span><span class="s2">&quot;# True logistic model: log-odds = -4 + 1.2 * hours\n&quot;</span><span class="p">,</span>
<span class="linenos">432</span><span class="w">    </span><span class="s2">&quot;true_b0, true_b1 = -4.0, 1.2\n&quot;</span><span class="p">,</span>
<span class="linenos">433</span><span class="w">    </span><span class="s2">&quot;log_odds = true_b0 + true_b1 * hours\n&quot;</span><span class="p">,</span>
<span class="linenos">434</span><span class="w">    </span><span class="s2">&quot;prob_pass = 1 / (1 + np.exp(-log_odds))\n&quot;</span><span class="p">,</span>
<span class="linenos">435</span><span class="w">    </span><span class="s2">&quot;passed = rng.binomial(1, prob_pass)                        # 0 = fail, 1 = pass\n&quot;</span><span class="p">,</span>
<span class="linenos">436</span><span class="w">    </span><span class="s2">&quot;\n&quot;</span><span class="p">,</span>
<span class="linenos">437</span><span class="w">    </span><span class="s2">&quot;# Fit via scikit-learn (or manual gradient descent fallback)\n&quot;</span><span class="p">,</span>
<span class="linenos">438</span><span class="w">    </span><span class="s2">&quot;if SKLEARN:\n&quot;</span><span class="p">,</span>
<span class="linenos">439</span><span class="w">    </span><span class="s2">&quot;    X_logit = hours.reshape(-1, 1)\n&quot;</span><span class="p">,</span>
<span class="linenos">440</span><span class="w">    </span><span class="s2">&quot;    clf = LogisticRegression(solver=&#39;lbfgs&#39;, C=1e6)        # large C ‚Üí minimal regularisation\n&quot;</span><span class="p">,</span>
<span class="linenos">441</span><span class="w">    </span><span class="s2">&quot;    clf.fit(X_logit, passed)\n&quot;</span><span class="p">,</span>
<span class="linenos">442</span><span class="w">    </span><span class="s2">&quot;    b0_hat = clf.intercept_[0]\n&quot;</span><span class="p">,</span>
<span class="linenos">443</span><span class="w">    </span><span class="s2">&quot;    b1_hat = clf.coef_[0, 0]\n&quot;</span><span class="p">,</span>
<span class="linenos">444</span><span class="w">    </span><span class="s2">&quot;    print(f&#39;Fitted (sklearn):   Œ≤‚ÇÄ={b0_hat:.4f}, Œ≤‚ÇÅ={b1_hat:.4f}&#39;)\n&quot;</span><span class="p">,</span>
<span class="linenos">445</span><span class="w">    </span><span class="s2">&quot;else:\n&quot;</span><span class="p">,</span>
<span class="linenos">446</span><span class="w">    </span><span class="s2">&quot;    # Manual gradient descent\n&quot;</span><span class="p">,</span>
<span class="linenos">447</span><span class="w">    </span><span class="s2">&quot;    def sigmoid(z): return 1 / (1 + np.exp(-z))\n&quot;</span><span class="p">,</span>
<span class="linenos">448</span><span class="w">    </span><span class="s2">&quot;    b0_hat, b1_hat = 0.0, 0.0\n&quot;</span><span class="p">,</span>
<span class="linenos">449</span><span class="w">    </span><span class="s2">&quot;    lr = 0.05\n&quot;</span><span class="p">,</span>
<span class="linenos">450</span><span class="w">    </span><span class="s2">&quot;    for _ in range(3000):\n&quot;</span><span class="p">,</span>
<span class="linenos">451</span><span class="w">    </span><span class="s2">&quot;        p = sigmoid(b0_hat + b1_hat * hours)\n&quot;</span><span class="p">,</span>
<span class="linenos">452</span><span class="w">    </span><span class="s2">&quot;        err = p - passed\n&quot;</span><span class="p">,</span>
<span class="linenos">453</span><span class="w">    </span><span class="s2">&quot;        b0_hat -= lr * err.mean()\n&quot;</span><span class="p">,</span>
<span class="linenos">454</span><span class="w">    </span><span class="s2">&quot;        b1_hat -= lr * (err * hours).mean()\n&quot;</span><span class="p">,</span>
<span class="linenos">455</span><span class="w">    </span><span class="s2">&quot;    print(f&#39;Fitted (grad desc): Œ≤‚ÇÄ={b0_hat:.4f}, Œ≤‚ÇÅ={b1_hat:.4f}&#39;)\n&quot;</span><span class="p">,</span>
<span class="linenos">456</span><span class="w">    </span><span class="s2">&quot;\n&quot;</span><span class="p">,</span>
<span class="linenos">457</span><span class="w">    </span><span class="s2">&quot;print(f&#39;True coefficients:  Œ≤‚ÇÄ={true_b0}, Œ≤‚ÇÅ={true_b1}&#39;)\n&quot;</span><span class="p">,</span>
<span class="linenos">458</span><span class="w">    </span><span class="s2">&quot;\n&quot;</span><span class="p">,</span>
<span class="linenos">459</span><span class="w">    </span><span class="s2">&quot;# Decision boundary: b0 + b1*x = 0  ‚Üí  x = -b0/b1\n&quot;</span><span class="p">,</span>
<span class="linenos">460</span><span class="w">    </span><span class="s2">&quot;boundary = -b0_hat / b1_hat\n&quot;</span><span class="p">,</span>
<span class="linenos">461</span><span class="w">    </span><span class="s2">&quot;print(f&#39;Decision boundary   : {boundary:.2f} hours&#39;)\n&quot;</span><span class="p">,</span>
<span class="linenos">462</span><span class="w">    </span><span class="s2">&quot;print(f&#39;Odds ratio e^Œ≤‚ÇÅ     : {np.exp(b1_hat):.4f}&#39;)\n&quot;</span><span class="p">,</span>
<span class="linenos">463</span><span class="w">    </span><span class="s2">&quot;\n&quot;</span><span class="p">,</span>
<span class="linenos">464</span><span class="w">    </span><span class="s2">&quot;# Visualise\n&quot;</span><span class="p">,</span>
<span class="linenos">465</span><span class="w">    </span><span class="s2">&quot;x_range = np.linspace(0, 10, 300)\n&quot;</span><span class="p">,</span>
<span class="linenos">466</span><span class="w">    </span><span class="s2">&quot;p_hat   = 1 / (1 + np.exp(-(b0_hat + b1_hat * x_range)))\n&quot;</span><span class="p">,</span>
<span class="linenos">467</span><span class="w">    </span><span class="s2">&quot;\n&quot;</span><span class="p">,</span>
<span class="linenos">468</span><span class="w">    </span><span class="s2">&quot;fig, axes = plt.subplots(1, 2, figsize=(12, 4))\n&quot;</span><span class="p">,</span>
<span class="linenos">469</span><span class="w">    </span><span class="s2">&quot;\n&quot;</span><span class="p">,</span>
<span class="linenos">470</span><span class="w">    </span><span class="s2">&quot;# Left: sigmoid curve\n&quot;</span><span class="p">,</span>
<span class="linenos">471</span><span class="w">    </span><span class="s2">&quot;ax = axes[0]\n&quot;</span><span class="p">,</span>
<span class="linenos">472</span><span class="w">    </span><span class="s2">&quot;ax.scatter(hours[passed == 0], passed[passed == 0], color=&#39;tomato&#39;,    alpha=0.5, s=30,\n&quot;</span><span class="p">,</span>
<span class="linenos">473</span><span class="w">    </span><span class="s2">&quot;           label=&#39;Fail (0)&#39;, zorder=3)\n&quot;</span><span class="p">,</span>
<span class="linenos">474</span><span class="w">    </span><span class="s2">&quot;ax.scatter(hours[passed == 1], passed[passed == 1], color=&#39;steelblue&#39;, alpha=0.5, s=30,\n&quot;</span><span class="p">,</span>
<span class="linenos">475</span><span class="w">    </span><span class="s2">&quot;           label=&#39;Pass (1)&#39;, zorder=3)\n&quot;</span><span class="p">,</span>
<span class="linenos">476</span><span class="w">    </span><span class="s2">&quot;ax.plot(x_range, p_hat, &#39;k-&#39;, lw=2, label=&#39;Fitted P(pass)&#39;)\n&quot;</span><span class="p">,</span>
<span class="linenos">477</span><span class="w">    </span><span class="s2">&quot;ax.axvline(boundary, color=&#39;green&#39;, ls=&#39;--&#39;, lw=1.5, label=f&#39;Boundary = {boundary:.1f}h&#39;)\n&quot;</span><span class="p">,</span>
<span class="linenos">478</span><span class="w">    </span><span class="s2">&quot;ax.axhline(0.5, color=&#39;gray&#39;, ls=&#39;:&#39;, lw=1)\n&quot;</span><span class="p">,</span>
<span class="linenos">479</span><span class="w">    </span><span class="s2">&quot;ax.set_xlabel(&#39;Hours studied&#39;)\n&quot;</span><span class="p">,</span>
<span class="linenos">480</span><span class="w">    </span><span class="s2">&quot;ax.set_ylabel(&#39;P(pass)&#39;)\n&quot;</span><span class="p">,</span>
<span class="linenos">481</span><span class="w">    </span><span class="s2">&quot;ax.set_title(&#39;Logistic Regression: Sigmoid Curve&#39;)\n&quot;</span><span class="p">,</span>
<span class="linenos">482</span><span class="w">    </span><span class="s2">&quot;ax.legend()\n&quot;</span><span class="p">,</span>
<span class="linenos">483</span><span class="w">    </span><span class="s2">&quot;\n&quot;</span><span class="p">,</span>
<span class="linenos">484</span><span class="w">    </span><span class="s2">&quot;# Right: log-odds plot\n&quot;</span><span class="p">,</span>
<span class="linenos">485</span><span class="w">    </span><span class="s2">&quot;ax = axes[1]\n&quot;</span><span class="p">,</span>
<span class="linenos">486</span><span class="w">    </span><span class="s2">&quot;ax.plot(x_range, b0_hat + b1_hat * x_range, &#39;purple&#39;, lw=2, label=&#39;Log-odds&#39;)\n&quot;</span><span class="p">,</span>
<span class="linenos">487</span><span class="w">    </span><span class="s2">&quot;ax.axhline(0, color=&#39;gray&#39;, ls=&#39;--&#39;)\n&quot;</span><span class="p">,</span>
<span class="linenos">488</span><span class="w">    </span><span class="s2">&quot;ax.axvline(boundary, color=&#39;green&#39;, ls=&#39;--&#39;, lw=1.5, label=f&#39;Boundary ({boundary:.1f}h)&#39;)\n&quot;</span><span class="p">,</span>
<span class="linenos">489</span><span class="w">    </span><span class="s2">&quot;ax.fill_between(x_range, b0_hat + b1_hat * x_range, 0,\n&quot;</span><span class="p">,</span>
<span class="linenos">490</span><span class="w">    </span><span class="s2">&quot;                where=(b0_hat + b1_hat * x_range &gt; 0), alpha=0.15, color=&#39;steelblue&#39;, label=&#39;Predict pass&#39;)\n&quot;</span><span class="p">,</span>
<span class="linenos">491</span><span class="w">    </span><span class="s2">&quot;ax.fill_between(x_range, b0_hat + b1_hat * x_range, 0,\n&quot;</span><span class="p">,</span>
<span class="linenos">492</span><span class="w">    </span><span class="s2">&quot;                where=(b0_hat + b1_hat * x_range &lt; 0), alpha=0.15, color=&#39;tomato&#39;, label=&#39;Predict fail&#39;)\n&quot;</span><span class="p">,</span>
<span class="linenos">493</span><span class="w">    </span><span class="s2">&quot;ax.set_xlabel(&#39;Hours studied&#39;)\n&quot;</span><span class="p">,</span>
<span class="linenos">494</span><span class="w">    </span><span class="s2">&quot;ax.set_ylabel(&#39;Log-odds&#39;)\n&quot;</span><span class="p">,</span>
<span class="linenos">495</span><span class="w">    </span><span class="s2">&quot;ax.set_title(&#39;Log-odds and Decision Boundary&#39;)\n&quot;</span><span class="p">,</span>
<span class="linenos">496</span><span class="w">    </span><span class="s2">&quot;ax.legend(fontsize=9)\n&quot;</span><span class="p">,</span>
<span class="linenos">497</span><span class="w">    </span><span class="s2">&quot;\n&quot;</span><span class="p">,</span>
<span class="linenos">498</span><span class="w">    </span><span class="s2">&quot;plt.tight_layout()\n&quot;</span><span class="p">,</span>
<span class="linenos">499</span><span class="w">    </span><span class="s2">&quot;plt.show()&quot;</span>
<span class="linenos">500</span><span class="w">   </span><span class="p">]</span>
<span class="linenos">501</span><span class="w">  </span><span class="p">},</span>
<span class="linenos">502</span><span class="w">  </span><span class="p">{</span>
<span class="linenos">503</span><span class="w">   </span><span class="nt">&quot;cell_type&quot;</span><span class="p">:</span><span class="w"> </span><span class="s2">&quot;markdown&quot;</span><span class="p">,</span>
<span class="linenos">504</span><span class="w">   </span><span class="nt">&quot;id&quot;</span><span class="p">:</span><span class="w"> </span><span class="s2">&quot;b2c3d4e5-0013-4000-8000-000000000013&quot;</span><span class="p">,</span>
<span class="linenos">505</span><span class="w">   </span><span class="nt">&quot;metadata&quot;</span><span class="p">:</span><span class="w"> </span><span class="p">{},</span>
<span class="linenos">506</span><span class="w">   </span><span class="nt">&quot;source&quot;</span><span class="p">:</span><span class="w"> </span><span class="p">[</span>
<span class="linenos">507</span><span class="w">    </span><span class="s2">&quot;## 6. Checking Regression Assumptions\n&quot;</span><span class="p">,</span>
<span class="linenos">508</span><span class="w">    </span><span class="s2">&quot;\n&quot;</span><span class="p">,</span>
<span class="linenos">509</span><span class="w">    </span><span class="s2">&quot;OLS inference is valid only when the Gauss-Markov assumptions hold. Violations affect different inferential properties:\n&quot;</span><span class="p">,</span>
<span class="linenos">510</span><span class="w">    </span><span class="s2">&quot;\n&quot;</span><span class="p">,</span>
<span class="linenos">511</span><span class="w">    </span><span class="s2">&quot;| Assumption | Consequence if violated | Diagnostic |\n&quot;</span><span class="p">,</span>
<span class="linenos">512</span><span class="w">    </span><span class="s2">&quot;|------------|------------------------|------------|\n&quot;</span><span class="p">,</span>
<span class="linenos">513</span><span class="w">    </span><span class="s2">&quot;| **Linearity** | Biased estimates | Residuals vs Fitted ‚Äî look for curvature |\n&quot;</span><span class="p">,</span>
<span class="linenos">514</span><span class="w">    </span><span class="s2">&quot;| **Independence** | Underestimated SEs | Durbin-Watson test (time-series) |\n&quot;</span><span class="p">,</span>
<span class="linenos">515</span><span class="w">    </span><span class="s2">&quot;| **Homoscedasticity** | Inefficient estimates, invalid SEs | Scale-Location plot, Breusch-Pagan |\n&quot;</span><span class="p">,</span>
<span class="linenos">516</span><span class="w">    </span><span class="s2">&quot;| **Normality of errors** | Invalid t/F intervals (small n) | Q-Q plot, Shapiro-Wilk test |\n&quot;</span><span class="p">,</span>
<span class="linenos">517</span><span class="w">    </span><span class="s2">&quot;| **No perfect multicollinearity** | Singular (X&#39;X), unreliable estimates | VIF &gt; 10 is problematic |\n&quot;</span><span class="p">,</span>
<span class="linenos">518</span><span class="w">    </span><span class="s2">&quot;\n&quot;</span><span class="p">,</span>
<span class="linenos">519</span><span class="w">    </span><span class="s2">&quot;Below we deliberately create two datasets ‚Äî one that satisfies assumptions and one that violates **homoscedasticity** (heteroscedastic errors) ‚Äî to compare the diagnostic plots.&quot;</span>
<span class="linenos">520</span><span class="w">   </span><span class="p">]</span>
<span class="linenos">521</span><span class="w">  </span><span class="p">},</span>
<span class="linenos">522</span><span class="w">  </span><span class="p">{</span>
<span class="linenos">523</span><span class="w">   </span><span class="nt">&quot;cell_type&quot;</span><span class="p">:</span><span class="w"> </span><span class="s2">&quot;code&quot;</span><span class="p">,</span>
<span class="linenos">524</span><span class="w">   </span><span class="nt">&quot;execution_count&quot;</span><span class="p">:</span><span class="w"> </span><span class="kc">null</span><span class="p">,</span>
<span class="linenos">525</span><span class="w">   </span><span class="nt">&quot;id&quot;</span><span class="p">:</span><span class="w"> </span><span class="s2">&quot;b2c3d4e5-0014-4000-8000-000000000014&quot;</span><span class="p">,</span>
<span class="linenos">526</span><span class="w">   </span><span class="nt">&quot;metadata&quot;</span><span class="p">:</span><span class="w"> </span><span class="p">{},</span>
<span class="linenos">527</span><span class="w">   </span><span class="nt">&quot;outputs&quot;</span><span class="p">:</span><span class="w"> </span><span class="p">[],</span>
<span class="linenos">528</span><span class="w">   </span><span class="nt">&quot;source&quot;</span><span class="p">:</span><span class="w"> </span><span class="p">[</span>
<span class="linenos">529</span><span class="w">    </span><span class="s2">&quot;n_diag = 100\n&quot;</span><span class="p">,</span>
<span class="linenos">530</span><span class="w">    </span><span class="s2">&quot;x_diag = rng.uniform(1, 10, n_diag)\n&quot;</span><span class="p">,</span>
<span class="linenos">531</span><span class="w">    </span><span class="s2">&quot;\n&quot;</span><span class="p">,</span>
<span class="linenos">532</span><span class="w">    </span><span class="s2">&quot;# --- Dataset A: well-behaved (homoscedastic, normal errors)\n&quot;</span><span class="p">,</span>
<span class="linenos">533</span><span class="w">    </span><span class="s2">&quot;y_good = 3 + 2 * x_diag + rng.normal(0, 1.5, n_diag)\n&quot;</span><span class="p">,</span>
<span class="linenos">534</span><span class="w">    </span><span class="s2">&quot;\n&quot;</span><span class="p">,</span>
<span class="linenos">535</span><span class="w">    </span><span class="s2">&quot;# --- Dataset B: heteroscedastic (variance grows with x)\n&quot;</span><span class="p">,</span>
<span class="linenos">536</span><span class="w">    </span><span class="s2">&quot;y_hetero = 3 + 2 * x_diag + rng.normal(0, 0.5 * x_diag, n_diag)\n&quot;</span><span class="p">,</span>
<span class="linenos">537</span><span class="w">    </span><span class="s2">&quot;\n&quot;</span><span class="p">,</span>
<span class="linenos">538</span><span class="w">    </span><span class="s2">&quot;def fit_ols_1d(x, y):\n&quot;</span><span class="p">,</span>
<span class="linenos">539</span><span class="w">    </span><span class="s2">&quot;    slope, intercept, *_ = stats.linregress(x, y)\n&quot;</span><span class="p">,</span>
<span class="linenos">540</span><span class="w">    </span><span class="s2">&quot;    y_hat = intercept + slope * x\n&quot;</span><span class="p">,</span>
<span class="linenos">541</span><span class="w">    </span><span class="s2">&quot;    resid = y - y_hat\n&quot;</span><span class="p">,</span>
<span class="linenos">542</span><span class="w">    </span><span class="s2">&quot;    s = np.sqrt((resid**2).sum() / (len(x) - 2))\n&quot;</span><span class="p">,</span>
<span class="linenos">543</span><span class="w">    </span><span class="s2">&quot;    return y_hat, resid, s\n&quot;</span><span class="p">,</span>
<span class="linenos">544</span><span class="w">    </span><span class="s2">&quot;\n&quot;</span><span class="p">,</span>
<span class="linenos">545</span><span class="w">    </span><span class="s2">&quot;y_hat_good,   resid_good,   s_good   = fit_ols_1d(x_diag, y_good)\n&quot;</span><span class="p">,</span>
<span class="linenos">546</span><span class="w">    </span><span class="s2">&quot;y_hat_hetero, resid_hetero, s_hetero = fit_ols_1d(x_diag, y_hetero)\n&quot;</span><span class="p">,</span>
<span class="linenos">547</span><span class="w">    </span><span class="s2">&quot;\n&quot;</span><span class="p">,</span>
<span class="linenos">548</span><span class="w">    </span><span class="s2">&quot;def add_lowess_line(ax, x, y, frac=0.5, color=&#39;red&#39;):\n&quot;</span><span class="p">,</span>
<span class="linenos">549</span><span class="w">    </span><span class="s2">&quot;    \&quot;\&quot;\&quot;Simple moving average as a smoother (no statsmodels dependency).\&quot;\&quot;\&quot;\n&quot;</span><span class="p">,</span>
<span class="linenos">550</span><span class="w">    </span><span class="s2">&quot;    idx    = np.argsort(x)\n&quot;</span><span class="p">,</span>
<span class="linenos">551</span><span class="w">    </span><span class="s2">&quot;    x_s, y_s = x[idx], y[idx]\n&quot;</span><span class="p">,</span>
<span class="linenos">552</span><span class="w">    </span><span class="s2">&quot;    w = max(1, int(frac * len(x)))\n&quot;</span><span class="p">,</span>
<span class="linenos">553</span><span class="w">    </span><span class="s2">&quot;    y_smooth = np.convolve(y_s, np.ones(w)/w, mode=&#39;same&#39;)\n&quot;</span><span class="p">,</span>
<span class="linenos">554</span><span class="w">    </span><span class="s2">&quot;    ax.plot(x_s, y_smooth, color=color, lw=1.5, ls=&#39;--&#39;)\n&quot;</span><span class="p">,</span>
<span class="linenos">555</span><span class="w">    </span><span class="s2">&quot;\n&quot;</span><span class="p">,</span>
<span class="linenos">556</span><span class="w">    </span><span class="s2">&quot;fig, axes = plt.subplots(2, 3, figsize=(13, 8))\n&quot;</span><span class="p">,</span>
<span class="linenos">557</span><span class="w">    </span><span class="s2">&quot;titles_row = [&#39;Homoscedastic (well-behaved)&#39;, &#39;Heteroscedastic (variance grows with x)&#39;]\n&quot;</span><span class="p">,</span>
<span class="linenos">558</span><span class="w">    </span><span class="s2">&quot;\n&quot;</span><span class="p">,</span>
<span class="linenos">559</span><span class="w">    </span><span class="s2">&quot;for row, (x, y, y_hat, resid, s, label) in enumerate([\n&quot;</span><span class="p">,</span>
<span class="linenos">560</span><span class="w">    </span><span class="s2">&quot;        (x_diag, y_good,   y_hat_good,   resid_good,   s_good,   &#39;A: Well-behaved&#39;),\n&quot;</span><span class="p">,</span>
<span class="linenos">561</span><span class="w">    </span><span class="s2">&quot;        (x_diag, y_hetero, y_hat_hetero, resid_hetero, s_hetero, &#39;B: Heteroscedastic&#39;)]):\n&quot;</span><span class="p">,</span>
<span class="linenos">562</span><span class="w">    </span><span class="s2">&quot;\n&quot;</span><span class="p">,</span>
<span class="linenos">563</span><span class="w">    </span><span class="s2">&quot;    std_resid = resid / s\n&quot;</span><span class="p">,</span>
<span class="linenos">564</span><span class="w">    </span><span class="s2">&quot;\n&quot;</span><span class="p">,</span>
<span class="linenos">565</span><span class="w">    </span><span class="s2">&quot;    # (1) Residuals vs Fitted\n&quot;</span><span class="p">,</span>
<span class="linenos">566</span><span class="w">    </span><span class="s2">&quot;    ax = axes[row, 0]\n&quot;</span><span class="p">,</span>
<span class="linenos">567</span><span class="w">    </span><span class="s2">&quot;    ax.scatter(y_hat, resid, alpha=0.5, color=&#39;steelblue&#39;, s=20)\n&quot;</span><span class="p">,</span>
<span class="linenos">568</span><span class="w">    </span><span class="s2">&quot;    ax.axhline(0, color=&#39;red&#39;, lw=1.5, ls=&#39;--&#39;)\n&quot;</span><span class="p">,</span>
<span class="linenos">569</span><span class="w">    </span><span class="s2">&quot;    add_lowess_line(ax, y_hat, resid)\n&quot;</span><span class="p">,</span>
<span class="linenos">570</span><span class="w">    </span><span class="s2">&quot;    ax.set_xlabel(&#39;Fitted&#39;); ax.set_ylabel(&#39;Residuals&#39;)\n&quot;</span><span class="p">,</span>
<span class="linenos">571</span><span class="w">    </span><span class="s2">&quot;    ax.set_title(f&#39;{label}\\nResiduals vs Fitted&#39;)\n&quot;</span><span class="p">,</span>
<span class="linenos">572</span><span class="w">    </span><span class="s2">&quot;\n&quot;</span><span class="p">,</span>
<span class="linenos">573</span><span class="w">    </span><span class="s2">&quot;    # (2) Q-Q plot\n&quot;</span><span class="p">,</span>
<span class="linenos">574</span><span class="w">    </span><span class="s2">&quot;    ax = axes[row, 1]\n&quot;</span><span class="p">,</span>
<span class="linenos">575</span><span class="w">    </span><span class="s2">&quot;    (osm, osr), (sq, iq, rq) = stats.probplot(resid, dist=&#39;norm&#39;)\n&quot;</span><span class="p">,</span>
<span class="linenos">576</span><span class="w">    </span><span class="s2">&quot;    ax.scatter(osm, osr, alpha=0.5, color=&#39;steelblue&#39;, s=20)\n&quot;</span><span class="p">,</span>
<span class="linenos">577</span><span class="w">    </span><span class="s2">&quot;    ax.plot(osm, sq * np.array(osm) + iq, &#39;r-&#39;, lw=1.5)\n&quot;</span><span class="p">,</span>
<span class="linenos">578</span><span class="w">    </span><span class="s2">&quot;    ax.set_xlabel(&#39;Theoretical quantiles&#39;); ax.set_ylabel(&#39;Sample quantiles&#39;)\n&quot;</span><span class="p">,</span>
<span class="linenos">579</span><span class="w">    </span><span class="s2">&quot;    ax.set_title(&#39;Normal Q-Q&#39;)\n&quot;</span><span class="p">,</span>
<span class="linenos">580</span><span class="w">    </span><span class="s2">&quot;\n&quot;</span><span class="p">,</span>
<span class="linenos">581</span><span class="w">    </span><span class="s2">&quot;    # (3) Scale-Location\n&quot;</span><span class="p">,</span>
<span class="linenos">582</span><span class="w">    </span><span class="s2">&quot;    ax = axes[row, 2]\n&quot;</span><span class="p">,</span>
<span class="linenos">583</span><span class="w">    </span><span class="s2">&quot;    ax.scatter(y_hat, np.sqrt(np.abs(std_resid)), alpha=0.5, color=&#39;steelblue&#39;, s=20)\n&quot;</span><span class="p">,</span>
<span class="linenos">584</span><span class="w">    </span><span class="s2">&quot;    add_lowess_line(ax, y_hat, np.sqrt(np.abs(std_resid)))\n&quot;</span><span class="p">,</span>
<span class="linenos">585</span><span class="w">    </span><span class="s2">&quot;    ax.set_xlabel(&#39;Fitted&#39;); ax.set_ylabel(&#39;‚àö|Std residuals|&#39;)\n&quot;</span><span class="p">,</span>
<span class="linenos">586</span><span class="w">    </span><span class="s2">&quot;    ax.set_title(&#39;Scale-Location&#39;)\n&quot;</span><span class="p">,</span>
<span class="linenos">587</span><span class="w">    </span><span class="s2">&quot;\n&quot;</span><span class="p">,</span>
<span class="linenos">588</span><span class="w">    </span><span class="s2">&quot;    # Breusch-Pagan-like test: correlation between |residuals| and fitted values\n&quot;</span><span class="p">,</span>
<span class="linenos">589</span><span class="w">    </span><span class="s2">&quot;    corr, bp_p = stats.pearsonr(y_hat, np.abs(resid))\n&quot;</span><span class="p">,</span>
<span class="linenos">590</span><span class="w">    </span><span class="s2">&quot;    print(f&#39;{label}: |residual| ~ fitted  r={corr:.3f}, p={bp_p:.4f}&#39;)\n&quot;</span><span class="p">,</span>
<span class="linenos">591</span><span class="w">    </span><span class="s2">&quot;\n&quot;</span><span class="p">,</span>
<span class="linenos">592</span><span class="w">    </span><span class="s2">&quot;plt.suptitle(&#39;Regression Diagnostic Plots: Well-behaved vs Heteroscedastic&#39;, fontsize=13)\n&quot;</span><span class="p">,</span>
<span class="linenos">593</span><span class="w">    </span><span class="s2">&quot;plt.tight_layout()\n&quot;</span><span class="p">,</span>
<span class="linenos">594</span><span class="w">    </span><span class="s2">&quot;plt.show()&quot;</span>
<span class="linenos">595</span><span class="w">   </span><span class="p">]</span>
<span class="linenos">596</span><span class="w">  </span><span class="p">},</span>
<span class="linenos">597</span><span class="w">  </span><span class="p">{</span>
<span class="linenos">598</span><span class="w">   </span><span class="nt">&quot;cell_type&quot;</span><span class="p">:</span><span class="w"> </span><span class="s2">&quot;markdown&quot;</span><span class="p">,</span>
<span class="linenos">599</span><span class="w">   </span><span class="nt">&quot;id&quot;</span><span class="p">:</span><span class="w"> </span><span class="s2">&quot;b2c3d4e5-0015-4000-8000-000000000015&quot;</span><span class="p">,</span>
<span class="linenos">600</span><span class="w">   </span><span class="nt">&quot;metadata&quot;</span><span class="p">:</span><span class="w"> </span><span class="p">{},</span>
<span class="linenos">601</span><span class="w">   </span><span class="nt">&quot;source&quot;</span><span class="p">:</span><span class="w"> </span><span class="p">[</span>
<span class="linenos">602</span><span class="w">    </span><span class="s2">&quot;## Summary\n&quot;</span><span class="p">,</span>
<span class="linenos">603</span><span class="w">    </span><span class="s2">&quot;\n&quot;</span><span class="p">,</span>
<span class="linenos">604</span><span class="w">    </span><span class="s2">&quot;### Regression Model Comparison\n&quot;</span><span class="p">,</span>
<span class="linenos">605</span><span class="w">    </span><span class="s2">&quot;\n&quot;</span><span class="p">,</span>
<span class="linenos">606</span><span class="w">    </span><span class="s2">&quot;| Model | Response | Key idea | Fitting method | Interpretation |\n&quot;</span><span class="p">,</span>
<span class="linenos">607</span><span class="w">    </span><span class="s2">&quot;|-------|----------|----------|----------------|----------------|\n&quot;</span><span class="p">,</span>
<span class="linenos">608</span><span class="w">    </span><span class="s2">&quot;| Simple linear | Continuous | One predictor | OLS | Œ≤‚ÇÅ: change in ≈∑ per unit x |\n&quot;</span><span class="p">,</span>
<span class="linenos">609</span><span class="w">    </span><span class="s2">&quot;| Multiple linear | Continuous | Multiple predictors | OLS | Œ≤_j: partial effect of x_j |\n&quot;</span><span class="p">,</span>
<span class="linenos">610</span><span class="w">    </span><span class="s2">&quot;| Polynomial | Continuous | Non-linear with OLS | OLS in transformed space | Compare adj-R¬≤ across degrees |\n&quot;</span><span class="p">,</span>
<span class="linenos">611</span><span class="w">    </span><span class="s2">&quot;| Logistic | Binary (0/1) | Log-odds linear in x | MLE (iterative) | e^Œ≤_j: odds ratio |\n&quot;</span><span class="p">,</span>
<span class="linenos">612</span><span class="w">    </span><span class="s2">&quot;\n&quot;</span><span class="p">,</span>
<span class="linenos">613</span><span class="w">    </span><span class="s2">&quot;### Workflow for Any Regression Problem\n&quot;</span><span class="p">,</span>
<span class="linenos">614</span><span class="w">    </span><span class="s2">&quot;\n&quot;</span><span class="p">,</span>
<span class="linenos">615</span><span class="w">    </span><span class="s2">&quot;1. **Explore** ‚Äî scatter plots, correlation matrix, check for outliers\n&quot;</span><span class="p">,</span>
<span class="linenos">616</span><span class="w">    </span><span class="s2">&quot;2. **Fit** ‚Äî choose model family (linear / polynomial / logistic / etc.)\n&quot;</span><span class="p">,</span>
<span class="linenos">617</span><span class="w">    </span><span class="s2">&quot;3. **Evaluate** ‚Äî R¬≤, adjusted R¬≤, AIC/BIC, cross-validation MSE\n&quot;</span><span class="p">,</span>
<span class="linenos">618</span><span class="w">    </span><span class="s2">&quot;4. **Diagnose** ‚Äî residual plots, Q-Q plot, scale-location, leverage\n&quot;</span><span class="p">,</span>
<span class="linenos">619</span><span class="w">    </span><span class="s2">&quot;5. **Iterate** ‚Äî transform variables, add/remove predictors, consider regularisation (Ridge/Lasso)\n&quot;</span><span class="p">,</span>
<span class="linenos">620</span><span class="w">    </span><span class="s2">&quot;6. **Report** ‚Äî coefficient estimates, confidence intervals, effect sizes\n&quot;</span><span class="p">,</span>
<span class="linenos">621</span><span class="w">    </span><span class="s2">&quot;\n&quot;</span><span class="p">,</span>
<span class="linenos">622</span><span class="w">    </span><span class="s2">&quot;### Extensions to Explore\n&quot;</span><span class="p">,</span>
<span class="linenos">623</span><span class="w">    </span><span class="s2">&quot;- **Regularised regression**: Ridge (L2), Lasso (L1), ElasticNet ‚Äî handle multicollinearity and feature selection\n&quot;</span><span class="p">,</span>
<span class="linenos">624</span><span class="w">    </span><span class="s2">&quot;- **Generalised Linear Models (GLM)**: Poisson (count data), Negative Binomial, Gamma ‚Äî when errors are non-normal\n&quot;</span><span class="p">,</span>
<span class="linenos">625</span><span class="w">    </span><span class="s2">&quot;- **Mixed-effects models**: repeated measures and nested data structures\n&quot;</span><span class="p">,</span>
<span class="linenos">626</span><span class="w">    </span><span class="s2">&quot;- **Robust regression**: Huber loss ‚Äî reduces sensitivity to outliers\n&quot;</span><span class="p">,</span>
<span class="linenos">627</span><span class="w">    </span><span class="s2">&quot;- **Non-parametric regression**: Kernel regression, GAM ‚Äî flexible functional forms&quot;</span>
<span class="linenos">628</span><span class="w">   </span><span class="p">]</span>
<span class="linenos">629</span><span class="w">  </span><span class="p">}</span>
<span class="linenos">630</span><span class="w"> </span><span class="p">],</span>
<span class="linenos">631</span><span class="w"> </span><span class="nt">&quot;metadata&quot;</span><span class="p">:</span><span class="w"> </span><span class="p">{</span>
<span class="linenos">632</span><span class="w">  </span><span class="nt">&quot;kernelspec&quot;</span><span class="p">:</span><span class="w"> </span><span class="p">{</span>
<span class="linenos">633</span><span class="w">   </span><span class="nt">&quot;display_name&quot;</span><span class="p">:</span><span class="w"> </span><span class="s2">&quot;Python 3&quot;</span><span class="p">,</span>
<span class="linenos">634</span><span class="w">   </span><span class="nt">&quot;language&quot;</span><span class="p">:</span><span class="w"> </span><span class="s2">&quot;python&quot;</span><span class="p">,</span>
<span class="linenos">635</span><span class="w">   </span><span class="nt">&quot;name&quot;</span><span class="p">:</span><span class="w"> </span><span class="s2">&quot;python3&quot;</span>
<span class="linenos">636</span><span class="w">  </span><span class="p">},</span>
<span class="linenos">637</span><span class="w">  </span><span class="nt">&quot;language_info&quot;</span><span class="p">:</span><span class="w"> </span><span class="p">{</span>
<span class="linenos">638</span><span class="w">   </span><span class="nt">&quot;name&quot;</span><span class="p">:</span><span class="w"> </span><span class="s2">&quot;python&quot;</span><span class="p">,</span>
<span class="linenos">639</span><span class="w">   </span><span class="nt">&quot;version&quot;</span><span class="p">:</span><span class="w"> </span><span class="s2">&quot;3.11.0&quot;</span>
<span class="linenos">640</span><span class="w">  </span><span class="p">}</span>
<span class="linenos">641</span><span class="w"> </span><span class="p">},</span>
<span class="linenos">642</span><span class="w"> </span><span class="nt">&quot;nbformat&quot;</span><span class="p">:</span><span class="w"> </span><span class="mi">4</span><span class="p">,</span>
<span class="linenos">643</span><span class="w"> </span><span class="nt">&quot;nbformat_minor&quot;</span><span class="p">:</span><span class="w"> </span><span class="mi">5</span>
<span class="linenos">644</span><span class="p">}</span>
</code></pre></div>

    </div>
</article>

            </div>
        </main>
    </div>

    <script src="/study/static/js/app.js"></script>
    <script>
        function switchLanguage(newLang) {
            const path = window.location.pathname;
            const base = '/study';
            const currentLang = 'en';
            const newPath = path.replace(base + '/' + currentLang + '/', base + '/' + newLang + '/');
            window.location.href = newPath + window.location.search;
        }
    </script>
    
<script>
    document.getElementById('copy-code-btn').addEventListener('click', function() {
        var code = document.querySelector('.example-code .highlight');
        // Extract just the code text, not line numbers
        var lines = code.querySelectorAll('.code pre span.line');
        var text;
        if (lines.length > 0) {
            text = Array.from(lines).map(function(l) { return l.textContent; }).join('\n');
        } else {
            // Fallback: get all code text
            var codeEl = code.querySelector('pre code') || code.querySelector('pre');
            text = codeEl ? codeEl.textContent : code.textContent;
        }
        navigator.clipboard.writeText(text);
        this.textContent = 'Copied!';
        var btn = this;
        setTimeout(function() { btn.textContent = 'Copy code'; }, 2000);
    });
</script>

</body>
</html>
{% endraw %}