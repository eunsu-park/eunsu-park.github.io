{% raw %}
<!DOCTYPE html>
<html lang="en" data-theme="light">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>12_regression_analysis.ipynb - Examples</title>
    <link rel="stylesheet" href="/study/static/css/style.css">
    <link rel="stylesheet" href="/study/static/css/highlight.css">
    <!-- KaTeX for LaTeX math rendering -->
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.11/dist/katex.min.css">
    <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.11/dist/katex.min.js"></script>
    <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.11/dist/contrib/auto-render.min.js"
        onload="renderMathInElement(document.body, {
            delimiters: [
                {left: '$$', right: '$$', display: true},
                {left: '$', right: '$', display: false}
            ],
            throwOnError: false
        });"></script>
    
</head>
<body>
    <div class="app-container">
        <!-- Sidebar -->
        <aside class="sidebar" id="sidebar">
            <div class="sidebar-header">
                <a href="/study/en/" class="logo">Study Materials</a>
                <button class="sidebar-toggle" id="sidebar-toggle" aria-label="Toggle sidebar">
                    <span></span>
                </button>
            </div>

            <nav class="sidebar-nav">
                <a href="/study/en/" class="nav-item ">
                    <span class="nav-icon">üè†</span>
                    <span class="nav-text">Home</span>
                </a>
                <a href="/study/examples/" class="nav-item active">
                    <span class="nav-icon">üíª</span>
                    <span class="nav-text">Examples</span>
                </a>
            </nav>

            <div class="sidebar-search">
                <form action="/study/en/search.html" method="get" id="search-form">
                    <input type="search" name="q" placeholder="Search..." id="search-sidebar-input">
                </form>
            </div>

            <!-- Language Selector -->
            <div class="sidebar-lang">
                <select id="lang-select" class="lang-selector" onchange="switchLanguage(this.value)">
                    
                    <option value="en" selected>
                        English
                    </option>
                    
                    <option value="ko" >
                        ÌïúÍµ≠Ïñ¥
                    </option>
                    
                </select>
            </div>

            <div class="sidebar-footer">
                <button class="theme-toggle" id="theme-toggle" aria-label="Toggle theme">
                    <span class="theme-icon light">‚òÄÔ∏è</span>
                    <span class="theme-icon dark">üåô</span>
                </button>
            </div>
        </aside>

        <!-- Main Content -->
        <main class="main-content">
            <header class="main-header">
                <button class="menu-toggle" id="menu-toggle" aria-label="Open menu">
                    <span></span>
                </button>
                
<nav class="breadcrumb">
    <a href="/study/examples/">Examples</a>
    <span class="separator">/</span>
    <a href="/study/examples/Data_Science/">Data Science</a>
    <span class="separator">/</span>
    <span class="current">12_regression_analysis.ipynb</span>
</nav>

            </header>

            <div class="content">
                
<article class="example-article">
    <header class="example-header">
        <h1>12_regression_analysis.ipynb</h1>
        <div class="example-actions">
            <a href="12_regression_analysis.ipynb" download class="btn">Download</a>
            <button class="btn" id="copy-code-btn">Copy code</button>
        </div>
    </header>

    <div class="example-meta">
        <span>json</span>
        <span>645 lines</span>
        <span>28.2 KB</span>
    </div>

    <div class="example-code markdown-body">
        <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span class="normal">  1</span>
<span class="normal">  2</span>
<span class="normal">  3</span>
<span class="normal">  4</span>
<span class="normal">  5</span>
<span class="normal">  6</span>
<span class="normal">  7</span>
<span class="normal">  8</span>
<span class="normal">  9</span>
<span class="normal"> 10</span>
<span class="normal"> 11</span>
<span class="normal"> 12</span>
<span class="normal"> 13</span>
<span class="normal"> 14</span>
<span class="normal"> 15</span>
<span class="normal"> 16</span>
<span class="normal"> 17</span>
<span class="normal"> 18</span>
<span class="normal"> 19</span>
<span class="normal"> 20</span>
<span class="normal"> 21</span>
<span class="normal"> 22</span>
<span class="normal"> 23</span>
<span class="normal"> 24</span>
<span class="normal"> 25</span>
<span class="normal"> 26</span>
<span class="normal"> 27</span>
<span class="normal"> 28</span>
<span class="normal"> 29</span>
<span class="normal"> 30</span>
<span class="normal"> 31</span>
<span class="normal"> 32</span>
<span class="normal"> 33</span>
<span class="normal"> 34</span>
<span class="normal"> 35</span>
<span class="normal"> 36</span>
<span class="normal"> 37</span>
<span class="normal"> 38</span>
<span class="normal"> 39</span>
<span class="normal"> 40</span>
<span class="normal"> 41</span>
<span class="normal"> 42</span>
<span class="normal"> 43</span>
<span class="normal"> 44</span>
<span class="normal"> 45</span>
<span class="normal"> 46</span>
<span class="normal"> 47</span>
<span class="normal"> 48</span>
<span class="normal"> 49</span>
<span class="normal"> 50</span>
<span class="normal"> 51</span>
<span class="normal"> 52</span>
<span class="normal"> 53</span>
<span class="normal"> 54</span>
<span class="normal"> 55</span>
<span class="normal"> 56</span>
<span class="normal"> 57</span>
<span class="normal"> 58</span>
<span class="normal"> 59</span>
<span class="normal"> 60</span>
<span class="normal"> 61</span>
<span class="normal"> 62</span>
<span class="normal"> 63</span>
<span class="normal"> 64</span>
<span class="normal"> 65</span>
<span class="normal"> 66</span>
<span class="normal"> 67</span>
<span class="normal"> 68</span>
<span class="normal"> 69</span>
<span class="normal"> 70</span>
<span class="normal"> 71</span>
<span class="normal"> 72</span>
<span class="normal"> 73</span>
<span class="normal"> 74</span>
<span class="normal"> 75</span>
<span class="normal"> 76</span>
<span class="normal"> 77</span>
<span class="normal"> 78</span>
<span class="normal"> 79</span>
<span class="normal"> 80</span>
<span class="normal"> 81</span>
<span class="normal"> 82</span>
<span class="normal"> 83</span>
<span class="normal"> 84</span>
<span class="normal"> 85</span>
<span class="normal"> 86</span>
<span class="normal"> 87</span>
<span class="normal"> 88</span>
<span class="normal"> 89</span>
<span class="normal"> 90</span>
<span class="normal"> 91</span>
<span class="normal"> 92</span>
<span class="normal"> 93</span>
<span class="normal"> 94</span>
<span class="normal"> 95</span>
<span class="normal"> 96</span>
<span class="normal"> 97</span>
<span class="normal"> 98</span>
<span class="normal"> 99</span>
<span class="normal">100</span>
<span class="normal">101</span>
<span class="normal">102</span>
<span class="normal">103</span>
<span class="normal">104</span>
<span class="normal">105</span>
<span class="normal">106</span>
<span class="normal">107</span>
<span class="normal">108</span>
<span class="normal">109</span>
<span class="normal">110</span>
<span class="normal">111</span>
<span class="normal">112</span>
<span class="normal">113</span>
<span class="normal">114</span>
<span class="normal">115</span>
<span class="normal">116</span>
<span class="normal">117</span>
<span class="normal">118</span>
<span class="normal">119</span>
<span class="normal">120</span>
<span class="normal">121</span>
<span class="normal">122</span>
<span class="normal">123</span>
<span class="normal">124</span>
<span class="normal">125</span>
<span class="normal">126</span>
<span class="normal">127</span>
<span class="normal">128</span>
<span class="normal">129</span>
<span class="normal">130</span>
<span class="normal">131</span>
<span class="normal">132</span>
<span class="normal">133</span>
<span class="normal">134</span>
<span class="normal">135</span>
<span class="normal">136</span>
<span class="normal">137</span>
<span class="normal">138</span>
<span class="normal">139</span>
<span class="normal">140</span>
<span class="normal">141</span>
<span class="normal">142</span>
<span class="normal">143</span>
<span class="normal">144</span>
<span class="normal">145</span>
<span class="normal">146</span>
<span class="normal">147</span>
<span class="normal">148</span>
<span class="normal">149</span>
<span class="normal">150</span>
<span class="normal">151</span>
<span class="normal">152</span>
<span class="normal">153</span>
<span class="normal">154</span>
<span class="normal">155</span>
<span class="normal">156</span>
<span class="normal">157</span>
<span class="normal">158</span>
<span class="normal">159</span>
<span class="normal">160</span>
<span class="normal">161</span>
<span class="normal">162</span>
<span class="normal">163</span>
<span class="normal">164</span>
<span class="normal">165</span>
<span class="normal">166</span>
<span class="normal">167</span>
<span class="normal">168</span>
<span class="normal">169</span>
<span class="normal">170</span>
<span class="normal">171</span>
<span class="normal">172</span>
<span class="normal">173</span>
<span class="normal">174</span>
<span class="normal">175</span>
<span class="normal">176</span>
<span class="normal">177</span>
<span class="normal">178</span>
<span class="normal">179</span>
<span class="normal">180</span>
<span class="normal">181</span>
<span class="normal">182</span>
<span class="normal">183</span>
<span class="normal">184</span>
<span class="normal">185</span>
<span class="normal">186</span>
<span class="normal">187</span>
<span class="normal">188</span>
<span class="normal">189</span>
<span class="normal">190</span>
<span class="normal">191</span>
<span class="normal">192</span>
<span class="normal">193</span>
<span class="normal">194</span>
<span class="normal">195</span>
<span class="normal">196</span>
<span class="normal">197</span>
<span class="normal">198</span>
<span class="normal">199</span>
<span class="normal">200</span>
<span class="normal">201</span>
<span class="normal">202</span>
<span class="normal">203</span>
<span class="normal">204</span>
<span class="normal">205</span>
<span class="normal">206</span>
<span class="normal">207</span>
<span class="normal">208</span>
<span class="normal">209</span>
<span class="normal">210</span>
<span class="normal">211</span>
<span class="normal">212</span>
<span class="normal">213</span>
<span class="normal">214</span>
<span class="normal">215</span>
<span class="normal">216</span>
<span class="normal">217</span>
<span class="normal">218</span>
<span class="normal">219</span>
<span class="normal">220</span>
<span class="normal">221</span>
<span class="normal">222</span>
<span class="normal">223</span>
<span class="normal">224</span>
<span class="normal">225</span>
<span class="normal">226</span>
<span class="normal">227</span>
<span class="normal">228</span>
<span class="normal">229</span>
<span class="normal">230</span>
<span class="normal">231</span>
<span class="normal">232</span>
<span class="normal">233</span>
<span class="normal">234</span>
<span class="normal">235</span>
<span class="normal">236</span>
<span class="normal">237</span>
<span class="normal">238</span>
<span class="normal">239</span>
<span class="normal">240</span>
<span class="normal">241</span>
<span class="normal">242</span>
<span class="normal">243</span>
<span class="normal">244</span>
<span class="normal">245</span>
<span class="normal">246</span>
<span class="normal">247</span>
<span class="normal">248</span>
<span class="normal">249</span>
<span class="normal">250</span>
<span class="normal">251</span>
<span class="normal">252</span>
<span class="normal">253</span>
<span class="normal">254</span>
<span class="normal">255</span>
<span class="normal">256</span>
<span class="normal">257</span>
<span class="normal">258</span>
<span class="normal">259</span>
<span class="normal">260</span>
<span class="normal">261</span>
<span class="normal">262</span>
<span class="normal">263</span>
<span class="normal">264</span>
<span class="normal">265</span>
<span class="normal">266</span>
<span class="normal">267</span>
<span class="normal">268</span>
<span class="normal">269</span>
<span class="normal">270</span>
<span class="normal">271</span>
<span class="normal">272</span>
<span class="normal">273</span>
<span class="normal">274</span>
<span class="normal">275</span>
<span class="normal">276</span>
<span class="normal">277</span>
<span class="normal">278</span>
<span class="normal">279</span>
<span class="normal">280</span>
<span class="normal">281</span>
<span class="normal">282</span>
<span class="normal">283</span>
<span class="normal">284</span>
<span class="normal">285</span>
<span class="normal">286</span>
<span class="normal">287</span>
<span class="normal">288</span>
<span class="normal">289</span>
<span class="normal">290</span>
<span class="normal">291</span>
<span class="normal">292</span>
<span class="normal">293</span>
<span class="normal">294</span>
<span class="normal">295</span>
<span class="normal">296</span>
<span class="normal">297</span>
<span class="normal">298</span>
<span class="normal">299</span>
<span class="normal">300</span>
<span class="normal">301</span>
<span class="normal">302</span>
<span class="normal">303</span>
<span class="normal">304</span>
<span class="normal">305</span>
<span class="normal">306</span>
<span class="normal">307</span>
<span class="normal">308</span>
<span class="normal">309</span>
<span class="normal">310</span>
<span class="normal">311</span>
<span class="normal">312</span>
<span class="normal">313</span>
<span class="normal">314</span>
<span class="normal">315</span>
<span class="normal">316</span>
<span class="normal">317</span>
<span class="normal">318</span>
<span class="normal">319</span>
<span class="normal">320</span>
<span class="normal">321</span>
<span class="normal">322</span>
<span class="normal">323</span>
<span class="normal">324</span>
<span class="normal">325</span>
<span class="normal">326</span>
<span class="normal">327</span>
<span class="normal">328</span>
<span class="normal">329</span>
<span class="normal">330</span>
<span class="normal">331</span>
<span class="normal">332</span>
<span class="normal">333</span>
<span class="normal">334</span>
<span class="normal">335</span>
<span class="normal">336</span>
<span class="normal">337</span>
<span class="normal">338</span>
<span class="normal">339</span>
<span class="normal">340</span>
<span class="normal">341</span>
<span class="normal">342</span>
<span class="normal">343</span>
<span class="normal">344</span>
<span class="normal">345</span>
<span class="normal">346</span>
<span class="normal">347</span>
<span class="normal">348</span>
<span class="normal">349</span>
<span class="normal">350</span>
<span class="normal">351</span>
<span class="normal">352</span>
<span class="normal">353</span>
<span class="normal">354</span>
<span class="normal">355</span>
<span class="normal">356</span>
<span class="normal">357</span>
<span class="normal">358</span>
<span class="normal">359</span>
<span class="normal">360</span>
<span class="normal">361</span>
<span class="normal">362</span>
<span class="normal">363</span>
<span class="normal">364</span>
<span class="normal">365</span>
<span class="normal">366</span>
<span class="normal">367</span>
<span class="normal">368</span>
<span class="normal">369</span>
<span class="normal">370</span>
<span class="normal">371</span>
<span class="normal">372</span>
<span class="normal">373</span>
<span class="normal">374</span>
<span class="normal">375</span>
<span class="normal">376</span>
<span class="normal">377</span>
<span class="normal">378</span>
<span class="normal">379</span>
<span class="normal">380</span>
<span class="normal">381</span>
<span class="normal">382</span>
<span class="normal">383</span>
<span class="normal">384</span>
<span class="normal">385</span>
<span class="normal">386</span>
<span class="normal">387</span>
<span class="normal">388</span>
<span class="normal">389</span>
<span class="normal">390</span>
<span class="normal">391</span>
<span class="normal">392</span>
<span class="normal">393</span>
<span class="normal">394</span>
<span class="normal">395</span>
<span class="normal">396</span>
<span class="normal">397</span>
<span class="normal">398</span>
<span class="normal">399</span>
<span class="normal">400</span>
<span class="normal">401</span>
<span class="normal">402</span>
<span class="normal">403</span>
<span class="normal">404</span>
<span class="normal">405</span>
<span class="normal">406</span>
<span class="normal">407</span>
<span class="normal">408</span>
<span class="normal">409</span>
<span class="normal">410</span>
<span class="normal">411</span>
<span class="normal">412</span>
<span class="normal">413</span>
<span class="normal">414</span>
<span class="normal">415</span>
<span class="normal">416</span>
<span class="normal">417</span>
<span class="normal">418</span>
<span class="normal">419</span>
<span class="normal">420</span>
<span class="normal">421</span>
<span class="normal">422</span>
<span class="normal">423</span>
<span class="normal">424</span>
<span class="normal">425</span>
<span class="normal">426</span>
<span class="normal">427</span>
<span class="normal">428</span>
<span class="normal">429</span>
<span class="normal">430</span>
<span class="normal">431</span>
<span class="normal">432</span>
<span class="normal">433</span>
<span class="normal">434</span>
<span class="normal">435</span>
<span class="normal">436</span>
<span class="normal">437</span>
<span class="normal">438</span>
<span class="normal">439</span>
<span class="normal">440</span>
<span class="normal">441</span>
<span class="normal">442</span>
<span class="normal">443</span>
<span class="normal">444</span>
<span class="normal">445</span>
<span class="normal">446</span>
<span class="normal">447</span>
<span class="normal">448</span>
<span class="normal">449</span>
<span class="normal">450</span>
<span class="normal">451</span>
<span class="normal">452</span>
<span class="normal">453</span>
<span class="normal">454</span>
<span class="normal">455</span>
<span class="normal">456</span>
<span class="normal">457</span>
<span class="normal">458</span>
<span class="normal">459</span>
<span class="normal">460</span>
<span class="normal">461</span>
<span class="normal">462</span>
<span class="normal">463</span>
<span class="normal">464</span>
<span class="normal">465</span>
<span class="normal">466</span>
<span class="normal">467</span>
<span class="normal">468</span>
<span class="normal">469</span>
<span class="normal">470</span>
<span class="normal">471</span>
<span class="normal">472</span>
<span class="normal">473</span>
<span class="normal">474</span>
<span class="normal">475</span>
<span class="normal">476</span>
<span class="normal">477</span>
<span class="normal">478</span>
<span class="normal">479</span>
<span class="normal">480</span>
<span class="normal">481</span>
<span class="normal">482</span>
<span class="normal">483</span>
<span class="normal">484</span>
<span class="normal">485</span>
<span class="normal">486</span>
<span class="normal">487</span>
<span class="normal">488</span>
<span class="normal">489</span>
<span class="normal">490</span>
<span class="normal">491</span>
<span class="normal">492</span>
<span class="normal">493</span>
<span class="normal">494</span>
<span class="normal">495</span>
<span class="normal">496</span>
<span class="normal">497</span>
<span class="normal">498</span>
<span class="normal">499</span>
<span class="normal">500</span>
<span class="normal">501</span>
<span class="normal">502</span>
<span class="normal">503</span>
<span class="normal">504</span>
<span class="normal">505</span>
<span class="normal">506</span>
<span class="normal">507</span>
<span class="normal">508</span>
<span class="normal">509</span>
<span class="normal">510</span>
<span class="normal">511</span>
<span class="normal">512</span>
<span class="normal">513</span>
<span class="normal">514</span>
<span class="normal">515</span>
<span class="normal">516</span>
<span class="normal">517</span>
<span class="normal">518</span>
<span class="normal">519</span>
<span class="normal">520</span>
<span class="normal">521</span>
<span class="normal">522</span>
<span class="normal">523</span>
<span class="normal">524</span>
<span class="normal">525</span>
<span class="normal">526</span>
<span class="normal">527</span>
<span class="normal">528</span>
<span class="normal">529</span>
<span class="normal">530</span>
<span class="normal">531</span>
<span class="normal">532</span>
<span class="normal">533</span>
<span class="normal">534</span>
<span class="normal">535</span>
<span class="normal">536</span>
<span class="normal">537</span>
<span class="normal">538</span>
<span class="normal">539</span>
<span class="normal">540</span>
<span class="normal">541</span>
<span class="normal">542</span>
<span class="normal">543</span>
<span class="normal">544</span>
<span class="normal">545</span>
<span class="normal">546</span>
<span class="normal">547</span>
<span class="normal">548</span>
<span class="normal">549</span>
<span class="normal">550</span>
<span class="normal">551</span>
<span class="normal">552</span>
<span class="normal">553</span>
<span class="normal">554</span>
<span class="normal">555</span>
<span class="normal">556</span>
<span class="normal">557</span>
<span class="normal">558</span>
<span class="normal">559</span>
<span class="normal">560</span>
<span class="normal">561</span>
<span class="normal">562</span>
<span class="normal">563</span>
<span class="normal">564</span>
<span class="normal">565</span>
<span class="normal">566</span>
<span class="normal">567</span>
<span class="normal">568</span>
<span class="normal">569</span>
<span class="normal">570</span>
<span class="normal">571</span>
<span class="normal">572</span>
<span class="normal">573</span>
<span class="normal">574</span>
<span class="normal">575</span>
<span class="normal">576</span>
<span class="normal">577</span>
<span class="normal">578</span>
<span class="normal">579</span>
<span class="normal">580</span>
<span class="normal">581</span>
<span class="normal">582</span>
<span class="normal">583</span>
<span class="normal">584</span>
<span class="normal">585</span>
<span class="normal">586</span>
<span class="normal">587</span>
<span class="normal">588</span>
<span class="normal">589</span>
<span class="normal">590</span>
<span class="normal">591</span>
<span class="normal">592</span>
<span class="normal">593</span>
<span class="normal">594</span>
<span class="normal">595</span>
<span class="normal">596</span>
<span class="normal">597</span>
<span class="normal">598</span>
<span class="normal">599</span>
<span class="normal">600</span>
<span class="normal">601</span>
<span class="normal">602</span>
<span class="normal">603</span>
<span class="normal">604</span>
<span class="normal">605</span>
<span class="normal">606</span>
<span class="normal">607</span>
<span class="normal">608</span>
<span class="normal">609</span>
<span class="normal">610</span>
<span class="normal">611</span>
<span class="normal">612</span>
<span class="normal">613</span>
<span class="normal">614</span>
<span class="normal">615</span>
<span class="normal">616</span>
<span class="normal">617</span>
<span class="normal">618</span>
<span class="normal">619</span>
<span class="normal">620</span>
<span class="normal">621</span>
<span class="normal">622</span>
<span class="normal">623</span>
<span class="normal">624</span>
<span class="normal">625</span>
<span class="normal">626</span>
<span class="normal">627</span>
<span class="normal">628</span>
<span class="normal">629</span>
<span class="normal">630</span>
<span class="normal">631</span>
<span class="normal">632</span>
<span class="normal">633</span>
<span class="normal">634</span>
<span class="normal">635</span>
<span class="normal">636</span>
<span class="normal">637</span>
<span class="normal">638</span>
<span class="normal">639</span>
<span class="normal">640</span>
<span class="normal">641</span>
<span class="normal">642</span>
<span class="normal">643</span>
<span class="normal">644</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="p">{</span>
<span class="w"> </span><span class="nt">&quot;cells&quot;</span><span class="p">:</span><span class="w"> </span><span class="p">[</span>
<span class="w">  </span><span class="p">{</span>
<span class="w">   </span><span class="nt">&quot;cell_type&quot;</span><span class="p">:</span><span class="w"> </span><span class="s2">&quot;markdown&quot;</span><span class="p">,</span>
<span class="w">   </span><span class="nt">&quot;id&quot;</span><span class="p">:</span><span class="w"> </span><span class="s2">&quot;b2c3d4e5-0001-4000-8000-000000000001&quot;</span><span class="p">,</span>
<span class="w">   </span><span class="nt">&quot;metadata&quot;</span><span class="p">:</span><span class="w"> </span><span class="p">{},</span>
<span class="w">   </span><span class="nt">&quot;source&quot;</span><span class="p">:</span><span class="w"> </span><span class="p">[</span>
<span class="w">    </span><span class="s2">&quot;# Regression Analysis\n&quot;</span><span class="p">,</span>
<span class="w">    </span><span class="s2">&quot;\n&quot;</span><span class="p">,</span>
<span class="w">    </span><span class="s2">&quot;Regression analysis models the relationship between a **response variable** (dependent) and one or more **predictor variables** (independent). It is the workhorse of statistical modelling and machine learning.\n&quot;</span><span class="p">,</span>
<span class="w">    </span><span class="s2">&quot;\n&quot;</span><span class="p">,</span>
<span class="w">    </span><span class="s2">&quot;**Topics covered in this notebook:**\n&quot;</span><span class="p">,</span>
<span class="w">    </span><span class="s2">&quot;1. Simple linear regression ‚Äî fitting a line, OLS estimation\n&quot;</span><span class="p">,</span>
<span class="w">    </span><span class="s2">&quot;2. Interpreting coefficients ‚Äî R¬≤, p-values, residual diagnostics\n&quot;</span><span class="p">,</span>
<span class="w">    </span><span class="s2">&quot;3. Multiple linear regression ‚Äî multiple predictors\n&quot;</span><span class="p">,</span>
<span class="w">    </span><span class="s2">&quot;4. Polynomial regression ‚Äî non-linear patterns, bias-variance tradeoff\n&quot;</span><span class="p">,</span>
<span class="w">    </span><span class="s2">&quot;5. Logistic regression ‚Äî binary outcomes, sigmoid function\n&quot;</span><span class="p">,</span>
<span class="w">    </span><span class="s2">&quot;6. Regression assumptions ‚Äî linearity, normality, homoscedasticity\n&quot;</span><span class="p">,</span>
<span class="w">    </span><span class="s2">&quot;\n&quot;</span><span class="p">,</span>
<span class="w">    </span><span class="s2">&quot;**Mathematical foundation ‚Äî Ordinary Least Squares (OLS):**\n&quot;</span><span class="p">,</span>
<span class="w">    </span><span class="s2">&quot;\n&quot;</span><span class="p">,</span>
<span class="w">    </span><span class="s2">&quot;$$\\hat{\\boldsymbol{\\beta}} = (\\mathbf{X}^\\top \\mathbf{X})^{-1} \\mathbf{X}^\\top \\mathbf{y}$$\n&quot;</span><span class="p">,</span>
<span class="w">    </span><span class="s2">&quot;\n&quot;</span><span class="p">,</span>
<span class="w">    </span><span class="s2">&quot;OLS minimises the **residual sum of squares (RSS)** $\\sum_{i=1}^{n}(y_i - \\hat{y}_i)^2$ and is the **BLUE** (Best Linear Unbiased Estimator) under the Gauss-Markov assumptions.&quot;</span>
<span class="w">   </span><span class="p">]</span>
<span class="w">  </span><span class="p">},</span>
<span class="w">  </span><span class="p">{</span>
<span class="w">   </span><span class="nt">&quot;cell_type&quot;</span><span class="p">:</span><span class="w"> </span><span class="s2">&quot;code&quot;</span><span class="p">,</span>
<span class="w">   </span><span class="nt">&quot;execution_count&quot;</span><span class="p">:</span><span class="w"> </span><span class="kc">null</span><span class="p">,</span>
<span class="w">   </span><span class="nt">&quot;id&quot;</span><span class="p">:</span><span class="w"> </span><span class="s2">&quot;b2c3d4e5-0002-4000-8000-000000000002&quot;</span><span class="p">,</span>
<span class="w">   </span><span class="nt">&quot;metadata&quot;</span><span class="p">:</span><span class="w"> </span><span class="p">{},</span>
<span class="w">   </span><span class="nt">&quot;outputs&quot;</span><span class="p">:</span><span class="w"> </span><span class="p">[],</span>
<span class="w">   </span><span class="nt">&quot;source&quot;</span><span class="p">:</span><span class="w"> </span><span class="p">[</span>
<span class="w">    </span><span class="s2">&quot;import numpy as np\n&quot;</span><span class="p">,</span>
<span class="w">    </span><span class="s2">&quot;import scipy.stats as stats\n&quot;</span><span class="p">,</span>
<span class="w">    </span><span class="s2">&quot;import matplotlib.pyplot as plt\n&quot;</span><span class="p">,</span>
<span class="w">    </span><span class="s2">&quot;import matplotlib.gridspec as gridspec\n&quot;</span><span class="p">,</span>
<span class="w">    </span><span class="s2">&quot;\n&quot;</span><span class="p">,</span>
<span class="w">    </span><span class="s2">&quot;# Optional: scikit-learn (used where noted; gracefully falls back to manual)\n&quot;</span><span class="p">,</span>
<span class="w">    </span><span class="s2">&quot;try:\n&quot;</span><span class="p">,</span>
<span class="w">    </span><span class="s2">&quot;    from sklearn.linear_model import LinearRegression, LogisticRegression\n&quot;</span><span class="p">,</span>
<span class="w">    </span><span class="s2">&quot;    from sklearn.preprocessing import PolynomialFeatures, StandardScaler\n&quot;</span><span class="p">,</span>
<span class="w">    </span><span class="s2">&quot;    from sklearn.pipeline import make_pipeline\n&quot;</span><span class="p">,</span>
<span class="w">    </span><span class="s2">&quot;    from sklearn.metrics import r2_score\n&quot;</span><span class="p">,</span>
<span class="w">    </span><span class="s2">&quot;    SKLEARN = True\n&quot;</span><span class="p">,</span>
<span class="w">    </span><span class="s2">&quot;    print(&#39;scikit-learn is available ‚Äî using sklearn where convenient.&#39;)\n&quot;</span><span class="p">,</span>
<span class="w">    </span><span class="s2">&quot;except ImportError:\n&quot;</span><span class="p">,</span>
<span class="w">    </span><span class="s2">&quot;    SKLEARN = False\n&quot;</span><span class="p">,</span>
<span class="w">    </span><span class="s2">&quot;    print(&#39;scikit-learn not found ‚Äî using NumPy/SciPy only.&#39;)\n&quot;</span><span class="p">,</span>
<span class="w">    </span><span class="s2">&quot;\n&quot;</span><span class="p">,</span>
<span class="w">    </span><span class="s2">&quot;rng = np.random.default_rng(seed=0)\n&quot;</span><span class="p">,</span>
<span class="w">    </span><span class="s2">&quot;\n&quot;</span><span class="p">,</span>
<span class="w">    </span><span class="s2">&quot;plt.rcParams.update({\n&quot;</span><span class="p">,</span>
<span class="w">    </span><span class="s2">&quot;    &#39;figure.dpi&#39;: 100,\n&quot;</span><span class="p">,</span>
<span class="w">    </span><span class="s2">&quot;    &#39;axes.spines.top&#39;: False,\n&quot;</span><span class="p">,</span>
<span class="w">    </span><span class="s2">&quot;    &#39;axes.spines.right&#39;: False,\n&quot;</span><span class="p">,</span>
<span class="w">    </span><span class="s2">&quot;    &#39;font.size&#39;: 11,\n&quot;</span><span class="p">,</span>
<span class="w">    </span><span class="s2">&quot;})\n&quot;</span><span class="p">,</span>
<span class="w">    </span><span class="s2">&quot;\n&quot;</span><span class="p">,</span>
<span class="w">    </span><span class="s2">&quot;print(&#39;NumPy:&#39;, np.__version__)\n&quot;</span><span class="p">,</span>
<span class="w">    </span><span class="s2">&quot;print(&#39;SciPy:&#39;, __import__(&#39;scipy&#39;).__version__)&quot;</span>
<span class="w">   </span><span class="p">]</span>
<span class="w">  </span><span class="p">},</span>
<span class="w">  </span><span class="p">{</span>
<span class="w">   </span><span class="nt">&quot;cell_type&quot;</span><span class="p">:</span><span class="w"> </span><span class="s2">&quot;markdown&quot;</span><span class="p">,</span>
<span class="w">   </span><span class="nt">&quot;id&quot;</span><span class="p">:</span><span class="w"> </span><span class="s2">&quot;b2c3d4e5-0003-4000-8000-000000000003&quot;</span><span class="p">,</span>
<span class="w">   </span><span class="nt">&quot;metadata&quot;</span><span class="p">:</span><span class="w"> </span><span class="p">{},</span>
<span class="w">   </span><span class="nt">&quot;source&quot;</span><span class="p">:</span><span class="w"> </span><span class="p">[</span>
<span class="w">    </span><span class="s2">&quot;## 1. Simple Linear Regression\n&quot;</span><span class="p">,</span>
<span class="w">    </span><span class="s2">&quot;\n&quot;</span><span class="p">,</span>
<span class="w">    </span><span class="s2">&quot;The simple linear model relates a single predictor $x$ to the response $y$:\n&quot;</span><span class="p">,</span>
<span class="w">    </span><span class="s2">&quot;\n&quot;</span><span class="p">,</span>
<span class="w">    </span><span class="s2">&quot;$$y_i = \\beta_0 + \\beta_1 x_i + \\varepsilon_i, \\qquad \\varepsilon_i \\overset{\\text{i.i.d.}}{\\sim} \\mathcal{N}(0, \\sigma^2)$$\n&quot;</span><span class="p">,</span>
<span class="w">    </span><span class="s2">&quot;\n&quot;</span><span class="p">,</span>
<span class="w">    </span><span class="s2">&quot;OLS closed-form estimates:\n&quot;</span><span class="p">,</span>
<span class="w">    </span><span class="s2">&quot;\n&quot;</span><span class="p">,</span>
<span class="w">    </span><span class="s2">&quot;$$\\hat{\\beta}_1 = \\frac{\\sum(x_i - \\bar{x})(y_i - \\bar{y})}{\\sum(x_i - \\bar{x})^2} = \\frac{S_{xy}}{S_{xx}}, \\qquad \\hat{\\beta}_0 = \\bar{y} - \\hat{\\beta}_1 \\bar{x}$$\n&quot;</span><span class="p">,</span>
<span class="w">    </span><span class="s2">&quot;\n&quot;</span><span class="p">,</span>
<span class="w">    </span><span class="s2">&quot;The **95% confidence band** around the regression line reflects uncertainty in the estimated mean response $E[y \\mid x]$. The wider **prediction interval** additionally accounts for individual observation noise $\\sigma^2$.&quot;</span>
<span class="w">   </span><span class="p">]</span>
<span class="w">  </span><span class="p">},</span>
<span class="w">  </span><span class="p">{</span>
<span class="w">   </span><span class="nt">&quot;cell_type&quot;</span><span class="p">:</span><span class="w"> </span><span class="s2">&quot;code&quot;</span><span class="p">,</span>
<span class="w">   </span><span class="nt">&quot;execution_count&quot;</span><span class="p">:</span><span class="w"> </span><span class="kc">null</span><span class="p">,</span>
<span class="w">   </span><span class="nt">&quot;id&quot;</span><span class="p">:</span><span class="w"> </span><span class="s2">&quot;b2c3d4e5-0004-4000-8000-000000000004&quot;</span><span class="p">,</span>
<span class="w">   </span><span class="nt">&quot;metadata&quot;</span><span class="p">:</span><span class="w"> </span><span class="p">{},</span>
<span class="w">   </span><span class="nt">&quot;outputs&quot;</span><span class="p">:</span><span class="w"> </span><span class="p">[],</span>
<span class="w">   </span><span class="nt">&quot;source&quot;</span><span class="p">:</span><span class="w"> </span><span class="p">[</span>
<span class="w">    </span><span class="s2">&quot;# Scenario: predict house price (‚Ç¨ thousands) from size (m¬≤)\n&quot;</span><span class="p">,</span>
<span class="w">    </span><span class="s2">&quot;n = 80\n&quot;</span><span class="p">,</span>
<span class="w">    </span><span class="s2">&quot;x = rng.uniform(40, 200, size=n)                            # house size (m¬≤)\n&quot;</span><span class="p">,</span>
<span class="w">    </span><span class="s2">&quot;true_beta0, true_beta1 = 30.0, 2.5                         # true intercept, slope\n&quot;</span><span class="p">,</span>
<span class="w">    </span><span class="s2">&quot;noise_std = 25.0\n&quot;</span><span class="p">,</span>
<span class="w">    </span><span class="s2">&quot;y = true_beta0 + true_beta1 * x + rng.normal(0, noise_std, size=n)\n&quot;</span><span class="p">,</span>
<span class="w">    </span><span class="s2">&quot;\n&quot;</span><span class="p">,</span>
<span class="w">    </span><span class="s2">&quot;# OLS via scipy.stats.linregress\n&quot;</span><span class="p">,</span>
<span class="w">    </span><span class="s2">&quot;slope, intercept, r_value, p_value, stderr = stats.linregress(x, y)\n&quot;</span><span class="p">,</span>
<span class="w">    </span><span class="s2">&quot;\n&quot;</span><span class="p">,</span>
<span class="w">    </span><span class="s2">&quot;print(f&#39;Fitted intercept Œ≤‚ÇÄ : {intercept:.4f}  (true: {true_beta0})&#39;)\n&quot;</span><span class="p">,</span>
<span class="w">    </span><span class="s2">&quot;print(f&#39;Fitted slope     Œ≤‚ÇÅ : {slope:.4f}  (true: {true_beta1})&#39;)\n&quot;</span><span class="p">,</span>
<span class="w">    </span><span class="s2">&quot;print(f&#39;R                   : {r_value:.4f}&#39;)\n&quot;</span><span class="p">,</span>
<span class="w">    </span><span class="s2">&quot;print(f&#39;R¬≤                  : {r_value**2:.4f}&#39;)\n&quot;</span><span class="p">,</span>
<span class="w">    </span><span class="s2">&quot;print(f&#39;p-value (slope)     : {p_value:.6f}&#39;)\n&quot;</span><span class="p">,</span>
<span class="w">    </span><span class="s2">&quot;print(f&#39;Std error (slope)   : {stderr:.4f}&#39;)\n&quot;</span><span class="p">,</span>
<span class="w">    </span><span class="s2">&quot;\n&quot;</span><span class="p">,</span>
<span class="w">    </span><span class="s2">&quot;# Predictions + 95% confidence band (analytical formula)\n&quot;</span><span class="p">,</span>
<span class="w">    </span><span class="s2">&quot;x_grid = np.linspace(x.min(), x.max(), 300)\n&quot;</span><span class="p">,</span>
<span class="w">    </span><span class="s2">&quot;y_hat  = intercept + slope * x_grid\n&quot;</span><span class="p">,</span>
<span class="w">    </span><span class="s2">&quot;\n&quot;</span><span class="p">,</span>
<span class="w">    </span><span class="s2">&quot;x_mean = x.mean()\n&quot;</span><span class="p">,</span>
<span class="w">    </span><span class="s2">&quot;n_obs  = len(x)\n&quot;</span><span class="p">,</span>
<span class="w">    </span><span class="s2">&quot;Sxx = ((x - x_mean) ** 2).sum()\n&quot;</span><span class="p">,</span>
<span class="w">    </span><span class="s2">&quot;residuals = y - (intercept + slope * x)\n&quot;</span><span class="p">,</span>
<span class="w">    </span><span class="s2">&quot;s_e = np.sqrt((residuals ** 2).sum() / (n_obs - 2))       # residual std error\n&quot;</span><span class="p">,</span>
<span class="w">    </span><span class="s2">&quot;\n&quot;</span><span class="p">,</span>
<span class="w">    </span><span class="s2">&quot;t_crit = stats.t.ppf(0.975, df=n_obs - 2)\n&quot;</span><span class="p">,</span>
<span class="w">    </span><span class="s2">&quot;se_mean = s_e * np.sqrt(1 / n_obs + (x_grid - x_mean) ** 2 / Sxx)\n&quot;</span><span class="p">,</span>
<span class="w">    </span><span class="s2">&quot;se_pred = s_e * np.sqrt(1 + 1 / n_obs + (x_grid - x_mean) ** 2 / Sxx)\n&quot;</span><span class="p">,</span>
<span class="w">    </span><span class="s2">&quot;\n&quot;</span><span class="p">,</span>
<span class="w">    </span><span class="s2">&quot;ci_lower = y_hat - t_crit * se_mean\n&quot;</span><span class="p">,</span>
<span class="w">    </span><span class="s2">&quot;ci_upper = y_hat + t_crit * se_mean\n&quot;</span><span class="p">,</span>
<span class="w">    </span><span class="s2">&quot;pi_lower = y_hat - t_crit * se_pred\n&quot;</span><span class="p">,</span>
<span class="w">    </span><span class="s2">&quot;pi_upper = y_hat + t_crit * se_pred\n&quot;</span><span class="p">,</span>
<span class="w">    </span><span class="s2">&quot;\n&quot;</span><span class="p">,</span>
<span class="w">    </span><span class="s2">&quot;# Plot\n&quot;</span><span class="p">,</span>
<span class="w">    </span><span class="s2">&quot;fig, ax = plt.subplots(figsize=(8, 5))\n&quot;</span><span class="p">,</span>
<span class="w">    </span><span class="s2">&quot;ax.scatter(x, y, color=&#39;steelblue&#39;, alpha=0.6, s=30, label=&#39;Observations&#39;)\n&quot;</span><span class="p">,</span>
<span class="w">    </span><span class="s2">&quot;ax.plot(x_grid, y_hat, &#39;r-&#39;, lw=2, label=f&#39;OLS fit: y = {intercept:.1f} + {slope:.2f}x&#39;)\n&quot;</span><span class="p">,</span>
<span class="w">    </span><span class="s2">&quot;ax.fill_between(x_grid, ci_lower, ci_upper, color=&#39;red&#39;, alpha=0.15,\n&quot;</span><span class="p">,</span>
<span class="w">    </span><span class="s2">&quot;                label=&#39;95% confidence band&#39;)\n&quot;</span><span class="p">,</span>
<span class="w">    </span><span class="s2">&quot;ax.fill_between(x_grid, pi_lower, pi_upper, color=&#39;orange&#39;, alpha=0.10,\n&quot;</span><span class="p">,</span>
<span class="w">    </span><span class="s2">&quot;                label=&#39;95% prediction interval&#39;)\n&quot;</span><span class="p">,</span>
<span class="w">    </span><span class="s2">&quot;ax.set_xlabel(&#39;House size (m¬≤)&#39;)\n&quot;</span><span class="p">,</span>
<span class="w">    </span><span class="s2">&quot;ax.set_ylabel(&#39;Price (‚Ç¨ thousands)&#39;)\n&quot;</span><span class="p">,</span>
<span class="w">    </span><span class="s2">&quot;ax.set_title(f&#39;Simple Linear Regression  |  R¬≤={r_value**2:.3f}, p={p_value:.4f}&#39;)\n&quot;</span><span class="p">,</span>
<span class="w">    </span><span class="s2">&quot;ax.legend()\n&quot;</span><span class="p">,</span>
<span class="w">    </span><span class="s2">&quot;plt.tight_layout()\n&quot;</span><span class="p">,</span>
<span class="w">    </span><span class="s2">&quot;plt.show()&quot;</span>
<span class="w">   </span><span class="p">]</span>
<span class="w">  </span><span class="p">},</span>
<span class="w">  </span><span class="p">{</span>
<span class="w">   </span><span class="nt">&quot;cell_type&quot;</span><span class="p">:</span><span class="w"> </span><span class="s2">&quot;markdown&quot;</span><span class="p">,</span>
<span class="w">   </span><span class="nt">&quot;id&quot;</span><span class="p">:</span><span class="w"> </span><span class="s2">&quot;b2c3d4e5-0005-4000-8000-000000000005&quot;</span><span class="p">,</span>
<span class="w">   </span><span class="nt">&quot;metadata&quot;</span><span class="p">:</span><span class="w"> </span><span class="p">{},</span>
<span class="w">   </span><span class="nt">&quot;source&quot;</span><span class="p">:</span><span class="w"> </span><span class="p">[</span>
<span class="w">    </span><span class="s2">&quot;## 2. Interpreting Coefficients: R¬≤, Residuals, and Q-Q Plot\n&quot;</span><span class="p">,</span>
<span class="w">    </span><span class="s2">&quot;\n&quot;</span><span class="p">,</span>
<span class="w">    </span><span class="s2">&quot;**R¬≤ (coefficient of determination)** measures the proportion of variance in $y$ explained by the model:\n&quot;</span><span class="p">,</span>
<span class="w">    </span><span class="s2">&quot;\n&quot;</span><span class="p">,</span>
<span class="w">    </span><span class="s2">&quot;$$R^2 = 1 - \\frac{\\text{RSS}}{\\text{TSS}} = 1 - \\frac{\\sum(y_i - \\hat{y}_i)^2}{\\sum(y_i - \\bar{y})^2}$$\n&quot;</span><span class="p">,</span>
<span class="w">    </span><span class="s2">&quot;\n&quot;</span><span class="p">,</span>
<span class="w">    </span><span class="s2">&quot;**Residual diagnostics** are essential for validating OLS assumptions:\n&quot;</span><span class="p">,</span>
<span class="w">    </span><span class="s2">&quot;\n&quot;</span><span class="p">,</span>
<span class="w">    </span><span class="s2">&quot;| Plot | What to look for | Violation suggests |\n&quot;</span><span class="p">,</span>
<span class="w">    </span><span class="s2">&quot;|------|------------------|-----------------------|\n&quot;</span><span class="p">,</span>
<span class="w">    </span><span class="s2">&quot;| Residuals vs Fitted | Random scatter around 0 | Non-linearity or heteroscedasticity |\n&quot;</span><span class="p">,</span>
<span class="w">    </span><span class="s2">&quot;| Q-Q plot | Points on diagonal | Non-normality of errors |\n&quot;</span><span class="p">,</span>
<span class="w">    </span><span class="s2">&quot;| Scale-Location | Horizontal band | Heteroscedasticity |\n&quot;</span><span class="p">,</span>
<span class="w">    </span><span class="s2">&quot;| Residuals vs Leverage | No high-leverage outliers | Influential observations |\n&quot;</span><span class="p">,</span>
<span class="w">    </span><span class="s2">&quot;\n&quot;</span><span class="p">,</span>
<span class="w">    </span><span class="s2">&quot;The **p-value of the slope** tests H‚ÇÄ: Œ≤‚ÇÅ = 0 (no linear relationship). Reject when p &lt; Œ±.&quot;</span>
<span class="w">   </span><span class="p">]</span>
<span class="w">  </span><span class="p">},</span>
<span class="w">  </span><span class="p">{</span>
<span class="w">   </span><span class="nt">&quot;cell_type&quot;</span><span class="p">:</span><span class="w"> </span><span class="s2">&quot;code&quot;</span><span class="p">,</span>
<span class="w">   </span><span class="nt">&quot;execution_count&quot;</span><span class="p">:</span><span class="w"> </span><span class="kc">null</span><span class="p">,</span>
<span class="w">   </span><span class="nt">&quot;id&quot;</span><span class="p">:</span><span class="w"> </span><span class="s2">&quot;b2c3d4e5-0006-4000-8000-000000000006&quot;</span><span class="p">,</span>
<span class="w">   </span><span class="nt">&quot;metadata&quot;</span><span class="p">:</span><span class="w"> </span><span class="p">{},</span>
<span class="w">   </span><span class="nt">&quot;outputs&quot;</span><span class="p">:</span><span class="w"> </span><span class="p">[],</span>
<span class="w">   </span><span class="nt">&quot;source&quot;</span><span class="p">:</span><span class="w"> </span><span class="p">[</span>
<span class="w">    </span><span class="s2">&quot;# Re-use the house price data from Cell 4\n&quot;</span><span class="p">,</span>
<span class="w">    </span><span class="s2">&quot;y_fitted   = intercept + slope * x\n&quot;</span><span class="p">,</span>
<span class="w">    </span><span class="s2">&quot;residuals  = y - y_fitted\n&quot;</span><span class="p">,</span>
<span class="w">    </span><span class="s2">&quot;std_resid  = residuals / s_e                               # standardised residuals\n&quot;</span><span class="p">,</span>
<span class="w">    </span><span class="s2">&quot;\n&quot;</span><span class="p">,</span>
<span class="w">    </span><span class="s2">&quot;RSS = (residuals ** 2).sum()\n&quot;</span><span class="p">,</span>
<span class="w">    </span><span class="s2">&quot;TSS = ((y - y.mean()) ** 2).sum()\n&quot;</span><span class="p">,</span>
<span class="w">    </span><span class="s2">&quot;R2  = 1 - RSS / TSS\n&quot;</span><span class="p">,</span>
<span class="w">    </span><span class="s2">&quot;adj_R2 = 1 - (1 - R2) * (n_obs - 1) / (n_obs - 2)\n&quot;</span><span class="p">,</span>
<span class="w">    </span><span class="s2">&quot;\n&quot;</span><span class="p">,</span>
<span class="w">    </span><span class="s2">&quot;print(f&#39;R¬≤          : {R2:.4f}&#39;)\n&quot;</span><span class="p">,</span>
<span class="w">    </span><span class="s2">&quot;print(f&#39;Adjusted R¬≤ : {adj_R2:.4f}&#39;)\n&quot;</span><span class="p">,</span>
<span class="w">    </span><span class="s2">&quot;print(f&#39;RSS         : {RSS:.2f}&#39;)\n&quot;</span><span class="p">,</span>
<span class="w">    </span><span class="s2">&quot;print(f&#39;Residual SE : {s_e:.4f}&#39;)\n&quot;</span><span class="p">,</span>
<span class="w">    </span><span class="s2">&quot;\n&quot;</span><span class="p">,</span>
<span class="w">    </span><span class="s2">&quot;# Shapiro-Wilk test on residuals\n&quot;</span><span class="p">,</span>
<span class="w">    </span><span class="s2">&quot;sw_stat, sw_p = stats.shapiro(residuals)\n&quot;</span><span class="p">,</span>
<span class="w">    </span><span class="s2">&quot;print(f&#39;\\nShapiro-Wilk test on residuals: W={sw_stat:.4f}, p={sw_p:.4f}&#39;)\n&quot;</span><span class="p">,</span>
<span class="w">    </span><span class="s2">&quot;print(&#39;Residuals appear normal.&#39; if sw_p &gt; 0.05 else &#39;Residuals may NOT be normal.&#39;)\n&quot;</span><span class="p">,</span>
<span class="w">    </span><span class="s2">&quot;\n&quot;</span><span class="p">,</span>
<span class="w">    </span><span class="s2">&quot;fig = plt.figure(figsize=(11, 8))\n&quot;</span><span class="p">,</span>
<span class="w">    </span><span class="s2">&quot;gs  = gridspec.GridSpec(2, 2, hspace=0.40, wspace=0.35)\n&quot;</span><span class="p">,</span>
<span class="w">    </span><span class="s2">&quot;\n&quot;</span><span class="p">,</span>
<span class="w">    </span><span class="s2">&quot;# 1. Residuals vs Fitted\n&quot;</span><span class="p">,</span>
<span class="w">    </span><span class="s2">&quot;ax1 = fig.add_subplot(gs[0, 0])\n&quot;</span><span class="p">,</span>
<span class="w">    </span><span class="s2">&quot;ax1.scatter(y_fitted, residuals, alpha=0.6, color=&#39;steelblue&#39;, s=25)\n&quot;</span><span class="p">,</span>
<span class="w">    </span><span class="s2">&quot;ax1.axhline(0, color=&#39;red&#39;, lw=1.5, ls=&#39;--&#39;)\n&quot;</span><span class="p">,</span>
<span class="w">    </span><span class="s2">&quot;ax1.set_xlabel(&#39;Fitted values&#39;); ax1.set_ylabel(&#39;Residuals&#39;)\n&quot;</span><span class="p">,</span>
<span class="w">    </span><span class="s2">&quot;ax1.set_title(&#39;Residuals vs Fitted&#39;)\n&quot;</span><span class="p">,</span>
<span class="w">    </span><span class="s2">&quot;\n&quot;</span><span class="p">,</span>
<span class="w">    </span><span class="s2">&quot;# 2. Q-Q plot\n&quot;</span><span class="p">,</span>
<span class="w">    </span><span class="s2">&quot;ax2 = fig.add_subplot(gs[0, 1])\n&quot;</span><span class="p">,</span>
<span class="w">    </span><span class="s2">&quot;(osm, osr), (slope_qq, intercept_qq, r_qq) = stats.probplot(residuals, dist=&#39;norm&#39;)\n&quot;</span><span class="p">,</span>
<span class="w">    </span><span class="s2">&quot;ax2.scatter(osm, osr, alpha=0.6, color=&#39;steelblue&#39;, s=25)\n&quot;</span><span class="p">,</span>
<span class="w">    </span><span class="s2">&quot;ax2.plot(osm, slope_qq * np.array(osm) + intercept_qq, &#39;r-&#39;, lw=1.5)\n&quot;</span><span class="p">,</span>
<span class="w">    </span><span class="s2">&quot;ax2.set_xlabel(&#39;Theoretical quantiles&#39;); ax2.set_ylabel(&#39;Sample quantiles&#39;)\n&quot;</span><span class="p">,</span>
<span class="w">    </span><span class="s2">&quot;ax2.set_title(&#39;Normal Q-Q Plot&#39;)\n&quot;</span><span class="p">,</span>
<span class="w">    </span><span class="s2">&quot;\n&quot;</span><span class="p">,</span>
<span class="w">    </span><span class="s2">&quot;# 3. Scale-Location (sqrt |standardised residuals| vs fitted)\n&quot;</span><span class="p">,</span>
<span class="w">    </span><span class="s2">&quot;ax3 = fig.add_subplot(gs[1, 0])\n&quot;</span><span class="p">,</span>
<span class="w">    </span><span class="s2">&quot;ax3.scatter(y_fitted, np.sqrt(np.abs(std_resid)), alpha=0.6, color=&#39;steelblue&#39;, s=25)\n&quot;</span><span class="p">,</span>
<span class="w">    </span><span class="s2">&quot;ax3.set_xlabel(&#39;Fitted values&#39;); ax3.set_ylabel(&#39;‚àö|Standardised residuals|&#39;)\n&quot;</span><span class="p">,</span>
<span class="w">    </span><span class="s2">&quot;ax3.set_title(&#39;Scale-Location&#39;)\n&quot;</span><span class="p">,</span>
<span class="w">    </span><span class="s2">&quot;\n&quot;</span><span class="p">,</span>
<span class="w">    </span><span class="s2">&quot;# 4. Residual histogram\n&quot;</span><span class="p">,</span>
<span class="w">    </span><span class="s2">&quot;ax4 = fig.add_subplot(gs[1, 1])\n&quot;</span><span class="p">,</span>
<span class="w">    </span><span class="s2">&quot;ax4.hist(residuals, bins=14, color=&#39;steelblue&#39;, edgecolor=&#39;white&#39;, alpha=0.8, density=True)\n&quot;</span><span class="p">,</span>
<span class="w">    </span><span class="s2">&quot;xr = np.linspace(residuals.min(), residuals.max(), 200)\n&quot;</span><span class="p">,</span>
<span class="w">    </span><span class="s2">&quot;ax4.plot(xr, stats.norm.pdf(xr, residuals.mean(), residuals.std()), &#39;r-&#39;, lw=2)\n&quot;</span><span class="p">,</span>
<span class="w">    </span><span class="s2">&quot;ax4.set_xlabel(&#39;Residual&#39;); ax4.set_ylabel(&#39;Density&#39;)\n&quot;</span><span class="p">,</span>
<span class="w">    </span><span class="s2">&quot;ax4.set_title(&#39;Residual Distribution&#39;)\n&quot;</span><span class="p">,</span>
<span class="w">    </span><span class="s2">&quot;\n&quot;</span><span class="p">,</span>
<span class="w">    </span><span class="s2">&quot;fig.suptitle(f&#39;Residual Diagnostics  |  R¬≤={R2:.3f}&#39;, fontsize=13, y=1.01)\n&quot;</span><span class="p">,</span>
<span class="w">    </span><span class="s2">&quot;plt.show()&quot;</span>
<span class="w">   </span><span class="p">]</span>
<span class="w">  </span><span class="p">},</span>
<span class="w">  </span><span class="p">{</span>
<span class="w">   </span><span class="nt">&quot;cell_type&quot;</span><span class="p">:</span><span class="w"> </span><span class="s2">&quot;markdown&quot;</span><span class="p">,</span>
<span class="w">   </span><span class="nt">&quot;id&quot;</span><span class="p">:</span><span class="w"> </span><span class="s2">&quot;b2c3d4e5-0007-4000-8000-000000000007&quot;</span><span class="p">,</span>
<span class="w">   </span><span class="nt">&quot;metadata&quot;</span><span class="p">:</span><span class="w"> </span><span class="p">{},</span>
<span class="w">   </span><span class="nt">&quot;source&quot;</span><span class="p">:</span><span class="w"> </span><span class="p">[</span>
<span class="w">    </span><span class="s2">&quot;## 3. Multiple Linear Regression\n&quot;</span><span class="p">,</span>
<span class="w">    </span><span class="s2">&quot;\n&quot;</span><span class="p">,</span>
<span class="w">    </span><span class="s2">&quot;With $p$ predictors the model becomes:\n&quot;</span><span class="p">,</span>
<span class="w">    </span><span class="s2">&quot;\n&quot;</span><span class="p">,</span>
<span class="w">    </span><span class="s2">&quot;$$\\mathbf{y} = \\mathbf{X}\\boldsymbol{\\beta} + \\boldsymbol{\\varepsilon}, \\qquad \\hat{\\boldsymbol{\\beta}} = (\\mathbf{X}^\\top\\mathbf{X})^{-1}\\mathbf{X}^\\top\\mathbf{y}$$\n&quot;</span><span class="p">,</span>
<span class="w">    </span><span class="s2">&quot;\n&quot;</span><span class="p">,</span>
<span class="w">    </span><span class="s2">&quot;where $\\mathbf{X}$ is the $n \\times (p+1)$ design matrix (first column all ones for the intercept).\n&quot;</span><span class="p">,</span>
<span class="w">    </span><span class="s2">&quot;\n&quot;</span><span class="p">,</span>
<span class="w">    </span><span class="s2">&quot;**Key considerations:**\n&quot;</span><span class="p">,</span>
<span class="w">    </span><span class="s2">&quot;- **Multicollinearity**: when predictors are correlated, coefficient estimates become unstable. Check with Variance Inflation Factor (VIF).\n&quot;</span><span class="p">,</span>
<span class="w">    </span><span class="s2">&quot;- **Adjusted R¬≤**: penalises adding uninformative predictors ‚Äî always report alongside R¬≤.\n&quot;</span><span class="p">,</span>
<span class="w">    </span><span class="s2">&quot;- **F-statistic**: tests whether *all* coefficients (except intercept) are simultaneously zero.\n&quot;</span><span class="p">,</span>
<span class="w">    </span><span class="s2">&quot;\n&quot;</span><span class="p">,</span>
<span class="w">    </span><span class="s2">&quot;$$F = \\frac{(\\text{TSS} - \\text{RSS})/p}{\\text{RSS}/(n - p - 1)}$$&quot;</span>
<span class="w">   </span><span class="p">]</span>
<span class="w">  </span><span class="p">},</span>
<span class="w">  </span><span class="p">{</span>
<span class="w">   </span><span class="nt">&quot;cell_type&quot;</span><span class="p">:</span><span class="w"> </span><span class="s2">&quot;code&quot;</span><span class="p">,</span>
<span class="w">   </span><span class="nt">&quot;execution_count&quot;</span><span class="p">:</span><span class="w"> </span><span class="kc">null</span><span class="p">,</span>
<span class="w">   </span><span class="nt">&quot;id&quot;</span><span class="p">:</span><span class="w"> </span><span class="s2">&quot;b2c3d4e5-0008-4000-8000-000000000008&quot;</span><span class="p">,</span>
<span class="w">   </span><span class="nt">&quot;metadata&quot;</span><span class="p">:</span><span class="w"> </span><span class="p">{},</span>
<span class="w">   </span><span class="nt">&quot;outputs&quot;</span><span class="p">:</span><span class="w"> </span><span class="p">[],</span>
<span class="w">   </span><span class="nt">&quot;source&quot;</span><span class="p">:</span><span class="w"> </span><span class="p">[</span>
<span class="w">    </span><span class="s2">&quot;# Scenario: predict salary (‚Ç¨k) from years of experience, education level (0-3), and performance score\n&quot;</span><span class="p">,</span>
<span class="w">    </span><span class="s2">&quot;n = 120\n&quot;</span><span class="p">,</span>
<span class="w">    </span><span class="s2">&quot;experience  = rng.uniform(0, 30, n)\n&quot;</span><span class="p">,</span>
<span class="w">    </span><span class="s2">&quot;education   = rng.integers(0, 4, n).astype(float)\n&quot;</span><span class="p">,</span>
<span class="w">    </span><span class="s2">&quot;performance = rng.uniform(50, 100, n)\n&quot;</span><span class="p">,</span>
<span class="w">    </span><span class="s2">&quot;\n&quot;</span><span class="p">,</span>
<span class="w">    </span><span class="s2">&quot;# True model: salary = 25 + 1.8*exp + 5*edu + 0.3*perf + noise\n&quot;</span><span class="p">,</span>
<span class="w">    </span><span class="s2">&quot;true_betas = np.array([25.0, 1.8, 5.0, 0.3])\n&quot;</span><span class="p">,</span>
<span class="w">    </span><span class="s2">&quot;noise = rng.normal(0, 8, n)\n&quot;</span><span class="p">,</span>
<span class="w">    </span><span class="s2">&quot;y = true_betas[0] + true_betas[1]*experience + true_betas[2]*education + true_betas[3]*performance + noise\n&quot;</span><span class="p">,</span>
<span class="w">    </span><span class="s2">&quot;\n&quot;</span><span class="p">,</span>
<span class="w">    </span><span class="s2">&quot;# Build design matrix (with intercept column)\n&quot;</span><span class="p">,</span>
<span class="w">    </span><span class="s2">&quot;X_raw = np.column_stack([experience, education, performance])\n&quot;</span><span class="p">,</span>
<span class="w">    </span><span class="s2">&quot;ones  = np.ones((n, 1))\n&quot;</span><span class="p">,</span>
<span class="w">    </span><span class="s2">&quot;X     = np.hstack([ones, X_raw])                           # shape: (n, 4)\n&quot;</span><span class="p">,</span>
<span class="w">    </span><span class="s2">&quot;\n&quot;</span><span class="p">,</span>
<span class="w">    </span><span class="s2">&quot;# OLS via normal equations\n&quot;</span><span class="p">,</span>
<span class="w">    </span><span class="s2">&quot;beta_hat = np.linalg.lstsq(X, y, rcond=None)[0]\n&quot;</span><span class="p">,</span>
<span class="w">    </span><span class="s2">&quot;\n&quot;</span><span class="p">,</span>
<span class="w">    </span><span class="s2">&quot;y_hat    = X @ beta_hat\n&quot;</span><span class="p">,</span>
<span class="w">    </span><span class="s2">&quot;residuals = y - y_hat\n&quot;</span><span class="p">,</span>
<span class="w">    </span><span class="s2">&quot;p_pred   = X.shape[1] - 1                                  # number of predictors (excl. intercept)\n&quot;</span><span class="p">,</span>
<span class="w">    </span><span class="s2">&quot;RSS      = (residuals ** 2).sum()\n&quot;</span><span class="p">,</span>
<span class="w">    </span><span class="s2">&quot;TSS      = ((y - y.mean()) ** 2).sum()\n&quot;</span><span class="p">,</span>
<span class="w">    </span><span class="s2">&quot;R2       = 1 - RSS / TSS\n&quot;</span><span class="p">,</span>
<span class="w">    </span><span class="s2">&quot;adj_R2   = 1 - (1 - R2) * (n - 1) / (n - p_pred - 1)\n&quot;</span><span class="p">,</span>
<span class="w">    </span><span class="s2">&quot;s2       = RSS / (n - p_pred - 1)\n&quot;</span><span class="p">,</span>
<span class="w">    </span><span class="s2">&quot;\n&quot;</span><span class="p">,</span>
<span class="w">    </span><span class="s2">&quot;# Standard errors of coefficients\n&quot;</span><span class="p">,</span>
<span class="w">    </span><span class="s2">&quot;XtX_inv  = np.linalg.inv(X.T @ X)\n&quot;</span><span class="p">,</span>
<span class="w">    </span><span class="s2">&quot;se_betas = np.sqrt(np.diag(XtX_inv) * s2)\n&quot;</span><span class="p">,</span>
<span class="w">    </span><span class="s2">&quot;\n&quot;</span><span class="p">,</span>
<span class="w">    </span><span class="s2">&quot;# t-statistics and p-values\n&quot;</span><span class="p">,</span>
<span class="w">    </span><span class="s2">&quot;t_stats = beta_hat / se_betas\n&quot;</span><span class="p">,</span>
<span class="w">    </span><span class="s2">&quot;df_res  = n - p_pred - 1\n&quot;</span><span class="p">,</span>
<span class="w">    </span><span class="s2">&quot;p_vals  = 2 * stats.t.sf(np.abs(t_stats), df=df_res)\n&quot;</span><span class="p">,</span>
<span class="w">    </span><span class="s2">&quot;\n&quot;</span><span class="p">,</span>
<span class="w">    </span><span class="s2">&quot;# F-statistic for overall model significance\n&quot;</span><span class="p">,</span>
<span class="w">    </span><span class="s2">&quot;F_stat = ((TSS - RSS) / p_pred) / (RSS / df_res)\n&quot;</span><span class="p">,</span>
<span class="w">    </span><span class="s2">&quot;F_pval = stats.f.sf(F_stat, p_pred, df_res)\n&quot;</span><span class="p">,</span>
<span class="w">    </span><span class="s2">&quot;\n&quot;</span><span class="p">,</span>
<span class="w">    </span><span class="s2">&quot;print(&#39;Multiple Linear Regression Results&#39;)\n&quot;</span><span class="p">,</span>
<span class="w">    </span><span class="s2">&quot;print(&#39;=&#39; * 55)\n&quot;</span><span class="p">,</span>
<span class="w">    </span><span class="s2">&quot;print(f&#39;R¬≤: {R2:.4f}   Adjusted R¬≤: {adj_R2:.4f}&#39;)\n&quot;</span><span class="p">,</span>
<span class="w">    </span><span class="s2">&quot;print(f&#39;F-statistic: {F_stat:.2f}  (p={F_pval:.2e})&#39;)\n&quot;</span><span class="p">,</span>
<span class="w">    </span><span class="s2">&quot;print(f&#39;n={n}, p={p_pred}&#39;)\n&quot;</span><span class="p">,</span>
<span class="w">    </span><span class="s2">&quot;print()\n&quot;</span><span class="p">,</span>
<span class="w">    </span><span class="s2">&quot;labels = [&#39;Intercept&#39;, &#39;Experience&#39;, &#39;Education&#39;, &#39;Performance&#39;]\n&quot;</span><span class="p">,</span>
<span class="w">    </span><span class="s2">&quot;print(f&#39;{\&quot;Variable\&quot;:&lt;14} {\&quot;Estimate\&quot;:&gt;10} {\&quot;Std Err\&quot;:&gt;10} {\&quot;t-stat\&quot;:&gt;10} {\&quot;p-value\&quot;:&gt;12}&#39;)\n&quot;</span><span class="p">,</span>
<span class="w">    </span><span class="s2">&quot;print(&#39;-&#39; * 58)\n&quot;</span><span class="p">,</span>
<span class="w">    </span><span class="s2">&quot;for lbl, b, se, t, p in zip(labels, beta_hat, se_betas, t_stats, p_vals):\n&quot;</span><span class="p">,</span>
<span class="w">    </span><span class="s2">&quot;    sig = &#39;***&#39; if p &lt; 0.001 else (&#39;**&#39; if p &lt; 0.01 else (&#39;*&#39; if p &lt; 0.05 else &#39;&#39;))\n&quot;</span><span class="p">,</span>
<span class="w">    </span><span class="s2">&quot;    print(f&#39;{lbl:&lt;14} {b:&gt;10.4f} {se:&gt;10.4f} {t:&gt;10.4f} {p:&gt;12.4f} {sig}&#39;)\n&quot;</span><span class="p">,</span>
<span class="w">    </span><span class="s2">&quot;\n&quot;</span><span class="p">,</span>
<span class="w">    </span><span class="s2">&quot;# Coefficient plot\n&quot;</span><span class="p">,</span>
<span class="w">    </span><span class="s2">&quot;fig, ax = plt.subplots(figsize=(7, 4))\n&quot;</span><span class="p">,</span>
<span class="w">    </span><span class="s2">&quot;ci_mult = stats.t.ppf(0.975, df=df_res)\n&quot;</span><span class="p">,</span>
<span class="w">    </span><span class="s2">&quot;ci_half = ci_mult * se_betas[1:]                           # skip intercept\n&quot;</span><span class="p">,</span>
<span class="w">    </span><span class="s2">&quot;y_pos = np.arange(p_pred)\n&quot;</span><span class="p">,</span>
<span class="w">    </span><span class="s2">&quot;ax.barh(y_pos, beta_hat[1:], xerr=ci_half, color=&#39;steelblue&#39;, alpha=0.7,\n&quot;</span><span class="p">,</span>
<span class="w">    </span><span class="s2">&quot;        edgecolor=&#39;white&#39;, capsize=4)\n&quot;</span><span class="p">,</span>
<span class="w">    </span><span class="s2">&quot;ax.axvline(0, color=&#39;black&#39;, lw=1)\n&quot;</span><span class="p">,</span>
<span class="w">    </span><span class="s2">&quot;ax.set_yticks(y_pos)\n&quot;</span><span class="p">,</span>
<span class="w">    </span><span class="s2">&quot;ax.set_yticklabels([&#39;Experience&#39;, &#39;Education&#39;, &#39;Performance&#39;])\n&quot;</span><span class="p">,</span>
<span class="w">    </span><span class="s2">&quot;ax.set_xlabel(&#39;Coefficient estimate (with 95% CI)&#39;)\n&quot;</span><span class="p">,</span>
<span class="w">    </span><span class="s2">&quot;ax.set_title(&#39;Multiple Regression Coefficients&#39;)\n&quot;</span><span class="p">,</span>
<span class="w">    </span><span class="s2">&quot;plt.tight_layout()\n&quot;</span><span class="p">,</span>
<span class="w">    </span><span class="s2">&quot;plt.show()&quot;</span>
<span class="w">   </span><span class="p">]</span>
<span class="w">  </span><span class="p">},</span>
<span class="w">  </span><span class="p">{</span>
<span class="w">   </span><span class="nt">&quot;cell_type&quot;</span><span class="p">:</span><span class="w"> </span><span class="s2">&quot;markdown&quot;</span><span class="p">,</span>
<span class="w">   </span><span class="nt">&quot;id&quot;</span><span class="p">:</span><span class="w"> </span><span class="s2">&quot;b2c3d4e5-0009-4000-8000-000000000009&quot;</span><span class="p">,</span>
<span class="w">   </span><span class="nt">&quot;metadata&quot;</span><span class="p">:</span><span class="w"> </span><span class="p">{},</span>
<span class="w">   </span><span class="nt">&quot;source&quot;</span><span class="p">:</span><span class="w"> </span><span class="p">[</span>
<span class="w">    </span><span class="s2">&quot;## 4. Polynomial Regression\n&quot;</span><span class="p">,</span>
<span class="w">    </span><span class="s2">&quot;\n&quot;</span><span class="p">,</span>
<span class="w">    </span><span class="s2">&quot;When the relationship between $x$ and $y$ is non-linear, we can extend OLS by including polynomial features:\n&quot;</span><span class="p">,</span>
<span class="w">    </span><span class="s2">&quot;\n&quot;</span><span class="p">,</span>
<span class="w">    </span><span class="s2">&quot;$$y = \\beta_0 + \\beta_1 x + \\beta_2 x^2 + \\cdots + \\beta_d x^d + \\varepsilon$$\n&quot;</span><span class="p">,</span>
<span class="w">    </span><span class="s2">&quot;\n&quot;</span><span class="p">,</span>
<span class="w">    </span><span class="s2">&quot;This is still *linear* in the parameters $\\boldsymbol{\\beta}$ ‚Äî just a linear model in a transformed feature space.\n&quot;</span><span class="p">,</span>
<span class="w">    </span><span class="s2">&quot;\n&quot;</span><span class="p">,</span>
<span class="w">    </span><span class="s2">&quot;**Bias-variance tradeoff:**\n&quot;</span><span class="p">,</span>
<span class="w">    </span><span class="s2">&quot;- Low-degree polynomial ‚Üí high bias (underfitting)\n&quot;</span><span class="p">,</span>
<span class="w">    </span><span class="s2">&quot;- High-degree polynomial ‚Üí high variance (overfitting)\n&quot;</span><span class="p">,</span>
<span class="w">    </span><span class="s2">&quot;- The optimal degree minimises prediction error on unseen data (use cross-validation)\n&quot;</span><span class="p">,</span>
<span class="w">    </span><span class="s2">&quot;\n&quot;</span><span class="p">,</span>
<span class="w">    </span><span class="s2">&quot;The training R¬≤ always increases with degree, so use **adjusted R¬≤** or **AIC/BIC** to compare models.&quot;</span>
<span class="w">   </span><span class="p">]</span>
<span class="w">  </span><span class="p">},</span>
<span class="w">  </span><span class="p">{</span>
<span class="w">   </span><span class="nt">&quot;cell_type&quot;</span><span class="p">:</span><span class="w"> </span><span class="s2">&quot;code&quot;</span><span class="p">,</span>
<span class="w">   </span><span class="nt">&quot;execution_count&quot;</span><span class="p">:</span><span class="w"> </span><span class="kc">null</span><span class="p">,</span>
<span class="w">   </span><span class="nt">&quot;id&quot;</span><span class="p">:</span><span class="w"> </span><span class="s2">&quot;b2c3d4e5-0010-4000-8000-000000000010&quot;</span><span class="p">,</span>
<span class="w">   </span><span class="nt">&quot;metadata&quot;</span><span class="p">:</span><span class="w"> </span><span class="p">{},</span>
<span class="w">   </span><span class="nt">&quot;outputs&quot;</span><span class="p">:</span><span class="w"> </span><span class="p">[],</span>
<span class="w">   </span><span class="nt">&quot;source&quot;</span><span class="p">:</span><span class="w"> </span><span class="p">[</span>
<span class="w">    </span><span class="s2">&quot;# Scenario: model a noisy sinusoidal relationship\n&quot;</span><span class="p">,</span>
<span class="w">    </span><span class="s2">&quot;n = 60\n&quot;</span><span class="p">,</span>
<span class="w">    </span><span class="s2">&quot;x_poly = rng.uniform(-3, 3, size=n)\n&quot;</span><span class="p">,</span>
<span class="w">    </span><span class="s2">&quot;y_poly = np.sin(x_poly) + rng.normal(0, 0.3, size=n)\n&quot;</span><span class="p">,</span>
<span class="w">    </span><span class="s2">&quot;\n&quot;</span><span class="p">,</span>
<span class="w">    </span><span class="s2">&quot;x_grid_poly = np.linspace(-3.2, 3.2, 300)\n&quot;</span><span class="p">,</span>
<span class="w">    </span><span class="s2">&quot;degrees = [1, 3, 7, 15]\n&quot;</span><span class="p">,</span>
<span class="w">    </span><span class="s2">&quot;\n&quot;</span><span class="p">,</span>
<span class="w">    </span><span class="s2">&quot;fig, axes = plt.subplots(2, 2, figsize=(11, 8), sharey=True)\n&quot;</span><span class="p">,</span>
<span class="w">    </span><span class="s2">&quot;axes = axes.ravel()\n&quot;</span><span class="p">,</span>
<span class="w">    </span><span class="s2">&quot;\n&quot;</span><span class="p">,</span>
<span class="w">    </span><span class="s2">&quot;r2_scores = []\n&quot;</span><span class="p">,</span>
<span class="w">    </span><span class="s2">&quot;for ax, deg in zip(axes, degrees):\n&quot;</span><span class="p">,</span>
<span class="w">    </span><span class="s2">&quot;    # Build Vandermonde matrix for training data and grid\n&quot;</span><span class="p">,</span>
<span class="w">    </span><span class="s2">&quot;    X_train = np.vander(x_poly,      N=deg + 1, increasing=True)\n&quot;</span><span class="p">,</span>
<span class="w">    </span><span class="s2">&quot;    X_grid  = np.vander(x_grid_poly, N=deg + 1, increasing=True)\n&quot;</span><span class="p">,</span>
<span class="w">    </span><span class="s2">&quot;\n&quot;</span><span class="p">,</span>
<span class="w">    </span><span class="s2">&quot;    coeffs = np.linalg.lstsq(X_train, y_poly, rcond=None)[0]\n&quot;</span><span class="p">,</span>
<span class="w">    </span><span class="s2">&quot;    y_pred_train = X_train @ coeffs\n&quot;</span><span class="p">,</span>
<span class="w">    </span><span class="s2">&quot;    y_pred_grid  = X_grid  @ coeffs\n&quot;</span><span class="p">,</span>
<span class="w">    </span><span class="s2">&quot;\n&quot;</span><span class="p">,</span>
<span class="w">    </span><span class="s2">&quot;    rss = ((y_poly - y_pred_train) ** 2).sum()\n&quot;</span><span class="p">,</span>
<span class="w">    </span><span class="s2">&quot;    tss = ((y_poly - y_poly.mean()) ** 2).sum()\n&quot;</span><span class="p">,</span>
<span class="w">    </span><span class="s2">&quot;    r2  = 1 - rss / tss\n&quot;</span><span class="p">,</span>
<span class="w">    </span><span class="s2">&quot;    adj_r2 = 1 - (1 - r2) * (n - 1) / (n - deg - 1)\n&quot;</span><span class="p">,</span>
<span class="w">    </span><span class="s2">&quot;    r2_scores.append(r2)\n&quot;</span><span class="p">,</span>
<span class="w">    </span><span class="s2">&quot;\n&quot;</span><span class="p">,</span>
<span class="w">    </span><span class="s2">&quot;    ax.scatter(x_poly, y_poly, color=&#39;steelblue&#39;, alpha=0.5, s=20, label=&#39;Data&#39;)\n&quot;</span><span class="p">,</span>
<span class="w">    </span><span class="s2">&quot;    ax.plot(x_grid_poly, np.sin(x_grid_poly), &#39;g--&#39;, lw=1.5, label=&#39;True sin(x)&#39;)\n&quot;</span><span class="p">,</span>
<span class="w">    </span><span class="s2">&quot;    ax.plot(x_grid_poly, y_pred_grid, &#39;r-&#39;, lw=2,\n&quot;</span><span class="p">,</span>
<span class="w">    </span><span class="s2">&quot;            label=f&#39;Degree {deg}&#39;)\n&quot;</span><span class="p">,</span>
<span class="w">    </span><span class="s2">&quot;    ax.set_ylim(-2.5, 2.5)\n&quot;</span><span class="p">,</span>
<span class="w">    </span><span class="s2">&quot;    ax.set_title(f&#39;Degree {deg}  |  R¬≤={r2:.3f}, adj-R¬≤={adj_r2:.3f}&#39;)\n&quot;</span><span class="p">,</span>
<span class="w">    </span><span class="s2">&quot;    ax.set_xlabel(&#39;x&#39;)\n&quot;</span><span class="p">,</span>
<span class="w">    </span><span class="s2">&quot;    ax.set_ylabel(&#39;y&#39;)\n&quot;</span><span class="p">,</span>
<span class="w">    </span><span class="s2">&quot;    ax.legend(fontsize=9)\n&quot;</span><span class="p">,</span>
<span class="w">    </span><span class="s2">&quot;\n&quot;</span><span class="p">,</span>
<span class="w">    </span><span class="s2">&quot;plt.suptitle(&#39;Polynomial Regression: Underfitting ‚Üí Overfitting&#39;, fontsize=13)\n&quot;</span><span class="p">,</span>
<span class="w">    </span><span class="s2">&quot;plt.tight_layout()\n&quot;</span><span class="p">,</span>
<span class="w">    </span><span class="s2">&quot;plt.show()\n&quot;</span><span class="p">,</span>
<span class="w">    </span><span class="s2">&quot;\n&quot;</span><span class="p">,</span>
<span class="w">    </span><span class="s2">&quot;print(&#39;Training R¬≤ by degree:&#39;)\n&quot;</span><span class="p">,</span>
<span class="w">    </span><span class="s2">&quot;for d, r2 in zip(degrees, r2_scores):\n&quot;</span><span class="p">,</span>
<span class="w">    </span><span class="s2">&quot;    bar = &#39;#&#39; * int(r2 * 40)\n&quot;</span><span class="p">,</span>
<span class="w">    </span><span class="s2">&quot;    print(f&#39;  Degree {d:&gt;2}: R¬≤={r2:.4f}  |{bar}&#39;)&quot;</span>
<span class="w">   </span><span class="p">]</span>
<span class="w">  </span><span class="p">},</span>
<span class="w">  </span><span class="p">{</span>
<span class="w">   </span><span class="nt">&quot;cell_type&quot;</span><span class="p">:</span><span class="w"> </span><span class="s2">&quot;markdown&quot;</span><span class="p">,</span>
<span class="w">   </span><span class="nt">&quot;id&quot;</span><span class="p">:</span><span class="w"> </span><span class="s2">&quot;b2c3d4e5-0011-4000-8000-000000000011&quot;</span><span class="p">,</span>
<span class="w">   </span><span class="nt">&quot;metadata&quot;</span><span class="p">:</span><span class="w"> </span><span class="p">{},</span>
<span class="w">   </span><span class="nt">&quot;source&quot;</span><span class="p">:</span><span class="w"> </span><span class="p">[</span>
<span class="w">    </span><span class="s2">&quot;## 5. Logistic Regression (Binary Classification)\n&quot;</span><span class="p">,</span>
<span class="w">    </span><span class="s2">&quot;\n&quot;</span><span class="p">,</span>
<span class="w">    </span><span class="s2">&quot;When the response is binary ($y \\in \\{0, 1\\}$), linear regression is inappropriate. **Logistic regression** models the log-odds:\n&quot;</span><span class="p">,</span>
<span class="w">    </span><span class="s2">&quot;\n&quot;</span><span class="p">,</span>
<span class="w">    </span><span class="s2">&quot;$$\\log\\frac{P(y=1 \\mid x)}{1 - P(y=1 \\mid x)} = \\beta_0 + \\beta_1 x$$\n&quot;</span><span class="p">,</span>
<span class="w">    </span><span class="s2">&quot;\n&quot;</span><span class="p">,</span>
<span class="w">    </span><span class="s2">&quot;Transforming back via the **sigmoid function**:\n&quot;</span><span class="p">,</span>
<span class="w">    </span><span class="s2">&quot;\n&quot;</span><span class="p">,</span>
<span class="w">    </span><span class="s2">&quot;$$P(y=1 \\mid x) = \\sigma(\\beta_0 + \\beta_1 x) = \\frac{1}{1 + e^{-(\\beta_0 + \\beta_1 x)}}$$\n&quot;</span><span class="p">,</span>
<span class="w">    </span><span class="s2">&quot;\n&quot;</span><span class="p">,</span>
<span class="w">    </span><span class="s2">&quot;Parameters are estimated by **maximum likelihood** (not OLS). The decision boundary is where $P = 0.5$, i.e., $\\beta_0 + \\beta_1 x = 0$.\n&quot;</span><span class="p">,</span>
<span class="w">    </span><span class="s2">&quot;\n&quot;</span><span class="p">,</span>
<span class="w">    </span><span class="s2">&quot;**Interpretation of coefficients:** $e^{\\beta_j}$ is the **odds ratio** ‚Äî the factor by which the odds of $y=1$ multiply for a unit increase in $x_j$, holding other predictors constant.&quot;</span>
<span class="w">   </span><span class="p">]</span>
<span class="w">  </span><span class="p">},</span>
<span class="w">  </span><span class="p">{</span>
<span class="w">   </span><span class="nt">&quot;cell_type&quot;</span><span class="p">:</span><span class="w"> </span><span class="s2">&quot;code&quot;</span><span class="p">,</span>
<span class="w">   </span><span class="nt">&quot;execution_count&quot;</span><span class="p">:</span><span class="w"> </span><span class="kc">null</span><span class="p">,</span>
<span class="w">   </span><span class="nt">&quot;id&quot;</span><span class="p">:</span><span class="w"> </span><span class="s2">&quot;b2c3d4e5-0012-4000-8000-000000000012&quot;</span><span class="p">,</span>
<span class="w">   </span><span class="nt">&quot;metadata&quot;</span><span class="p">:</span><span class="w"> </span><span class="p">{},</span>
<span class="w">   </span><span class="nt">&quot;outputs&quot;</span><span class="p">:</span><span class="w"> </span><span class="p">[],</span>
<span class="w">   </span><span class="nt">&quot;source&quot;</span><span class="p">:</span><span class="w"> </span><span class="p">[</span>
<span class="w">    </span><span class="s2">&quot;# Scenario: predict exam pass/fail from hours studied\n&quot;</span><span class="p">,</span>
<span class="w">    </span><span class="s2">&quot;n_logit = 100\n&quot;</span><span class="p">,</span>
<span class="w">    </span><span class="s2">&quot;hours   = rng.uniform(0, 10, n_logit)\n&quot;</span><span class="p">,</span>
<span class="w">    </span><span class="s2">&quot;\n&quot;</span><span class="p">,</span>
<span class="w">    </span><span class="s2">&quot;# True logistic model: log-odds = -4 + 1.2 * hours\n&quot;</span><span class="p">,</span>
<span class="w">    </span><span class="s2">&quot;true_b0, true_b1 = -4.0, 1.2\n&quot;</span><span class="p">,</span>
<span class="w">    </span><span class="s2">&quot;log_odds = true_b0 + true_b1 * hours\n&quot;</span><span class="p">,</span>
<span class="w">    </span><span class="s2">&quot;prob_pass = 1 / (1 + np.exp(-log_odds))\n&quot;</span><span class="p">,</span>
<span class="w">    </span><span class="s2">&quot;passed = rng.binomial(1, prob_pass)                        # 0 = fail, 1 = pass\n&quot;</span><span class="p">,</span>
<span class="w">    </span><span class="s2">&quot;\n&quot;</span><span class="p">,</span>
<span class="w">    </span><span class="s2">&quot;# Fit via scikit-learn (or manual gradient descent fallback)\n&quot;</span><span class="p">,</span>
<span class="w">    </span><span class="s2">&quot;if SKLEARN:\n&quot;</span><span class="p">,</span>
<span class="w">    </span><span class="s2">&quot;    X_logit = hours.reshape(-1, 1)\n&quot;</span><span class="p">,</span>
<span class="w">    </span><span class="s2">&quot;    clf = LogisticRegression(solver=&#39;lbfgs&#39;, C=1e6)        # large C ‚Üí minimal regularisation\n&quot;</span><span class="p">,</span>
<span class="w">    </span><span class="s2">&quot;    clf.fit(X_logit, passed)\n&quot;</span><span class="p">,</span>
<span class="w">    </span><span class="s2">&quot;    b0_hat = clf.intercept_[0]\n&quot;</span><span class="p">,</span>
<span class="w">    </span><span class="s2">&quot;    b1_hat = clf.coef_[0, 0]\n&quot;</span><span class="p">,</span>
<span class="w">    </span><span class="s2">&quot;    print(f&#39;Fitted (sklearn):   Œ≤‚ÇÄ={b0_hat:.4f}, Œ≤‚ÇÅ={b1_hat:.4f}&#39;)\n&quot;</span><span class="p">,</span>
<span class="w">    </span><span class="s2">&quot;else:\n&quot;</span><span class="p">,</span>
<span class="w">    </span><span class="s2">&quot;    # Manual gradient descent\n&quot;</span><span class="p">,</span>
<span class="w">    </span><span class="s2">&quot;    def sigmoid(z): return 1 / (1 + np.exp(-z))\n&quot;</span><span class="p">,</span>
<span class="w">    </span><span class="s2">&quot;    b0_hat, b1_hat = 0.0, 0.0\n&quot;</span><span class="p">,</span>
<span class="w">    </span><span class="s2">&quot;    lr = 0.05\n&quot;</span><span class="p">,</span>
<span class="w">    </span><span class="s2">&quot;    for _ in range(3000):\n&quot;</span><span class="p">,</span>
<span class="w">    </span><span class="s2">&quot;        p = sigmoid(b0_hat + b1_hat * hours)\n&quot;</span><span class="p">,</span>
<span class="w">    </span><span class="s2">&quot;        err = p - passed\n&quot;</span><span class="p">,</span>
<span class="w">    </span><span class="s2">&quot;        b0_hat -= lr * err.mean()\n&quot;</span><span class="p">,</span>
<span class="w">    </span><span class="s2">&quot;        b1_hat -= lr * (err * hours).mean()\n&quot;</span><span class="p">,</span>
<span class="w">    </span><span class="s2">&quot;    print(f&#39;Fitted (grad desc): Œ≤‚ÇÄ={b0_hat:.4f}, Œ≤‚ÇÅ={b1_hat:.4f}&#39;)\n&quot;</span><span class="p">,</span>
<span class="w">    </span><span class="s2">&quot;\n&quot;</span><span class="p">,</span>
<span class="w">    </span><span class="s2">&quot;print(f&#39;True coefficients:  Œ≤‚ÇÄ={true_b0}, Œ≤‚ÇÅ={true_b1}&#39;)\n&quot;</span><span class="p">,</span>
<span class="w">    </span><span class="s2">&quot;\n&quot;</span><span class="p">,</span>
<span class="w">    </span><span class="s2">&quot;# Decision boundary: b0 + b1*x = 0  ‚Üí  x = -b0/b1\n&quot;</span><span class="p">,</span>
<span class="w">    </span><span class="s2">&quot;boundary = -b0_hat / b1_hat\n&quot;</span><span class="p">,</span>
<span class="w">    </span><span class="s2">&quot;print(f&#39;Decision boundary   : {boundary:.2f} hours&#39;)\n&quot;</span><span class="p">,</span>
<span class="w">    </span><span class="s2">&quot;print(f&#39;Odds ratio e^Œ≤‚ÇÅ     : {np.exp(b1_hat):.4f}&#39;)\n&quot;</span><span class="p">,</span>
<span class="w">    </span><span class="s2">&quot;\n&quot;</span><span class="p">,</span>
<span class="w">    </span><span class="s2">&quot;# Visualise\n&quot;</span><span class="p">,</span>
<span class="w">    </span><span class="s2">&quot;x_range = np.linspace(0, 10, 300)\n&quot;</span><span class="p">,</span>
<span class="w">    </span><span class="s2">&quot;p_hat   = 1 / (1 + np.exp(-(b0_hat + b1_hat * x_range)))\n&quot;</span><span class="p">,</span>
<span class="w">    </span><span class="s2">&quot;\n&quot;</span><span class="p">,</span>
<span class="w">    </span><span class="s2">&quot;fig, axes = plt.subplots(1, 2, figsize=(12, 4))\n&quot;</span><span class="p">,</span>
<span class="w">    </span><span class="s2">&quot;\n&quot;</span><span class="p">,</span>
<span class="w">    </span><span class="s2">&quot;# Left: sigmoid curve\n&quot;</span><span class="p">,</span>
<span class="w">    </span><span class="s2">&quot;ax = axes[0]\n&quot;</span><span class="p">,</span>
<span class="w">    </span><span class="s2">&quot;ax.scatter(hours[passed == 0], passed[passed == 0], color=&#39;tomato&#39;,    alpha=0.5, s=30,\n&quot;</span><span class="p">,</span>
<span class="w">    </span><span class="s2">&quot;           label=&#39;Fail (0)&#39;, zorder=3)\n&quot;</span><span class="p">,</span>
<span class="w">    </span><span class="s2">&quot;ax.scatter(hours[passed == 1], passed[passed == 1], color=&#39;steelblue&#39;, alpha=0.5, s=30,\n&quot;</span><span class="p">,</span>
<span class="w">    </span><span class="s2">&quot;           label=&#39;Pass (1)&#39;, zorder=3)\n&quot;</span><span class="p">,</span>
<span class="w">    </span><span class="s2">&quot;ax.plot(x_range, p_hat, &#39;k-&#39;, lw=2, label=&#39;Fitted P(pass)&#39;)\n&quot;</span><span class="p">,</span>
<span class="w">    </span><span class="s2">&quot;ax.axvline(boundary, color=&#39;green&#39;, ls=&#39;--&#39;, lw=1.5, label=f&#39;Boundary = {boundary:.1f}h&#39;)\n&quot;</span><span class="p">,</span>
<span class="w">    </span><span class="s2">&quot;ax.axhline(0.5, color=&#39;gray&#39;, ls=&#39;:&#39;, lw=1)\n&quot;</span><span class="p">,</span>
<span class="w">    </span><span class="s2">&quot;ax.set_xlabel(&#39;Hours studied&#39;)\n&quot;</span><span class="p">,</span>
<span class="w">    </span><span class="s2">&quot;ax.set_ylabel(&#39;P(pass)&#39;)\n&quot;</span><span class="p">,</span>
<span class="w">    </span><span class="s2">&quot;ax.set_title(&#39;Logistic Regression: Sigmoid Curve&#39;)\n&quot;</span><span class="p">,</span>
<span class="w">    </span><span class="s2">&quot;ax.legend()\n&quot;</span><span class="p">,</span>
<span class="w">    </span><span class="s2">&quot;\n&quot;</span><span class="p">,</span>
<span class="w">    </span><span class="s2">&quot;# Right: log-odds plot\n&quot;</span><span class="p">,</span>
<span class="w">    </span><span class="s2">&quot;ax = axes[1]\n&quot;</span><span class="p">,</span>
<span class="w">    </span><span class="s2">&quot;ax.plot(x_range, b0_hat + b1_hat * x_range, &#39;purple&#39;, lw=2, label=&#39;Log-odds&#39;)\n&quot;</span><span class="p">,</span>
<span class="w">    </span><span class="s2">&quot;ax.axhline(0, color=&#39;gray&#39;, ls=&#39;--&#39;)\n&quot;</span><span class="p">,</span>
<span class="w">    </span><span class="s2">&quot;ax.axvline(boundary, color=&#39;green&#39;, ls=&#39;--&#39;, lw=1.5, label=f&#39;Boundary ({boundary:.1f}h)&#39;)\n&quot;</span><span class="p">,</span>
<span class="w">    </span><span class="s2">&quot;ax.fill_between(x_range, b0_hat + b1_hat * x_range, 0,\n&quot;</span><span class="p">,</span>
<span class="w">    </span><span class="s2">&quot;                where=(b0_hat + b1_hat * x_range &gt; 0), alpha=0.15, color=&#39;steelblue&#39;, label=&#39;Predict pass&#39;)\n&quot;</span><span class="p">,</span>
<span class="w">    </span><span class="s2">&quot;ax.fill_between(x_range, b0_hat + b1_hat * x_range, 0,\n&quot;</span><span class="p">,</span>
<span class="w">    </span><span class="s2">&quot;                where=(b0_hat + b1_hat * x_range &lt; 0), alpha=0.15, color=&#39;tomato&#39;, label=&#39;Predict fail&#39;)\n&quot;</span><span class="p">,</span>
<span class="w">    </span><span class="s2">&quot;ax.set_xlabel(&#39;Hours studied&#39;)\n&quot;</span><span class="p">,</span>
<span class="w">    </span><span class="s2">&quot;ax.set_ylabel(&#39;Log-odds&#39;)\n&quot;</span><span class="p">,</span>
<span class="w">    </span><span class="s2">&quot;ax.set_title(&#39;Log-odds and Decision Boundary&#39;)\n&quot;</span><span class="p">,</span>
<span class="w">    </span><span class="s2">&quot;ax.legend(fontsize=9)\n&quot;</span><span class="p">,</span>
<span class="w">    </span><span class="s2">&quot;\n&quot;</span><span class="p">,</span>
<span class="w">    </span><span class="s2">&quot;plt.tight_layout()\n&quot;</span><span class="p">,</span>
<span class="w">    </span><span class="s2">&quot;plt.show()&quot;</span>
<span class="w">   </span><span class="p">]</span>
<span class="w">  </span><span class="p">},</span>
<span class="w">  </span><span class="p">{</span>
<span class="w">   </span><span class="nt">&quot;cell_type&quot;</span><span class="p">:</span><span class="w"> </span><span class="s2">&quot;markdown&quot;</span><span class="p">,</span>
<span class="w">   </span><span class="nt">&quot;id&quot;</span><span class="p">:</span><span class="w"> </span><span class="s2">&quot;b2c3d4e5-0013-4000-8000-000000000013&quot;</span><span class="p">,</span>
<span class="w">   </span><span class="nt">&quot;metadata&quot;</span><span class="p">:</span><span class="w"> </span><span class="p">{},</span>
<span class="w">   </span><span class="nt">&quot;source&quot;</span><span class="p">:</span><span class="w"> </span><span class="p">[</span>
<span class="w">    </span><span class="s2">&quot;## 6. Checking Regression Assumptions\n&quot;</span><span class="p">,</span>
<span class="w">    </span><span class="s2">&quot;\n&quot;</span><span class="p">,</span>
<span class="w">    </span><span class="s2">&quot;OLS inference is valid only when the Gauss-Markov assumptions hold. Violations affect different inferential properties:\n&quot;</span><span class="p">,</span>
<span class="w">    </span><span class="s2">&quot;\n&quot;</span><span class="p">,</span>
<span class="w">    </span><span class="s2">&quot;| Assumption | Consequence if violated | Diagnostic |\n&quot;</span><span class="p">,</span>
<span class="w">    </span><span class="s2">&quot;|------------|------------------------|------------|\n&quot;</span><span class="p">,</span>
<span class="w">    </span><span class="s2">&quot;| **Linearity** | Biased estimates | Residuals vs Fitted ‚Äî look for curvature |\n&quot;</span><span class="p">,</span>
<span class="w">    </span><span class="s2">&quot;| **Independence** | Underestimated SEs | Durbin-Watson test (time-series) |\n&quot;</span><span class="p">,</span>
<span class="w">    </span><span class="s2">&quot;| **Homoscedasticity** | Inefficient estimates, invalid SEs | Scale-Location plot, Breusch-Pagan |\n&quot;</span><span class="p">,</span>
<span class="w">    </span><span class="s2">&quot;| **Normality of errors** | Invalid t/F intervals (small n) | Q-Q plot, Shapiro-Wilk test |\n&quot;</span><span class="p">,</span>
<span class="w">    </span><span class="s2">&quot;| **No perfect multicollinearity** | Singular (X&#39;X), unreliable estimates | VIF &gt; 10 is problematic |\n&quot;</span><span class="p">,</span>
<span class="w">    </span><span class="s2">&quot;\n&quot;</span><span class="p">,</span>
<span class="w">    </span><span class="s2">&quot;Below we deliberately create two datasets ‚Äî one that satisfies assumptions and one that violates **homoscedasticity** (heteroscedastic errors) ‚Äî to compare the diagnostic plots.&quot;</span>
<span class="w">   </span><span class="p">]</span>
<span class="w">  </span><span class="p">},</span>
<span class="w">  </span><span class="p">{</span>
<span class="w">   </span><span class="nt">&quot;cell_type&quot;</span><span class="p">:</span><span class="w"> </span><span class="s2">&quot;code&quot;</span><span class="p">,</span>
<span class="w">   </span><span class="nt">&quot;execution_count&quot;</span><span class="p">:</span><span class="w"> </span><span class="kc">null</span><span class="p">,</span>
<span class="w">   </span><span class="nt">&quot;id&quot;</span><span class="p">:</span><span class="w"> </span><span class="s2">&quot;b2c3d4e5-0014-4000-8000-000000000014&quot;</span><span class="p">,</span>
<span class="w">   </span><span class="nt">&quot;metadata&quot;</span><span class="p">:</span><span class="w"> </span><span class="p">{},</span>
<span class="w">   </span><span class="nt">&quot;outputs&quot;</span><span class="p">:</span><span class="w"> </span><span class="p">[],</span>
<span class="w">   </span><span class="nt">&quot;source&quot;</span><span class="p">:</span><span class="w"> </span><span class="p">[</span>
<span class="w">    </span><span class="s2">&quot;n_diag = 100\n&quot;</span><span class="p">,</span>
<span class="w">    </span><span class="s2">&quot;x_diag = rng.uniform(1, 10, n_diag)\n&quot;</span><span class="p">,</span>
<span class="w">    </span><span class="s2">&quot;\n&quot;</span><span class="p">,</span>
<span class="w">    </span><span class="s2">&quot;# --- Dataset A: well-behaved (homoscedastic, normal errors)\n&quot;</span><span class="p">,</span>
<span class="w">    </span><span class="s2">&quot;y_good = 3 + 2 * x_diag + rng.normal(0, 1.5, n_diag)\n&quot;</span><span class="p">,</span>
<span class="w">    </span><span class="s2">&quot;\n&quot;</span><span class="p">,</span>
<span class="w">    </span><span class="s2">&quot;# --- Dataset B: heteroscedastic (variance grows with x)\n&quot;</span><span class="p">,</span>
<span class="w">    </span><span class="s2">&quot;y_hetero = 3 + 2 * x_diag + rng.normal(0, 0.5 * x_diag, n_diag)\n&quot;</span><span class="p">,</span>
<span class="w">    </span><span class="s2">&quot;\n&quot;</span><span class="p">,</span>
<span class="w">    </span><span class="s2">&quot;def fit_ols_1d(x, y):\n&quot;</span><span class="p">,</span>
<span class="w">    </span><span class="s2">&quot;    slope, intercept, *_ = stats.linregress(x, y)\n&quot;</span><span class="p">,</span>
<span class="w">    </span><span class="s2">&quot;    y_hat = intercept + slope * x\n&quot;</span><span class="p">,</span>
<span class="w">    </span><span class="s2">&quot;    resid = y - y_hat\n&quot;</span><span class="p">,</span>
<span class="w">    </span><span class="s2">&quot;    s = np.sqrt((resid**2).sum() / (len(x) - 2))\n&quot;</span><span class="p">,</span>
<span class="w">    </span><span class="s2">&quot;    return y_hat, resid, s\n&quot;</span><span class="p">,</span>
<span class="w">    </span><span class="s2">&quot;\n&quot;</span><span class="p">,</span>
<span class="w">    </span><span class="s2">&quot;y_hat_good,   resid_good,   s_good   = fit_ols_1d(x_diag, y_good)\n&quot;</span><span class="p">,</span>
<span class="w">    </span><span class="s2">&quot;y_hat_hetero, resid_hetero, s_hetero = fit_ols_1d(x_diag, y_hetero)\n&quot;</span><span class="p">,</span>
<span class="w">    </span><span class="s2">&quot;\n&quot;</span><span class="p">,</span>
<span class="w">    </span><span class="s2">&quot;def add_lowess_line(ax, x, y, frac=0.5, color=&#39;red&#39;):\n&quot;</span><span class="p">,</span>
<span class="w">    </span><span class="s2">&quot;    \&quot;\&quot;\&quot;Simple moving average as a smoother (no statsmodels dependency).\&quot;\&quot;\&quot;\n&quot;</span><span class="p">,</span>
<span class="w">    </span><span class="s2">&quot;    idx    = np.argsort(x)\n&quot;</span><span class="p">,</span>
<span class="w">    </span><span class="s2">&quot;    x_s, y_s = x[idx], y[idx]\n&quot;</span><span class="p">,</span>
<span class="w">    </span><span class="s2">&quot;    w = max(1, int(frac * len(x)))\n&quot;</span><span class="p">,</span>
<span class="w">    </span><span class="s2">&quot;    y_smooth = np.convolve(y_s, np.ones(w)/w, mode=&#39;same&#39;)\n&quot;</span><span class="p">,</span>
<span class="w">    </span><span class="s2">&quot;    ax.plot(x_s, y_smooth, color=color, lw=1.5, ls=&#39;--&#39;)\n&quot;</span><span class="p">,</span>
<span class="w">    </span><span class="s2">&quot;\n&quot;</span><span class="p">,</span>
<span class="w">    </span><span class="s2">&quot;fig, axes = plt.subplots(2, 3, figsize=(13, 8))\n&quot;</span><span class="p">,</span>
<span class="w">    </span><span class="s2">&quot;titles_row = [&#39;Homoscedastic (well-behaved)&#39;, &#39;Heteroscedastic (variance grows with x)&#39;]\n&quot;</span><span class="p">,</span>
<span class="w">    </span><span class="s2">&quot;\n&quot;</span><span class="p">,</span>
<span class="w">    </span><span class="s2">&quot;for row, (x, y, y_hat, resid, s, label) in enumerate([\n&quot;</span><span class="p">,</span>
<span class="w">    </span><span class="s2">&quot;        (x_diag, y_good,   y_hat_good,   resid_good,   s_good,   &#39;A: Well-behaved&#39;),\n&quot;</span><span class="p">,</span>
<span class="w">    </span><span class="s2">&quot;        (x_diag, y_hetero, y_hat_hetero, resid_hetero, s_hetero, &#39;B: Heteroscedastic&#39;)]):\n&quot;</span><span class="p">,</span>
<span class="w">    </span><span class="s2">&quot;\n&quot;</span><span class="p">,</span>
<span class="w">    </span><span class="s2">&quot;    std_resid = resid / s\n&quot;</span><span class="p">,</span>
<span class="w">    </span><span class="s2">&quot;\n&quot;</span><span class="p">,</span>
<span class="w">    </span><span class="s2">&quot;    # (1) Residuals vs Fitted\n&quot;</span><span class="p">,</span>
<span class="w">    </span><span class="s2">&quot;    ax = axes[row, 0]\n&quot;</span><span class="p">,</span>
<span class="w">    </span><span class="s2">&quot;    ax.scatter(y_hat, resid, alpha=0.5, color=&#39;steelblue&#39;, s=20)\n&quot;</span><span class="p">,</span>
<span class="w">    </span><span class="s2">&quot;    ax.axhline(0, color=&#39;red&#39;, lw=1.5, ls=&#39;--&#39;)\n&quot;</span><span class="p">,</span>
<span class="w">    </span><span class="s2">&quot;    add_lowess_line(ax, y_hat, resid)\n&quot;</span><span class="p">,</span>
<span class="w">    </span><span class="s2">&quot;    ax.set_xlabel(&#39;Fitted&#39;); ax.set_ylabel(&#39;Residuals&#39;)\n&quot;</span><span class="p">,</span>
<span class="w">    </span><span class="s2">&quot;    ax.set_title(f&#39;{label}\\nResiduals vs Fitted&#39;)\n&quot;</span><span class="p">,</span>
<span class="w">    </span><span class="s2">&quot;\n&quot;</span><span class="p">,</span>
<span class="w">    </span><span class="s2">&quot;    # (2) Q-Q plot\n&quot;</span><span class="p">,</span>
<span class="w">    </span><span class="s2">&quot;    ax = axes[row, 1]\n&quot;</span><span class="p">,</span>
<span class="w">    </span><span class="s2">&quot;    (osm, osr), (sq, iq, rq) = stats.probplot(resid, dist=&#39;norm&#39;)\n&quot;</span><span class="p">,</span>
<span class="w">    </span><span class="s2">&quot;    ax.scatter(osm, osr, alpha=0.5, color=&#39;steelblue&#39;, s=20)\n&quot;</span><span class="p">,</span>
<span class="w">    </span><span class="s2">&quot;    ax.plot(osm, sq * np.array(osm) + iq, &#39;r-&#39;, lw=1.5)\n&quot;</span><span class="p">,</span>
<span class="w">    </span><span class="s2">&quot;    ax.set_xlabel(&#39;Theoretical quantiles&#39;); ax.set_ylabel(&#39;Sample quantiles&#39;)\n&quot;</span><span class="p">,</span>
<span class="w">    </span><span class="s2">&quot;    ax.set_title(&#39;Normal Q-Q&#39;)\n&quot;</span><span class="p">,</span>
<span class="w">    </span><span class="s2">&quot;\n&quot;</span><span class="p">,</span>
<span class="w">    </span><span class="s2">&quot;    # (3) Scale-Location\n&quot;</span><span class="p">,</span>
<span class="w">    </span><span class="s2">&quot;    ax = axes[row, 2]\n&quot;</span><span class="p">,</span>
<span class="w">    </span><span class="s2">&quot;    ax.scatter(y_hat, np.sqrt(np.abs(std_resid)), alpha=0.5, color=&#39;steelblue&#39;, s=20)\n&quot;</span><span class="p">,</span>
<span class="w">    </span><span class="s2">&quot;    add_lowess_line(ax, y_hat, np.sqrt(np.abs(std_resid)))\n&quot;</span><span class="p">,</span>
<span class="w">    </span><span class="s2">&quot;    ax.set_xlabel(&#39;Fitted&#39;); ax.set_ylabel(&#39;‚àö|Std residuals|&#39;)\n&quot;</span><span class="p">,</span>
<span class="w">    </span><span class="s2">&quot;    ax.set_title(&#39;Scale-Location&#39;)\n&quot;</span><span class="p">,</span>
<span class="w">    </span><span class="s2">&quot;\n&quot;</span><span class="p">,</span>
<span class="w">    </span><span class="s2">&quot;    # Breusch-Pagan-like test: correlation between |residuals| and fitted values\n&quot;</span><span class="p">,</span>
<span class="w">    </span><span class="s2">&quot;    corr, bp_p = stats.pearsonr(y_hat, np.abs(resid))\n&quot;</span><span class="p">,</span>
<span class="w">    </span><span class="s2">&quot;    print(f&#39;{label}: |residual| ~ fitted  r={corr:.3f}, p={bp_p:.4f}&#39;)\n&quot;</span><span class="p">,</span>
<span class="w">    </span><span class="s2">&quot;\n&quot;</span><span class="p">,</span>
<span class="w">    </span><span class="s2">&quot;plt.suptitle(&#39;Regression Diagnostic Plots: Well-behaved vs Heteroscedastic&#39;, fontsize=13)\n&quot;</span><span class="p">,</span>
<span class="w">    </span><span class="s2">&quot;plt.tight_layout()\n&quot;</span><span class="p">,</span>
<span class="w">    </span><span class="s2">&quot;plt.show()&quot;</span>
<span class="w">   </span><span class="p">]</span>
<span class="w">  </span><span class="p">},</span>
<span class="w">  </span><span class="p">{</span>
<span class="w">   </span><span class="nt">&quot;cell_type&quot;</span><span class="p">:</span><span class="w"> </span><span class="s2">&quot;markdown&quot;</span><span class="p">,</span>
<span class="w">   </span><span class="nt">&quot;id&quot;</span><span class="p">:</span><span class="w"> </span><span class="s2">&quot;b2c3d4e5-0015-4000-8000-000000000015&quot;</span><span class="p">,</span>
<span class="w">   </span><span class="nt">&quot;metadata&quot;</span><span class="p">:</span><span class="w"> </span><span class="p">{},</span>
<span class="w">   </span><span class="nt">&quot;source&quot;</span><span class="p">:</span><span class="w"> </span><span class="p">[</span>
<span class="w">    </span><span class="s2">&quot;## Summary\n&quot;</span><span class="p">,</span>
<span class="w">    </span><span class="s2">&quot;\n&quot;</span><span class="p">,</span>
<span class="w">    </span><span class="s2">&quot;### Regression Model Comparison\n&quot;</span><span class="p">,</span>
<span class="w">    </span><span class="s2">&quot;\n&quot;</span><span class="p">,</span>
<span class="w">    </span><span class="s2">&quot;| Model | Response | Key idea | Fitting method | Interpretation |\n&quot;</span><span class="p">,</span>
<span class="w">    </span><span class="s2">&quot;|-------|----------|----------|----------------|----------------|\n&quot;</span><span class="p">,</span>
<span class="w">    </span><span class="s2">&quot;| Simple linear | Continuous | One predictor | OLS | Œ≤‚ÇÅ: change in ≈∑ per unit x |\n&quot;</span><span class="p">,</span>
<span class="w">    </span><span class="s2">&quot;| Multiple linear | Continuous | Multiple predictors | OLS | Œ≤_j: partial effect of x_j |\n&quot;</span><span class="p">,</span>
<span class="w">    </span><span class="s2">&quot;| Polynomial | Continuous | Non-linear with OLS | OLS in transformed space | Compare adj-R¬≤ across degrees |\n&quot;</span><span class="p">,</span>
<span class="w">    </span><span class="s2">&quot;| Logistic | Binary (0/1) | Log-odds linear in x | MLE (iterative) | e^Œ≤_j: odds ratio |\n&quot;</span><span class="p">,</span>
<span class="w">    </span><span class="s2">&quot;\n&quot;</span><span class="p">,</span>
<span class="w">    </span><span class="s2">&quot;### Workflow for Any Regression Problem\n&quot;</span><span class="p">,</span>
<span class="w">    </span><span class="s2">&quot;\n&quot;</span><span class="p">,</span>
<span class="w">    </span><span class="s2">&quot;1. **Explore** ‚Äî scatter plots, correlation matrix, check for outliers\n&quot;</span><span class="p">,</span>
<span class="w">    </span><span class="s2">&quot;2. **Fit** ‚Äî choose model family (linear / polynomial / logistic / etc.)\n&quot;</span><span class="p">,</span>
<span class="w">    </span><span class="s2">&quot;3. **Evaluate** ‚Äî R¬≤, adjusted R¬≤, AIC/BIC, cross-validation MSE\n&quot;</span><span class="p">,</span>
<span class="w">    </span><span class="s2">&quot;4. **Diagnose** ‚Äî residual plots, Q-Q plot, scale-location, leverage\n&quot;</span><span class="p">,</span>
<span class="w">    </span><span class="s2">&quot;5. **Iterate** ‚Äî transform variables, add/remove predictors, consider regularisation (Ridge/Lasso)\n&quot;</span><span class="p">,</span>
<span class="w">    </span><span class="s2">&quot;6. **Report** ‚Äî coefficient estimates, confidence intervals, effect sizes\n&quot;</span><span class="p">,</span>
<span class="w">    </span><span class="s2">&quot;\n&quot;</span><span class="p">,</span>
<span class="w">    </span><span class="s2">&quot;### Extensions to Explore\n&quot;</span><span class="p">,</span>
<span class="w">    </span><span class="s2">&quot;- **Regularised regression**: Ridge (L2), Lasso (L1), ElasticNet ‚Äî handle multicollinearity and feature selection\n&quot;</span><span class="p">,</span>
<span class="w">    </span><span class="s2">&quot;- **Generalised Linear Models (GLM)**: Poisson (count data), Negative Binomial, Gamma ‚Äî when errors are non-normal\n&quot;</span><span class="p">,</span>
<span class="w">    </span><span class="s2">&quot;- **Mixed-effects models**: repeated measures and nested data structures\n&quot;</span><span class="p">,</span>
<span class="w">    </span><span class="s2">&quot;- **Robust regression**: Huber loss ‚Äî reduces sensitivity to outliers\n&quot;</span><span class="p">,</span>
<span class="w">    </span><span class="s2">&quot;- **Non-parametric regression**: Kernel regression, GAM ‚Äî flexible functional forms&quot;</span>
<span class="w">   </span><span class="p">]</span>
<span class="w">  </span><span class="p">}</span>
<span class="w"> </span><span class="p">],</span>
<span class="w"> </span><span class="nt">&quot;metadata&quot;</span><span class="p">:</span><span class="w"> </span><span class="p">{</span>
<span class="w">  </span><span class="nt">&quot;kernelspec&quot;</span><span class="p">:</span><span class="w"> </span><span class="p">{</span>
<span class="w">   </span><span class="nt">&quot;display_name&quot;</span><span class="p">:</span><span class="w"> </span><span class="s2">&quot;Python 3&quot;</span><span class="p">,</span>
<span class="w">   </span><span class="nt">&quot;language&quot;</span><span class="p">:</span><span class="w"> </span><span class="s2">&quot;python&quot;</span><span class="p">,</span>
<span class="w">   </span><span class="nt">&quot;name&quot;</span><span class="p">:</span><span class="w"> </span><span class="s2">&quot;python3&quot;</span>
<span class="w">  </span><span class="p">},</span>
<span class="w">  </span><span class="nt">&quot;language_info&quot;</span><span class="p">:</span><span class="w"> </span><span class="p">{</span>
<span class="w">   </span><span class="nt">&quot;name&quot;</span><span class="p">:</span><span class="w"> </span><span class="s2">&quot;python&quot;</span><span class="p">,</span>
<span class="w">   </span><span class="nt">&quot;version&quot;</span><span class="p">:</span><span class="w"> </span><span class="s2">&quot;3.11.0&quot;</span>
<span class="w">  </span><span class="p">}</span>
<span class="w"> </span><span class="p">},</span>
<span class="w"> </span><span class="nt">&quot;nbformat&quot;</span><span class="p">:</span><span class="w"> </span><span class="mi">4</span><span class="p">,</span>
<span class="w"> </span><span class="nt">&quot;nbformat_minor&quot;</span><span class="p">:</span><span class="w"> </span><span class="mi">5</span>
<span class="p">}</span>
</code></pre></div></td></tr></table></div>

    </div>
</article>

            </div>
        </main>
    </div>

    <script src="/study/static/js/app.js"></script>
    <script>
        function switchLanguage(newLang) {
            const path = window.location.pathname;
            const base = '/study';
            const currentLang = 'en';
            const newPath = path.replace(base + '/' + currentLang + '/', base + '/' + newLang + '/');
            window.location.href = newPath + window.location.search;
        }
    </script>
    
<script>
    document.getElementById('copy-code-btn').addEventListener('click', function() {
        var code = document.querySelector('.example-code .highlight');
        // Extract just the code text, not line numbers
        var lines = code.querySelectorAll('.code pre span.line');
        var text;
        if (lines.length > 0) {
            text = Array.from(lines).map(function(l) { return l.textContent; }).join('\n');
        } else {
            // Fallback: get all code text
            var codeEl = code.querySelector('pre code') || code.querySelector('pre');
            text = codeEl ? codeEl.textContent : code.textContent;
        }
        navigator.clipboard.writeText(text);
        this.textContent = 'Copied!';
        var btn = this;
        setTimeout(function() { btn.textContent = 'Copy code'; }, 2000);
    });
</script>

</body>
</html>
{% endraw %}