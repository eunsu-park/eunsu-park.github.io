% custom_macros.tex
% Demonstrates custom commands, environments, and macro creation
% Shows how to create reusable LaTeX components for efficient document writing

\documentclass[12pt,a4paper]{article}

\usepackage{amsmath, amssymb, amsthm}
\usepackage{xcolor}
\usepackage{tcolorbox}
\usepackage[margin=1in]{geometry}
\usepackage{tikz}
\usepackage{enumitem}

% ==================== Custom Math Commands ====================

% Vector notation
\newcommand{\vect}[1]{\mathbf{#1}}
\newcommand{\uvect}[1]{\hat{\mathbf{#1}}}  % Unit vector

% Matrix notation
\newcommand{\mat}[1]{\mathbf{#1}}
\newcommand{\trans}[1]{#1^{\mathsf{T}}}  % Transpose
\newcommand{\inv}[1]{#1^{-1}}  % Inverse

% Common sets
\newcommand{\R}{\mathbb{R}}  % Real numbers
\newcommand{\C}{\mathbb{C}}  % Complex numbers
\newcommand{\N}{\mathbb{N}}  % Natural numbers
\newcommand{\Z}{\mathbb{Z}}  % Integers
\newcommand{\Q}{\mathbb{Q}}  % Rational numbers

% Probability and statistics
\newcommand{\E}[1]{\mathbb{E}\left[#1\right]}  % Expectation
\newcommand{\Var}[1]{\text{Var}\left[#1\right]}  % Variance
\newcommand{\Cov}[2]{\text{Cov}\left[#1, #2\right]}  % Covariance
\newcommand{\Prob}[1]{\mathbb{P}\left(#1\right)}  % Probability

% Calculus operators
\newcommand{\diff}[2]{\frac{d#1}{d#2}}  % Derivative
\newcommand{\pdiff}[2]{\frac{\partial #1}{\partial #2}}  % Partial derivative
\newcommand{\grad}{\nabla}  % Gradient
\newcommand{\divg}{\nabla \cdot}  % Divergence
\newcommand{\curl}{\nabla \times}  % Curl

% Norms
\newcommand{\norm}[1]{\left\lVert #1 \right\rVert}  % General norm
\newcommand{\abs}[1]{\left| #1 \right|}  % Absolute value
\newcommand{\inner}[2]{\langle #1, #2 \rangle}  % Inner product

% Machine learning specific
\newcommand{\loss}{\mathcal{L}}  % Loss function
\newcommand{\hypothesis}{\mathcal{H}}  % Hypothesis space
\newcommand{\weights}{\vect{w}}  % Weight vector
\newcommand{\features}{\vect{x}}  % Feature vector
\newcommand{\labels}{\vect{y}}  % Label vector

% Argmax and argmin
\DeclareMathOperator*{\argmax}{arg\,max}
\DeclareMathOperator*{\argmin}{arg\,min}

% ==================== Custom Commands with Optional Arguments ====================

% Define a command with default and optional values
\newcommand{\derivative}[3][1]{%
    \ifnum#1=1
        \frac{d#2}{d#3}%
    \else
        \frac{d^{#1}#2}{d#3^{#1}}%
    \fi
}

% Matrix with size specification
\newcommand{\bigmat}[2][r]{%
    \begin{bmatrix}
        #2
    \end{bmatrix}_{#1}
}

% ==================== Custom Environments ====================

% Theorem-like environments
\newtheorem{theorem}{Theorem}[section]
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{proposition}[theorem]{Proposition}
\newtheorem{corollary}[theorem]{Corollary}

\theoremstyle{definition}
\newtheorem{definition}[theorem]{Definition}
\newtheorem{example}[theorem]{Example}

\theoremstyle{remark}
\newtheorem{remark}[theorem]{Remark}

% Custom colored boxes
\newtcolorbox{notebox}{
    colback=blue!5!white,
    colframe=blue!75!black,
    title=Note,
    fonttitle=\bfseries
}

\newtcolorbox{warningbox}{
    colback=red!5!white,
    colframe=red!75!black,
    title=Warning,
    fonttitle=\bfseries
}

\newtcolorbox{tipbox}{
    colback=green!5!white,
    colframe=green!75!black,
    title=Tip,
    fonttitle=\bfseries
}

% Custom proof-like environment with custom ending
\newenvironment{solution}
    {\begin{proof}[Solution]}
    {\end{proof}}

% Algorithm environment
\newcounter{algorithm}
\newenvironment{algorithm}[1][]{%
    \refstepcounter{algorithm}%
    \begin{tcolorbox}[
        colback=yellow!10!white,
        colframe=orange!75!black,
        title=Algorithm \thealgorithm: #1,
        fonttitle=\bfseries
    ]
}{%
    \end{tcolorbox}
}

% ==================== Renewing Existing Commands ====================

% Redefine the vec command to use bold instead of arrow
\renewcommand{\vec}[1]{\mathbf{#1}}

% Customize the equation numbering format
\renewcommand{\theequation}{\thesection.\arabic{equation}}

% ==================== Document Content ====================

\title{Custom \LaTeX\ Commands and Environments}
\author{LaTeX Student}
\date{\today}

\begin{document}

\maketitle

\tableofcontents

\section{Custom Math Commands}

\subsection{Vector and Matrix Notation}

Using custom vector commands makes equations cleaner and easier to modify:

\begin{equation}
    \vect{v} = \begin{bmatrix} v_1 \\ v_2 \\ v_3 \end{bmatrix}, \quad
    \uvect{n} = \frac{\vect{v}}{\norm{\vect{v}}}
\end{equation}

Matrix operations become more readable:

\begin{equation}
    \mat{A}\vect{x} = \vect{b}, \quad
    \vect{x} = \inv{\mat{A}}\vect{b}, \quad
    \mat{B} = \trans{\mat{A}}\mat{A}
\end{equation}

\subsection{Common Mathematical Sets}

The custom set commands simplify notation:

\begin{itemize}
    \item Real numbers: $\R$, $\R^n$
    \item Complex numbers: $\C$
    \item Integers: $\Z$
    \item Natural numbers: $\N$
    \item Rational numbers: $\Q$
\end{itemize}

\subsection{Probability and Statistics}

Custom probability commands ensure consistent notation:

\begin{align}
    \E{X} &= \sum_{i} x_i p(x_i) \\
    \Var{X} &= \E{X^2} - \left(\E{X}\right)^2 \\
    \Cov{X}{Y} &= \E{XY} - \E{X}\E{Y} \\
    \Prob{X = x} &= p(x)
\end{align}

\subsection{Calculus Operators}

\begin{example}[Derivatives and Gradients]
First derivative: $\diff{f}{x}$

Partial derivative: $\pdiff{f}{x}$

Gradient: $\grad f = \begin{bmatrix} \pdiff{f}{x_1} \\ \pdiff{f}{x_2} \\ \vdots \\ \pdiff{f}{x_n} \end{bmatrix}$

Divergence: $\divg \vect{F} = \pdiff{F_x}{x} + \pdiff{F_y}{y} + \pdiff{F_z}{z}$

Curl: $\curl \vect{F} = \grad \times \vect{F}$
\end{example}

\subsection{Norms and Inner Products}

\begin{equation}
    \norm{\vect{x}}_2 = \sqrt{\sum_{i=1}^n x_i^2}, \quad
    \abs{x}, \quad
    \inner{\vect{u}}{\vect{v}} = \sum_{i=1}^n u_i v_i
\end{equation}

\section{Machine Learning Notation}

Using custom ML commands ensures consistency across documents:

\begin{theorem}[Linear Regression]
The optimal weights for linear regression minimize:
\begin{equation}
    \weights^* = \argmin_{\weights} \loss(\weights) = \argmin_{\weights} \sum_{i=1}^n (y_i - \trans{\weights}\features_i)^2
\end{equation}
\end{theorem}

\begin{solution}
Taking the gradient and setting it to zero:
\begin{align}
    \grad_{\weights} \loss(\weights) &= 0 \\
    \weights^* &= \inv{(\trans{\mat{X}}\mat{X})}\trans{\mat{X}}\labels
\end{align}
\end{solution}

\section{Commands with Optional Arguments}

\subsection{Higher-Order Derivatives}

The \texttt{derivative} command supports optional arguments for higher-order derivatives:

\begin{itemize}
    \item First derivative: $\derivative{f}{x}$
    \item Second derivative: $\derivative[2]{f}{x}$
    \item Third derivative: $\derivative[3]{f}{x}$
    \item $n$-th derivative: $\derivative[n]{f}{x}$
\end{itemize}

\section{Custom Environments}

\subsection{Theorem Environments}

\begin{theorem}[Pythagorean Theorem]
In a right-angled triangle, the square of the hypotenuse equals the sum of squares of the other two sides:
\begin{equation}
    c^2 = a^2 + b^2
\end{equation}
\end{theorem}

\begin{proof}
This can be proven using similar triangles or area calculations.
\end{proof}

\begin{lemma}
If $f$ is differentiable at $x = a$, then $f$ is continuous at $x = a$.
\end{lemma}

\begin{definition}[Convex Function]
A function $f: \R^n \to \R$ is convex if for all $\vect{x}, \vect{y} \in \R^n$ and $\lambda \in [0,1]$:
\begin{equation}
    f(\lambda\vect{x} + (1-\lambda)\vect{y}) \leq \lambda f(\vect{x}) + (1-\lambda)f(\vect{y})
\end{equation}
\end{definition}

\subsection{Colored Boxes}

\begin{notebox}
Custom environments like this can highlight important information and make documents more readable.
\end{notebox}

\begin{warningbox}
Be careful when redefining existing commands. Always check that your new definition doesn't break existing functionality.
\end{warningbox}

\begin{tipbox}
Use \texttt{\textbackslash newcommand} for new commands and \texttt{\textbackslash renewcommand} to modify existing ones. Use \texttt{\textbackslash providecommand} if you want to define a command only if it doesn't already exist.
\end{tipbox}

\subsection{Algorithm Environment}

\begin{algorithm}[Gradient Descent]
\textbf{Input:} Initial point $\weights_0$, learning rate $\eta$, tolerance $\epsilon$

\textbf{Output:} Optimal weights $\weights^*$

\begin{enumerate}[label=\arabic*.]
    \item Initialize $k = 0$
    \item \textbf{while} $\norm{\grad \loss(\weights_k)} > \epsilon$ \textbf{do}
    \begin{enumerate}[label=\alph*.]
        \item Compute gradient: $\vect{g}_k = \grad \loss(\weights_k)$
        \item Update weights: $\weights_{k+1} = \weights_k - \eta \vect{g}_k$
        \item $k \leftarrow k + 1$
    \end{enumerate}
    \item \textbf{end while}
    \item \textbf{return} $\weights_k$
\end{enumerate}
\end{algorithm}

\section{Advanced Examples}

\subsection{Neural Network Forward Pass}

Using our custom commands, the forward pass equations become cleaner:

\begin{align}
    \vect{h}_1 &= \sigma(\mat{W}_1\features + \vect{b}_1) \\
    \vect{h}_2 &= \sigma(\mat{W}_2\vect{h}_1 + \vect{b}_2) \\
    \vect{y} &= \text{softmax}(\mat{W}_3\vect{h}_2 + \vect{b}_3)
\end{align}

where $\sigma$ is the activation function.

\subsection{Optimization Problem}

\begin{equation}
    \min_{\weights \in \R^d} \left\{ \loss(\weights) = \E{\ell(f_{\weights}(\features), y)} + \lambda\norm{\weights}^2 \right\}
\end{equation}

The optimal solution satisfies:
\begin{equation}
    \weights^* = \argmin_{\weights} \left[ \loss(\weights) \right]
\end{equation}

\section{Best Practices}

\begin{enumerate}
    \item \textbf{Consistency:} Use custom commands to ensure consistent notation throughout your document
    \item \textbf{Maintainability:} If you need to change notation, you only need to modify the command definition
    \item \textbf{Readability:} Well-named commands make your \LaTeX\ source more readable
    \item \textbf{Reusability:} Save common commands in a separate \texttt{.sty} file for reuse across projects
    \item \textbf{Documentation:} Comment your custom commands to remember their purpose
\end{enumerate}

\begin{warningbox}
Avoid redefining standard \LaTeX\ commands unless absolutely necessary, as this can lead to compatibility issues with packages.
\end{warningbox}

\section{Creating Your Own Style File}

To reuse these commands across multiple documents, save them in a file called \texttt{mycommands.sty}:

\begin{verbatim}
\NeedsTeXFormat{LaTeX2e}
\ProvidesPackage{mycommands}[2024/01/01 Custom Commands]

% Include all your \newcommand and \newenvironment definitions here

\endinput
\end{verbatim}

Then in your main document:

\begin{verbatim}
\documentclass{article}
\usepackage{mycommands}
\begin{document}
...
\end{document}
\end{verbatim}

\section{Compilation Notes}

To compile this document:
\begin{verbatim}
pdflatex custom_macros.tex
pdflatex custom_macros.tex  # Run twice for cross-references
\end{verbatim}

\end{document}
