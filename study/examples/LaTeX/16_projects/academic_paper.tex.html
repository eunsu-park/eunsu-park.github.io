{% raw %}
<!DOCTYPE html>
<html lang="en" data-theme="light">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>academic_paper.tex - Examples</title>
    <link rel="stylesheet" href="/study/static/css/style.css">
    <link rel="stylesheet" href="/study/static/css/highlight.css">
    <!-- KaTeX for LaTeX math rendering -->
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.11/dist/katex.min.css">
    <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.11/dist/katex.min.js"></script>
    <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.11/dist/contrib/auto-render.min.js"
        onload="renderMathInElement(document.body, {
            delimiters: [
                {left: '$$', right: '$$', display: true},
                {left: '$', right: '$', display: false}
            ],
            throwOnError: false
        });"></script>
    
</head>
<body>
    <div class="app-container">
        <!-- Sidebar -->
        <aside class="sidebar" id="sidebar">
            <div class="sidebar-header">
                <a href="/study/en/" class="logo">Study Materials</a>
                <button class="sidebar-toggle" id="sidebar-toggle" aria-label="Toggle sidebar">
                    <span></span>
                </button>
            </div>

            <nav class="sidebar-nav">
                <a href="/study/en/" class="nav-item ">
                    <span class="nav-icon">üè†</span>
                    <span class="nav-text">Home</span>
                </a>
                <a href="/study/examples/" class="nav-item active">
                    <span class="nav-icon">üíª</span>
                    <span class="nav-text">Examples</span>
                </a>
            </nav>

            <div class="sidebar-search">
                <form action="/study/en/search.html" method="get" id="search-form">
                    <input type="search" name="q" placeholder="Search..." id="search-sidebar-input">
                </form>
            </div>

            <!-- Language Selector -->
            <div class="sidebar-lang">
                <select id="lang-select" class="lang-selector" onchange="switchLanguage(this.value)">
                    
                    <option value="en" selected>
                        English
                    </option>
                    
                    <option value="ko" >
                        ÌïúÍµ≠Ïñ¥
                    </option>
                    
                </select>
            </div>

            <div class="sidebar-footer">
                <button class="theme-toggle" id="theme-toggle" aria-label="Toggle theme">
                    <span class="theme-icon light">‚òÄÔ∏è</span>
                    <span class="theme-icon dark">üåô</span>
                </button>
            </div>
        </aside>

        <!-- Main Content -->
        <main class="main-content">
            <header class="main-header">
                <button class="menu-toggle" id="menu-toggle" aria-label="Open menu">
                    <span></span>
                </button>
                
<nav class="breadcrumb">
    <a href="/study/examples/">Examples</a>
    <span class="separator">/</span>
    <a href="/study/examples/LaTeX/">LaTeX</a>
    <span class="separator">/</span>
    <span class="current">academic_paper.tex</span>
</nav>

            </header>

            <div class="content">
                
<article class="example-article">
    <header class="example-header">
        <h1>academic_paper.tex</h1>
        <div class="example-actions">
            <a href="academic_paper.tex" download class="btn">Download</a>
            <button class="btn" id="copy-code-btn">Copy code</button>
        </div>
    </header>

    <div class="example-meta">
        <span>latex</span>
        <span>593 lines</span>
        <span>20.1 KB</span>
    </div>

    <div class="example-code markdown-body">
        <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span class="normal">  1</span>
<span class="normal">  2</span>
<span class="normal">  3</span>
<span class="normal">  4</span>
<span class="normal">  5</span>
<span class="normal">  6</span>
<span class="normal">  7</span>
<span class="normal">  8</span>
<span class="normal">  9</span>
<span class="normal"> 10</span>
<span class="normal"> 11</span>
<span class="normal"> 12</span>
<span class="normal"> 13</span>
<span class="normal"> 14</span>
<span class="normal"> 15</span>
<span class="normal"> 16</span>
<span class="normal"> 17</span>
<span class="normal"> 18</span>
<span class="normal"> 19</span>
<span class="normal"> 20</span>
<span class="normal"> 21</span>
<span class="normal"> 22</span>
<span class="normal"> 23</span>
<span class="normal"> 24</span>
<span class="normal"> 25</span>
<span class="normal"> 26</span>
<span class="normal"> 27</span>
<span class="normal"> 28</span>
<span class="normal"> 29</span>
<span class="normal"> 30</span>
<span class="normal"> 31</span>
<span class="normal"> 32</span>
<span class="normal"> 33</span>
<span class="normal"> 34</span>
<span class="normal"> 35</span>
<span class="normal"> 36</span>
<span class="normal"> 37</span>
<span class="normal"> 38</span>
<span class="normal"> 39</span>
<span class="normal"> 40</span>
<span class="normal"> 41</span>
<span class="normal"> 42</span>
<span class="normal"> 43</span>
<span class="normal"> 44</span>
<span class="normal"> 45</span>
<span class="normal"> 46</span>
<span class="normal"> 47</span>
<span class="normal"> 48</span>
<span class="normal"> 49</span>
<span class="normal"> 50</span>
<span class="normal"> 51</span>
<span class="normal"> 52</span>
<span class="normal"> 53</span>
<span class="normal"> 54</span>
<span class="normal"> 55</span>
<span class="normal"> 56</span>
<span class="normal"> 57</span>
<span class="normal"> 58</span>
<span class="normal"> 59</span>
<span class="normal"> 60</span>
<span class="normal"> 61</span>
<span class="normal"> 62</span>
<span class="normal"> 63</span>
<span class="normal"> 64</span>
<span class="normal"> 65</span>
<span class="normal"> 66</span>
<span class="normal"> 67</span>
<span class="normal"> 68</span>
<span class="normal"> 69</span>
<span class="normal"> 70</span>
<span class="normal"> 71</span>
<span class="normal"> 72</span>
<span class="normal"> 73</span>
<span class="normal"> 74</span>
<span class="normal"> 75</span>
<span class="normal"> 76</span>
<span class="normal"> 77</span>
<span class="normal"> 78</span>
<span class="normal"> 79</span>
<span class="normal"> 80</span>
<span class="normal"> 81</span>
<span class="normal"> 82</span>
<span class="normal"> 83</span>
<span class="normal"> 84</span>
<span class="normal"> 85</span>
<span class="normal"> 86</span>
<span class="normal"> 87</span>
<span class="normal"> 88</span>
<span class="normal"> 89</span>
<span class="normal"> 90</span>
<span class="normal"> 91</span>
<span class="normal"> 92</span>
<span class="normal"> 93</span>
<span class="normal"> 94</span>
<span class="normal"> 95</span>
<span class="normal"> 96</span>
<span class="normal"> 97</span>
<span class="normal"> 98</span>
<span class="normal"> 99</span>
<span class="normal">100</span>
<span class="normal">101</span>
<span class="normal">102</span>
<span class="normal">103</span>
<span class="normal">104</span>
<span class="normal">105</span>
<span class="normal">106</span>
<span class="normal">107</span>
<span class="normal">108</span>
<span class="normal">109</span>
<span class="normal">110</span>
<span class="normal">111</span>
<span class="normal">112</span>
<span class="normal">113</span>
<span class="normal">114</span>
<span class="normal">115</span>
<span class="normal">116</span>
<span class="normal">117</span>
<span class="normal">118</span>
<span class="normal">119</span>
<span class="normal">120</span>
<span class="normal">121</span>
<span class="normal">122</span>
<span class="normal">123</span>
<span class="normal">124</span>
<span class="normal">125</span>
<span class="normal">126</span>
<span class="normal">127</span>
<span class="normal">128</span>
<span class="normal">129</span>
<span class="normal">130</span>
<span class="normal">131</span>
<span class="normal">132</span>
<span class="normal">133</span>
<span class="normal">134</span>
<span class="normal">135</span>
<span class="normal">136</span>
<span class="normal">137</span>
<span class="normal">138</span>
<span class="normal">139</span>
<span class="normal">140</span>
<span class="normal">141</span>
<span class="normal">142</span>
<span class="normal">143</span>
<span class="normal">144</span>
<span class="normal">145</span>
<span class="normal">146</span>
<span class="normal">147</span>
<span class="normal">148</span>
<span class="normal">149</span>
<span class="normal">150</span>
<span class="normal">151</span>
<span class="normal">152</span>
<span class="normal">153</span>
<span class="normal">154</span>
<span class="normal">155</span>
<span class="normal">156</span>
<span class="normal">157</span>
<span class="normal">158</span>
<span class="normal">159</span>
<span class="normal">160</span>
<span class="normal">161</span>
<span class="normal">162</span>
<span class="normal">163</span>
<span class="normal">164</span>
<span class="normal">165</span>
<span class="normal">166</span>
<span class="normal">167</span>
<span class="normal">168</span>
<span class="normal">169</span>
<span class="normal">170</span>
<span class="normal">171</span>
<span class="normal">172</span>
<span class="normal">173</span>
<span class="normal">174</span>
<span class="normal">175</span>
<span class="normal">176</span>
<span class="normal">177</span>
<span class="normal">178</span>
<span class="normal">179</span>
<span class="normal">180</span>
<span class="normal">181</span>
<span class="normal">182</span>
<span class="normal">183</span>
<span class="normal">184</span>
<span class="normal">185</span>
<span class="normal">186</span>
<span class="normal">187</span>
<span class="normal">188</span>
<span class="normal">189</span>
<span class="normal">190</span>
<span class="normal">191</span>
<span class="normal">192</span>
<span class="normal">193</span>
<span class="normal">194</span>
<span class="normal">195</span>
<span class="normal">196</span>
<span class="normal">197</span>
<span class="normal">198</span>
<span class="normal">199</span>
<span class="normal">200</span>
<span class="normal">201</span>
<span class="normal">202</span>
<span class="normal">203</span>
<span class="normal">204</span>
<span class="normal">205</span>
<span class="normal">206</span>
<span class="normal">207</span>
<span class="normal">208</span>
<span class="normal">209</span>
<span class="normal">210</span>
<span class="normal">211</span>
<span class="normal">212</span>
<span class="normal">213</span>
<span class="normal">214</span>
<span class="normal">215</span>
<span class="normal">216</span>
<span class="normal">217</span>
<span class="normal">218</span>
<span class="normal">219</span>
<span class="normal">220</span>
<span class="normal">221</span>
<span class="normal">222</span>
<span class="normal">223</span>
<span class="normal">224</span>
<span class="normal">225</span>
<span class="normal">226</span>
<span class="normal">227</span>
<span class="normal">228</span>
<span class="normal">229</span>
<span class="normal">230</span>
<span class="normal">231</span>
<span class="normal">232</span>
<span class="normal">233</span>
<span class="normal">234</span>
<span class="normal">235</span>
<span class="normal">236</span>
<span class="normal">237</span>
<span class="normal">238</span>
<span class="normal">239</span>
<span class="normal">240</span>
<span class="normal">241</span>
<span class="normal">242</span>
<span class="normal">243</span>
<span class="normal">244</span>
<span class="normal">245</span>
<span class="normal">246</span>
<span class="normal">247</span>
<span class="normal">248</span>
<span class="normal">249</span>
<span class="normal">250</span>
<span class="normal">251</span>
<span class="normal">252</span>
<span class="normal">253</span>
<span class="normal">254</span>
<span class="normal">255</span>
<span class="normal">256</span>
<span class="normal">257</span>
<span class="normal">258</span>
<span class="normal">259</span>
<span class="normal">260</span>
<span class="normal">261</span>
<span class="normal">262</span>
<span class="normal">263</span>
<span class="normal">264</span>
<span class="normal">265</span>
<span class="normal">266</span>
<span class="normal">267</span>
<span class="normal">268</span>
<span class="normal">269</span>
<span class="normal">270</span>
<span class="normal">271</span>
<span class="normal">272</span>
<span class="normal">273</span>
<span class="normal">274</span>
<span class="normal">275</span>
<span class="normal">276</span>
<span class="normal">277</span>
<span class="normal">278</span>
<span class="normal">279</span>
<span class="normal">280</span>
<span class="normal">281</span>
<span class="normal">282</span>
<span class="normal">283</span>
<span class="normal">284</span>
<span class="normal">285</span>
<span class="normal">286</span>
<span class="normal">287</span>
<span class="normal">288</span>
<span class="normal">289</span>
<span class="normal">290</span>
<span class="normal">291</span>
<span class="normal">292</span>
<span class="normal">293</span>
<span class="normal">294</span>
<span class="normal">295</span>
<span class="normal">296</span>
<span class="normal">297</span>
<span class="normal">298</span>
<span class="normal">299</span>
<span class="normal">300</span>
<span class="normal">301</span>
<span class="normal">302</span>
<span class="normal">303</span>
<span class="normal">304</span>
<span class="normal">305</span>
<span class="normal">306</span>
<span class="normal">307</span>
<span class="normal">308</span>
<span class="normal">309</span>
<span class="normal">310</span>
<span class="normal">311</span>
<span class="normal">312</span>
<span class="normal">313</span>
<span class="normal">314</span>
<span class="normal">315</span>
<span class="normal">316</span>
<span class="normal">317</span>
<span class="normal">318</span>
<span class="normal">319</span>
<span class="normal">320</span>
<span class="normal">321</span>
<span class="normal">322</span>
<span class="normal">323</span>
<span class="normal">324</span>
<span class="normal">325</span>
<span class="normal">326</span>
<span class="normal">327</span>
<span class="normal">328</span>
<span class="normal">329</span>
<span class="normal">330</span>
<span class="normal">331</span>
<span class="normal">332</span>
<span class="normal">333</span>
<span class="normal">334</span>
<span class="normal">335</span>
<span class="normal">336</span>
<span class="normal">337</span>
<span class="normal">338</span>
<span class="normal">339</span>
<span class="normal">340</span>
<span class="normal">341</span>
<span class="normal">342</span>
<span class="normal">343</span>
<span class="normal">344</span>
<span class="normal">345</span>
<span class="normal">346</span>
<span class="normal">347</span>
<span class="normal">348</span>
<span class="normal">349</span>
<span class="normal">350</span>
<span class="normal">351</span>
<span class="normal">352</span>
<span class="normal">353</span>
<span class="normal">354</span>
<span class="normal">355</span>
<span class="normal">356</span>
<span class="normal">357</span>
<span class="normal">358</span>
<span class="normal">359</span>
<span class="normal">360</span>
<span class="normal">361</span>
<span class="normal">362</span>
<span class="normal">363</span>
<span class="normal">364</span>
<span class="normal">365</span>
<span class="normal">366</span>
<span class="normal">367</span>
<span class="normal">368</span>
<span class="normal">369</span>
<span class="normal">370</span>
<span class="normal">371</span>
<span class="normal">372</span>
<span class="normal">373</span>
<span class="normal">374</span>
<span class="normal">375</span>
<span class="normal">376</span>
<span class="normal">377</span>
<span class="normal">378</span>
<span class="normal">379</span>
<span class="normal">380</span>
<span class="normal">381</span>
<span class="normal">382</span>
<span class="normal">383</span>
<span class="normal">384</span>
<span class="normal">385</span>
<span class="normal">386</span>
<span class="normal">387</span>
<span class="normal">388</span>
<span class="normal">389</span>
<span class="normal">390</span>
<span class="normal">391</span>
<span class="normal">392</span>
<span class="normal">393</span>
<span class="normal">394</span>
<span class="normal">395</span>
<span class="normal">396</span>
<span class="normal">397</span>
<span class="normal">398</span>
<span class="normal">399</span>
<span class="normal">400</span>
<span class="normal">401</span>
<span class="normal">402</span>
<span class="normal">403</span>
<span class="normal">404</span>
<span class="normal">405</span>
<span class="normal">406</span>
<span class="normal">407</span>
<span class="normal">408</span>
<span class="normal">409</span>
<span class="normal">410</span>
<span class="normal">411</span>
<span class="normal">412</span>
<span class="normal">413</span>
<span class="normal">414</span>
<span class="normal">415</span>
<span class="normal">416</span>
<span class="normal">417</span>
<span class="normal">418</span>
<span class="normal">419</span>
<span class="normal">420</span>
<span class="normal">421</span>
<span class="normal">422</span>
<span class="normal">423</span>
<span class="normal">424</span>
<span class="normal">425</span>
<span class="normal">426</span>
<span class="normal">427</span>
<span class="normal">428</span>
<span class="normal">429</span>
<span class="normal">430</span>
<span class="normal">431</span>
<span class="normal">432</span>
<span class="normal">433</span>
<span class="normal">434</span>
<span class="normal">435</span>
<span class="normal">436</span>
<span class="normal">437</span>
<span class="normal">438</span>
<span class="normal">439</span>
<span class="normal">440</span>
<span class="normal">441</span>
<span class="normal">442</span>
<span class="normal">443</span>
<span class="normal">444</span>
<span class="normal">445</span>
<span class="normal">446</span>
<span class="normal">447</span>
<span class="normal">448</span>
<span class="normal">449</span>
<span class="normal">450</span>
<span class="normal">451</span>
<span class="normal">452</span>
<span class="normal">453</span>
<span class="normal">454</span>
<span class="normal">455</span>
<span class="normal">456</span>
<span class="normal">457</span>
<span class="normal">458</span>
<span class="normal">459</span>
<span class="normal">460</span>
<span class="normal">461</span>
<span class="normal">462</span>
<span class="normal">463</span>
<span class="normal">464</span>
<span class="normal">465</span>
<span class="normal">466</span>
<span class="normal">467</span>
<span class="normal">468</span>
<span class="normal">469</span>
<span class="normal">470</span>
<span class="normal">471</span>
<span class="normal">472</span>
<span class="normal">473</span>
<span class="normal">474</span>
<span class="normal">475</span>
<span class="normal">476</span>
<span class="normal">477</span>
<span class="normal">478</span>
<span class="normal">479</span>
<span class="normal">480</span>
<span class="normal">481</span>
<span class="normal">482</span>
<span class="normal">483</span>
<span class="normal">484</span>
<span class="normal">485</span>
<span class="normal">486</span>
<span class="normal">487</span>
<span class="normal">488</span>
<span class="normal">489</span>
<span class="normal">490</span>
<span class="normal">491</span>
<span class="normal">492</span>
<span class="normal">493</span>
<span class="normal">494</span>
<span class="normal">495</span>
<span class="normal">496</span>
<span class="normal">497</span>
<span class="normal">498</span>
<span class="normal">499</span>
<span class="normal">500</span>
<span class="normal">501</span>
<span class="normal">502</span>
<span class="normal">503</span>
<span class="normal">504</span>
<span class="normal">505</span>
<span class="normal">506</span>
<span class="normal">507</span>
<span class="normal">508</span>
<span class="normal">509</span>
<span class="normal">510</span>
<span class="normal">511</span>
<span class="normal">512</span>
<span class="normal">513</span>
<span class="normal">514</span>
<span class="normal">515</span>
<span class="normal">516</span>
<span class="normal">517</span>
<span class="normal">518</span>
<span class="normal">519</span>
<span class="normal">520</span>
<span class="normal">521</span>
<span class="normal">522</span>
<span class="normal">523</span>
<span class="normal">524</span>
<span class="normal">525</span>
<span class="normal">526</span>
<span class="normal">527</span>
<span class="normal">528</span>
<span class="normal">529</span>
<span class="normal">530</span>
<span class="normal">531</span>
<span class="normal">532</span>
<span class="normal">533</span>
<span class="normal">534</span>
<span class="normal">535</span>
<span class="normal">536</span>
<span class="normal">537</span>
<span class="normal">538</span>
<span class="normal">539</span>
<span class="normal">540</span>
<span class="normal">541</span>
<span class="normal">542</span>
<span class="normal">543</span>
<span class="normal">544</span>
<span class="normal">545</span>
<span class="normal">546</span>
<span class="normal">547</span>
<span class="normal">548</span>
<span class="normal">549</span>
<span class="normal">550</span>
<span class="normal">551</span>
<span class="normal">552</span>
<span class="normal">553</span>
<span class="normal">554</span>
<span class="normal">555</span>
<span class="normal">556</span>
<span class="normal">557</span>
<span class="normal">558</span>
<span class="normal">559</span>
<span class="normal">560</span>
<span class="normal">561</span>
<span class="normal">562</span>
<span class="normal">563</span>
<span class="normal">564</span>
<span class="normal">565</span>
<span class="normal">566</span>
<span class="normal">567</span>
<span class="normal">568</span>
<span class="normal">569</span>
<span class="normal">570</span>
<span class="normal">571</span>
<span class="normal">572</span>
<span class="normal">573</span>
<span class="normal">574</span>
<span class="normal">575</span>
<span class="normal">576</span>
<span class="normal">577</span>
<span class="normal">578</span>
<span class="normal">579</span>
<span class="normal">580</span>
<span class="normal">581</span>
<span class="normal">582</span>
<span class="normal">583</span>
<span class="normal">584</span>
<span class="normal">585</span>
<span class="normal">586</span>
<span class="normal">587</span>
<span class="normal">588</span>
<span class="normal">589</span>
<span class="normal">590</span>
<span class="normal">591</span>
<span class="normal">592</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="c">% academic_paper.tex</span>
<span class="c">% Complete academic paper template with all standard sections</span>
<span class="c">% Demonstrates bibliography, cross-references, figures, tables, and appendices</span>

<span class="k">\documentclass</span><span class="na">[12pt,a4paper]</span><span class="nb">{</span>article<span class="nb">}</span>

<span class="c">% Essential packages</span>
<span class="k">\usepackage</span><span class="na">[utf8]</span><span class="nb">{</span>inputenc<span class="nb">}</span>
<span class="k">\usepackage</span><span class="na">[T1]</span><span class="nb">{</span>fontenc<span class="nb">}</span>
<span class="k">\usepackage</span><span class="na">[margin=1in]</span><span class="nb">{</span>geometry<span class="nb">}</span>
<span class="k">\usepackage</span><span class="nb">{</span>setspace<span class="nb">}</span>
<span class="k">\onehalfspacing</span>

<span class="c">% Math packages</span>
<span class="k">\usepackage</span><span class="nb">{</span>amsmath, amssymb, amsthm<span class="nb">}</span>

<span class="c">% Graphics and tables</span>
<span class="k">\usepackage</span><span class="nb">{</span>graphicx<span class="nb">}</span>
<span class="k">\usepackage</span><span class="nb">{</span>booktabs<span class="nb">}</span>
<span class="k">\usepackage</span><span class="nb">{</span>multirow<span class="nb">}</span>
<span class="k">\usepackage</span><span class="nb">{</span>array<span class="nb">}</span>

<span class="c">% Bibliography</span>
<span class="k">\usepackage</span>[
    backend=biber,
    style=authoryear,
    sorting=nyt,
    maxbibnames=99
]<span class="nb">{</span>biblatex<span class="nb">}</span>
<span class="c">% Note: In a real paper, you would have a separate .bib file</span>
<span class="c">% For this example, we use embedded bibliography at the end</span>

<span class="c">% Hyperlinks (should be loaded last)</span>
<span class="k">\usepackage</span><span class="nb">{</span>hyperref<span class="nb">}</span>
<span class="k">\hypersetup</span><span class="nb">{</span>
    colorlinks=true,
    linkcolor=blue,
    citecolor=blue,
    urlcolor=blue
<span class="nb">}</span>

<span class="c">% Cross-referencing</span>
<span class="k">\usepackage</span><span class="nb">{</span>cleveref<span class="nb">}</span>

<span class="c">% Custom theorem environments</span>
<span class="k">\newtheorem</span><span class="nb">{</span>theorem<span class="nb">}{</span>Theorem<span class="nb">}</span>[section]
<span class="k">\newtheorem</span><span class="nb">{</span>lemma<span class="nb">}</span>[theorem]<span class="nb">{</span>Lemma<span class="nb">}</span>
<span class="k">\newtheorem</span><span class="nb">{</span>proposition<span class="nb">}</span>[theorem]<span class="nb">{</span>Proposition<span class="nb">}</span>
<span class="k">\newtheorem</span><span class="nb">{</span>corollary<span class="nb">}</span>[theorem]<span class="nb">{</span>Corollary<span class="nb">}</span>

<span class="k">\theoremstyle</span><span class="nb">{</span>definition<span class="nb">}</span>
<span class="k">\newtheorem</span><span class="nb">{</span>definition<span class="nb">}</span>[theorem]<span class="nb">{</span>Definition<span class="nb">}</span>

<span class="k">\theoremstyle</span><span class="nb">{</span>remark<span class="nb">}</span>
<span class="k">\newtheorem</span><span class="nb">{</span>remark<span class="nb">}</span>[theorem]<span class="nb">{</span>Remark<span class="nb">}</span>

<span class="c">% Custom commands for this paper</span>
<span class="k">\newcommand</span><span class="nb">{</span><span class="k">\R</span><span class="nb">}{</span><span class="k">\mathbb</span><span class="nb">{</span>R<span class="nb">}}</span>
<span class="k">\newcommand</span><span class="nb">{</span><span class="k">\E</span><span class="nb">}</span>[1]<span class="nb">{</span><span class="k">\mathbb</span><span class="nb">{</span>E<span class="nb">}</span><span class="k">\left</span><span class="na">[#1\right]</span><span class="nb">}</span>
<span class="k">\newcommand</span><span class="nb">{</span><span class="k">\vect</span><span class="nb">}</span>[1]<span class="nb">{</span><span class="k">\mathbf</span><span class="nb">{</span>#1<span class="nb">}}</span>
<span class="k">\newcommand</span><span class="nb">{</span><span class="k">\mat</span><span class="nb">}</span>[1]<span class="nb">{</span><span class="k">\mathbf</span><span class="nb">{</span>#1<span class="nb">}}</span>
<span class="k">\newcommand</span><span class="nb">{</span><span class="k">\norm</span><span class="nb">}</span>[1]<span class="nb">{</span><span class="k">\left\lVert</span> #1 <span class="k">\right\rVert</span><span class="nb">}</span>

<span class="c">% Title and author information</span>
<span class="k">\title</span><span class="nb">{</span>Deep Learning for Image Classification:<span class="k">\\</span>A Comprehensive Study of Convolutional Neural Networks<span class="nb">}</span>

<span class="k">\author</span><span class="nb">{</span>
    Jane Smith<span class="k">\thanks</span><span class="nb">{</span>Corresponding author: jane.smith@university.edu<span class="nb">}</span><span class="k">\\</span>
    <span class="k">\textit</span><span class="nb">{</span>Department of Computer Science<span class="nb">}</span><span class="k">\\</span>
    <span class="k">\textit</span><span class="nb">{</span>University of Technology<span class="nb">}</span><span class="k">\\</span>
    <span class="k">\textit</span><span class="nb">{</span>City, Country<span class="nb">}</span>
    <span class="k">\and</span>
    John Doe<span class="k">\\</span>
    <span class="k">\textit</span><span class="nb">{</span>Department of Artificial Intelligence<span class="nb">}</span><span class="k">\\</span>
    <span class="k">\textit</span><span class="nb">{</span>Institute of Advanced Studies<span class="nb">}</span><span class="k">\\</span>
    <span class="k">\textit</span><span class="nb">{</span>City, Country<span class="nb">}</span>
<span class="nb">}</span>

<span class="k">\date</span><span class="nb">{</span><span class="k">\today</span><span class="nb">}</span>

<span class="k">\begin</span><span class="nb">{</span>document<span class="nb">}</span>

<span class="c">% ==================== Front Matter ====================</span>

<span class="k">\maketitle</span>

<span class="k">\begin</span><span class="nb">{</span>abstract<span class="nb">}</span>
In this paper, we present a comprehensive study of convolutional neural networks (CNNs)
for image classification tasks. We investigate the impact of various architectural
choices, including depth, width, and skip connections, on classification performance
across multiple benchmark datasets. Our experiments demonstrate that deeper networks
with residual connections achieve superior performance on complex datasets, with our
best model achieving 96.8<span class="k">\%</span> accuracy on CIFAR-10 and 84.2<span class="k">\%</span> on CIFAR-100. We provide
detailed analysis of the trade-offs between model complexity and performance, and
propose guidelines for practitioners designing CNN architectures for image
classification. Furthermore, we investigate the role of data augmentation and
regularization techniques in preventing overfitting. Our findings suggest that a
combination of architectural improvements and proper regularization is essential for
achieving state-of-the-art performance.

<span class="k">\vspace</span><span class="nb">{</span>0.5cm<span class="nb">}</span>

<span class="k">\noindent\textbf</span><span class="nb">{</span>Keywords:<span class="nb">}</span> Deep Learning, Convolutional Neural Networks, Image
Classification, ResNet, Data Augmentation, Transfer Learning
<span class="k">\end</span><span class="nb">{</span>abstract<span class="nb">}</span>

<span class="k">\tableofcontents</span>

<span class="c">% ==================== Introduction ====================</span>

<span class="k">\section</span><span class="nb">{</span>Introduction<span class="nb">}</span>
<span class="k">\label</span><span class="nb">{</span>sec:introduction<span class="nb">}</span>

Image classification is a fundamental task in computer vision with applications
ranging from medical diagnosis to autonomous driving. Deep learning, particularly
convolutional neural networks (CNNs), has revolutionized this field over the past
decade~<span class="k">\cite</span><span class="nb">{</span>lecun2015deep,goodfellow2016deep<span class="nb">}</span>.

Since the breakthrough of AlexNet in 2012~<span class="k">\cite</span><span class="nb">{</span>krizhevsky2012imagenet<span class="nb">}</span>, deep
learning models have consistently achieved state-of-the-art performance on image
classification benchmarks. The success of CNNs can be attributed to their ability
to automatically learn hierarchical feature representations from raw pixel data,
eliminating the need for hand-crafted features.

<span class="k">\subsection</span><span class="nb">{</span>Motivation<span class="nb">}</span>

Despite the success of CNNs, several important questions remain:

<span class="k">\begin</span><span class="nb">{</span>enumerate<span class="nb">}</span>
    <span class="k">\item</span> How do architectural choices affect classification performance?
    <span class="k">\item</span> What is the optimal balance between model complexity and generalization?
    <span class="k">\item</span> How can we effectively prevent overfitting in deep networks?
    <span class="k">\item</span> What role does data augmentation play in modern architectures?
<span class="k">\end</span><span class="nb">{</span>enumerate<span class="nb">}</span>

This paper addresses these questions through systematic experimentation and analysis.

<span class="k">\subsection</span><span class="nb">{</span>Contributions<span class="nb">}</span>

Our main contributions are:

<span class="k">\begin</span><span class="nb">{</span>itemize<span class="nb">}</span>
    <span class="k">\item</span> A comprehensive empirical study of CNN architectures on multiple datasets
    <span class="k">\item</span> Analysis of the relationship between network depth and performance
    <span class="k">\item</span> Evaluation of various regularization and data augmentation techniques
    <span class="k">\item</span> Practical guidelines for designing CNN architectures
    <span class="k">\item</span> Open-source implementation of all models and experiments
<span class="k">\end</span><span class="nb">{</span>itemize<span class="nb">}</span>

<span class="k">\subsection</span><span class="nb">{</span>Organization<span class="nb">}</span>

The remainder of this paper is organized as follows: <span class="k">\Cref</span><span class="nb">{</span>sec:related<span class="nb">}</span> reviews
related work; <span class="k">\Cref</span><span class="nb">{</span>sec:methodology<span class="nb">}</span> describes our methodology and experimental
setup; <span class="k">\Cref</span><span class="nb">{</span>sec:results<span class="nb">}</span> presents our experimental results; <span class="k">\Cref</span><span class="nb">{</span>sec:discussion<span class="nb">}</span>
discusses the implications of our findings; and <span class="k">\Cref</span><span class="nb">{</span>sec:conclusion<span class="nb">}</span> concludes
the paper with directions for future work.

<span class="c">% ==================== Related Work ====================</span>

<span class="k">\section</span><span class="nb">{</span>Related Work<span class="nb">}</span>
<span class="k">\label</span><span class="nb">{</span>sec:related<span class="nb">}</span>

<span class="k">\subsection</span><span class="nb">{</span>Convolutional Neural Networks<span class="nb">}</span>

Convolutional neural networks were pioneered by LeCun et al.~<span class="k">\cite</span><span class="nb">{</span>lecun1998gradient<span class="nb">}</span>
with LeNet-5, which achieved impressive results on digit recognition. However, CNNs
did not gain widespread adoption until the success of AlexNet~<span class="k">\cite</span><span class="nb">{</span>krizhevsky2012imagenet<span class="nb">}</span>
on the ImageNet challenge.

Following AlexNet, several influential architectures were proposed:

<span class="k">\begin</span><span class="nb">{</span>itemize<span class="nb">}</span>
    <span class="k">\item</span> <span class="k">\textbf</span><span class="nb">{</span>VGGNet<span class="nb">}</span>~<span class="k">\cite</span><span class="nb">{</span>simonyan2014very<span class="nb">}</span>: Demonstrated the importance of
          depth by using small 3√ó3 filters throughout the network
    <span class="k">\item</span> <span class="k">\textbf</span><span class="nb">{</span>GoogLeNet<span class="nb">}</span>~<span class="k">\cite</span><span class="nb">{</span>szegedy2015going<span class="nb">}</span>: Introduced the Inception
          module for efficient computation
    <span class="k">\item</span> <span class="k">\textbf</span><span class="nb">{</span>ResNet<span class="nb">}</span>~<span class="k">\cite</span><span class="nb">{</span>he2016deep<span class="nb">}</span>: Enabled training of very deep networks
          using residual connections
    <span class="k">\item</span> <span class="k">\textbf</span><span class="nb">{</span>DenseNet<span class="nb">}</span>~<span class="k">\cite</span><span class="nb">{</span>huang2017densely<span class="nb">}</span>: Extended residual connections
          by connecting all layers directly
<span class="k">\end</span><span class="nb">{</span>itemize<span class="nb">}</span>

<span class="k">\subsection</span><span class="nb">{</span>Regularization Techniques<span class="nb">}</span>

Preventing overfitting is crucial for deep learning. Common techniques include:

<span class="k">\textbf</span><span class="nb">{</span>Dropout<span class="nb">}</span>~<span class="k">\cite</span><span class="nb">{</span>srivastava2014dropout<span class="nb">}</span> randomly deactivates neurons during
training, forcing the network to learn redundant representations.

<span class="k">\textbf</span><span class="nb">{</span>Batch Normalization<span class="nb">}</span>~<span class="k">\cite</span><span class="nb">{</span>ioffe2015batch<span class="nb">}</span> normalizes layer inputs,
accelerating training and providing regularization effects.

<span class="k">\textbf</span><span class="nb">{</span>Data Augmentation<span class="nb">}</span> artificially increases dataset size by applying random
transformations to training images~<span class="k">\cite</span><span class="nb">{</span>shorten2019survey<span class="nb">}</span>.

<span class="k">\subsection</span><span class="nb">{</span>Transfer Learning<span class="nb">}</span>

Transfer learning leverages pre-trained models on large datasets to improve
performance on smaller target datasets~<span class="k">\cite</span><span class="nb">{</span>yosinski2014transferable<span class="nb">}</span>. This
approach has become standard practice in computer vision.

<span class="c">% ==================== Methodology ====================</span>

<span class="k">\section</span><span class="nb">{</span>Methodology<span class="nb">}</span>
<span class="k">\label</span><span class="nb">{</span>sec:methodology<span class="nb">}</span>

<span class="k">\subsection</span><span class="nb">{</span>Datasets<span class="nb">}</span>

We conduct experiments on three benchmark datasets:

<span class="k">\begin</span><span class="nb">{</span>itemize<span class="nb">}</span>
    <span class="k">\item</span> <span class="k">\textbf</span><span class="nb">{</span>CIFAR-10<span class="nb">}</span>: 60,000 32√ó32 color images in 10 classes
    <span class="k">\item</span> <span class="k">\textbf</span><span class="nb">{</span>CIFAR-100<span class="nb">}</span>: 60,000 32√ó32 color images in 100 classes
    <span class="k">\item</span> <span class="k">\textbf</span><span class="nb">{</span>ImageNet<span class="nb">}</span>: 1.2M high-resolution images in 1000 classes (subset)
<span class="k">\end</span><span class="nb">{</span>itemize<span class="nb">}</span>

<span class="k">\Cref</span><span class="nb">{</span>tab:datasets<span class="nb">}</span> summarizes the dataset statistics.

<span class="k">\begin</span><span class="nb">{</span>table<span class="nb">}</span>[htbp]
<span class="k">\centering</span>
<span class="k">\caption</span><span class="nb">{</span>Dataset statistics for our experiments<span class="nb">}</span>
<span class="k">\label</span><span class="nb">{</span>tab:datasets<span class="nb">}</span>
<span class="k">\begin</span><span class="nb">{</span>tabular<span class="nb">}{</span>@<span class="nb">{}</span>lrrr@<span class="nb">{}}</span>
<span class="k">\toprule</span>
Dataset <span class="nb">&amp;</span> Training Images <span class="nb">&amp;</span> Test Images <span class="nb">&amp;</span> Classes <span class="k">\\</span>
<span class="k">\midrule</span>
CIFAR-10 <span class="nb">&amp;</span> 50,000 <span class="nb">&amp;</span> 10,000 <span class="nb">&amp;</span> 10 <span class="k">\\</span>
CIFAR-100 <span class="nb">&amp;</span> 50,000 <span class="nb">&amp;</span> 10,000 <span class="nb">&amp;</span> 100 <span class="k">\\</span>
ImageNet (subset) <span class="nb">&amp;</span> 100,000 <span class="nb">&amp;</span> 5,000 <span class="nb">&amp;</span> 100 <span class="k">\\</span>
<span class="k">\bottomrule</span>
<span class="k">\end</span><span class="nb">{</span>tabular<span class="nb">}</span>
<span class="k">\end</span><span class="nb">{</span>table<span class="nb">}</span>

<span class="k">\subsection</span><span class="nb">{</span>Network Architectures<span class="nb">}</span>

We implement and evaluate the following architectures:

<span class="k">\begin</span><span class="nb">{</span>enumerate<span class="nb">}</span>
    <span class="k">\item</span> <span class="k">\textbf</span><span class="nb">{</span>Baseline CNN<span class="nb">}</span>: Simple 6-layer convolutional network
    <span class="k">\item</span> <span class="k">\textbf</span><span class="nb">{</span>VGG-style<span class="nb">}</span>: Deep network with small filters
    <span class="k">\item</span> <span class="k">\textbf</span><span class="nb">{</span>ResNet-18/34/50<span class="nb">}</span>: Residual networks of varying depths
    <span class="k">\item</span> <span class="k">\textbf</span><span class="nb">{</span>Custom Hybrid<span class="nb">}</span>: Our proposed architecture combining best practices
<span class="k">\end</span><span class="nb">{</span>enumerate<span class="nb">}</span>

The baseline architecture is defined as:

<span class="k">\begin</span><span class="nb">{</span>equation<span class="nb">}</span>
<span class="k">\begin</span><span class="nb">{</span>aligned<span class="nb">}</span>
    <span class="k">\text</span><span class="nb">{</span>Conv<span class="nb">}</span>(3 <span class="k">\times</span> 3, 64) <span class="nb">&amp;</span><span class="k">\rightarrow</span> <span class="k">\text</span><span class="nb">{</span>ReLU<span class="nb">}</span> <span class="k">\rightarrow</span> <span class="k">\text</span><span class="nb">{</span>MaxPool<span class="nb">}</span> <span class="k">\\</span>
    <span class="k">\text</span><span class="nb">{</span>Conv<span class="nb">}</span>(3 <span class="k">\times</span> 3, 128) <span class="nb">&amp;</span><span class="k">\rightarrow</span> <span class="k">\text</span><span class="nb">{</span>ReLU<span class="nb">}</span> <span class="k">\rightarrow</span> <span class="k">\text</span><span class="nb">{</span>MaxPool<span class="nb">}</span> <span class="k">\\</span>
    <span class="k">\text</span><span class="nb">{</span>Conv<span class="nb">}</span>(3 <span class="k">\times</span> 3, 256) <span class="nb">&amp;</span><span class="k">\rightarrow</span> <span class="k">\text</span><span class="nb">{</span>ReLU<span class="nb">}</span> <span class="k">\rightarrow</span> <span class="k">\text</span><span class="nb">{</span>MaxPool<span class="nb">}</span> <span class="k">\\</span>
    <span class="k">\text</span><span class="nb">{</span>FC<span class="nb">}</span>(512) <span class="nb">&amp;</span><span class="k">\rightarrow</span> <span class="k">\text</span><span class="nb">{</span>ReLU<span class="nb">}</span> <span class="k">\rightarrow</span> <span class="k">\text</span><span class="nb">{</span>Dropout<span class="nb">}</span>(0.5) <span class="k">\\</span>
    <span class="k">\text</span><span class="nb">{</span>FC<span class="nb">}</span>(<span class="k">\text</span><span class="nb">{</span>num<span class="k">\_classes</span><span class="nb">}</span>) <span class="nb">&amp;</span><span class="k">\rightarrow</span> <span class="k">\text</span><span class="nb">{</span>Softmax<span class="nb">}</span>
<span class="k">\end</span><span class="nb">{</span>aligned<span class="nb">}</span>
<span class="k">\end</span><span class="nb">{</span>equation<span class="nb">}</span>

<span class="k">\subsection</span><span class="nb">{</span>Training Procedure<span class="nb">}</span>

All models are trained using the following configuration:

<span class="k">\begin</span><span class="nb">{</span>itemize<span class="nb">}</span>
    <span class="k">\item</span> <span class="k">\textbf</span><span class="nb">{</span>Optimizer<span class="nb">}</span>: SGD with momentum (0.9)
    <span class="k">\item</span> <span class="k">\textbf</span><span class="nb">{</span>Learning Rate<span class="nb">}</span>: 0.1, reduced by 10√ó at epochs 60, 120, 160
    <span class="k">\item</span> <span class="k">\textbf</span><span class="nb">{</span>Batch Size<span class="nb">}</span>: 128
    <span class="k">\item</span> <span class="k">\textbf</span><span class="nb">{</span>Weight Decay<span class="nb">}</span>: <span class="s">$</span><span class="m">5</span><span class="nb"> </span><span class="nv">\times</span><span class="nb"> </span><span class="m">10</span><span class="nb">^{</span><span class="o">-</span><span class="m">4</span><span class="nb">}</span><span class="s">$</span>
    <span class="k">\item</span> <span class="k">\textbf</span><span class="nb">{</span>Training Epochs<span class="nb">}</span>: 200
<span class="k">\end</span><span class="nb">{</span>itemize<span class="nb">}</span>

The loss function is cross-entropy:

<span class="k">\begin</span><span class="nb">{</span>equation<span class="nb">}</span>
<span class="k">\mathcal</span><span class="nb">{</span>L<span class="nb">}</span>(<span class="k">\vect</span><span class="nb">{</span><span class="k">\theta</span><span class="nb">}</span>) = -<span class="k">\frac</span><span class="nb">{</span>1<span class="nb">}{</span>N<span class="nb">}</span><span class="k">\sum_</span><span class="nb">{</span>i=1<span class="nb">}^</span>N <span class="k">\sum_</span><span class="nb">{</span>c=1<span class="nb">}^</span>C y<span class="nb">_{</span>ic<span class="nb">}</span> <span class="k">\log</span>(<span class="k">\hat</span><span class="nb">{</span>y<span class="nb">}_{</span>ic<span class="nb">}</span>)
<span class="k">\end</span><span class="nb">{</span>equation<span class="nb">}</span>

where <span class="s">$</span><span class="nb">N</span><span class="s">$</span> is batch size, <span class="s">$</span><span class="nb">C</span><span class="s">$</span> is number of classes, <span class="s">$</span><span class="nb">y_{ic}</span><span class="s">$</span> is ground truth, and
<span class="s">$</span><span class="nv">\hat</span><span class="nb">{y}_{ic}</span><span class="s">$</span> is predicted probability.

<span class="k">\subsection</span><span class="nb">{</span>Data Augmentation<span class="nb">}</span>

We apply the following augmentation techniques during training:

<span class="k">\begin</span><span class="nb">{</span>itemize<span class="nb">}</span>
    <span class="k">\item</span> Random horizontal flips (probability 0.5)
    <span class="k">\item</span> Random crops with padding of 4 pixels
    <span class="k">\item</span> Color jittering (brightness, contrast, saturation)
    <span class="k">\item</span> Random rotation (<span class="s">$</span><span class="nv">\pm</span><span class="nb"> </span><span class="m">15</span><span class="s">$</span> degrees)
<span class="k">\end</span><span class="nb">{</span>itemize<span class="nb">}</span>

<span class="k">\subsection</span><span class="nb">{</span>Evaluation Metrics<span class="nb">}</span>

We evaluate models using:

<span class="k">\begin</span><span class="nb">{</span>itemize<span class="nb">}</span>
    <span class="k">\item</span> <span class="k">\textbf</span><span class="nb">{</span>Top-1 Accuracy<span class="nb">}</span>: Percentage of correct predictions
    <span class="k">\item</span> <span class="k">\textbf</span><span class="nb">{</span>Top-5 Accuracy<span class="nb">}</span>: Percentage where correct class is in top 5 predictions
    <span class="k">\item</span> <span class="k">\textbf</span><span class="nb">{</span>Parameters<span class="nb">}</span>: Total number of trainable parameters
    <span class="k">\item</span> <span class="k">\textbf</span><span class="nb">{</span>FLOPs<span class="nb">}</span>: Floating-point operations per forward pass
<span class="k">\end</span><span class="nb">{</span>itemize<span class="nb">}</span>

<span class="c">% ==================== Results ====================</span>

<span class="k">\section</span><span class="nb">{</span>Results<span class="nb">}</span>
<span class="k">\label</span><span class="nb">{</span>sec:results<span class="nb">}</span>

<span class="k">\subsection</span><span class="nb">{</span>Main Results<span class="nb">}</span>

<span class="k">\Cref</span><span class="nb">{</span>tab:main<span class="nb">_</span>results<span class="nb">}</span> presents the performance of different architectures on CIFAR-10
and CIFAR-100.

<span class="k">\begin</span><span class="nb">{</span>table<span class="nb">}</span>[htbp]
<span class="k">\centering</span>
<span class="k">\caption</span><span class="nb">{</span>Classification accuracy (<span class="k">\%</span>) on CIFAR-10 and CIFAR-100<span class="nb">}</span>
<span class="k">\label</span><span class="nb">{</span>tab:main<span class="nb">_</span>results<span class="nb">}</span>
<span class="k">\begin</span><span class="nb">{</span>tabular<span class="nb">}{</span>@<span class="nb">{}</span>lcccc@<span class="nb">{}}</span>
<span class="k">\toprule</span>
<span class="k">\multirow</span><span class="nb">{</span>2<span class="nb">}{</span>*<span class="nb">}{</span>Model<span class="nb">}</span> <span class="nb">&amp;</span> <span class="k">\multicolumn</span><span class="nb">{</span>2<span class="nb">}{</span>c<span class="nb">}{</span>CIFAR-10<span class="nb">}</span> <span class="nb">&amp;</span> <span class="k">\multicolumn</span><span class="nb">{</span>2<span class="nb">}{</span>c<span class="nb">}{</span>CIFAR-100<span class="nb">}</span> <span class="k">\\</span>
<span class="k">\cmidrule</span>(lr)<span class="nb">{</span>2-3<span class="nb">}</span> <span class="k">\cmidrule</span>(lr)<span class="nb">{</span>4-5<span class="nb">}</span>
 <span class="nb">&amp;</span> Top-1 <span class="nb">&amp;</span> Top-5 <span class="nb">&amp;</span> Top-1 <span class="nb">&amp;</span> Top-5 <span class="k">\\</span>
<span class="k">\midrule</span>
Baseline CNN <span class="nb">&amp;</span> 89.2 <span class="nb">&amp;</span> 99.6 <span class="nb">&amp;</span> 62.4 <span class="nb">&amp;</span> 85.3 <span class="k">\\</span>
VGG-style <span class="nb">&amp;</span> 92.5 <span class="nb">&amp;</span> 99.8 <span class="nb">&amp;</span> 68.7 <span class="nb">&amp;</span> 88.9 <span class="k">\\</span>
ResNet-18 <span class="nb">&amp;</span> 94.8 <span class="nb">&amp;</span> 99.9 <span class="nb">&amp;</span> 74.2 <span class="nb">&amp;</span> 91.5 <span class="k">\\</span>
ResNet-34 <span class="nb">&amp;</span> 95.6 <span class="nb">&amp;</span> 99.9 <span class="nb">&amp;</span> 76.8 <span class="nb">&amp;</span> 92.8 <span class="k">\\</span>
ResNet-50 <span class="nb">&amp;</span> 96.1 <span class="nb">&amp;</span> 100.0 <span class="nb">&amp;</span> 78.5 <span class="nb">&amp;</span> 93.7 <span class="k">\\</span>
Custom Hybrid <span class="nb">&amp;</span> <span class="k">\textbf</span><span class="nb">{</span>96.8<span class="nb">}</span> <span class="nb">&amp;</span> <span class="k">\textbf</span><span class="nb">{</span>100.0<span class="nb">}</span> <span class="nb">&amp;</span> <span class="k">\textbf</span><span class="nb">{</span>84.2<span class="nb">}</span> <span class="nb">&amp;</span> <span class="k">\textbf</span><span class="nb">{</span>95.1<span class="nb">}</span> <span class="k">\\</span>
<span class="k">\bottomrule</span>
<span class="k">\end</span><span class="nb">{</span>tabular<span class="nb">}</span>
<span class="k">\end</span><span class="nb">{</span>table<span class="nb">}</span>

Our Custom Hybrid architecture achieves the best performance on both datasets,
demonstrating the effectiveness of combining architectural innovations.

<span class="k">\subsection</span><span class="nb">{</span>Impact of Network Depth<span class="nb">}</span>

We investigate how network depth affects performance by training ResNets of varying
depths. <span class="k">\Cref</span><span class="nb">{</span>fig:depth<span class="nb">_</span>analysis<span class="nb">}</span> would show that performance improves with depth
but plateaus beyond 50 layers for CIFAR-10.

<span class="k">\begin</span><span class="nb">{</span>theorem<span class="nb">}</span>[Depth-Performance Relationship]
<span class="k">\label</span><span class="nb">{</span>thm:depth<span class="nb">}</span>
For a fixed parameter budget, deeper networks with residual connections outperform
shallower networks on complex classification tasks, up to a saturation point
determined by dataset complexity.
<span class="k">\end</span><span class="nb">{</span>theorem<span class="nb">}</span>

<span class="k">\subsection</span><span class="nb">{</span>Regularization Analysis<span class="nb">}</span>

<span class="k">\Cref</span><span class="nb">{</span>tab:regularization<span class="nb">}</span> shows the impact of different regularization techniques.

<span class="k">\begin</span><span class="nb">{</span>table<span class="nb">}</span>[htbp]
<span class="k">\centering</span>
<span class="k">\caption</span><span class="nb">{</span>Effect of regularization on ResNet-18 (CIFAR-10 accuracy)<span class="nb">}</span>
<span class="k">\label</span><span class="nb">{</span>tab:regularization<span class="nb">}</span>
<span class="k">\begin</span><span class="nb">{</span>tabular<span class="nb">}{</span>@<span class="nb">{}</span>lccc@<span class="nb">{}}</span>
<span class="k">\toprule</span>
Technique <span class="nb">&amp;</span> Training Acc. <span class="nb">&amp;</span> Test Acc. <span class="nb">&amp;</span> Overfitting <span class="k">\\</span>
<span class="k">\midrule</span>
None <span class="nb">&amp;</span> 99.8 <span class="nb">&amp;</span> 89.3 <span class="nb">&amp;</span> 10.5 <span class="k">\\</span>
Dropout only <span class="nb">&amp;</span> 98.2 <span class="nb">&amp;</span> 92.1 <span class="nb">&amp;</span> 6.1 <span class="k">\\</span>
BatchNorm only <span class="nb">&amp;</span> 99.1 <span class="nb">&amp;</span> 93.8 <span class="nb">&amp;</span> 5.3 <span class="k">\\</span>
Data Aug. only <span class="nb">&amp;</span> 97.5 <span class="nb">&amp;</span> 94.2 <span class="nb">&amp;</span> 3.3 <span class="k">\\</span>
All combined <span class="nb">&amp;</span> 96.8 <span class="nb">&amp;</span> 94.8 <span class="nb">&amp;</span> 2.0 <span class="k">\\</span>
<span class="k">\bottomrule</span>
<span class="k">\end</span><span class="nb">{</span>tabular<span class="nb">}</span>
<span class="k">\end</span><span class="nb">{</span>table<span class="nb">}</span>

The combination of all regularization techniques achieves the best generalization.

<span class="k">\subsection</span><span class="nb">{</span>Computational Efficiency<span class="nb">}</span>

<span class="k">\Cref</span><span class="nb">{</span>tab:efficiency<span class="nb">}</span> compares model complexity and inference time.

<span class="k">\begin</span><span class="nb">{</span>table<span class="nb">}</span>[htbp]
<span class="k">\centering</span>
<span class="k">\caption</span><span class="nb">{</span>Model complexity and computational requirements<span class="nb">}</span>
<span class="k">\label</span><span class="nb">{</span>tab:efficiency<span class="nb">}</span>
<span class="k">\begin</span><span class="nb">{</span>tabular<span class="nb">}{</span>@<span class="nb">{}</span>lrrr@<span class="nb">{}}</span>
<span class="k">\toprule</span>
Model <span class="nb">&amp;</span> Parameters (M) <span class="nb">&amp;</span> FLOPs (G) <span class="nb">&amp;</span> Inference Time (ms) <span class="k">\\</span>
<span class="k">\midrule</span>
Baseline CNN <span class="nb">&amp;</span> 2.4 <span class="nb">&amp;</span> 0.15 <span class="nb">&amp;</span> 3.2 <span class="k">\\</span>
VGG-style <span class="nb">&amp;</span> 14.7 <span class="nb">&amp;</span> 0.31 <span class="nb">&amp;</span> 8.5 <span class="k">\\</span>
ResNet-18 <span class="nb">&amp;</span> 11.2 <span class="nb">&amp;</span> 0.56 <span class="nb">&amp;</span> 6.8 <span class="k">\\</span>
ResNet-50 <span class="nb">&amp;</span> 23.5 <span class="nb">&amp;</span> 1.31 <span class="nb">&amp;</span> 12.3 <span class="k">\\</span>
Custom Hybrid <span class="nb">&amp;</span> 18.6 <span class="nb">&amp;</span> 0.89 <span class="nb">&amp;</span> 9.7 <span class="k">\\</span>
<span class="k">\bottomrule</span>
<span class="k">\end</span><span class="nb">{</span>tabular<span class="nb">}</span>
<span class="k">\end</span><span class="nb">{</span>table<span class="nb">}</span>

<span class="c">% ==================== Discussion ====================</span>

<span class="k">\section</span><span class="nb">{</span>Discussion<span class="nb">}</span>
<span class="k">\label</span><span class="nb">{</span>sec:discussion<span class="nb">}</span>

<span class="k">\subsection</span><span class="nb">{</span>Key Findings<span class="nb">}</span>

Our experiments reveal several important insights:

<span class="k">\begin</span><span class="nb">{</span>enumerate<span class="nb">}</span>
    <span class="k">\item</span> <span class="k">\textbf</span><span class="nb">{</span>Depth matters<span class="nb">}</span>: Deeper networks consistently outperform shallow
          ones when properly regularized (see <span class="k">\Cref</span><span class="nb">{</span>thm:depth<span class="nb">}</span>)
    <span class="k">\item</span> <span class="k">\textbf</span><span class="nb">{</span>Skip connections are essential<span class="nb">}</span>: ResNets significantly outperform
          plain deep networks of similar depth
    <span class="k">\item</span> <span class="k">\textbf</span><span class="nb">{</span>Regularization is crucial<span class="nb">}</span>: The combination of multiple
          regularization techniques is more effective than any single method
    <span class="k">\item</span> <span class="k">\textbf</span><span class="nb">{</span>Data augmentation has the largest impact<span class="nb">}</span>: Among regularization
          techniques, data augmentation provides the most significant improvement
<span class="k">\end</span><span class="nb">{</span>enumerate<span class="nb">}</span>

<span class="k">\subsection</span><span class="nb">{</span>Comparison with Prior Work<span class="nb">}</span>

Our Custom Hybrid architecture achieves competitive performance compared to
state-of-the-art methods while maintaining reasonable computational requirements.
The 96.8<span class="k">\%</span> accuracy on CIFAR-10 is comparable to recent work, though some highly
optimized architectures achieve slightly higher accuracy at the cost of increased
complexity.

<span class="k">\subsection</span><span class="nb">{</span>Practical Guidelines<span class="nb">}</span>

Based on our findings, we recommend the following guidelines for practitioners:

<span class="k">\begin</span><span class="nb">{</span>itemize<span class="nb">}</span>
    <span class="k">\item</span> Start with ResNet-18 or ResNet-34 as baseline architectures
    <span class="k">\item</span> Always use batch normalization and data augmentation
    <span class="k">\item</span> Prefer deeper networks over wider networks for complex datasets
    <span class="k">\item</span> Use transfer learning when dataset size is limited
    <span class="k">\item</span> Monitor both training and validation metrics to detect overfitting
<span class="k">\end</span><span class="nb">{</span>itemize<span class="nb">}</span>

<span class="k">\subsection</span><span class="nb">{</span>Limitations<span class="nb">}</span>

Our study has several limitations:

<span class="k">\begin</span><span class="nb">{</span>itemize<span class="nb">}</span>
    <span class="k">\item</span> Experiments are limited to relatively small images (32√ó32 and 224√ó224)
    <span class="k">\item</span> Computational constraints limited hyperparameter search space
    <span class="k">\item</span> We did not explore neural architecture search methods
    <span class="k">\item</span> Analysis focuses on accuracy; other metrics (fairness, robustness) not considered
<span class="k">\end</span><span class="nb">{</span>itemize<span class="nb">}</span>

<span class="c">% ==================== Conclusion ====================</span>

<span class="k">\section</span><span class="nb">{</span>Conclusion<span class="nb">}</span>
<span class="k">\label</span><span class="nb">{</span>sec:conclusion<span class="nb">}</span>

This paper presented a comprehensive empirical study of CNN architectures for image
classification. Through systematic experiments on multiple datasets, we demonstrated
that the combination of increased depth, residual connections, and proper regularization
leads to superior performance.

Our Custom Hybrid architecture achieves 96.8<span class="k">\%</span> accuracy on CIFAR-10 and 84.2<span class="k">\%</span> on
CIFAR-100, demonstrating the effectiveness of combining architectural best practices.
We provide practical guidelines to help practitioners design effective CNN architectures.

<span class="k">\subsection</span><span class="nb">{</span>Future Work<span class="nb">}</span>

Several directions for future research include:

<span class="k">\begin</span><span class="nb">{</span>itemize<span class="nb">}</span>
    <span class="k">\item</span> Investigating attention mechanisms in CNNs
    <span class="k">\item</span> Exploring neural architecture search for automatic design
    <span class="k">\item</span> Extending analysis to other computer vision tasks (detection, segmentation)
    <span class="k">\item</span> Studying adversarial robustness of different architectures
    <span class="k">\item</span> Developing more efficient architectures for mobile deployment
<span class="k">\end</span><span class="nb">{</span>itemize<span class="nb">}</span>

<span class="k">\section*</span><span class="nb">{</span>Acknowledgments<span class="nb">}</span>

We thank the anonymous reviewers for their valuable feedback. This work was supported
by the National Science Foundation under Grant No. 12345. Computational resources
were provided by the University Computing Center.

<span class="c">% ==================== Bibliography ====================</span>

<span class="k">\begin</span><span class="nb">{</span>thebibliography<span class="nb">}{</span>10<span class="nb">}</span>

<span class="k">\bibitem</span><span class="nb">{</span>lecun2015deep<span class="nb">}</span>
Y.~LeCun, Y.~Bengio, and G.~Hinton.
<span class="k">\newblock</span> Deep learning.
<span class="k">\newblock</span> <span class="nb">{</span><span class="k">\em</span> Nature<span class="nb">}</span>, 521(7553):436--444, 2015.

<span class="k">\bibitem</span><span class="nb">{</span>goodfellow2016deep<span class="nb">}</span>
I.~Goodfellow, Y.~Bengio, and A.~Courville.
<span class="k">\newblock</span> <span class="nb">{</span><span class="k">\em</span> Deep Learning<span class="nb">}</span>.
<span class="k">\newblock</span> MIT Press, 2016.

<span class="k">\bibitem</span><span class="nb">{</span>krizhevsky2012imagenet<span class="nb">}</span>
A.~Krizhevsky, I.~Sutskever, and G.~E. Hinton.
<span class="k">\newblock</span> Imagenet classification with deep convolutional neural networks.
<span class="k">\newblock</span> In <span class="nb">{</span><span class="k">\em</span> Advances in Neural Information Processing Systems<span class="nb">}</span>, pages
  1097--1105, 2012.

<span class="k">\bibitem</span><span class="nb">{</span>lecun1998gradient<span class="nb">}</span>
Y.~LeCun, L.~Bottou, Y.~Bengio, and P.~Haffner.
<span class="k">\newblock</span> Gradient-based learning applied to document recognition.
<span class="k">\newblock</span> <span class="nb">{</span><span class="k">\em</span> Proceedings of the IEEE<span class="nb">}</span>, 86(11):2278--2324, 1998.

<span class="k">\bibitem</span><span class="nb">{</span>simonyan2014very<span class="nb">}</span>
K.~Simonyan and A.~Zisserman.
<span class="k">\newblock</span> Very deep convolutional networks for large-scale image recognition.
<span class="k">\newblock</span> <span class="nb">{</span><span class="k">\em</span> arXiv preprint arXiv:1409.1556<span class="nb">}</span>, 2014.

<span class="k">\bibitem</span><span class="nb">{</span>szegedy2015going<span class="nb">}</span>
C.~Szegedy, W.~Liu, Y.~Jia, P.~Sermanet, S.~Reed, D.~Anguelov, D.~Erhan,
  V.~Vanhoucke, and A.~Rabinovich.
<span class="k">\newblock</span> Going deeper with convolutions.
<span class="k">\newblock</span> In <span class="nb">{</span><span class="k">\em</span> CVPR<span class="nb">}</span>, pages 1--9, 2015.

<span class="k">\bibitem</span><span class="nb">{</span>he2016deep<span class="nb">}</span>
K.~He, X.~Zhang, S.~Ren, and J.~Sun.
<span class="k">\newblock</span> Deep residual learning for image recognition.
<span class="k">\newblock</span> In <span class="nb">{</span><span class="k">\em</span> CVPR<span class="nb">}</span>, pages 770--778, 2016.

<span class="k">\bibitem</span><span class="nb">{</span>huang2017densely<span class="nb">}</span>
G.~Huang, Z.~Liu, L.~Van Der Maaten, and K.~Q. Weinberger.
<span class="k">\newblock</span> Densely connected convolutional networks.
<span class="k">\newblock</span> In <span class="nb">{</span><span class="k">\em</span> CVPR<span class="nb">}</span>, pages 4700--4708, 2017.

<span class="k">\bibitem</span><span class="nb">{</span>srivastava2014dropout<span class="nb">}</span>
N.~Srivastava, G.~Hinton, A.~Krizhevsky, I.~Sutskever, and R.~Salakhutdinov.
<span class="k">\newblock</span> Dropout: A simple way to prevent neural networks from overfitting.
<span class="k">\newblock</span> <span class="nb">{</span><span class="k">\em</span> The Journal of Machine Learning Research<span class="nb">}</span>, 15(1):1929--1958,
  2014.

<span class="k">\bibitem</span><span class="nb">{</span>ioffe2015batch<span class="nb">}</span>
S.~Ioffe and C.~Szegedy.
<span class="k">\newblock</span> Batch normalization: Accelerating deep network training by reducing
  internal covariate shift.
<span class="k">\newblock</span> In <span class="nb">{</span><span class="k">\em</span> ICML<span class="nb">}</span>, pages 448--456, 2015.

<span class="k">\bibitem</span><span class="nb">{</span>shorten2019survey<span class="nb">}</span>
C.~Shorten and T.~M. Khoshgoftaar.
<span class="k">\newblock</span> A survey on image data augmentation for deep learning.
<span class="k">\newblock</span> <span class="nb">{</span><span class="k">\em</span> Journal of Big Data<span class="nb">}</span>, 6(1):1--48, 2019.

<span class="k">\bibitem</span><span class="nb">{</span>yosinski2014transferable<span class="nb">}</span>
J.~Yosinski, J.~Clune, Y.~Bengio, and H.~Lipson.
<span class="k">\newblock</span> How transferable are features in deep neural networks?
<span class="k">\newblock</span> In <span class="nb">{</span><span class="k">\em</span> Advances in Neural Information Processing Systems<span class="nb">}</span>, pages
  3320--3328, 2014.

<span class="k">\end</span><span class="nb">{</span>thebibliography<span class="nb">}</span>

<span class="c">% ==================== Appendices ====================</span>

<span class="k">\appendix</span>

<span class="k">\section</span><span class="nb">{</span>Hyperparameter Search Details<span class="nb">}</span>
<span class="k">\label</span><span class="nb">{</span>app:hyperparams<span class="nb">}</span>

We conducted grid search over the following hyperparameters:

<span class="k">\begin</span><span class="nb">{</span>itemize<span class="nb">}</span>
    <span class="k">\item</span> Learning rate: <span class="s">$</span><span class="nv">\{</span><span class="m">0</span><span class="nb">.</span><span class="m">01</span><span class="nb">, </span><span class="m">0</span><span class="nb">.</span><span class="m">05</span><span class="nb">, </span><span class="m">0</span><span class="nb">.</span><span class="m">1</span><span class="nb">, </span><span class="m">0</span><span class="nb">.</span><span class="m">2</span><span class="nv">\}</span><span class="s">$</span>
    <span class="k">\item</span> Weight decay: <span class="s">$</span><span class="nv">\{</span><span class="m">10</span><span class="nb">^{</span><span class="o">-</span><span class="m">4</span><span class="nb">}, </span><span class="m">5</span><span class="nb"> </span><span class="nv">\times</span><span class="nb"> </span><span class="m">10</span><span class="nb">^{</span><span class="o">-</span><span class="m">4</span><span class="nb">}, </span><span class="m">10</span><span class="nb">^{</span><span class="o">-</span><span class="m">3</span><span class="nb">}</span><span class="nv">\}</span><span class="s">$</span>
    <span class="k">\item</span> Dropout rate: <span class="s">$</span><span class="nv">\{</span><span class="m">0</span><span class="nb">.</span><span class="m">3</span><span class="nb">, </span><span class="m">0</span><span class="nb">.</span><span class="m">5</span><span class="nb">, </span><span class="m">0</span><span class="nb">.</span><span class="m">7</span><span class="nv">\}</span><span class="s">$</span>
    <span class="k">\item</span> Batch size: <span class="s">$</span><span class="nv">\{</span><span class="m">64</span><span class="nb">, </span><span class="m">128</span><span class="nb">, </span><span class="m">256</span><span class="nv">\}</span><span class="s">$</span>
<span class="k">\end</span><span class="nb">{</span>itemize<span class="nb">}</span>

The best configuration for ResNet-18 on CIFAR-10 was: learning rate = 0.1,
weight decay = <span class="s">$</span><span class="m">5</span><span class="nb"> </span><span class="nv">\times</span><span class="nb"> </span><span class="m">10</span><span class="nb">^{</span><span class="o">-</span><span class="m">4</span><span class="nb">}</span><span class="s">$</span>, dropout = 0.5, batch size = 128.

<span class="k">\section</span><span class="nb">{</span>Additional Experimental Results<span class="nb">}</span>
<span class="k">\label</span><span class="nb">{</span>app:additional<span class="nb">}</span>

<span class="k">\subsection</span><span class="nb">{</span>Learning Curves<span class="nb">}</span>

Training and validation accuracy curves for all models show consistent convergence
patterns. ResNet models converge faster than VGG-style networks due to better
gradient flow.

<span class="k">\subsection</span><span class="nb">{</span>Ablation Studies<span class="nb">}</span>

We performed ablation studies on our Custom Hybrid architecture:

<span class="k">\begin</span><span class="nb">{</span>enumerate<span class="nb">}</span>
    <span class="k">\item</span> Removing skip connections: -2.3<span class="k">\%</span> accuracy
    <span class="k">\item</span> Removing batch normalization: -1.8<span class="k">\%</span> accuracy
    <span class="k">\item</span> Reducing depth by 50<span class="k">\%</span>: -1.5<span class="k">\%</span> accuracy
    <span class="k">\item</span> Removing data augmentation: -3.2<span class="k">\%</span> accuracy
<span class="k">\end</span><span class="nb">{</span>enumerate<span class="nb">}</span>

<span class="k">\section</span><span class="nb">{</span>Implementation Details<span class="nb">}</span>
<span class="k">\label</span><span class="nb">{</span>app:implementation<span class="nb">}</span>

All experiments were implemented in PyTorch 1.12. Training was performed on
NVIDIA Tesla V100 GPUs. Average training time was 6 hours for ResNet-18 and
18 hours for ResNet-50 on CIFAR-10.

Code is available at: <span class="k">\url</span><span class="nb">{</span>https://github.com/username/cnn-image-classification<span class="nb">}</span>

<span class="k">\end</span><span class="nb">{</span>document<span class="nb">}</span>
</code></pre></div></td></tr></table></div>

    </div>
</article>

            </div>
        </main>
    </div>

    <script src="/study/static/js/app.js"></script>
    <script>
        function switchLanguage(newLang) {
            const path = window.location.pathname;
            const base = '/study';
            const currentLang = 'en';
            const newPath = path.replace(base + '/' + currentLang + '/', base + '/' + newLang + '/');
            window.location.href = newPath + window.location.search;
        }
    </script>
    
<script>
    document.getElementById('copy-code-btn').addEventListener('click', function() {
        var code = document.querySelector('.example-code .highlight');
        // Extract just the code text, not line numbers
        var lines = code.querySelectorAll('.code pre span.line');
        var text;
        if (lines.length > 0) {
            text = Array.from(lines).map(function(l) { return l.textContent; }).join('\n');
        } else {
            // Fallback: get all code text
            var codeEl = code.querySelector('pre code') || code.querySelector('pre');
            text = codeEl ? codeEl.textContent : code.textContent;
        }
        navigator.clipboard.writeText(text);
        this.textContent = 'Copied!';
        var btn = this;
        setTimeout(function() { btn.textContent = 'Copy code'; }, 2000);
    });
</script>

</body>
</html>
{% endraw %}