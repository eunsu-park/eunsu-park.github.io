{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 07. 랜덤 포레스트 (Random Forest)\n",
    "\n",
    "## 학습 목표\n",
    "- 앙상블 학습과 배깅 이해\n",
    "- 랜덤 포레스트 작동 원리\n",
    "- 하이퍼파라미터 튜닝"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.ensemble import RandomForestClassifier, RandomForestRegressor\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "from sklearn.datasets import load_iris, load_wine, fetch_california_housing\n",
    "import seaborn as sns\n",
    "\n",
    "plt.rcParams['font.family'] = 'DejaVu Sans'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. 랜덤 포레스트 분류"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Wine 데이터셋 로드\n",
    "wine = load_wine()\n",
    "X, y = wine.data, wine.target\n",
    "\n",
    "print(f\"Features: {wine.feature_names}\")\n",
    "print(f\"Classes: {wine.target_names}\")\n",
    "print(f\"Shape: {X.shape}\")\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.3, random_state=42\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 랜덤 포레스트 모델\n",
    "rf_clf = RandomForestClassifier(\n",
    "    n_estimators=100,\n",
    "    max_depth=5,\n",
    "    random_state=42,\n",
    "    n_jobs=-1\n",
    ")\n",
    "rf_clf.fit(X_train, y_train)\n",
    "\n",
    "y_pred = rf_clf.predict(X_test)\n",
    "print(f\"Accuracy: {accuracy_score(y_test, y_pred):.4f}\")\n",
    "print(f\"\\nClassification Report:\")\n",
    "print(classification_report(y_test, y_pred, target_names=wine.target_names))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. 트리 개수에 따른 성능"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 트리 개수에 따른 성능 변화\n",
    "n_trees = [1, 5, 10, 20, 50, 100, 200, 500]\n",
    "train_scores = []\n",
    "test_scores = []\n",
    "oob_scores = []\n",
    "\n",
    "for n in n_trees:\n",
    "    rf = RandomForestClassifier(n_estimators=n, random_state=42, oob_score=True)\n",
    "    rf.fit(X_train, y_train)\n",
    "    train_scores.append(rf.score(X_train, y_train))\n",
    "    test_scores.append(rf.score(X_test, y_test))\n",
    "    oob_scores.append(rf.oob_score_)\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(n_trees, train_scores, 'b-o', label='Train Score')\n",
    "plt.plot(n_trees, test_scores, 'r-o', label='Test Score')\n",
    "plt.plot(n_trees, oob_scores, 'g-o', label='OOB Score')\n",
    "plt.xlabel('Number of Trees')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.title('Random Forest: Performance vs Number of Trees')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.xscale('log')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. 특성 중요도"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 특성 중요도\n",
    "importance = pd.DataFrame({\n",
    "    'Feature': wine.feature_names,\n",
    "    'Importance': rf_clf.feature_importances_\n",
    "}).sort_values('Importance', ascending=True)\n",
    "\n",
    "plt.figure(figsize=(12, 8))\n",
    "plt.barh(importance['Feature'], importance['Importance'])\n",
    "plt.xlabel('Feature Importance')\n",
    "plt.title('Random Forest Feature Importance - Wine Dataset')\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. 하이퍼파라미터 튜닝"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_grid = {\n",
    "    'n_estimators': [50, 100, 200],\n",
    "    'max_depth': [3, 5, 7, None],\n",
    "    'min_samples_split': [2, 5, 10],\n",
    "    'max_features': ['sqrt', 'log2', None]\n",
    "}\n",
    "\n",
    "grid_search = GridSearchCV(\n",
    "    RandomForestClassifier(random_state=42, n_jobs=-1),\n",
    "    param_grid,\n",
    "    cv=5,\n",
    "    scoring='accuracy',\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "print(f\"Best Parameters: {grid_search.best_params_}\")\n",
    "print(f\"Best CV Score: {grid_search.best_score_:.4f}\")\n",
    "print(f\"Test Score: {grid_search.score(X_test, y_test):.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. 결정 트리 vs 랜덤 포레스트 비교"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "# 두 모델 비교\n",
    "dt = DecisionTreeClassifier(max_depth=5, random_state=42)\n",
    "rf = RandomForestClassifier(n_estimators=100, max_depth=5, random_state=42)\n",
    "\n",
    "dt_scores = cross_val_score(dt, X, y, cv=10)\n",
    "rf_scores = cross_val_score(rf, X, y, cv=10)\n",
    "\n",
    "print(f\"Decision Tree: {dt_scores.mean():.4f} (+/- {dt_scores.std()*2:.4f})\")\n",
    "print(f\"Random Forest: {rf_scores.mean():.4f} (+/- {rf_scores.std()*2:.4f})\")\n",
    "\n",
    "# 박스플롯 비교\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.boxplot([dt_scores, rf_scores], labels=['Decision Tree', 'Random Forest'])\n",
    "plt.ylabel('Accuracy')\n",
    "plt.title('Model Comparison: Decision Tree vs Random Forest')\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. 랜덤 포레스트 회귀"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# California Housing 데이터\n",
    "housing = fetch_california_housing()\n",
    "X_h, y_h = housing.data, housing.target\n",
    "\n",
    "X_train_h, X_test_h, y_train_h, y_test_h = train_test_split(\n",
    "    X_h, y_h, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "# 랜덤 포레스트 회귀\n",
    "rf_reg = RandomForestRegressor(n_estimators=100, max_depth=10, random_state=42, n_jobs=-1)\n",
    "rf_reg.fit(X_train_h, y_train_h)\n",
    "\n",
    "y_pred_h = rf_reg.predict(X_test_h)\n",
    "\n",
    "from sklearn.metrics import r2_score, mean_absolute_error\n",
    "\n",
    "print(f\"R² Score: {r2_score(y_test_h, y_pred_h):.4f}\")\n",
    "print(f\"RMSE: {np.sqrt(mean_squared_error(y_test_h, y_pred_h)):.4f}\")\n",
    "print(f\"MAE: {mean_absolute_error(y_test_h, y_pred_h):.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 특성 중요도 (회귀)\n",
    "importance_reg = pd.DataFrame({\n",
    "    'Feature': housing.feature_names,\n",
    "    'Importance': rf_reg.feature_importances_\n",
    "}).sort_values('Importance', ascending=True)\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.barh(importance_reg['Feature'], importance_reg['Importance'])\n",
    "plt.xlabel('Feature Importance')\n",
    "plt.title('Random Forest Regressor Feature Importance')\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 정리\n",
    "\n",
    "### 핵심 개념\n",
    "- **배깅 (Bagging)**: Bootstrap Aggregating, 여러 모델의 예측을 평균/투표\n",
    "- **랜덤 특성 선택**: 각 분할에서 일부 특성만 고려\n",
    "- **OOB (Out-of-Bag) Score**: 부트스트랩에 포함되지 않은 샘플로 평가\n",
    "\n",
    "### 주요 하이퍼파라미터\n",
    "- `n_estimators`: 트리 개수 (많을수록 좋지만 수익 체감)\n",
    "- `max_depth`: 트리 깊이 (과적합 방지)\n",
    "- `max_features`: 분할 시 고려할 특성 수\n",
    "- `min_samples_split`: 분할을 위한 최소 샘플 수\n",
    "\n",
    "### 다음 단계\n",
    "- Gradient Boosting (XGBoost, LightGBM)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
