{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cell-0",
   "metadata": {},
   "source": [
    "# 08. XGBoost & LightGBM\n",
    "\n",
    "## 학습 목표\n",
    "- Gradient Boosting 개념 이해\n",
    "- XGBoost 사용법과 하이퍼파라미터\n",
    "- LightGBM 특징과 최적화\n",
    "- CatBoost 개요\n",
    "- 모델 비교 및 선택"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV\n",
    "from sklearn.metrics import accuracy_score, classification_report, mean_squared_error, r2_score\n",
    "from sklearn.datasets import make_classification, load_breast_cancer, fetch_california_housing\n",
    "import time\n",
    "\n",
    "plt.rcParams['font.family'] = 'DejaVu Sans'\n",
    "plt.rcParams['axes.unicode_minus'] = False"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-2",
   "metadata": {},
   "source": [
    "## 1. Gradient Boosting 개념"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-3",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\"\"\n",
    "Gradient Boosting 알고리즘:\n",
    "\n",
    "1. 초기화: F_0(x) = argmin_γ Σ L(y_i, γ)\n",
    "\n",
    "2. 반복 (m = 1, 2, ..., M):\n",
    "   a. 의사 잔차(pseudo-residual) 계산:\n",
    "      r_im = -[∂L(y_i, F(x_i))/∂F(x_i)]_{F=F_{m-1}}\n",
    "   \n",
    "   b. 잔차에 대해 약한 학습기 h_m(x) 학습\n",
    "   \n",
    "   c. 최적 스텝 크기 계산:\n",
    "      γ_m = argmin_γ Σ L(y_i, F_{m-1}(x_i) + γ * h_m(x_i))\n",
    "   \n",
    "   d. 모델 업데이트:\n",
    "      F_m(x) = F_{m-1}(x) + learning_rate * γ_m * h_m(x)\n",
    "\n",
    "핵심:\n",
    "- 각 단계에서 이전 모델의 오차(잔차)를 학습\n",
    "- 손실 함수의 그래디언트 방향으로 최적화\n",
    "- learning_rate로 과적합 방지\n",
    "\"\"\")\n",
    "\n",
    "# 간단한 시각화 데이터\n",
    "np.random.seed(42)\n",
    "X_demo = np.linspace(0, 10, 100).reshape(-1, 1)\n",
    "y_demo = np.sin(X_demo).ravel() + np.random.randn(100) * 0.3\n",
    "\n",
    "plt.figure(figsize=(12, 4))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.scatter(X_demo, y_demo, alpha=0.5)\n",
    "plt.xlabel('X')\n",
    "plt.ylabel('y')\n",
    "plt.title('Sample Data for Gradient Boosting')\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "stages = [0, 1, 5, 20, 50]\n",
    "colors = ['red', 'orange', 'yellow', 'green', 'blue']\n",
    "for stage, color in zip(stages, colors):\n",
    "    if stage == 0:\n",
    "        plt.axhline(y=np.mean(y_demo), color=color, label=f'Stage {stage}', alpha=0.7)\n",
    "plt.scatter(X_demo, y_demo, alpha=0.3, color='gray')\n",
    "plt.xlabel('X')\n",
    "plt.ylabel('y')\n",
    "plt.title('Gradient Boosting: Sequential Learning')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-4",
   "metadata": {},
   "source": [
    "## 2. XGBoost (eXtreme Gradient Boosting)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# XGBoost 설치: pip install xgboost\n",
    "import xgboost as xgb\n",
    "from xgboost import XGBClassifier, XGBRegressor\n",
    "\n",
    "print(\"\"\"\n",
    "XGBoost 특징:\n",
    "\n",
    "1. 정규화:\n",
    "   - L1, L2 정규화로 과적합 방지\n",
    "   - 목표 함수: Σ L(y_i, ŷ_i) + Σ Ω(f_k)\n",
    "   - Ω(f) = γT + 0.5λ||w||²\n",
    "\n",
    "2. 효율적인 계산:\n",
    "   - 2차 테일러 전개 사용\n",
    "   - 히스토그램 기반 분할\n",
    "   - 캐시 최적화\n",
    "\n",
    "3. 결측치 처리:\n",
    "   - 자동으로 최적 방향 학습\n",
    "\n",
    "4. 병렬 처리:\n",
    "   - 특성별 병렬 분할점 탐색\n",
    "\"\"\")\n",
    "\n",
    "print(f\"XGBoost 버전: {xgb.__version__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-6",
   "metadata": {},
   "source": [
    "### 2.1 XGBoost 분류"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Breast Cancer 데이터 로드\n",
    "cancer = load_breast_cancer()\n",
    "X, y = cancer.data, cancer.target\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "print(f\"데이터 크기: {X.shape}\")\n",
    "print(f\"클래스: {cancer.target_names}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# XGBoost 분류기\n",
    "xgb_clf = XGBClassifier(\n",
    "    n_estimators=100,\n",
    "    learning_rate=0.1,\n",
    "    max_depth=6,\n",
    "    min_child_weight=1,     # 리프 노드 최소 가중치\n",
    "    gamma=0,                # 분할에 필요한 최소 손실 감소\n",
    "    subsample=1.0,          # 행 샘플링 비율\n",
    "    colsample_bytree=1.0,   # 트리별 열 샘플링 비율\n",
    "    reg_alpha=0,            # L1 정규화\n",
    "    reg_lambda=1,           # L2 정규화\n",
    "    random_state=42,\n",
    "    eval_metric='logloss'\n",
    ")\n",
    "\n",
    "# 학습\n",
    "start_time = time.time()\n",
    "xgb_clf.fit(X_train, y_train)\n",
    "train_time = time.time() - start_time\n",
    "\n",
    "# 예측 및 평가\n",
    "y_pred = xgb_clf.predict(X_test)\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "\n",
    "print(\"=== XGBoost 분류 결과 ===\")\n",
    "print(f\"훈련 정확도: {xgb_clf.score(X_train, y_train):.4f}\")\n",
    "print(f\"테스트 정확도: {accuracy:.4f}\")\n",
    "print(f\"학습 시간: {train_time:.4f}초\")\n",
    "print(f\"\\n분류 보고서:\")\n",
    "print(classification_report(y_test, y_pred, target_names=cancer.target_names))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-9",
   "metadata": {},
   "source": [
    "### 2.2 조기 종료 (Early Stopping)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-10",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 검증 데이터 분리\n",
    "X_train_sub, X_val, y_train_sub, y_val = train_test_split(\n",
    "    X_train, y_train, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "# 조기 종료 사용\n",
    "xgb_early = XGBClassifier(\n",
    "    n_estimators=1000,\n",
    "    learning_rate=0.1,\n",
    "    max_depth=6,\n",
    "    random_state=42,\n",
    "    early_stopping_rounds=10,  # 10 라운드 동안 개선 없으면 중지\n",
    "    eval_metric='logloss'\n",
    ")\n",
    "\n",
    "xgb_early.fit(\n",
    "    X_train_sub, y_train_sub,\n",
    "    eval_set=[(X_val, y_val)],\n",
    "    verbose=False\n",
    ")\n",
    "\n",
    "print(\"=== 조기 종료 결과 ===\")\n",
    "print(f\"최적 반복 횟수: {xgb_early.best_iteration}\")\n",
    "print(f\"최적 점수: {xgb_early.best_score:.4f}\")\n",
    "print(f\"테스트 정확도: {xgb_early.score(X_test, y_test):.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-11",
   "metadata": {},
   "source": [
    "### 2.3 특성 중요도"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-12",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 특성 중요도 시각화\n",
    "importance_types = ['weight', 'gain', 'cover']\n",
    "\n",
    "fig, axes = plt.subplots(1, 3, figsize=(18, 5))\n",
    "\n",
    "for ax, imp_type in zip(axes, importance_types):\n",
    "    importance_dict = xgb_clf.get_booster().get_score(importance_type=imp_type)\n",
    "    \n",
    "    if importance_dict:\n",
    "        # 상위 10개만 표시\n",
    "        sorted_importance = sorted(importance_dict.items(), key=lambda x: x[1], reverse=True)[:10]\n",
    "        features = [x[0] for x in sorted_importance]\n",
    "        values = [x[1] for x in sorted_importance]\n",
    "        \n",
    "        ax.barh(range(len(features)), values)\n",
    "        ax.set_yticks(range(len(features)))\n",
    "        ax.set_yticklabels(features)\n",
    "        ax.set_xlabel('Importance')\n",
    "        ax.set_title(f'Feature Importance ({imp_type})')\n",
    "        ax.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\"\"\n",
    "중요도 타입:\n",
    "- weight: 특성이 분할에 사용된 횟수\n",
    "- gain: 특성 사용 시 평균 이득\n",
    "- cover: 특성이 커버하는 평균 샘플 수\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-13",
   "metadata": {},
   "source": [
    "### 2.4 XGBoost 하이퍼파라미터 튜닝"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-14",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 파라미터 그리드\n",
    "param_grid_xgb = {\n",
    "    'max_depth': [3, 5, 7],\n",
    "    'learning_rate': [0.01, 0.1, 0.3],\n",
    "    'n_estimators': [100, 200],\n",
    "    'min_child_weight': [1, 3],\n",
    "    'subsample': [0.8, 1.0],\n",
    "    'colsample_bytree': [0.8, 1.0]\n",
    "}\n",
    "\n",
    "# Grid Search (시간이 오래 걸릴 수 있으므로 간소화된 그리드 사용)\n",
    "grid_search_xgb = GridSearchCV(\n",
    "    XGBClassifier(random_state=42, eval_metric='logloss'),\n",
    "    param_grid_xgb,\n",
    "    cv=3,\n",
    "    scoring='accuracy',\n",
    "    n_jobs=-1,\n",
    "    verbose=0\n",
    ")\n",
    "\n",
    "# 시간 절약을 위해 샘플 사용\n",
    "X_sample, _, y_sample, _ = train_test_split(X_train, y_train, train_size=0.3, random_state=42)\n",
    "grid_search_xgb.fit(X_sample, y_sample)\n",
    "\n",
    "print(\"=== XGBoost Grid Search 결과 ===\")\n",
    "print(f\"최적 파라미터: {grid_search_xgb.best_params_}\")\n",
    "print(f\"최적 CV 점수: {grid_search_xgb.best_score_:.4f}\")\n",
    "print(f\"테스트 점수: {grid_search_xgb.score(X_test, y_test):.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-15",
   "metadata": {},
   "source": [
    "## 3. LightGBM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-16",
   "metadata": {},
   "outputs": [],
   "source": [
    "# LightGBM 설치: pip install lightgbm\n",
    "import lightgbm as lgb\n",
    "from lightgbm import LGBMClassifier, LGBMRegressor\n",
    "\n",
    "print(\"\"\"\n",
    "LightGBM 특징:\n",
    "\n",
    "1. Leaf-wise 성장:\n",
    "   - 기존: Level-wise (수평 분할)\n",
    "   - LightGBM: Leaf-wise (손실 최대 감소 리프 분할)\n",
    "   - 더 빠르고 정확하지만 과적합 위험\n",
    "\n",
    "2. 히스토그램 기반 분할:\n",
    "   - 연속형 값을 이산화\n",
    "   - 메모리 효율적, 빠른 학습\n",
    "\n",
    "3. GOSS (Gradient-based One-Side Sampling):\n",
    "   - 그래디언트가 큰 샘플 위주로 샘플링\n",
    "\n",
    "4. EFB (Exclusive Feature Bundling):\n",
    "   - 상호 배타적 특성들을 묶음\n",
    "   - 희소 특성에 효과적\n",
    "\"\"\")\n",
    "\n",
    "print(f\"LightGBM 버전: {lgb.__version__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-17",
   "metadata": {},
   "source": [
    "### 3.1 LightGBM 분류"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-18",
   "metadata": {},
   "outputs": [],
   "source": [
    "# LightGBM 분류기\n",
    "lgb_clf = LGBMClassifier(\n",
    "    n_estimators=100,\n",
    "    learning_rate=0.1,\n",
    "    max_depth=-1,           # -1: 제한 없음\n",
    "    num_leaves=31,          # 리프 노드 최대 수\n",
    "    min_child_samples=20,   # 리프 노드 최소 샘플 수\n",
    "    subsample=1.0,          # 행 샘플링\n",
    "    colsample_bytree=1.0,   # 열 샘플링\n",
    "    reg_alpha=0,            # L1 정규화\n",
    "    reg_lambda=0,           # L2 정규화\n",
    "    random_state=42,\n",
    "    verbose=-1\n",
    ")\n",
    "\n",
    "# 학습\n",
    "start_time = time.time()\n",
    "lgb_clf.fit(X_train, y_train)\n",
    "train_time_lgb = time.time() - start_time\n",
    "\n",
    "# 평가\n",
    "y_pred_lgb = lgb_clf.predict(X_test)\n",
    "accuracy_lgb = accuracy_score(y_test, y_pred_lgb)\n",
    "\n",
    "print(\"=== LightGBM 분류 결과 ===\")\n",
    "print(f\"훈련 정확도: {lgb_clf.score(X_train, y_train):.4f}\")\n",
    "print(f\"테스트 정확도: {accuracy_lgb:.4f}\")\n",
    "print(f\"학습 시간: {train_time_lgb:.4f}초\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-19",
   "metadata": {},
   "source": [
    "### 3.2 num_leaves vs max_depth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-20",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\"\"\n",
    "num_leaves와 max_depth의 관계:\n",
    "- max_depth = d일 때, 최대 리프 수 = 2^d\n",
    "- num_leaves = 31이면 대략 max_depth = 5 수준\n",
    "- 과적합 방지: num_leaves < 2^max_depth\n",
    "\n",
    "권장 설정:\n",
    "- 대용량 데이터: num_leaves = 2^max_depth - 1 이하\n",
    "- 소규모 데이터: num_leaves를 작게 (15~31)\n",
    "\"\"\")\n",
    "\n",
    "# num_leaves에 따른 성능\n",
    "num_leaves_range = [15, 31, 63, 127, 255]\n",
    "train_scores_lgb = []\n",
    "test_scores_lgb = []\n",
    "\n",
    "for num_leaves in num_leaves_range:\n",
    "    lgb_temp = LGBMClassifier(\n",
    "        n_estimators=100,\n",
    "        num_leaves=num_leaves,\n",
    "        random_state=42,\n",
    "        verbose=-1\n",
    "    )\n",
    "    lgb_temp.fit(X_train, y_train)\n",
    "    train_scores_lgb.append(lgb_temp.score(X_train, y_train))\n",
    "    test_scores_lgb.append(lgb_temp.score(X_test, y_test))\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(num_leaves_range, train_scores_lgb, 'o-', label='Train')\n",
    "plt.plot(num_leaves_range, test_scores_lgb, 's-', label='Test')\n",
    "plt.xlabel('num_leaves')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.title('LightGBM: num_leaves Effect')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-21",
   "metadata": {},
   "source": [
    "### 3.3 특성 중요도"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-22",
   "metadata": {},
   "outputs": [],
   "source": [
    "# LightGBM 특성 중요도\n",
    "importance_lgb = pd.DataFrame({\n",
    "    'Feature': cancer.feature_names,\n",
    "    'Importance': lgb_clf.feature_importances_\n",
    "}).sort_values('Importance', ascending=True).tail(15)\n",
    "\n",
    "plt.figure(figsize=(10, 8))\n",
    "plt.barh(importance_lgb['Feature'], importance_lgb['Importance'])\n",
    "plt.xlabel('Importance')\n",
    "plt.title('LightGBM Feature Importance - Top 15')\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-23",
   "metadata": {},
   "source": [
    "## 4. CatBoost 개요"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-24",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\"\"\n",
    "CatBoost 특징:\n",
    "\n",
    "1. 범주형 특성 자동 처리:\n",
    "   - Target Encoding 자동 적용\n",
    "   - Ordered Target Statistics로 데이터 누수 방지\n",
    "\n",
    "2. Ordered Boosting:\n",
    "   - 학습 순서를 랜덤화하여 편향 감소\n",
    "   - 과적합 방지\n",
    "\n",
    "3. 대칭 트리:\n",
    "   - 같은 수준의 모든 노드가 동일한 분할 조건 사용\n",
    "   - 예측 속도 향상\n",
    "\n",
    "설치: pip install catboost\n",
    "\n",
    "기본 사용법:\n",
    "from catboost import CatBoostClassifier\n",
    "\n",
    "cat_clf = CatBoostClassifier(\n",
    "    iterations=100,\n",
    "    learning_rate=0.1,\n",
    "    depth=6,\n",
    "    l2_leaf_reg=3,\n",
    "    random_state=42,\n",
    "    verbose=False\n",
    ")\n",
    "\n",
    "cat_clf.fit(X_train, y_train)\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-25",
   "metadata": {},
   "source": [
    "## 5. 부스팅 알고리즘 비교"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-26",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "\n",
    "# 모델 정의\n",
    "models = {\n",
    "    'Gradient Boosting': GradientBoostingClassifier(n_estimators=100, random_state=42),\n",
    "    'XGBoost': XGBClassifier(n_estimators=100, random_state=42, eval_metric='logloss'),\n",
    "    'LightGBM': LGBMClassifier(n_estimators=100, random_state=42, verbose=-1)\n",
    "}\n",
    "\n",
    "# 비교\n",
    "print(\"부스팅 알고리즘 비교:\")\n",
    "print(\"-\" * 70)\n",
    "print(f\"{'모델':<20} {'훈련 정확도':>15} {'테스트 정확도':>15} {'학습시간(초)':>15}\")\n",
    "print(\"-\" * 70)\n",
    "\n",
    "results = {}\n",
    "for name, model in models.items():\n",
    "    start_time = time.time()\n",
    "    model.fit(X_train, y_train)\n",
    "    train_time = time.time() - start_time\n",
    "    \n",
    "    train_acc = model.score(X_train, y_train)\n",
    "    test_acc = model.score(X_test, y_test)\n",
    "    \n",
    "    results[name] = {\n",
    "        'train_accuracy': train_acc,\n",
    "        'test_accuracy': test_acc,\n",
    "        'time': train_time\n",
    "    }\n",
    "    \n",
    "    print(f\"{name:<20} {train_acc:>15.4f} {test_acc:>15.4f} {train_time:>15.4f}\")\n",
    "\n",
    "print(\"-\" * 70)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-27",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 시각화 비교\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# 정확도 비교\n",
    "names = list(results.keys())\n",
    "test_accuracies = [results[n]['test_accuracy'] for n in names]\n",
    "axes[0].barh(names, test_accuracies, color=['skyblue', 'salmon', 'lightgreen'])\n",
    "axes[0].set_xlabel('Test Accuracy')\n",
    "axes[0].set_title('Accuracy Comparison')\n",
    "axes[0].set_xlim([0.9, 1.0])\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "# 학습 시간 비교\n",
    "times = [results[n]['time'] for n in names]\n",
    "axes[1].barh(names, times, color=['skyblue', 'salmon', 'lightgreen'])\n",
    "axes[1].set_xlabel('Training Time (seconds)')\n",
    "axes[1].set_title('Training Time Comparison')\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-28",
   "metadata": {},
   "source": [
    "## 6. 회귀 예제"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-29",
   "metadata": {},
   "outputs": [],
   "source": [
    "# California Housing 데이터\n",
    "housing = fetch_california_housing()\n",
    "X_reg, y_reg = housing.data, housing.target\n",
    "\n",
    "X_train_reg, X_test_reg, y_train_reg, y_test_reg = train_test_split(\n",
    "    X_reg, y_reg, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "print(f\"데이터 크기: {X_reg.shape}\")\n",
    "print(f\"특성: {housing.feature_names}\")\n",
    "print(f\"타겟: Median house value (in $100,000s)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-30",
   "metadata": {},
   "outputs": [],
   "source": [
    "# XGBoost 회귀\n",
    "xgb_reg = XGBRegressor(\n",
    "    n_estimators=100,\n",
    "    learning_rate=0.1,\n",
    "    max_depth=6,\n",
    "    random_state=42\n",
    ")\n",
    "xgb_reg.fit(X_train_reg, y_train_reg)\n",
    "y_pred_xgb_reg = xgb_reg.predict(X_test_reg)\n",
    "\n",
    "# LightGBM 회귀\n",
    "lgb_reg = LGBMRegressor(\n",
    "    n_estimators=100,\n",
    "    learning_rate=0.1,\n",
    "    num_leaves=31,\n",
    "    random_state=42,\n",
    "    verbose=-1\n",
    ")\n",
    "lgb_reg.fit(X_train_reg, y_train_reg)\n",
    "y_pred_lgb_reg = lgb_reg.predict(X_test_reg)\n",
    "\n",
    "# 평가\n",
    "print(\"=== XGBoost 회귀 ===\")\n",
    "print(f\"R² Score: {r2_score(y_test_reg, y_pred_xgb_reg):.4f}\")\n",
    "print(f\"RMSE: {np.sqrt(mean_squared_error(y_test_reg, y_pred_xgb_reg)):.4f}\")\n",
    "\n",
    "print(\"\\n=== LightGBM 회귀 ===\")\n",
    "print(f\"R² Score: {r2_score(y_test_reg, y_pred_lgb_reg):.4f}\")\n",
    "print(f\"RMSE: {np.sqrt(mean_squared_error(y_test_reg, y_pred_lgb_reg)):.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-31",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 예측 vs 실제 시각화\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# XGBoost\n",
    "axes[0].scatter(y_test_reg, y_pred_xgb_reg, alpha=0.5)\n",
    "axes[0].plot([y_test_reg.min(), y_test_reg.max()], \n",
    "             [y_test_reg.min(), y_test_reg.max()], 'r--', lw=2)\n",
    "axes[0].set_xlabel('Actual')\n",
    "axes[0].set_ylabel('Predicted')\n",
    "axes[0].set_title(f'XGBoost (R²={r2_score(y_test_reg, y_pred_xgb_reg):.4f})')\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "# LightGBM\n",
    "axes[1].scatter(y_test_reg, y_pred_lgb_reg, alpha=0.5, color='green')\n",
    "axes[1].plot([y_test_reg.min(), y_test_reg.max()], \n",
    "             [y_test_reg.min(), y_test_reg.max()], 'r--', lw=2)\n",
    "axes[1].set_xlabel('Actual')\n",
    "axes[1].set_ylabel('Predicted')\n",
    "axes[1].set_title(f'LightGBM (R²={r2_score(y_test_reg, y_pred_lgb_reg):.4f})')\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-32",
   "metadata": {},
   "source": [
    "## 7. 하이퍼파라미터 가이드"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-33",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 하이퍼파라미터 비교표\n",
    "params_comparison = pd.DataFrame({\n",
    "    'Parameter': ['학습률', '트리 수', '깊이', '리프 수', 'L1 정규화', 'L2 정규화', '행 샘플링', '열 샘플링'],\n",
    "    'XGBoost': ['learning_rate', 'n_estimators', 'max_depth', '-', 'reg_alpha', 'reg_lambda', 'subsample', 'colsample_bytree'],\n",
    "    'LightGBM': ['learning_rate', 'n_estimators', 'max_depth', 'num_leaves', 'reg_alpha', 'reg_lambda', 'subsample', 'colsample_bytree'],\n",
    "    'Effect': ['낮으면 안정적', '많으면 정확', '깊으면 복잡', '많으면 복잡', '과적합 방지', '과적합 방지', '분산 감소', '다양성 증가']\n",
    "})\n",
    "\n",
    "print(\"하이퍼파라미터 가이드:\")\n",
    "print(params_comparison.to_string(index=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-34",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\"\"\n",
    "권장 튜닝 순서:\n",
    "\n",
    "1. 트리 구조 파라미터:\n",
    "   - max_depth, num_leaves\n",
    "   - min_child_weight, min_child_samples\n",
    "\n",
    "2. 샘플링 파라미터:\n",
    "   - subsample\n",
    "   - colsample_bytree\n",
    "\n",
    "3. 정규화 파라미터:\n",
    "   - reg_alpha, reg_lambda\n",
    "\n",
    "4. 학습률 조정:\n",
    "   - learning_rate 낮추고\n",
    "   - n_estimators 늘리기\n",
    "\n",
    "과적합 방지 전략:\n",
    "- 조기 종료 (early_stopping_rounds)\n",
    "- 정규화 (reg_alpha, reg_lambda)\n",
    "- 샘플링 (subsample, colsample_bytree)\n",
    "- 트리 제한 (max_depth, min_child_weight)\n",
    "- 학습률 낮추기 (learning_rate)\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-35",
   "metadata": {},
   "source": [
    "## 정리\n",
    "\n",
    "### 알고리즘 비교\n",
    "\n",
    "| 알고리즘 | 특징 | 장점 | 단점 |\n",
    "|----------|------|------|------|\n",
    "| Gradient Boosting | 잔차 학습 | 높은 정확도 | 느린 학습 |\n",
    "| XGBoost | 정규화 + 병렬화 | 빠름, 정확함 | 메모리 사용 |\n",
    "| LightGBM | Leaf-wise | 매우 빠름, 대용량 | 과적합 위험 |\n",
    "| CatBoost | 범주형 처리 | 튜닝 적게 필요 | 느린 시작 |\n",
    "\n",
    "### 선택 가이드\n",
    "\n",
    "- **작은 데이터 (<10K)**: XGBoost 또는 sklearn GradientBoosting\n",
    "- **중간 데이터 (10K-100K)**: XGBoost\n",
    "- **대용량 데이터 (>100K)**: LightGBM\n",
    "- **범주형 특성 많음**: CatBoost\n",
    "- **빠른 학습 필요**: LightGBM\n",
    "- **최고 정확도**: 모두 시도 후 앙상블\n",
    "\n",
    "### 주요 하이퍼파라미터\n",
    "\n",
    "**공통:**\n",
    "- `n_estimators`: 트리 개수\n",
    "- `learning_rate`: 학습률\n",
    "- `max_depth`: 트리 깊이\n",
    "\n",
    "**XGBoost 전용:**\n",
    "- `min_child_weight`: 리프 노드 최소 가중치\n",
    "- `gamma`: 분할 최소 손실 감소\n",
    "\n",
    "**LightGBM 전용:**\n",
    "- `num_leaves`: 리프 노드 최대 수\n",
    "- `min_child_samples`: 리프 노드 최소 샘플\n",
    "\n",
    "### 다음 단계\n",
    "- Stacking과 Blending\n",
    "- AutoML (Optuna, Hyperopt)\n",
    "- 실전 Kaggle 대회 참여"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
