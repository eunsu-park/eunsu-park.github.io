{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cell-0",
   "metadata": {},
   "source": [
    "# 09. 서포트 벡터 머신 (Support Vector Machine)\n",
    "\n",
    "## 학습 목표\n",
    "- SVM의 마진 최대화 원리 이해\n",
    "- 서포트 벡터의 역할 학습\n",
    "- 커널 트릭으로 비선형 문제 해결\n",
    "- 하이퍼파라미터 C와 gamma 튜닝\n",
    "- SVR로 회귀 문제 해결"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 라이브러리 임포트\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn import svm\n",
    "from sklearn.svm import SVC, SVR, LinearSVC\n",
    "from sklearn.datasets import (\n",
    "    make_blobs, make_classification, make_moons, make_circles,\n",
    "    load_iris, load_breast_cancer, load_diabetes\n",
    ")\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV, cross_val_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import accuracy_score, classification_report, mean_squared_error, r2_score\n",
    "\n",
    "# 한글 폰트 설정\n",
    "plt.rcParams['font.family'] = 'DejaVu Sans'\n",
    "plt.rcParams['axes.unicode_minus'] = False\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-2",
   "metadata": {},
   "source": [
    "## 1. 선형 SVM - 마진 최대화\n",
    "\n",
    "SVM의 핵심은 두 클래스를 분리하는 최적의 초평면(hyperplane)을 찾는 것입니다.\n",
    "마진(margin)을 최대화하여 일반화 성능을 높입니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 선형 분리 가능한 데이터 생성\n",
    "X, y = make_blobs(n_samples=100, centers=2, random_state=6)\n",
    "\n",
    "# 선형 SVM 학습\n",
    "clf = svm.SVC(kernel='linear', C=1000)\n",
    "clf.fit(X, y)\n",
    "\n",
    "print(f\"서포트 벡터 수: {len(clf.support_vectors_)}\")\n",
    "print(f\"가중치 (w): {clf.coef_}\")\n",
    "print(f\"절편 (b): {clf.intercept_}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 결정 경계와 마진 시각화\n",
    "plt.figure(figsize=(10, 8))\n",
    "\n",
    "# 데이터 포인트\n",
    "plt.scatter(X[:, 0], X[:, 1], c=y, cmap='coolwarm', s=100, edgecolors='black')\n",
    "\n",
    "# 결정 경계와 마진\n",
    "ax = plt.gca()\n",
    "xlim = ax.get_xlim()\n",
    "ylim = ax.get_ylim()\n",
    "\n",
    "# 그리드 생성\n",
    "xx = np.linspace(xlim[0], xlim[1], 30)\n",
    "yy = np.linspace(ylim[0], ylim[1], 30)\n",
    "YY, XX = np.meshgrid(yy, xx)\n",
    "xy = np.vstack([XX.ravel(), YY.ravel()]).T\n",
    "Z = clf.decision_function(xy).reshape(XX.shape)\n",
    "\n",
    "# 결정 경계와 마진 그리기\n",
    "ax.contour(XX, YY, Z, colors='k', levels=[-1, 0, 1],\n",
    "           linestyles=['--', '-', '--'], linewidths=[1, 2, 1])\n",
    "\n",
    "# 서포트 벡터 표시\n",
    "ax.scatter(clf.support_vectors_[:, 0], clf.support_vectors_[:, 1],\n",
    "           s=200, linewidth=2, facecolors='none', edgecolors='green',\n",
    "           label='Support Vectors')\n",
    "\n",
    "plt.xlabel('Feature 1')\n",
    "plt.ylabel('Feature 2')\n",
    "plt.title('Linear SVM: Maximum Margin Classifier')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-5",
   "metadata": {},
   "source": [
    "## 2. 소프트 마진 - C 파라미터\n",
    "\n",
    "실제 데이터는 완벽하게 선형 분리가 불가능합니다.\n",
    "C 파라미터로 오분류와 마진 크기의 균형을 조절합니다.\n",
    "\n",
    "- **C 큼**: 오분류 페널티 큼 → 좁은 마진, 과적합 위험\n",
    "- **C 작음**: 오분류 허용 → 넓은 마진, 일반화 향상"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 노이즈가 있는 데이터\n",
    "X, y = make_classification(\n",
    "    n_samples=200, n_features=2, n_redundant=0,\n",
    "    n_informative=2, n_clusters_per_class=1,\n",
    "    flip_y=0.1,  # 10% 노이즈\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "# 여러 C 값 비교\n",
    "fig, axes = plt.subplots(1, 3, figsize=(15, 5))\n",
    "C_values = [0.1, 1, 100]\n",
    "\n",
    "for ax, C in zip(axes, C_values):\n",
    "    clf = svm.SVC(kernel='linear', C=C)\n",
    "    clf.fit(X, y)\n",
    "\n",
    "    # 결정 경계\n",
    "    xlim = [X[:, 0].min() - 0.5, X[:, 0].max() + 0.5]\n",
    "    ylim = [X[:, 1].min() - 0.5, X[:, 1].max() + 0.5]\n",
    "    xx, yy = np.meshgrid(np.linspace(xlim[0], xlim[1], 100),\n",
    "                         np.linspace(ylim[0], ylim[1], 100))\n",
    "\n",
    "    Z = clf.decision_function(np.c_[xx.ravel(), yy.ravel()])\n",
    "    Z = Z.reshape(xx.shape)\n",
    "\n",
    "    ax.contourf(xx, yy, Z, alpha=0.3, cmap='coolwarm')\n",
    "    ax.contour(xx, yy, Z, colors='k', levels=[-1, 0, 1],\n",
    "               linestyles=['--', '-', '--'])\n",
    "    ax.scatter(X[:, 0], X[:, 1], c=y, cmap='coolwarm', edgecolors='black')\n",
    "    ax.scatter(clf.support_vectors_[:, 0], clf.support_vectors_[:, 1],\n",
    "               s=150, facecolors='none', edgecolors='green', linewidths=2)\n",
    "    ax.set_title(f'C = {C}\\nSupport Vectors: {len(clf.support_vectors_)}')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-7",
   "metadata": {},
   "source": [
    "## 3. 커널 트릭 - 비선형 분류\n",
    "\n",
    "커널 함수로 데이터를 고차원 공간에 매핑하여 비선형 패턴을 처리합니다.\n",
    "\n",
    "주요 커널:\n",
    "- **linear**: K(x, y) = x·y\n",
    "- **polynomial**: K(x, y) = (γ·x·y + r)^d\n",
    "- **rbf** (Gaussian): K(x, y) = exp(-γ||x - y||²)\n",
    "- **sigmoid**: K(x, y) = tanh(γ·x·y + r)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 비선형 데이터 생성\n",
    "X_moons, y_moons = make_moons(n_samples=200, noise=0.1, random_state=42)\n",
    "X_circles, y_circles = make_circles(n_samples=200, noise=0.1, factor=0.5, random_state=42)\n",
    "\n",
    "# 커널 비교\n",
    "kernels = ['linear', 'poly', 'rbf']\n",
    "\n",
    "fig, axes = plt.subplots(2, 3, figsize=(15, 10))\n",
    "\n",
    "for row, (X_data, y_data, name) in enumerate([(X_moons, y_moons, 'Moons'),\n",
    "                                                (X_circles, y_circles, 'Circles')]):\n",
    "    for col, kernel in enumerate(kernels):\n",
    "        ax = axes[row, col]\n",
    "\n",
    "        # SVM 학습\n",
    "        if kernel == 'poly':\n",
    "            clf = svm.SVC(kernel=kernel, degree=3, gamma='scale')\n",
    "        else:\n",
    "            clf = svm.SVC(kernel=kernel, gamma='scale')\n",
    "        clf.fit(X_data, y_data)\n",
    "\n",
    "        # 결정 경계\n",
    "        xlim = [X_data[:, 0].min() - 0.5, X_data[:, 0].max() + 0.5]\n",
    "        ylim = [X_data[:, 1].min() - 0.5, X_data[:, 1].max() + 0.5]\n",
    "        xx, yy = np.meshgrid(np.linspace(xlim[0], xlim[1], 100),\n",
    "                             np.linspace(ylim[0], ylim[1], 100))\n",
    "\n",
    "        Z = clf.predict(np.c_[xx.ravel(), yy.ravel()])\n",
    "        Z = Z.reshape(xx.shape)\n",
    "\n",
    "        ax.contourf(xx, yy, Z, alpha=0.3, cmap='coolwarm')\n",
    "        ax.scatter(X_data[:, 0], X_data[:, 1], c=y_data, cmap='coolwarm', edgecolors='black')\n",
    "        ax.set_title(f'{name} - {kernel}\\nAccuracy: {clf.score(X_data, y_data):.3f}')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-9",
   "metadata": {},
   "source": [
    "## 4. RBF 커널과 gamma 파라미터\n",
    "\n",
    "RBF 커널에서 gamma는 각 데이터 포인트의 영향 범위를 결정합니다.\n",
    "\n",
    "- **gamma 큼**: 영향 범위 좁음 → 복잡한 경계, 과적합 위험\n",
    "- **gamma 작음**: 영향 범위 넓음 → 단순한 경계, 과소적합 위험"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-10",
   "metadata": {},
   "outputs": [],
   "source": [
    "# gamma 효과 시각화\n",
    "fig, axes = plt.subplots(1, 4, figsize=(20, 5))\n",
    "gamma_values = [0.1, 1, 10, 100]\n",
    "\n",
    "X, y = make_moons(n_samples=200, noise=0.1, random_state=42)\n",
    "\n",
    "for ax, gamma in zip(axes, gamma_values):\n",
    "    clf = svm.SVC(kernel='rbf', gamma=gamma, C=1)\n",
    "    clf.fit(X, y)\n",
    "\n",
    "    xlim = [X[:, 0].min() - 0.5, X[:, 0].max() + 0.5]\n",
    "    ylim = [X[:, 1].min() - 0.5, X[:, 1].max() + 0.5]\n",
    "    xx, yy = np.meshgrid(np.linspace(xlim[0], xlim[1], 100),\n",
    "                         np.linspace(ylim[0], ylim[1], 100))\n",
    "\n",
    "    Z = clf.predict(np.c_[xx.ravel(), yy.ravel()])\n",
    "    Z = Z.reshape(xx.shape)\n",
    "\n",
    "    ax.contourf(xx, yy, Z, alpha=0.3, cmap='coolwarm')\n",
    "    ax.scatter(X[:, 0], X[:, 1], c=y, cmap='coolwarm', edgecolors='black')\n",
    "    ax.scatter(clf.support_vectors_[:, 0], clf.support_vectors_[:, 1],\n",
    "               s=100, facecolors='none', edgecolors='green', linewidths=2)\n",
    "    ax.set_title(f'gamma = {gamma}\\nSVs: {len(clf.support_vectors_)}')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-11",
   "metadata": {},
   "source": [
    "## 5. SVC - 실제 데이터 분류\n",
    "\n",
    "Iris 데이터셋으로 다중 클래스 분류를 수행합니다.\n",
    "**중요**: SVM은 특성 스케일에 민감하므로 스케일링이 필수입니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-12",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 데이터 로드\n",
    "iris = load_iris()\n",
    "X, y = iris.data, iris.target\n",
    "\n",
    "# 데이터 분할\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# 스케일링 (SVM은 스케일에 민감)\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# SVM 학습\n",
    "svm_clf = SVC(\n",
    "    C=1.0,\n",
    "    kernel='rbf',\n",
    "    gamma='scale',\n",
    "    probability=True,  # 확률 예측 활성화\n",
    "    random_state=42\n",
    ")\n",
    "svm_clf.fit(X_train_scaled, y_train)\n",
    "\n",
    "# 예측\n",
    "y_pred = svm_clf.predict(X_test_scaled)\n",
    "\n",
    "print(\"SVM 분류 결과:\")\n",
    "print(f\"  정확도: {accuracy_score(y_test, y_pred):.4f}\")\n",
    "print(f\"  서포트 벡터 수: {len(svm_clf.support_vectors_)}\")\n",
    "print(\"\\n분류 리포트:\")\n",
    "print(classification_report(y_test, y_pred, target_names=iris.target_names))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-13",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 확률 예측\n",
    "y_proba = svm_clf.predict_proba(X_test_scaled[:5])\n",
    "\n",
    "print(\"확률 예측 (처음 5개):\")\n",
    "print(f\"클래스: {iris.target_names}\")\n",
    "print(y_proba)\n",
    "print(f\"\\n예측 클래스: {y_pred[:5]}\")\n",
    "print(f\"실제 클래스: {y_test[:5]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-14",
   "metadata": {},
   "source": [
    "## 6. 스케일링의 중요성\n",
    "\n",
    "SVM은 거리 기반 알고리즘이므로 특성 스케일이 다르면 성능이 저하됩니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-15",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 스케일링 효과 비교\n",
    "cancer = load_breast_cancer()\n",
    "X_train_c, X_test_c, y_train_c, y_test_c = train_test_split(\n",
    "    cancer.data, cancer.target, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "# 스케일링 없이\n",
    "svm_no_scale = SVC(kernel='rbf', C=1, gamma='scale')\n",
    "svm_no_scale.fit(X_train_c, y_train_c)\n",
    "acc_no_scale = svm_no_scale.score(X_test_c, y_test_c)\n",
    "\n",
    "# 스케일링 후\n",
    "scaler = StandardScaler()\n",
    "X_train_c_scaled = scaler.fit_transform(X_train_c)\n",
    "X_test_c_scaled = scaler.transform(X_test_c)\n",
    "\n",
    "svm_scaled = SVC(kernel='rbf', C=1, gamma='scale')\n",
    "svm_scaled.fit(X_train_c_scaled, y_train_c)\n",
    "acc_scaled = svm_scaled.score(X_test_c_scaled, y_test_c)\n",
    "\n",
    "print(\"스케일링 효과:\")\n",
    "print(f\"  스케일링 없이: {acc_no_scale:.4f}\")\n",
    "print(f\"  스케일링 후:   {acc_scaled:.4f}\")\n",
    "print(f\"  성능 향상:     {(acc_scaled - acc_no_scale) * 100:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-16",
   "metadata": {},
   "source": [
    "## 7. 하이퍼파라미터 튜닝 - Grid Search\n",
    "\n",
    "C와 gamma를 동시에 튜닝하여 최적 조합을 찾습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-17",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 파라미터 그리드\n",
    "param_grid = {\n",
    "    'C': [0.1, 1, 10, 100],\n",
    "    'gamma': ['scale', 'auto', 0.01, 0.1, 1],\n",
    "    'kernel': ['rbf', 'poly']\n",
    "}\n",
    "\n",
    "# Grid Search\n",
    "grid_search = GridSearchCV(\n",
    "    SVC(random_state=42),\n",
    "    param_grid,\n",
    "    cv=5,\n",
    "    scoring='accuracy',\n",
    "    n_jobs=-1,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "grid_search.fit(X_train_scaled, y_train)\n",
    "\n",
    "print(\"\\nGrid Search 결과:\")\n",
    "print(f\"  최적 파라미터: {grid_search.best_params_}\")\n",
    "print(f\"  최적 CV 점수: {grid_search.best_score_:.4f}\")\n",
    "print(f\"  테스트 점수: {grid_search.score(X_test_scaled, y_test):.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-18",
   "metadata": {},
   "outputs": [],
   "source": [
    "# C와 gamma 동시 튜닝 시각화 (RBF 커널만)\n",
    "C_range = np.logspace(-2, 2, 5)\n",
    "gamma_range = np.logspace(-3, 1, 5)\n",
    "\n",
    "# 점수 계산\n",
    "scores = np.zeros((len(C_range), len(gamma_range)))\n",
    "\n",
    "for i, C in enumerate(C_range):\n",
    "    for j, gamma in enumerate(gamma_range):\n",
    "        svm_clf = SVC(C=C, gamma=gamma, kernel='rbf')\n",
    "        svm_clf.fit(X_train_c_scaled, y_train_c)\n",
    "        scores[i, j] = svm_clf.score(X_test_c_scaled, y_test_c)\n",
    "\n",
    "# 히트맵 시각화\n",
    "plt.figure(figsize=(10, 8))\n",
    "plt.imshow(scores, interpolation='nearest', cmap='viridis')\n",
    "plt.xlabel('gamma')\n",
    "plt.ylabel('C')\n",
    "plt.colorbar(label='Accuracy')\n",
    "plt.xticks(np.arange(len(gamma_range)), [f'{g:.3f}' for g in gamma_range])\n",
    "plt.yticks(np.arange(len(C_range)), [f'{c:.2f}' for c in C_range])\n",
    "plt.title('SVM Hyperparameter Tuning (RBF Kernel)')\n",
    "\n",
    "# 최적점 표시\n",
    "best_i, best_j = np.unravel_index(scores.argmax(), scores.shape)\n",
    "plt.scatter(best_j, best_i, marker='*', s=300, c='red', edgecolors='white')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"최적 C: {C_range[best_i]:.2f}\")\n",
    "print(f\"최적 gamma: {gamma_range[best_j]:.3f}\")\n",
    "print(f\"최고 정확도: {scores.max():.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-19",
   "metadata": {},
   "source": [
    "## 8. SVR - Support Vector Regression\n",
    "\n",
    "SVM을 회귀 문제에 적용합니다.\n",
    "epsilon-tube 내의 오차는 무시하고, 튜브 밖의 오차만 페널티를 줍니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-20",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 데이터 로드\n",
    "diabetes = load_diabetes()\n",
    "X_train_d, X_test_d, y_train_d, y_test_d = train_test_split(\n",
    "    diabetes.data, diabetes.target, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "# 스케일링\n",
    "scaler = StandardScaler()\n",
    "X_train_d_scaled = scaler.fit_transform(X_train_d)\n",
    "X_test_d_scaled = scaler.transform(X_test_d)\n",
    "\n",
    "# SVR 학습\n",
    "svr = SVR(\n",
    "    kernel='rbf',\n",
    "    C=100,\n",
    "    epsilon=0.1,  # 튜브 폭: 이 안의 오차는 무시\n",
    "    gamma='scale'\n",
    ")\n",
    "svr.fit(X_train_d_scaled, y_train_d)\n",
    "\n",
    "# 예측\n",
    "y_pred_d = svr.predict(X_test_d_scaled)\n",
    "\n",
    "print(\"SVR 회귀 결과:\")\n",
    "print(f\"  MSE: {mean_squared_error(y_test_d, y_pred_d):.4f}\")\n",
    "print(f\"  RMSE: {np.sqrt(mean_squared_error(y_test_d, y_pred_d)):.4f}\")\n",
    "print(f\"  R²: {r2_score(y_test_d, y_pred_d):.4f}\")\n",
    "print(f\"  서포트 벡터 수: {len(svr.support_vectors_)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-21",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 시각화\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.scatter(y_test_d, y_pred_d, alpha=0.7, edgecolors='black')\n",
    "plt.plot([y_test_d.min(), y_test_d.max()], [y_test_d.min(), y_test_d.max()], 'r--', lw=2)\n",
    "plt.xlabel('Actual')\n",
    "plt.ylabel('Predicted')\n",
    "plt.title(f'SVR Regression (R² = {r2_score(y_test_d, y_pred_d):.4f})')\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-22",
   "metadata": {},
   "source": [
    "## 9. 다중 클래스 분류 전략\n",
    "\n",
    "SVM은 이진 분류기이므로 다중 클래스는 다음 전략으로 처리합니다.\n",
    "\n",
    "- **OvO (One-vs-One)**: k(k-1)/2 개 분류기, SVC 기본값\n",
    "- **OvR (One-vs-Rest)**: k 개 분류기, LinearSVC 기본값"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-23",
   "metadata": {},
   "outputs": [],
   "source": [
    "# OvO (기본)\n",
    "svm_ovo = SVC(kernel='rbf', decision_function_shape='ovo')\n",
    "svm_ovo.fit(X_train_scaled, y_train)\n",
    "print(f\"OvO 정확도: {svm_ovo.score(X_test_scaled, y_test):.4f}\")\n",
    "\n",
    "# OvR\n",
    "svm_ovr = SVC(kernel='rbf', decision_function_shape='ovr')\n",
    "svm_ovr.fit(X_train_scaled, y_train)\n",
    "print(f\"OvR 정확도: {svm_ovr.score(X_test_scaled, y_test):.4f}\")\n",
    "\n",
    "# LinearSVC (OvR 기본)\n",
    "linear_svc = LinearSVC(dual=True, max_iter=10000)\n",
    "linear_svc.fit(X_train_scaled, y_train)\n",
    "print(f\"LinearSVC 정확도: {linear_svc.score(X_test_scaled, y_test):.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-24",
   "metadata": {},
   "source": [
    "## 10. 커널 비교 - 실전\n",
    "\n",
    "유방암 데이터로 여러 커널의 성능을 비교합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-25",
   "metadata": {},
   "outputs": [],
   "source": [
    "kernels = ['linear', 'poly', 'rbf', 'sigmoid']\n",
    "\n",
    "print(\"커널별 성능 비교 (Breast Cancer):\")\n",
    "print(\"-\" * 50)\n",
    "\n",
    "for kernel in kernels:\n",
    "    if kernel == 'poly':\n",
    "        svm_model = SVC(kernel=kernel, degree=3, gamma='scale')\n",
    "    else:\n",
    "        svm_model = SVC(kernel=kernel, gamma='scale')\n",
    "\n",
    "    svm_model.fit(X_train_c_scaled, y_train_c)\n",
    "    acc = svm_model.score(X_test_c_scaled, y_test_c)\n",
    "    print(f\"  {kernel:8s}: {acc:.4f} (SVs: {len(svm_model.support_vectors_)})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-26",
   "metadata": {},
   "source": [
    "## 정리\n",
    "\n",
    "### 핵심 개념\n",
    "\n",
    "| 개념 | 설명 |\n",
    "|------|------|\n",
    "| **서포트 벡터** | 마진 경계에 위치한 핵심 데이터 포인트 |\n",
    "| **마진** | 결정 경계와 서포트 벡터 사이의 거리 |\n",
    "| **C** | 규제 파라미터 (큼: 좁은 마진, 작음: 넓은 마진) |\n",
    "| **gamma** | RBF 커널 범위 (큼: 좁은 영향, 작음: 넓은 영향) |\n",
    "| **커널** | 데이터를 고차원으로 매핑하는 함수 |\n",
    "\n",
    "### SVM 사용 체크리스트\n",
    "\n",
    "1. ✅ **스케일링 필수**: StandardScaler 또는 MinMaxScaler 적용\n",
    "2. ✅ **커널 선택**: 선형 분리 가능 → linear, 비선형 → rbf\n",
    "3. ✅ **파라미터 튜닝**: C와 gamma를 GridSearchCV로 튜닝\n",
    "4. ✅ **대용량 데이터**: LinearSVC 또는 SGDClassifier 사용\n",
    "5. ✅ **확률 필요시**: probability=True 설정 (추가 비용 발생)\n",
    "\n",
    "### 장단점\n",
    "\n",
    "**장점**:\n",
    "- 고차원 데이터에 효과적\n",
    "- 메모리 효율적 (서포트 벡터만 저장)\n",
    "- 다양한 커널로 비선형 문제 해결\n",
    "\n",
    "**단점**:\n",
    "- 대용량 데이터에 느림 (O(n²) ~ O(n³))\n",
    "- 스케일링 필수\n",
    "- 파라미터 튜닝 필요\n",
    "\n",
    "### 다음 단계\n",
    "- k-Nearest Neighbors (kNN)\n",
    "- Naive Bayes\n",
    "- Ensemble methods"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
