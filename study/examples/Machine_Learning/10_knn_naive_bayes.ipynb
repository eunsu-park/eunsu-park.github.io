{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cell-0",
   "metadata": {},
   "source": [
    "# 10. k-최근접 이웃(kNN)과 나이브 베이즈\n",
    "\n",
    "## 학습 목표\n",
    "- kNN의 거리 기반 분류 원리 이해\n",
    "- 거리 메트릭 (Euclidean, Manhattan, Minkowski) 학습\n",
    "- 최적 k값 선택 방법 습득\n",
    "- 나이브 베이즈의 확률 기반 분류 이해\n",
    "- Gaussian, Multinomial, Bernoulli NB 비교\n",
    "- 텍스트 분류 적용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 라이브러리 임포트\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.neighbors import KNeighborsClassifier, KNeighborsRegressor\n",
    "from sklearn.naive_bayes import GaussianNB, MultinomialNB, BernoulliNB\n",
    "from sklearn.datasets import (\n",
    "    load_iris, load_breast_cancer, load_diabetes, load_digits,\n",
    "    make_classification, fetch_20newsgroups\n",
    ")\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import accuracy_score, classification_report, mean_squared_error, r2_score\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from scipy.spatial.distance import euclidean, cityblock, minkowski, chebyshev\n",
    "from time import time\n",
    "\n",
    "# 한글 폰트 설정\n",
    "plt.rcParams['font.family'] = 'DejaVu Sans'\n",
    "plt.rcParams['axes.unicode_minus'] = False\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-2",
   "metadata": {},
   "source": [
    "## 1. k-최근접 이웃 (kNN) 개념\n",
    "\n",
    "kNN은 게으른 학습(Lazy Learning) 알고리즘입니다.\n",
    "\n",
    "**동작 원리**:\n",
    "1. 새로운 데이터가 들어오면\n",
    "2. 학습 데이터에서 가장 가까운 k개의 이웃을 찾음\n",
    "3. k개 이웃의 다수결(분류) 또는 평균(회귀)으로 예측\n",
    "\n",
    "**특징**:\n",
    "- 학습 시 모델 생성 없음 (모든 데이터 저장)\n",
    "- 비모수적 방법 (데이터 분포 가정 불필요)\n",
    "- 예측 시간이 느림"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2D 데이터로 kNN 시각화\n",
    "X, y = make_classification(\n",
    "    n_samples=100, n_features=2, n_redundant=0,\n",
    "    n_informative=2, n_clusters_per_class=1, random_state=42\n",
    ")\n",
    "\n",
    "# 여러 k값 비교\n",
    "fig, axes = plt.subplots(1, 3, figsize=(15, 5))\n",
    "k_values = [1, 5, 15]\n",
    "\n",
    "for ax, k in zip(axes, k_values):\n",
    "    knn = KNeighborsClassifier(n_neighbors=k)\n",
    "    knn.fit(X, y)\n",
    "\n",
    "    # 결정 경계\n",
    "    x_min, x_max = X[:, 0].min() - 0.5, X[:, 0].max() + 0.5\n",
    "    y_min, y_max = X[:, 1].min() - 0.5, X[:, 1].max() + 0.5\n",
    "    xx, yy = np.meshgrid(np.linspace(x_min, x_max, 100),\n",
    "                         np.linspace(y_min, y_max, 100))\n",
    "\n",
    "    Z = knn.predict(np.c_[xx.ravel(), yy.ravel()])\n",
    "    Z = Z.reshape(xx.shape)\n",
    "\n",
    "    ax.contourf(xx, yy, Z, alpha=0.3, cmap='coolwarm')\n",
    "    ax.scatter(X[:, 0], X[:, 1], c=y, cmap='coolwarm', edgecolors='black')\n",
    "    ax.set_title(f'k = {k}\\nAccuracy = {knn.score(X, y):.3f}')\n",
    "    ax.set_xlabel('Feature 1')\n",
    "    ax.set_ylabel('Feature 2')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-4",
   "metadata": {},
   "source": [
    "## 2. kNN 기본 사용법"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 데이터 로드\n",
    "iris = load_iris()\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    iris.data, iris.target, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "# kNN 분류기\n",
    "knn = KNeighborsClassifier(\n",
    "    n_neighbors=5,           # k값\n",
    "    weights='uniform',       # 가중치: 'uniform' 또는 'distance'\n",
    "    algorithm='auto',        # 알고리즘: 'auto', 'ball_tree', 'kd_tree', 'brute'\n",
    "    metric='minkowski',      # 거리 측정: 'euclidean', 'manhattan', 'minkowski'\n",
    "    p=2                      # minkowski p값 (2=euclidean, 1=manhattan)\n",
    ")\n",
    "\n",
    "knn.fit(X_train, y_train)\n",
    "y_pred = knn.predict(X_test)\n",
    "\n",
    "print(\"kNN 분류 결과:\")\n",
    "print(f\"  정확도: {accuracy_score(y_test, y_pred):.4f}\")\n",
    "print(\"\\n분류 리포트:\")\n",
    "print(classification_report(y_test, y_pred, target_names=iris.target_names))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-6",
   "metadata": {},
   "source": [
    "## 3. 거리 측정 방법\n",
    "\n",
    "kNN의 핵심은 거리 계산입니다.\n",
    "\n",
    "주요 거리 메트릭:\n",
    "- **유클리드 (Euclidean, L2)**: d = √Σ(xi - yi)²\n",
    "- **맨해튼 (Manhattan, L1)**: d = Σ|xi - yi|\n",
    "- **민코프스키 (Minkowski)**: d = (Σ|xi - yi|^p)^(1/p)\n",
    "- **체비셰프 (Chebyshev, L∞)**: d = max(|xi - yi|)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 거리 측정 예시\n",
    "point1 = np.array([1, 2, 3])\n",
    "point2 = np.array([4, 5, 6])\n",
    "\n",
    "print(\"거리 측정 예시:\")\n",
    "print(f\"  Point 1: {point1}\")\n",
    "print(f\"  Point 2: {point2}\")\n",
    "print()\n",
    "print(f\"  유클리드 거리:     {euclidean(point1, point2):.4f}\")\n",
    "print(f\"  맨해튼 거리:       {cityblock(point1, point2):.4f}\")\n",
    "print(f\"  민코프스키 (p=3):  {minkowski(point1, point2, p=3):.4f}\")\n",
    "print(f\"  체비셰프 거리:     {chebyshev(point1, point2):.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 거리 메트릭별 성능 비교\n",
    "metrics = ['euclidean', 'manhattan', 'chebyshev']\n",
    "\n",
    "print(\"거리 메트릭별 성능 (Iris):\")\n",
    "print(\"-\" * 40)\n",
    "for metric in metrics:\n",
    "    knn = KNeighborsClassifier(n_neighbors=5, metric=metric)\n",
    "    knn.fit(X_train, y_train)\n",
    "    acc = knn.score(X_test, y_test)\n",
    "    print(f\"  {metric:12s}: {acc:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-9",
   "metadata": {},
   "source": [
    "## 4. 최적 k값 선택\n",
    "\n",
    "k값이 너무 작으면 과적합, 너무 크면 과소적합이 발생합니다.\n",
    "교차 검증으로 최적 k를 찾습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-10",
   "metadata": {},
   "outputs": [],
   "source": [
    "# k값에 따른 성능 변화\n",
    "k_range = range(1, 31)\n",
    "train_scores = []\n",
    "test_scores = []\n",
    "\n",
    "for k in k_range:\n",
    "    knn = KNeighborsClassifier(n_neighbors=k)\n",
    "    knn.fit(X_train, y_train)\n",
    "    train_scores.append(knn.score(X_train, y_train))\n",
    "    test_scores.append(knn.score(X_test, y_test))\n",
    "\n",
    "# 시각화\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(k_range, train_scores, 'o-', label='Train')\n",
    "plt.plot(k_range, test_scores, 's-', label='Test')\n",
    "plt.xlabel('k (Number of Neighbors)')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.title('kNN: k vs Accuracy')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.xticks(k_range[::2])\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# 최적 k 찾기\n",
    "best_k = k_range[np.argmax(test_scores)]\n",
    "print(f\"최적 k: {best_k}\")\n",
    "print(f\"최고 테스트 정확도: {max(test_scores):.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-11",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 교차 검증으로 k 선택\n",
    "k_range = range(1, 31)\n",
    "cv_scores = []\n",
    "\n",
    "for k in k_range:\n",
    "    knn = KNeighborsClassifier(n_neighbors=k)\n",
    "    scores = cross_val_score(knn, X_train, y_train, cv=5, scoring='accuracy')\n",
    "    cv_scores.append(scores.mean())\n",
    "\n",
    "# 시각화\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(k_range, cv_scores, 'o-', color='green')\n",
    "plt.xlabel('k')\n",
    "plt.ylabel('Cross-Validation Accuracy')\n",
    "plt.title('kNN: k Selection with 5-Fold Cross-Validation')\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.xticks(k_range[::2])\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "best_k_cv = k_range[np.argmax(cv_scores)]\n",
    "print(f\"교차 검증 최적 k: {best_k_cv}\")\n",
    "print(f\"최고 CV 정확도: {max(cv_scores):.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-12",
   "metadata": {},
   "source": [
    "## 5. 가중 kNN (Weighted kNN)\n",
    "\n",
    "거리에 따라 이웃의 가중치를 조절합니다.\n",
    "\n",
    "- **uniform**: 모든 이웃에 동일한 가중치\n",
    "- **distance**: 가까운 이웃에 더 큰 가중치 (weight = 1/distance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-13",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 가중치 방식 비교\n",
    "weights = ['uniform', 'distance']\n",
    "\n",
    "print(\"가중치 방식 비교:\")\n",
    "print(\"-\" * 40)\n",
    "for weight in weights:\n",
    "    knn = KNeighborsClassifier(n_neighbors=5, weights=weight)\n",
    "    knn.fit(X_train, y_train)\n",
    "    acc = knn.score(X_test, y_test)\n",
    "    print(f\"  {weight:10s}: {acc:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-14",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 거리 가중 kNN 시각화\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "for ax, weight in zip(axes, weights):\n",
    "    knn = KNeighborsClassifier(n_neighbors=15, weights=weight)\n",
    "    knn.fit(X[:, :2], y)\n",
    "\n",
    "    x_min, x_max = X[:, 0].min() - 0.5, X[:, 0].max() + 0.5\n",
    "    y_min, y_max = X[:, 1].min() - 0.5, X[:, 1].max() + 0.5\n",
    "    xx, yy = np.meshgrid(np.linspace(x_min, x_max, 100),\n",
    "                         np.linspace(y_min, y_max, 100))\n",
    "\n",
    "    Z = knn.predict(np.c_[xx.ravel(), yy.ravel()])\n",
    "    Z = Z.reshape(xx.shape)\n",
    "\n",
    "    ax.contourf(xx, yy, Z, alpha=0.3, cmap='coolwarm')\n",
    "    ax.scatter(X[:, 0], X[:, 1], c=y, cmap='coolwarm', edgecolors='black')\n",
    "    ax.set_title(f'weights = {weight}')\n",
    "    ax.set_xlabel('Feature 1')\n",
    "    ax.set_ylabel('Feature 2')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-15",
   "metadata": {},
   "source": [
    "## 6. kNN 회귀\n",
    "\n",
    "kNN은 회귀 문제에도 사용할 수 있습니다.\n",
    "k개 이웃의 평균으로 예측합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-16",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 데이터 로드\n",
    "diabetes = load_diabetes()\n",
    "X_train_d, X_test_d, y_train_d, y_test_d = train_test_split(\n",
    "    diabetes.data, diabetes.target, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "# 스케일링 (kNN은 거리 기반이므로 필수)\n",
    "scaler = StandardScaler()\n",
    "X_train_d_scaled = scaler.fit_transform(X_train_d)\n",
    "X_test_d_scaled = scaler.transform(X_test_d)\n",
    "\n",
    "# kNN 회귀\n",
    "knn_reg = KNeighborsRegressor(n_neighbors=5, weights='distance')\n",
    "knn_reg.fit(X_train_d_scaled, y_train_d)\n",
    "y_pred_d = knn_reg.predict(X_test_d_scaled)\n",
    "\n",
    "print(\"kNN 회귀 결과:\")\n",
    "print(f\"  MSE: {mean_squared_error(y_test_d, y_pred_d):.4f}\")\n",
    "print(f\"  RMSE: {np.sqrt(mean_squared_error(y_test_d, y_pred_d)):.4f}\")\n",
    "print(f\"  R²: {r2_score(y_test_d, y_pred_d):.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-17",
   "metadata": {},
   "source": [
    "## 7. kNN 알고리즘 비교\n",
    "\n",
    "대용량 데이터에서는 탐색 알고리즘 선택이 중요합니다.\n",
    "\n",
    "- **brute**: 전수 탐색 (O(n))\n",
    "- **kd_tree**: KD-Tree 사용 (저차원에 효율적)\n",
    "- **ball_tree**: Ball-Tree 사용 (고차원에 효율적)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-18",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 알고리즘별 시간 비교\n",
    "algorithms = ['brute', 'kd_tree', 'ball_tree']\n",
    "\n",
    "print(\"알고리즘별 시간 비교:\")\n",
    "print(\"-\" * 60)\n",
    "for algo in algorithms:\n",
    "    knn = KNeighborsClassifier(n_neighbors=5, algorithm=algo)\n",
    "\n",
    "    # 학습 시간\n",
    "    start = time()\n",
    "    knn.fit(X_train, y_train)\n",
    "    fit_time = time() - start\n",
    "\n",
    "    # 예측 시간\n",
    "    start = time()\n",
    "    knn.predict(X_test)\n",
    "    pred_time = time() - start\n",
    "\n",
    "    print(f\"  {algo:10s}: fit={fit_time:.4f}s, predict={pred_time:.4f}s\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-19",
   "metadata": {},
   "source": [
    "## 8. 나이브 베이즈 (Naive Bayes)\n",
    "\n",
    "### 베이즈 정리\n",
    "\n",
    "**P(y|X) = P(X|y) × P(y) / P(X)**\n",
    "\n",
    "- P(y|X): 사후 확률 (특성이 주어졌을 때 클래스 확률)\n",
    "- P(X|y): 우도 (클래스가 주어졌을 때 특성 확률)\n",
    "- P(y): 사전 확률 (클래스의 기본 확률)\n",
    "- P(X): 증거 (특성의 확률)\n",
    "\n",
    "### 나이브 가정\n",
    "\n",
    "모든 특성이 서로 독립적이라고 가정:\n",
    "**P(X|y) = P(x₁|y) × P(x₂|y) × ... × P(xₙ|y)**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-20",
   "metadata": {},
   "source": [
    "## 9. 가우시안 나이브 베이즈\n",
    "\n",
    "연속형 특성이 가우시안(정규) 분포를 따른다고 가정합니다.\n",
    "**P(xi|y) = N(xi; μy, σy)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-21",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 가우시안 나이브 베이즈\n",
    "gnb = GaussianNB()\n",
    "gnb.fit(X_train, y_train)\n",
    "y_pred_nb = gnb.predict(X_test)\n",
    "\n",
    "print(\"가우시안 나이브 베이즈 결과:\")\n",
    "print(f\"  정확도: {accuracy_score(y_test, y_pred_nb):.4f}\")\n",
    "\n",
    "# 학습된 파라미터 확인\n",
    "print(f\"\\n클래스 사전 확률: {gnb.class_prior_}\")\n",
    "print(f\"\\n클래스별 평균 (처음 2개 특성):\")\n",
    "print(gnb.theta_[:, :2])\n",
    "print(f\"\\n클래스별 분산 (처음 2개 특성):\")\n",
    "print(gnb.var_[:, :2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-22",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 확률 예측\n",
    "y_proba = gnb.predict_proba(X_test[:5])\n",
    "\n",
    "print(\"확률 예측 (처음 5개):\")\n",
    "print(f\"클래스: {iris.target_names}\")\n",
    "print(y_proba)\n",
    "print(f\"\\n예측 클래스: {gnb.predict(X_test[:5])}\")\n",
    "print(f\"실제 클래스: {y_test[:5]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-23",
   "metadata": {},
   "source": [
    "## 10. 다항 나이브 베이즈 - 텍스트 분류\n",
    "\n",
    "이산형/카운트 특성에 사용하며, 주로 텍스트 분류(단어 빈도)에 활용됩니다.\n",
    "\n",
    "**P(xi|y) = (Nyi + α) / (Ny + αn)**\n",
    "\n",
    "- α: Laplace smoothing 파라미터 (Zero frequency 문제 해결)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-24",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 뉴스 데이터 로드\n",
    "categories = ['sci.space', 'rec.sport.baseball', 'talk.politics.misc']\n",
    "newsgroups = fetch_20newsgroups(\n",
    "    subset='train',\n",
    "    categories=categories,\n",
    "    remove=('headers', 'footers', 'quotes'),\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "print(f\"뉴스 데이터: {len(newsgroups.data)} 기사\")\n",
    "print(f\"카테고리: {categories}\")\n",
    "print(f\"\\n첫 번째 기사 (일부):\\n{newsgroups.data[0][:200]}...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-25",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 텍스트 벡터화\n",
    "vectorizer = CountVectorizer(max_features=5000, stop_words='english')\n",
    "X_news = vectorizer.fit_transform(newsgroups.data)\n",
    "y_news = newsgroups.target\n",
    "\n",
    "print(f\"벡터 크기: {X_news.shape}\")\n",
    "print(f\"특성 수: {len(vectorizer.get_feature_names_out())}\")\n",
    "\n",
    "# 학습/테스트 분할\n",
    "X_train_news, X_test_news, y_train_news, y_test_news = train_test_split(\n",
    "    X_news, y_news, test_size=0.2, random_state=42\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-26",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 다항 나이브 베이즈\n",
    "mnb = MultinomialNB(alpha=1.0)  # alpha: Laplace smoothing\n",
    "mnb.fit(X_train_news, y_train_news)\n",
    "\n",
    "y_pred_news = mnb.predict(X_test_news)\n",
    "\n",
    "print(\"다항 나이브 베이즈 (텍스트 분류) 결과:\")\n",
    "print(f\"  정확도: {mnb.score(X_test_news, y_test_news):.4f}\")\n",
    "print(\"\\n분류 리포트:\")\n",
    "print(classification_report(y_test_news, y_pred_news, target_names=categories))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-27",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 각 클래스의 가장 중요한 단어\n",
    "feature_names = vectorizer.get_feature_names_out()\n",
    "\n",
    "print(\"각 클래스별 상위 10개 단어:\")\n",
    "print(\"=\" * 60)\n",
    "for i, category in enumerate(categories):\n",
    "    top_indices = mnb.feature_log_prob_[i].argsort()[-10:][::-1]\n",
    "    top_words = [feature_names[idx] for idx in top_indices]\n",
    "    print(f\"\\n{category}:\")\n",
    "    print(f\"  {', '.join(top_words)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-28",
   "metadata": {},
   "source": [
    "## 11. 베르누이 나이브 베이즈\n",
    "\n",
    "이진 특성(0/1)에 사용하며, 단어의 존재 여부로 텍스트를 분류합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-29",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 이진 벡터화 (단어 존재 여부만)\n",
    "binary_vectorizer = CountVectorizer(max_features=5000, binary=True, stop_words='english')\n",
    "X_binary = binary_vectorizer.fit_transform(newsgroups.data)\n",
    "\n",
    "X_train_bin, X_test_bin, y_train_bin, y_test_bin = train_test_split(\n",
    "    X_binary, y_news, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "# 베르누이 나이브 베이즈\n",
    "bnb = BernoulliNB(alpha=1.0)\n",
    "bnb.fit(X_train_bin, y_train_bin)\n",
    "\n",
    "print(\"베르누이 나이브 베이즈 결과:\")\n",
    "print(f\"  정확도: {bnb.score(X_test_bin, y_test_bin):.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-30",
   "metadata": {},
   "source": [
    "## 12. 나이브 베이즈 모델 비교"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-31",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 숫자 이미지 데이터\n",
    "digits = load_digits()\n",
    "X_train_dig, X_test_dig, y_train_dig, y_test_dig = train_test_split(\n",
    "    digits.data, digits.target, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "# 세 가지 나이브 베이즈 비교\n",
    "models = {\n",
    "    'Gaussian NB': GaussianNB(),\n",
    "    'Multinomial NB': MultinomialNB(),\n",
    "    'Bernoulli NB': BernoulliNB()\n",
    "}\n",
    "\n",
    "print(\"나이브 베이즈 모델 비교 (Digits):\")\n",
    "print(\"-\" * 50)\n",
    "for name, model in models.items():\n",
    "    model.fit(X_train_dig, y_train_dig)\n",
    "    acc = model.score(X_test_dig, y_test_dig)\n",
    "    print(f\"  {name:18s}: {acc:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-32",
   "metadata": {},
   "source": [
    "## 13. 온라인 학습 (Incremental Learning)\n",
    "\n",
    "나이브 베이즈는 `partial_fit`으로 온라인 학습이 가능합니다.\n",
    "대용량 데이터나 스트리밍 데이터에 유용합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-33",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 온라인 학습 시뮬레이션\n",
    "gnb_online = GaussianNB()\n",
    "\n",
    "# 배치 학습\n",
    "batch_size = 50\n",
    "n_batches = len(X_train) // batch_size\n",
    "\n",
    "for i in range(n_batches):\n",
    "    start = i * batch_size\n",
    "    end = start + batch_size\n",
    "    X_batch = X_train[start:end]\n",
    "    y_batch = y_train[start:end]\n",
    "\n",
    "    # 첫 배치에서 클래스 정의\n",
    "    if i == 0:\n",
    "        gnb_online.partial_fit(X_batch, y_batch, classes=np.unique(y_train))\n",
    "    else:\n",
    "        gnb_online.partial_fit(X_batch, y_batch)\n",
    "\n",
    "print(\"온라인 학습 결과:\")\n",
    "print(f\"  배치 수: {n_batches}\")\n",
    "print(f\"  배치 크기: {batch_size}\")\n",
    "print(f\"  정확도: {gnb_online.score(X_test, y_test):.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-34",
   "metadata": {},
   "source": [
    "## 14. kNN vs 나이브 베이즈 비교"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-35",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 유방암 데이터\n",
    "cancer = load_breast_cancer()\n",
    "X_train_c, X_test_c, y_train_c, y_test_c = train_test_split(\n",
    "    cancer.data, cancer.target, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "# 스케일링\n",
    "scaler = StandardScaler()\n",
    "X_train_c_scaled = scaler.fit_transform(X_train_c)\n",
    "X_test_c_scaled = scaler.transform(X_test_c)\n",
    "\n",
    "# 모델 비교\n",
    "models = {\n",
    "    'kNN (k=5)': KNeighborsClassifier(n_neighbors=5),\n",
    "    'kNN (weighted)': KNeighborsClassifier(n_neighbors=5, weights='distance'),\n",
    "    'Gaussian NB': GaussianNB()\n",
    "}\n",
    "\n",
    "print(\"kNN vs 나이브 베이즈 비교 (Breast Cancer):\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "for name, model in models.items():\n",
    "    if 'kNN' in name:\n",
    "        model.fit(X_train_c_scaled, y_train_c)\n",
    "        acc = model.score(X_test_c_scaled, y_test_c)\n",
    "    else:\n",
    "        model.fit(X_train_c, y_train_c)\n",
    "        acc = model.score(X_test_c, y_test_c)\n",
    "    print(f\"  {name:18s}: {acc:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-36",
   "metadata": {},
   "source": [
    "## 15. 간단한 텍스트 분류 예제"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-37",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 간단한 감성 분류\n",
    "texts = [\n",
    "    \"I love this movie\", \"Great film\", \"Excellent acting\",\n",
    "    \"Amazing performance\", \"Wonderful story\",\n",
    "    \"Terrible movie\", \"Bad film\", \"Worst movie ever\",\n",
    "    \"Horrible acting\", \"Disappointing story\"\n",
    "]\n",
    "labels = [1, 1, 1, 1, 1, 0, 0, 0, 0, 0]  # 1: positive, 0: negative\n",
    "\n",
    "# TF-IDF 벡터화\n",
    "tfidf = TfidfVectorizer()\n",
    "X_sentiment = tfidf.fit_transform(texts)\n",
    "\n",
    "# 나이브 베이즈 학습\n",
    "mnb_sentiment = MultinomialNB()\n",
    "mnb_sentiment.fit(X_sentiment, labels)\n",
    "\n",
    "# 새로운 텍스트 분류\n",
    "new_texts = [\n",
    "    \"This is a great movie\",\n",
    "    \"I hate this film\",\n",
    "    \"Excellent performance and story\",\n",
    "    \"Terrible and disappointing\"\n",
    "]\n",
    "X_new = tfidf.transform(new_texts)\n",
    "predictions = mnb_sentiment.predict(X_new)\n",
    "probabilities = mnb_sentiment.predict_proba(X_new)\n",
    "\n",
    "print(\"감성 분류 결과:\")\n",
    "print(\"=\" * 60)\n",
    "for text, pred, prob in zip(new_texts, predictions, probabilities):\n",
    "    sentiment = \"Positive\" if pred == 1 else \"Negative\"\n",
    "    confidence = max(prob) * 100\n",
    "    print(f\"'{text}'\")\n",
    "    print(f\"  → {sentiment} (신뢰도: {confidence:.1f}%)\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-38",
   "metadata": {},
   "source": [
    "## 정리\n",
    "\n",
    "### kNN 요약\n",
    "\n",
    "| 파라미터 | 설명 | 권장 |\n",
    "|----------|------|------|\n",
    "| **n_neighbors** | 이웃 수 (k) | 교차 검증으로 선택 |\n",
    "| **weights** | 가중치 방식 | 'distance' 추천 |\n",
    "| **metric** | 거리 측정 | 'euclidean' 기본 |\n",
    "| **algorithm** | 탐색 알고리즘 | 'auto' |\n",
    "\n",
    "**특징**:\n",
    "- 게으른 학습 (학습 시간 없음)\n",
    "- 예측 시간 느림 (O(n·d))\n",
    "- 스케일링 필수\n",
    "- 고차원에서 성능 저하 (차원의 저주)\n",
    "\n",
    "### 나이브 베이즈 요약\n",
    "\n",
    "| 종류 | 특성 타입 | 주요 용도 |\n",
    "|------|-----------|----------|\n",
    "| **GaussianNB** | 연속형 (정규 분포) | 일반 분류 |\n",
    "| **MultinomialNB** | 카운트/빈도 | 텍스트 분류 |\n",
    "| **BernoulliNB** | 이진 (0/1) | 단어 존재 여부 |\n",
    "\n",
    "**특징**:\n",
    "- 매우 빠름 (학습 O(n·d), 예측 O(d))\n",
    "- 적은 데이터로도 잘 작동\n",
    "- 고차원 데이터에 효과적\n",
    "- 온라인 학습 가능\n",
    "- 특성 독립성 가정 (현실에서 위반 가능)\n",
    "\n",
    "### kNN vs 나이브 베이즈\n",
    "\n",
    "| 특성 | kNN | 나이브 베이즈 |\n",
    "|------|-----|---------------|\n",
    "| **학습 시간** | O(1) | O(n·d) |\n",
    "| **예측 시간** | O(n·d) | O(d) |\n",
    "| **메모리** | 높음 | 낮음 |\n",
    "| **스케일링** | 필수 | 불필요 |\n",
    "| **고차원** | 약함 | 강함 |\n",
    "| **해석성** | 직관적 | 확률 기반 |\n",
    "\n",
    "### 다음 단계\n",
    "- Clustering (K-Means, DBSCAN)\n",
    "- Dimensionality Reduction (PCA, t-SNE)\n",
    "- Ensemble methods (Stacking, Voting)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
