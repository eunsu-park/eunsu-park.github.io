{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 파이프라인과 실무 (Pipeline & Practice)\n",
    "\n",
    "sklearn의 Pipeline과 ColumnTransformer를 사용하여 전처리와 모델링을 하나의 워크플로우로 통합하는 방법을 학습합니다.\n",
    "\n",
    "**학습 목표:**\n",
    "- Pipeline의 필요성과 장점 이해\n",
    "- ColumnTransformer로 다양한 타입의 특성 처리\n",
    "- 커스텀 Transformer 작성\n",
    "- Pipeline과 GridSearchCV 결합\n",
    "- 모델 저장 및 배포"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.pipeline import Pipeline, make_pipeline\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler, OneHotEncoder\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV\n",
    "from sklearn.datasets import load_iris, load_breast_cancer\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Pipeline 기초\n",
    "\n",
    "### Pipeline을 사용하지 않을 때의 문제점\n",
    "\n",
    "1. **데이터 누수 (Data Leakage)**: 테스트 데이터 정보가 학습에 반영될 위험\n",
    "2. **코드 복잡성**: 여러 단계를 수동으로 관리해야 함\n",
    "3. **재현성 문제**: 순서 실수, 파라미터 불일치 가능성\n",
    "\n",
    "### Pipeline의 장점\n",
    "\n",
    "1. 코드 간소화\n",
    "2. 데이터 누수 방지\n",
    "3. 교차 검증과 완벽 통합\n",
    "4. 하이퍼파라미터 튜닝 용이\n",
    "5. 모델 저장/배포 편리"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 데이터 로드\n",
    "iris = load_iris()\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    iris.data, iris.target, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "# Pipeline 생성 (명시적 이름)\n",
    "pipeline = Pipeline([\n",
    "    ('scaler', StandardScaler()),\n",
    "    ('pca', PCA(n_components=2)),\n",
    "    ('classifier', LogisticRegression())\n",
    "])\n",
    "\n",
    "# 학습 및 예측\n",
    "pipeline.fit(X_train, y_train)\n",
    "y_pred = pipeline.predict(X_test)\n",
    "score = pipeline.score(X_test, y_test)\n",
    "\n",
    "print(f\"Pipeline 정확도: {score:.4f}\")\n",
    "\n",
    "# make_pipeline (자동 이름 생성)\n",
    "pipeline_auto = make_pipeline(\n",
    "    StandardScaler(),\n",
    "    PCA(n_components=2),\n",
    "    LogisticRegression()\n",
    ")\n",
    "\n",
    "pipeline_auto.fit(X_train, y_train)\n",
    "print(f\"make_pipeline 정확도: {pipeline_auto.score(X_test, y_test):.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pipeline 단계 접근하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 단계 이름 확인\n",
    "print(\"Pipeline 단계:\")\n",
    "for name, step in pipeline.named_steps.items():\n",
    "    print(f\"  {name}: {type(step).__name__}\")\n",
    "\n",
    "# 특정 단계 접근\n",
    "print(f\"\\nPCA 설명된 분산: {pipeline.named_steps['pca'].explained_variance_ratio_}\")\n",
    "print(f\"로지스틱 회귀 계수 형상: {pipeline.named_steps['classifier'].coef_.shape}\")\n",
    "\n",
    "# 중간 단계 결과 얻기\n",
    "X_scaled = pipeline.named_steps['scaler'].transform(X_test)\n",
    "X_pca = pipeline.named_steps['pca'].transform(X_scaled)\n",
    "print(f\"\\n스케일링 후 형상: {X_scaled.shape}\")\n",
    "print(f\"PCA 후 형상: {X_pca.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. ColumnTransformer - 다양한 타입의 특성 처리\n",
    "\n",
    "실제 데이터에서는 수치형과 범주형 특성이 혼재되어 있습니다. ColumnTransformer를 사용하면 각 타입에 맞는 전처리를 적용할 수 있습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import OrdinalEncoder\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "# 샘플 데이터 생성\n",
    "data = {\n",
    "    'age': [25, 32, 47, 51, 62, 28, 35, 42, 55, 60],\n",
    "    'income': [50000, 60000, 80000, 120000, 95000, 55000, 70000, 85000, 110000, 100000],\n",
    "    'gender': ['M', 'F', 'M', 'F', 'M', 'F', 'M', 'F', 'M', 'F'],\n",
    "    'education': ['Bachelor', 'Master', 'PhD', 'Bachelor', 'Master', \n",
    "                  'Bachelor', 'PhD', 'Master', 'PhD', 'Bachelor'],\n",
    "    'purchased': [0, 1, 1, 1, 0, 0, 1, 1, 1, 0]\n",
    "}\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "X = df.drop('purchased', axis=1)\n",
    "y = df['purchased']\n",
    "\n",
    "print(\"데이터 타입:\")\n",
    "print(X.dtypes)\n",
    "print(\"\\n데이터 샘플:\")\n",
    "print(X.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 특성 분류\n",
    "numeric_features = ['age', 'income']\n",
    "categorical_features = ['gender', 'education']\n",
    "\n",
    "# ColumnTransformer 정의\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', StandardScaler(), numeric_features),\n",
    "        ('cat', OneHotEncoder(drop='first', sparse_output=False), categorical_features)\n",
    "    ],\n",
    "    remainder='passthrough'  # 나머지 특성 처리: 'drop' 또는 'passthrough'\n",
    ")\n",
    "\n",
    "# 변환\n",
    "X_transformed = preprocessor.fit_transform(X)\n",
    "\n",
    "print(f\"원본 형상: {X.shape}\")\n",
    "print(f\"변환 후 형상: {X_transformed.shape}\")\n",
    "\n",
    "# 변환된 특성 이름\n",
    "feature_names = (\n",
    "    numeric_features +\n",
    "    list(preprocessor.named_transformers_['cat'].get_feature_names_out(categorical_features))\n",
    ")\n",
    "print(f\"\\n특성 이름: {feature_names}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pipeline + ColumnTransformer 결합"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 전체 파이프라인\n",
    "full_pipeline = Pipeline([\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('classifier', RandomForestClassifier(n_estimators=100, random_state=42))\n",
    "])\n",
    "\n",
    "# 학습\n",
    "full_pipeline.fit(X, y)\n",
    "\n",
    "# 새로운 데이터로 예측\n",
    "new_data = pd.DataFrame({\n",
    "    'age': [30, 45],\n",
    "    'income': [70000, 90000],\n",
    "    'gender': ['F', 'M'],\n",
    "    'education': ['Master', 'PhD']\n",
    "})\n",
    "\n",
    "predictions = full_pipeline.predict(new_data)\n",
    "print(f\"예측 결과: {predictions}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. 결측치 처리를 포함한 복잡한 파이프라인"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "# 결측치가 있는 데이터 생성\n",
    "data_missing = {\n",
    "    'age': [25, np.nan, 47, 51, 62, 28, np.nan, 42, 55, 60],\n",
    "    'income': [50000, 60000, np.nan, 120000, 95000, np.nan, 70000, 85000, 110000, 100000],\n",
    "    'gender': ['M', 'F', 'M', None, 'M', 'F', 'M', None, 'M', 'F'],\n",
    "    'education': ['Bachelor', 'Master', 'PhD', 'Bachelor', None, \n",
    "                  'Bachelor', 'PhD', 'Master', None, 'Bachelor'],\n",
    "    'purchased': [0, 1, 1, 1, 0, 0, 1, 1, 1, 0]\n",
    "}\n",
    "df_missing = pd.DataFrame(data_missing)\n",
    "X_missing = df_missing.drop('purchased', axis=1)\n",
    "y_missing = df_missing['purchased']\n",
    "\n",
    "print(\"결측치 개수:\")\n",
    "print(X_missing.isnull().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 수치형 파이프라인 (결측치 처리 포함)\n",
    "numeric_transformer = Pipeline([\n",
    "    ('imputer', SimpleImputer(strategy='median')),\n",
    "    ('scaler', StandardScaler())\n",
    "])\n",
    "\n",
    "# 범주형 파이프라인 (결측치 처리 포함)\n",
    "categorical_transformer = Pipeline([\n",
    "    ('imputer', SimpleImputer(strategy='most_frequent')),\n",
    "    ('encoder', OneHotEncoder(drop='first', sparse_output=False, handle_unknown='ignore'))\n",
    "])\n",
    "\n",
    "# ColumnTransformer\n",
    "preprocessor_full = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', numeric_transformer, numeric_features),\n",
    "        ('cat', categorical_transformer, categorical_features)\n",
    "    ]\n",
    ")\n",
    "\n",
    "# 전체 파이프라인\n",
    "complete_pipeline = Pipeline([\n",
    "    ('preprocessor', preprocessor_full),\n",
    "    ('classifier', RandomForestClassifier(random_state=42))\n",
    "])\n",
    "\n",
    "complete_pipeline.fit(X_missing, y_missing)\n",
    "print(\"결측치 포함 파이프라인 학습 완료\")\n",
    "print(f\"학습 정확도: {complete_pipeline.score(X_missing, y_missing):.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Pipeline과 교차 검증 및 하이퍼파라미터 튜닝"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 실제 데이터셋으로 실습\n",
    "cancer = load_breast_cancer()\n",
    "X, y = cancer.data, cancer.target\n",
    "\n",
    "# 파이프라인 정의\n",
    "pipeline_cv = Pipeline([\n",
    "    ('scaler', StandardScaler()),\n",
    "    ('classifier', LogisticRegression(max_iter=1000))\n",
    "])\n",
    "\n",
    "# 교차 검증 (올바른 방법: 각 폴드에서 스케일러가 학습 데이터만으로 fit)\n",
    "scores = cross_val_score(pipeline_cv, X, y, cv=5, scoring='accuracy')\n",
    "\n",
    "print(\"교차 검증 결과:\")\n",
    "print(f\"  각 폴드: {scores}\")\n",
    "print(f\"  평균: {scores.mean():.4f} (+/- {scores.std():.4f})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GridSearchCV로 하이퍼파라미터 튜닝\n",
    "\n",
    "Pipeline에서 하이퍼파라미터 이름은 `step__parameter` 형식을 사용합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 파라미터 그리드 (step__parameter 형식)\n",
    "param_grid = {\n",
    "    'scaler': [StandardScaler(), MinMaxScaler()],\n",
    "    'classifier__C': [0.1, 1, 10],\n",
    "    'classifier__penalty': ['l1', 'l2'],\n",
    "    'classifier__solver': ['liblinear']\n",
    "}\n",
    "\n",
    "# Grid Search\n",
    "grid_search = GridSearchCV(\n",
    "    pipeline_cv,\n",
    "    param_grid,\n",
    "    cv=5,\n",
    "    scoring='accuracy',\n",
    "    n_jobs=-1,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "grid_search.fit(X, y)\n",
    "\n",
    "print(\"\\nGrid Search 결과:\")\n",
    "print(f\"  최적 파라미터: {grid_search.best_params_}\")\n",
    "print(f\"  최적 점수: {grid_search.best_score_:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 여러 모델 비교"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "# 여러 모델을 위한 파이프라인\n",
    "pipeline_multi = Pipeline([\n",
    "    ('scaler', StandardScaler()),\n",
    "    ('classifier', LogisticRegression())  # placeholder\n",
    "])\n",
    "\n",
    "# 모델별 다른 파라미터\n",
    "param_grid_multi = [\n",
    "    {\n",
    "        'classifier': [LogisticRegression(max_iter=1000)],\n",
    "        'classifier__C': [0.1, 1, 10]\n",
    "    },\n",
    "    {\n",
    "        'classifier': [RandomForestClassifier(random_state=42)],\n",
    "        'classifier__n_estimators': [50, 100],\n",
    "        'classifier__max_depth': [None, 5, 10]\n",
    "    },\n",
    "    {\n",
    "        'classifier': [SVC()],\n",
    "        'classifier__C': [0.1, 1],\n",
    "        'classifier__kernel': ['rbf', 'linear']\n",
    "    }\n",
    "]\n",
    "\n",
    "grid_search_multi = GridSearchCV(\n",
    "    pipeline_multi,\n",
    "    param_grid_multi,\n",
    "    cv=5,\n",
    "    scoring='accuracy',\n",
    "    n_jobs=-1,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "grid_search_multi.fit(X, y)\n",
    "\n",
    "print(\"\\n여러 모델 비교 결과:\")\n",
    "print(f\"  최적 모델: {type(grid_search_multi.best_params_['classifier']).__name__}\")\n",
    "print(f\"  최적 파라미터: {grid_search_multi.best_params_}\")\n",
    "print(f\"  최적 점수: {grid_search_multi.best_score_:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. 모델 저장과 로드\n",
    "\n",
    "학습된 파이프라인을 저장하고 나중에 다시 사용할 수 있습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import joblib\n",
    "import pickle\n",
    "import sklearn\n",
    "from datetime import datetime\n",
    "\n",
    "# 최적 모델\n",
    "best_pipeline = grid_search.best_estimator_\n",
    "\n",
    "# 1. joblib 저장 (권장)\n",
    "joblib.dump(best_pipeline, 'best_model.joblib')\n",
    "print(\"모델 저장 완료: best_model.joblib\")\n",
    "\n",
    "# 모델 로드\n",
    "loaded_model = joblib.load('best_model.joblib')\n",
    "\n",
    "# 테스트\n",
    "X_test_sample = X[:5]\n",
    "predictions = loaded_model.predict(X_test_sample)\n",
    "print(f\"로드된 모델 예측: {predictions}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. pickle 저장\n",
    "with open('model.pkl', 'wb') as f:\n",
    "    pickle.dump(best_pipeline, f)\n",
    "\n",
    "# pickle 로드\n",
    "with open('model.pkl', 'rb') as f:\n",
    "    loaded_model_pkl = pickle.load(f)\n",
    "\n",
    "print(\"pickle 모델 예측:\", loaded_model_pkl.predict(X[:3]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 메타데이터와 함께 저장 (권장)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 메타데이터와 함께 저장\n",
    "model_metadata = {\n",
    "    'model': best_pipeline,\n",
    "    'sklearn_version': sklearn.__version__,\n",
    "    'training_date': datetime.now().isoformat(),\n",
    "    'feature_names': list(cancer.feature_names),\n",
    "    'target_names': list(cancer.target_names),\n",
    "    'cv_score': grid_search.best_score_,\n",
    "    'best_params': grid_search.best_params_\n",
    "}\n",
    "\n",
    "joblib.dump(model_metadata, 'model_with_metadata.joblib')\n",
    "\n",
    "# 로드 및 검증\n",
    "loaded_metadata = joblib.load('model_with_metadata.joblib')\n",
    "print(\"모델 메타데이터:\")\n",
    "print(f\"  학습 날짜: {loaded_metadata['training_date']}\")\n",
    "print(f\"  sklearn 버전: {loaded_metadata['sklearn_version']}\")\n",
    "print(f\"  CV 점수: {loaded_metadata['cv_score']:.4f}\")\n",
    "print(f\"  최적 파라미터: {loaded_metadata['best_params']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. 커스텀 Transformer 작성\n",
    "\n",
    "sklearn의 BaseEstimator와 TransformerMixin을 상속하여 자신만의 Transformer를 만들 수 있습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "\n",
    "class OutlierRemover(BaseEstimator, TransformerMixin):\n",
    "    \"\"\"이상치를 경계값으로 대체하는 트랜스포머\"\"\"\n",
    "    \n",
    "    def __init__(self, threshold=3):\n",
    "        self.threshold = threshold\n",
    "        self.mean_ = None\n",
    "        self.std_ = None\n",
    "    \n",
    "    def fit(self, X, y=None):\n",
    "        self.mean_ = np.mean(X, axis=0)\n",
    "        self.std_ = np.std(X, axis=0)\n",
    "        return self\n",
    "    \n",
    "    def transform(self, X):\n",
    "        X = np.array(X)\n",
    "        z_scores = np.abs((X - self.mean_) / (self.std_ + 1e-10))\n",
    "        # 이상치를 경계값으로 대체\n",
    "        X_clipped = np.where(z_scores > self.threshold,\n",
    "                             self.mean_ + self.threshold * self.std_ * np.sign(X - self.mean_),\n",
    "                             X)\n",
    "        return X_clipped\n",
    "\n",
    "\n",
    "class FeatureSelector(BaseEstimator, TransformerMixin):\n",
    "    \"\"\"특성 선택 트랜스포머\"\"\"\n",
    "    \n",
    "    def __init__(self, feature_indices=None):\n",
    "        self.feature_indices = feature_indices\n",
    "    \n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "    \n",
    "    def transform(self, X):\n",
    "        X = np.array(X)\n",
    "        if self.feature_indices is not None:\n",
    "            return X[:, self.feature_indices]\n",
    "        return X\n",
    "\n",
    "\n",
    "# 커스텀 트랜스포머 사용\n",
    "custom_pipeline = Pipeline([\n",
    "    ('outlier', OutlierRemover(threshold=3)),\n",
    "    ('scaler', StandardScaler()),\n",
    "    ('classifier', LogisticRegression(max_iter=1000))\n",
    "])\n",
    "\n",
    "scores = cross_val_score(custom_pipeline, X, y, cv=5)\n",
    "print(f\"커스텀 트랜스포머 CV 점수: {scores.mean():.4f} (+/- {scores.std():.4f})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. 실전 템플릿 - 분류 문제용 파이프라인 생성 함수"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.compose import make_column_selector\n",
    "\n",
    "def create_classification_pipeline(model, numeric_features=None, categorical_features=None):\n",
    "    \"\"\"\n",
    "    분류 문제용 파이프라인 생성 함수\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    model : sklearn estimator\n",
    "        분류 모델\n",
    "    numeric_features : list, optional\n",
    "        수치형 특성 이름 리스트\n",
    "    categorical_features : list, optional\n",
    "        범주형 특성 이름 리스트\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    pipeline : Pipeline\n",
    "        전처리 + 모델 파이프라인\n",
    "    \"\"\"\n",
    "    \n",
    "    # 수치형 특성 파이프라인\n",
    "    numeric_transformer = Pipeline([\n",
    "        ('imputer', SimpleImputer(strategy='median')),\n",
    "        ('scaler', StandardScaler())\n",
    "    ])\n",
    "    \n",
    "    # 범주형 특성 파이프라인\n",
    "    categorical_transformer = Pipeline([\n",
    "        ('imputer', SimpleImputer(strategy='most_frequent')),\n",
    "        ('encoder', OneHotEncoder(drop='first', sparse_output=False, handle_unknown='ignore'))\n",
    "    ])\n",
    "    \n",
    "    # ColumnTransformer\n",
    "    if numeric_features is None and categorical_features is None:\n",
    "        # 자동 감지\n",
    "        preprocessor = ColumnTransformer(\n",
    "            transformers=[\n",
    "                ('num', numeric_transformer, make_column_selector(dtype_include=np.number)),\n",
    "                ('cat', categorical_transformer, make_column_selector(dtype_include=object))\n",
    "            ]\n",
    "        )\n",
    "    else:\n",
    "        preprocessor = ColumnTransformer(\n",
    "            transformers=[\n",
    "                ('num', numeric_transformer, numeric_features or []),\n",
    "                ('cat', categorical_transformer, categorical_features or [])\n",
    "            ]\n",
    "        )\n",
    "    \n",
    "    # 전체 파이프라인\n",
    "    pipeline = Pipeline([\n",
    "        ('preprocessor', preprocessor),\n",
    "        ('classifier', model)\n",
    "    ])\n",
    "    \n",
    "    return pipeline\n",
    "\n",
    "\n",
    "# 사용 예시\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "\n",
    "pipeline_template = create_classification_pipeline(\n",
    "    GradientBoostingClassifier(random_state=42),\n",
    "    numeric_features=['age', 'income'],\n",
    "    categorical_features=['gender', 'education']\n",
    ")\n",
    "\n",
    "print(\"분류 파이프라인 템플릿 생성 완료\")\n",
    "print(pipeline_template)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. 배포를 위한 모델 래퍼 클래스"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ModelWrapper:\n",
    "    \"\"\"배포용 모델 래퍼\"\"\"\n",
    "    \n",
    "    def __init__(self, model_path):\n",
    "        self.model = joblib.load(model_path)\n",
    "        self.feature_names = None\n",
    "    \n",
    "    def set_feature_names(self, names):\n",
    "        \"\"\"특성 이름 설정\"\"\"\n",
    "        self.feature_names = names\n",
    "    \n",
    "    def predict(self, input_data):\n",
    "        \"\"\"딕셔너리 또는 DataFrame 입력 처리\"\"\"\n",
    "        if isinstance(input_data, dict):\n",
    "            input_data = pd.DataFrame([input_data])\n",
    "        \n",
    "        if self.feature_names:\n",
    "            input_data = input_data[self.feature_names]\n",
    "        \n",
    "        return self.model.predict(input_data)\n",
    "    \n",
    "    def predict_proba(self, input_data):\n",
    "        \"\"\"확률 예측\"\"\"\n",
    "        if isinstance(input_data, dict):\n",
    "            input_data = pd.DataFrame([input_data])\n",
    "        \n",
    "        if self.feature_names:\n",
    "            input_data = input_data[self.feature_names]\n",
    "        \n",
    "        return self.model.predict_proba(input_data)\n",
    "\n",
    "\n",
    "# 사용 예시\n",
    "# wrapper = ModelWrapper('best_model.joblib')\n",
    "# wrapper.set_feature_names(cancer.feature_names)\n",
    "# prediction = wrapper.predict(X[0:1])\n",
    "# print(f\"예측 결과: {prediction}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 요약 및 Best Practices\n",
    "\n",
    "### Pipeline 사용 시 장점\n",
    "\n",
    "1. **데이터 누수 방지**: 교차 검증 시 각 폴드에서 전처리가 학습 데이터만으로 수행됨\n",
    "2. **코드 간소화**: 여러 단계를 하나의 객체로 관리\n",
    "3. **재현성**: 모든 전처리 단계가 저장되어 동일한 처리 보장\n",
    "4. **배포 용이**: 하나의 파일로 전체 워크플로우 저장 가능\n",
    "\n",
    "### 하이퍼파라미터 명명 규칙\n",
    "\n",
    "```python\n",
    "# 형식: step_name__parameter_name\n",
    "'classifier__C'  # classifier 단계의 C 파라미터\n",
    "'preprocessor__num__scaler__with_mean'  # 중첩된 파라미터\n",
    "```\n",
    "\n",
    "### 모델 저장 방법 비교\n",
    "\n",
    "| 방법 | 장점 | 단점 |\n",
    "|------|------|------|\n",
    "| joblib | 대용량 NumPy 배열 효율적 처리 | sklearn 전용 |\n",
    "| pickle | 파이썬 표준 라이브러리 | 대용량에서 느림 |\n",
    "| ONNX | 프레임워크 독립적, 다양한 언어 지원 | 변환 작업 필요 |\n",
    "\n",
    "### 실무 체크리스트\n",
    "\n",
    "- [ ] 항상 Pipeline 사용하여 데이터 누수 방지\n",
    "- [ ] ColumnTransformer로 수치형/범주형 전처리 분리\n",
    "- [ ] 모델 저장 시 메타데이터 포함 (버전, 날짜, 성능 등)\n",
    "- [ ] 입력 검증 함수 작성\n",
    "- [ ] 커스텀 Transformer는 BaseEstimator, TransformerMixin 상속\n",
    "- [ ] GridSearchCV로 전체 파이프라인 튜닝"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
