{% raw %}
<!DOCTYPE html>
<html lang="en" data-theme="light">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>README.md - Examples</title>
    <link rel="stylesheet" href="/study/static/css/style.css">
    <link rel="stylesheet" href="/study/static/css/highlight.css">
    <!-- KaTeX for LaTeX math rendering -->
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.11/dist/katex.min.css">
    <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.11/dist/katex.min.js"></script>
    <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.11/dist/contrib/auto-render.min.js"
        onload="renderMathInElement(document.body, {
            delimiters: [
                {left: '$$', right: '$$', display: true},
                {left: '$', right: '$', display: false}
            ],
            throwOnError: false
        });"></script>
    
</head>
<body>
    <div class="app-container">
        <!-- Sidebar -->
        <aside class="sidebar" id="sidebar">
            <div class="sidebar-header">
                <a href="/study/en/" class="logo">Study Materials</a>
                <button class="sidebar-toggle" id="sidebar-toggle" aria-label="Toggle sidebar">
                    <span></span>
                </button>
            </div>

            <nav class="sidebar-nav">
                <a href="/study/en/" class="nav-item ">
                    <span class="nav-icon">üè†</span>
                    <span class="nav-text">Home</span>
                </a>
                <a href="/study/examples/" class="nav-item active">
                    <span class="nav-icon">üíª</span>
                    <span class="nav-text">Examples</span>
                </a>
            </nav>

            <div class="sidebar-search">
                <form action="/study/en/search.html" method="get" id="search-form">
                    <input type="search" name="q" placeholder="Search..." id="search-sidebar-input">
                </form>
            </div>

            <!-- Language Selector -->
            <div class="sidebar-lang">
                <select id="lang-select" class="lang-selector" onchange="switchLanguage(this.value)">
                    
                    <option value="en" selected>
                        English
                    </option>
                    
                    <option value="ko" >
                        ÌïúÍµ≠Ïñ¥
                    </option>
                    
                </select>
            </div>

            <div class="sidebar-footer">
                <button class="theme-toggle" id="theme-toggle" aria-label="Toggle theme">
                    <span class="theme-icon light">‚òÄÔ∏è</span>
                    <span class="theme-icon dark">üåô</span>
                </button>
            </div>
        </aside>

        <!-- Main Content -->
        <main class="main-content">
            <header class="main-header">
                <button class="menu-toggle" id="menu-toggle" aria-label="Open menu">
                    <span></span>
                </button>
                
<nav class="breadcrumb">
    <a href="/study/examples/">Examples</a>
    <span class="separator">/</span>
    <a href="/study/examples/Math_for_AI/">Math for AI</a>
    <span class="separator">/</span>
    <span class="current">README.md</span>
</nav>

            </header>

            <div class="content">
                
<article class="example-article">
    <header class="example-header">
        <h1>README.md</h1>
        <div class="example-actions">
            <a href="README.md" download class="btn">Download</a>
            <button class="btn" id="copy-code-btn">Copy code</button>
        </div>
    </header>

    <div class="example-meta">
        <span>markdown</span>
        <span>311 lines</span>
        <span>9.7 KB</span>
    </div>

    <div class="example-code markdown-body">
        <div class="highlight"><pre><span></span><code><span class="linenos">  1</span><span class="gh"># Math for AI - Example Code</span>
<span class="linenos">  2</span>
<span class="linenos">  3</span>This directory contains Python examples demonstrating mathematical concepts used in AI/ML.
<span class="linenos">  4</span>
<span class="linenos">  5</span><span class="gu">## Files</span>
<span class="linenos">  6</span>
<span class="linenos">  7</span><span class="gu">### 01_vector_matrix_ops.py (283 lines)</span>
<span class="linenos">  8</span><span class="gs">**Linear Algebra Fundamentals**</span>
<span class="linenos">  9</span>
<span class="linenos"> 10</span>Demonstrates:
<span class="linenos"> 11</span><span class="k">-</span><span class="w"> </span>Vector space operations: basis, span, linear independence
<span class="linenos"> 12</span><span class="k">-</span><span class="w"> </span>Matrix operations: multiplication, transpose, inverse
<span class="linenos"> 13</span><span class="k">-</span><span class="w"> </span>Rank computation and properties
<span class="linenos"> 14</span><span class="k">-</span><span class="w"> </span>ML application: feature matrices and weight matrices
<span class="linenos"> 15</span><span class="k">-</span><span class="w"> </span>Visualization of vector addition and linear transformations
<span class="linenos"> 16</span>
<span class="linenos"> 17</span><span class="gs">**Key concepts:**</span>
<span class="linenos"> 18</span><span class="k">-</span><span class="w"> </span>Standard basis vectors in R^n
<span class="linenos"> 19</span><span class="k">-</span><span class="w"> </span>Linear independence checking via rank
<span class="linenos"> 20</span><span class="k">-</span><span class="w"> </span>Matrix multiplication and properties
<span class="linenos"> 21</span><span class="k">-</span><span class="w"> </span>Feature vectors as matrix rows in ML
<span class="linenos"> 22</span>
<span class="linenos"> 23</span><span class="gs">**Output:**</span> <span class="sb">`vector_ops.png`</span> - Vector operations visualization
<span class="linenos"> 24</span>
<span class="linenos"> 25</span>---
<span class="linenos"> 26</span>
<span class="linenos"> 27</span><span class="gu">### 02_svd_pca.py (302 lines)</span>
<span class="linenos"> 28</span><span class="gs">**SVD and Principal Component Analysis**</span>
<span class="linenos"> 29</span>
<span class="linenos"> 30</span>Demonstrates:
<span class="linenos"> 31</span><span class="k">-</span><span class="w"> </span>Singular Value Decomposition (SVD) with NumPy
<span class="linenos"> 32</span><span class="k">-</span><span class="w"> </span>Low-rank matrix approximation
<span class="linenos"> 33</span><span class="k">-</span><span class="w"> </span>PCA implementation from scratch (centering ‚Üí covariance ‚Üí eigen)
<span class="linenos"> 34</span><span class="k">-</span><span class="w"> </span>Comparison with sklearn PCA
<span class="linenos"> 35</span><span class="k">-</span><span class="w"> </span>Application to Iris dataset
<span class="linenos"> 36</span><span class="k">-</span><span class="w"> </span>Explained variance visualization
<span class="linenos"> 37</span>
<span class="linenos"> 38</span><span class="gs">**Key concepts:**</span>
<span class="linenos"> 39</span><span class="k">-</span><span class="w"> </span>SVD decomposition: A = U @ S @ V^T
<span class="linenos"> 40</span><span class="k">-</span><span class="w"> </span>Relationship between SVD and PCA
<span class="linenos"> 41</span><span class="k">-</span><span class="w"> </span>Principal components as eigenvectors
<span class="linenos"> 42</span><span class="k">-</span><span class="w"> </span>Dimensionality reduction in practice
<span class="linenos"> 43</span>
<span class="linenos"> 44</span><span class="gs">**Output:**</span> <span class="sb">`pca_visualization.png`</span> - PCA results on Iris dataset
<span class="linenos"> 45</span>
<span class="linenos"> 46</span>---
<span class="linenos"> 47</span>
<span class="linenos"> 48</span><span class="gu">### 03_matrix_calculus_autograd.py (408 lines)</span>
<span class="linenos"> 49</span><span class="gs">**Matrix Calculus and Automatic Differentiation**</span>
<span class="linenos"> 50</span>
<span class="linenos"> 51</span>Demonstrates:
<span class="linenos"> 52</span><span class="k">-</span><span class="w"> </span>Manual gradient computation for scalar and vector functions
<span class="linenos"> 53</span><span class="k">-</span><span class="w"> </span>Jacobian matrix computation (vector ‚Üí vector)
<span class="linenos"> 54</span><span class="k">-</span><span class="w"> </span>Hessian matrix computation (second derivatives)
<span class="linenos"> 55</span><span class="k">-</span><span class="w"> </span>PyTorch autograd comparison
<span class="linenos"> 56</span><span class="k">-</span><span class="w"> </span>Numerical gradient checking
<span class="linenos"> 57</span><span class="k">-</span><span class="w"> </span>MSE loss gradients for linear regression
<span class="linenos"> 58</span>
<span class="linenos"> 59</span><span class="gs">**Key concepts:**</span>
<span class="linenos"> 60</span><span class="k">-</span><span class="w"> </span>Partial derivatives and gradient vectors
<span class="linenos"> 61</span><span class="k">-</span><span class="w"> </span>Jacobian for vector-valued functions
<span class="linenos"> 62</span><span class="k">-</span><span class="w"> </span>Hessian for convexity analysis
<span class="linenos"> 63</span><span class="k">-</span><span class="w"> </span>Automatic differentiation with PyTorch
<span class="linenos"> 64</span><span class="k">-</span><span class="w"> </span>Gradient descent optimization
<span class="linenos"> 65</span>
<span class="linenos"> 66</span><span class="gs">**Output:**</span> <span class="sb">`gradient_descent.png`</span> - Gradient descent trajectory visualization
<span class="linenos"> 67</span>
<span class="linenos"> 68</span>---
<span class="linenos"> 69</span>
<span class="linenos"> 70</span><span class="gu">### 04_norms_regularization.py (400 lines)</span>
<span class="linenos"> 71</span><span class="gs">**Norms, Distances, and Regularization**</span>
<span class="linenos"> 72</span>
<span class="linenos"> 73</span>Demonstrates:
<span class="linenos"> 74</span><span class="k">-</span><span class="w"> </span>Lp norms (L1, L2, L‚àû) and their properties
<span class="linenos"> 75</span><span class="k">-</span><span class="w"> </span>Distance metrics: Euclidean, Manhattan, Cosine, Mahalanobis
<span class="linenos"> 76</span><span class="k">-</span><span class="w"> </span>L1 vs L2 regularization in linear regression
<span class="linenos"> 77</span><span class="k">-</span><span class="w"> </span>Sparsity-inducing property of L1
<span class="linenos"> 78</span><span class="k">-</span><span class="w"> </span>Unit ball visualization for different norms
<span class="linenos"> 79</span>
<span class="linenos"> 80</span><span class="gs">**Key concepts:**</span>
<span class="linenos"> 81</span><span class="k">-</span><span class="w"> </span>Norm properties: non-negativity, homogeneity, triangle inequality
<span class="linenos"> 82</span><span class="k">-</span><span class="w"> </span>L1 (Lasso) produces sparse solutions
<span class="linenos"> 83</span><span class="k">-</span><span class="w"> </span>L2 (Ridge) shrinks coefficients smoothly
<span class="linenos"> 84</span><span class="k">-</span><span class="w"> </span>Feature selection vs feature shrinkage
<span class="linenos"> 85</span><span class="k">-</span><span class="w"> </span>Regularization paths
<span class="linenos"> 86</span>
<span class="linenos"> 87</span><span class="gs">**Output:**</span>
<span class="linenos"> 88</span><span class="k">-</span><span class="w"> </span><span class="sb">`unit_balls.png`</span> - Unit ball shapes for L1, L2, L‚àû
<span class="linenos"> 89</span><span class="k">-</span><span class="w"> </span><span class="sb">`regularization_comparison.png`</span> - L1 vs L2 effects
<span class="linenos"> 90</span>
<span class="linenos"> 91</span>---
<span class="linenos"> 92</span>
<span class="linenos"> 93</span><span class="gu">### 05_gradient_descent.py (271 lines)</span>
<span class="linenos"> 94</span><span class="gs">**Gradient Descent Optimization Algorithms**</span>
<span class="linenos"> 95</span>
<span class="linenos"> 96</span>Demonstrates:
<span class="linenos"> 97</span><span class="k">-</span><span class="w"> </span>Basic Gradient Descent (GD) optimizer
<span class="linenos"> 98</span><span class="k">-</span><span class="w"> </span>Stochastic Gradient Descent (SGD)
<span class="linenos"> 99</span><span class="k">-</span><span class="w"> </span>Momentum-based optimization
<span class="linenos">100</span><span class="k">-</span><span class="w"> </span>Adam optimizer from scratch
<span class="linenos">101</span><span class="k">-</span><span class="w"> </span>Optimization of Rosenbrock function (non-convex with narrow valley)
<span class="linenos">102</span>
<span class="linenos">103</span><span class="gs">**Key concepts:**</span>
<span class="linenos">104</span><span class="k">-</span><span class="w"> </span>Vanilla gradient descent updates: Œ∏ = Œ∏ - lr * ‚àáŒ∏
<span class="linenos">105</span><span class="k">-</span><span class="w"> </span>Momentum accumulation for acceleration
<span class="linenos">106</span><span class="k">-</span><span class="w"> </span>Adaptive learning rates with Adam
<span class="linenos">107</span><span class="k">-</span><span class="w"> </span>Comparison of convergence behavior across optimizers
<span class="linenos">108</span>
<span class="linenos">109</span><span class="gs">**Output:**</span> <span class="sb">`gradient_optimizers.png`</span> - Optimization trajectories for different algorithms
<span class="linenos">110</span>
<span class="linenos">111</span>---
<span class="linenos">112</span>
<span class="linenos">113</span><span class="gu">### 06_optimization_constrained.py (320 lines)</span>
<span class="linenos">114</span><span class="gs">**Constrained and Unconstrained Optimization**</span>
<span class="linenos">115</span>
<span class="linenos">116</span>Demonstrates:
<span class="linenos">117</span><span class="k">-</span><span class="w"> </span>Unconstrained optimization with scipy.optimize methods (BFGS, CG, Newton-CG, L-BFGS-B)
<span class="linenos">118</span><span class="k">-</span><span class="w"> </span>Lagrange multipliers for equality constraints
<span class="linenos">119</span><span class="k">-</span><span class="w"> </span>KKT conditions for inequality constraints
<span class="linenos">120</span><span class="k">-</span><span class="w"> </span>Convex vs non-convex optimization comparison
<span class="linenos">121</span>
<span class="linenos">122</span><span class="gs">**Key concepts:**</span>
<span class="linenos">123</span><span class="k">-</span><span class="w"> </span>Analytical vs numerical optimization
<span class="linenos">124</span><span class="k">-</span><span class="w"> </span>Lagrangian formulation for constraints
<span class="linenos">125</span><span class="k">-</span><span class="w"> </span>First-order (gradient) and second-order (Hessian) methods
<span class="linenos">126</span><span class="k">-</span><span class="w"> </span>Constraint handling in optimization problems
<span class="linenos">127</span>
<span class="linenos">128</span><span class="gs">**Output:**</span> <span class="sb">`constrained_optimization.png`</span> - Constraint visualization and optimization paths
<span class="linenos">129</span>
<span class="linenos">130</span>---
<span class="linenos">131</span>
<span class="linenos">132</span><span class="gu">### 07_probability_distributions.py (367 lines)</span>
<span class="linenos">133</span><span class="gs">**Probability Distributions and Statistical Inference**</span>
<span class="linenos">134</span>
<span class="linenos">135</span>Demonstrates:
<span class="linenos">136</span><span class="k">-</span><span class="w"> </span>Common probability distributions: Gaussian, Bernoulli, Poisson, Exponential
<span class="linenos">137</span><span class="k">-</span><span class="w"> </span>Maximum Likelihood Estimation (MLE) for Gaussian parameters
<span class="linenos">138</span><span class="k">-</span><span class="w"> </span>Maximum A Posteriori (MAP) estimation with Gaussian prior
<span class="linenos">139</span><span class="k">-</span><span class="w"> </span>Bayesian update visualization
<span class="linenos">140</span>
<span class="linenos">141</span><span class="gs">**Key concepts:**</span>
<span class="linenos">142</span><span class="k">-</span><span class="w"> </span>PDF/PMF properties for different distributions
<span class="linenos">143</span><span class="k">-</span><span class="w"> </span>MLE: argmax_Œ∏ P(data | Œ∏)
<span class="linenos">144</span><span class="k">-</span><span class="w"> </span>MAP: argmax_Œ∏ P(Œ∏ | data) = argmax_Œ∏ P(data | Œ∏) P(Œ∏)
<span class="linenos">145</span><span class="k">-</span><span class="w"> </span>Prior √ó Likelihood = Posterior (Bayes&#39; rule)
<span class="linenos">146</span>
<span class="linenos">147</span><span class="gs">**Output:**</span> <span class="sb">`probability_distributions.png`</span> - Distribution PDFs and Bayesian inference
<span class="linenos">148</span>
<span class="linenos">149</span>---
<span class="linenos">150</span>
<span class="linenos">151</span><span class="gu">### 08_information_theory.py (462 lines)</span>
<span class="linenos">152</span><span class="gs">**Information Theory for Machine Learning**</span>
<span class="linenos">153</span>
<span class="linenos">154</span>Demonstrates:
<span class="linenos">155</span><span class="k">-</span><span class="w"> </span>Entropy as measure of uncertainty: H(X) = -Œ£ p(x) log p(x)
<span class="linenos">156</span><span class="k">-</span><span class="w"> </span>Cross-entropy and KL divergence
<span class="linenos">157</span><span class="k">-</span><span class="w"> </span>Mutual information between variables
<span class="linenos">158</span><span class="k">-</span><span class="w"> </span>Connection to ML loss functions (cross-entropy loss)
<span class="linenos">159</span><span class="k">-</span><span class="w"> </span>ELBO (Evidence Lower Bound) visualization for VAEs
<span class="linenos">160</span>
<span class="linenos">161</span><span class="gs">**Key concepts:**</span>
<span class="linenos">162</span><span class="k">-</span><span class="w"> </span>Maximum entropy for uniform distributions
<span class="linenos">163</span><span class="k">-</span><span class="w"> </span>KL divergence as measure of distribution difference
<span class="linenos">164</span><span class="k">-</span><span class="w"> </span>Cross-entropy = Entropy + KL divergence
<span class="linenos">165</span><span class="k">-</span><span class="w"> </span>ELBO = log p(x) - KL(q||p) used in variational inference
<span class="linenos">166</span>
<span class="linenos">167</span><span class="gs">**Output:**</span> <span class="sb">`information_theory.png`</span> - Entropy, KL divergence, and ELBO visualization
<span class="linenos">168</span>
<span class="linenos">169</span>---
<span class="linenos">170</span>
<span class="linenos">171</span><span class="gu">### 09_mcmc_sampling.py (308 lines)</span>
<span class="linenos">172</span><span class="gs">**MCMC Sampling and Advanced Sampling Techniques**</span>
<span class="linenos">173</span>
<span class="linenos">174</span>Demonstrates:
<span class="linenos">175</span><span class="k">-</span><span class="w"> </span>Rejection sampling from target distribution
<span class="linenos">176</span><span class="k">-</span><span class="w"> </span>Importance sampling for expectation estimation
<span class="linenos">177</span><span class="k">-</span><span class="w"> </span>Metropolis-Hastings MCMC algorithm
<span class="linenos">178</span><span class="k">-</span><span class="w"> </span>Reparameterization trick (VAE-style)
<span class="linenos">179</span>
<span class="linenos">180</span><span class="gs">**Key concepts:**</span>
<span class="linenos">181</span><span class="k">-</span><span class="w"> </span>Rejection sampling: accept/reject based on proposal distribution
<span class="linenos">182</span><span class="k">-</span><span class="w"> </span>Importance sampling: weighted samples for expectation
<span class="linenos">183</span><span class="k">-</span><span class="w"> </span>MCMC: Markov chain converging to target distribution
<span class="linenos">184</span><span class="k">-</span><span class="w"> </span>Reparameterization: z = Œº + œÉ * Œµ for gradient flow in VAEs
<span class="linenos">185</span>
<span class="linenos">186</span><span class="gs">**Output:**</span> <span class="sb">`mcmc_sampling.png`</span> - Sampling method comparisons and MCMC convergence
<span class="linenos">187</span>
<span class="linenos">188</span>---
<span class="linenos">189</span>
<span class="linenos">190</span><span class="gu">### 10_tensor_ops_einsum.py (298 lines)</span>
<span class="linenos">191</span><span class="gs">**Tensor Operations and Einstein Summation**</span>
<span class="linenos">192</span>
<span class="linenos">193</span>Demonstrates:
<span class="linenos">194</span><span class="k">-</span><span class="w"> </span>Tensor creation and manipulation in NumPy and PyTorch
<span class="linenos">195</span><span class="k">-</span><span class="w"> </span>Einstein summation notation (einsum) for efficient operations
<span class="linenos">196</span><span class="k">-</span><span class="w"> </span>Broadcasting rules and examples
<span class="linenos">197</span><span class="k">-</span><span class="w"> </span>Numerical stability techniques (log-sum-exp, softmax)
<span class="linenos">198</span>
<span class="linenos">199</span><span class="gs">**Key concepts:**</span>
<span class="linenos">200</span><span class="k">-</span><span class="w"> </span>einsum notation: implicit summation over repeated indices
<span class="linenos">201</span><span class="k">-</span><span class="w"> </span>Common operations: matrix multiply, batch operations, trace, transpose
<span class="linenos">202</span><span class="k">-</span><span class="w"> </span>Broadcasting: automatic shape alignment for element-wise ops
<span class="linenos">203</span><span class="k">-</span><span class="w"> </span>Numerical stability: avoid overflow/underflow in exp/log
<span class="linenos">204</span>
<span class="linenos">205</span><span class="gs">**Output:**</span> Console output demonstrating einsum equivalences and timing comparisons
<span class="linenos">206</span>
<span class="linenos">207</span>---
<span class="linenos">208</span>
<span class="linenos">209</span><span class="gu">### 11_graph_spectral.py (372 lines)</span>
<span class="linenos">210</span><span class="gs">**Graph Theory and Spectral Graph Theory**</span>
<span class="linenos">211</span>
<span class="linenos">212</span>Demonstrates:
<span class="linenos">213</span><span class="k">-</span><span class="w"> </span>Graph matrix construction: adjacency, degree, Laplacian
<span class="linenos">214</span><span class="k">-</span><span class="w"> </span>Spectral decomposition of graph Laplacian
<span class="linenos">215</span><span class="k">-</span><span class="w"> </span>Spectral clustering algorithm
<span class="linenos">216</span><span class="k">-</span><span class="w"> </span>Simple Graph Neural Network (GNN) message passing
<span class="linenos">217</span><span class="k">-</span><span class="w"> </span>PageRank computation
<span class="linenos">218</span>
<span class="linenos">219</span><span class="gs">**Key concepts:**</span>
<span class="linenos">220</span><span class="k">-</span><span class="w"> </span>Laplacian eigenvalues/eigenvectors encode graph structure
<span class="linenos">221</span><span class="k">-</span><span class="w"> </span>Normalized Laplacian: L_norm = I - D^(-1/2) A D^(-1/2)
<span class="linenos">222</span><span class="k">-</span><span class="w"> </span>Spectral clustering uses eigenvectors for community detection
<span class="linenos">223</span><span class="k">-</span><span class="w"> </span>GNN message passing aggregates neighbor features
<span class="linenos">224</span>
<span class="linenos">225</span><span class="gs">**Output:**</span> <span class="sb">`graph_spectral.png`</span> - Graph visualization, spectral clustering, and PageRank
<span class="linenos">226</span>
<span class="linenos">227</span>---
<span class="linenos">228</span>
<span class="linenos">229</span><span class="gu">### 12_attention_math.py (419 lines)</span>
<span class="linenos">230</span><span class="gs">**Attention Mechanism Mathematics**</span>
<span class="linenos">231</span>
<span class="linenos">232</span>Demonstrates:
<span class="linenos">233</span><span class="k">-</span><span class="w"> </span>Scaled dot-product attention from scratch
<span class="linenos">234</span><span class="k">-</span><span class="w"> </span>Multi-head attention implementation
<span class="linenos">235</span><span class="k">-</span><span class="w"> </span>Positional encoding (sinusoidal)
<span class="linenos">236</span><span class="k">-</span><span class="w"> </span>Attention weight visualization
<span class="linenos">237</span><span class="k">-</span><span class="w"> </span>Comparison with PyTorch nn.MultiheadAttention
<span class="linenos">238</span>
<span class="linenos">239</span><span class="gs">**Key concepts:**</span>
<span class="linenos">240</span><span class="k">-</span><span class="w"> </span>Attention formula: softmax(Q @ K^T / sqrt(d_k)) @ V
<span class="linenos">241</span><span class="k">-</span><span class="w"> </span>Scaling factor sqrt(d_k) prevents softmax saturation
<span class="linenos">242</span><span class="k">-</span><span class="w"> </span>Multi-head attention: parallel attention with different projections
<span class="linenos">243</span><span class="k">-</span><span class="w"> </span>Positional encoding adds sequence order information to embeddings
<span class="linenos">244</span>
<span class="linenos">245</span><span class="gs">**Output:**</span> <span class="sb">`attention_weights.png`</span> - Heatmap visualization of attention weights
<span class="linenos">246</span>
<span class="linenos">247</span>---
<span class="linenos">248</span>
<span class="linenos">249</span><span class="gu">## Running the Examples</span>
<span class="linenos">250</span>
<span class="linenos">251</span>Each file is standalone and can be run independently:
<span class="linenos">252</span>
<span class="linenos">253</span><span class="sb">```bash</span>
<span class="linenos">254</span>python<span class="w"> </span>01_vector_matrix_ops.py
<span class="linenos">255</span>python<span class="w"> </span>02_svd_pca.py
<span class="linenos">256</span><span class="c1"># ... through 12_attention_math.py</span>
<span class="linenos">257</span><span class="sb">```</span>
<span class="linenos">258</span>
<span class="linenos">259</span><span class="gu">## Dependencies</span>
<span class="linenos">260</span>
<span class="linenos">261</span>All examples require:
<span class="linenos">262</span><span class="k">-</span><span class="w"> </span>numpy
<span class="linenos">263</span><span class="k">-</span><span class="w"> </span>matplotlib
<span class="linenos">264</span><span class="k">-</span><span class="w"> </span>torch (PyTorch)
<span class="linenos">265</span><span class="k">-</span><span class="w"> </span>sklearn (scikit-learn)
<span class="linenos">266</span><span class="k">-</span><span class="w"> </span>scipy
<span class="linenos">267</span>
<span class="linenos">268</span>Install with:
<span class="linenos">269</span><span class="sb">```bash</span>
<span class="linenos">270</span>pip<span class="w"> </span>install<span class="w"> </span>numpy<span class="w"> </span>matplotlib<span class="w"> </span>torch<span class="w"> </span>scikit-learn<span class="w"> </span>scipy
<span class="linenos">271</span><span class="sb">```</span>
<span class="linenos">272</span>
<span class="linenos">273</span><span class="gu">## Learning Path</span>
<span class="linenos">274</span>
<span class="linenos">275</span><span class="gs">**Recommended order:**</span>
<span class="linenos">276</span><span class="k">1.</span> <span class="sb">`01_vector_matrix_ops.py`</span> - Linear algebra fundamentals
<span class="linenos">277</span><span class="k">2.</span> <span class="sb">`02_svd_pca.py`</span> - Matrix factorization and dimensionality reduction
<span class="linenos">278</span><span class="k">3.</span> <span class="sb">`03_matrix_calculus_autograd.py`</span> - Gradients and automatic differentiation
<span class="linenos">279</span><span class="k">4.</span> <span class="sb">`04_norms_regularization.py`</span> - Norms and regularization
<span class="linenos">280</span><span class="k">5.</span> <span class="sb">`05_gradient_descent.py`</span> - Optimization algorithms
<span class="linenos">281</span><span class="k">6.</span> <span class="sb">`06_optimization_constrained.py`</span> - Constrained optimization
<span class="linenos">282</span><span class="k">7.</span> <span class="sb">`07_probability_distributions.py`</span> - Probability and inference
<span class="linenos">283</span><span class="k">8.</span> <span class="sb">`08_information_theory.py`</span> - Information theory for ML
<span class="linenos">284</span><span class="k">9.</span> <span class="sb">`09_mcmc_sampling.py`</span> - Sampling techniques
<span class="linenos">285</span><span class="k">10.</span> <span class="sb">`10_tensor_ops_einsum.py`</span> - Tensor operations and einsum
<span class="linenos">286</span><span class="k">11.</span> <span class="sb">`11_graph_spectral.py`</span> - Graph theory and spectral methods
<span class="linenos">287</span><span class="k">12.</span> <span class="sb">`12_attention_math.py`</span> - Attention mechanism mathematics
<span class="linenos">288</span>
<span class="linenos">289</span><span class="gu">## Output Files</span>
<span class="linenos">290</span>
<span class="linenos">291</span>Running the scripts will generate PNG visualizations:
<span class="linenos">292</span><span class="k">-</span><span class="w"> </span><span class="sb">`vector_ops.png`</span> - Vector addition and transformations
<span class="linenos">293</span><span class="k">-</span><span class="w"> </span><span class="sb">`pca_visualization.png`</span> - PCA projection and explained variance
<span class="linenos">294</span><span class="k">-</span><span class="w"> </span><span class="sb">`gradient_descent.png`</span> - Optimization trajectory
<span class="linenos">295</span><span class="k">-</span><span class="w"> </span><span class="sb">`unit_balls.png`</span> - Norm visualizations
<span class="linenos">296</span><span class="k">-</span><span class="w"> </span><span class="sb">`regularization_comparison.png`</span> - Regularization effects
<span class="linenos">297</span><span class="k">-</span><span class="w"> </span><span class="sb">`gradient_optimizers.png`</span> - Optimizer comparison
<span class="linenos">298</span><span class="k">-</span><span class="w"> </span><span class="sb">`constrained_optimization.png`</span> - Constrained optimization
<span class="linenos">299</span><span class="k">-</span><span class="w"> </span><span class="sb">`probability_distributions.png`</span> - Distribution PDFs
<span class="linenos">300</span><span class="k">-</span><span class="w"> </span><span class="sb">`information_theory.png`</span> - Entropy and KL divergence
<span class="linenos">301</span><span class="k">-</span><span class="w"> </span><span class="sb">`mcmc_sampling.png`</span> - Sampling methods
<span class="linenos">302</span><span class="k">-</span><span class="w"> </span><span class="sb">`graph_spectral.png`</span> - Graph spectral analysis
<span class="linenos">303</span><span class="k">-</span><span class="w"> </span><span class="sb">`attention_weights.png`</span> - Attention weight heatmaps
<span class="linenos">304</span>
<span class="linenos">305</span><span class="gu">## Notes</span>
<span class="linenos">306</span>
<span class="linenos">307</span><span class="k">-</span><span class="w"> </span>All examples include extensive comments explaining mathematical concepts
<span class="linenos">308</span><span class="k">-</span><span class="w"> </span>Print statements show intermediate results for learning
<span class="linenos">309</span><span class="k">-</span><span class="w"> </span>Visualizations are automatically saved to the current directory
<span class="linenos">310</span><span class="k">-</span><span class="w"> </span>Each file has a <span class="sb">`if __name__ == &quot;__main__&quot;:`</span> block for modularity
</code></pre></div>

    </div>
</article>

            </div>
        </main>
    </div>

    <script src="/study/static/js/app.js"></script>
    <script>
        function switchLanguage(newLang) {
            const path = window.location.pathname;
            const base = '/study';
            const currentLang = 'en';
            const newPath = path.replace(base + '/' + currentLang + '/', base + '/' + newLang + '/');
            window.location.href = newPath + window.location.search;
        }
    </script>
    
<script>
    document.getElementById('copy-code-btn').addEventListener('click', function() {
        var code = document.querySelector('.example-code .highlight');
        // Extract just the code text, not line numbers
        var lines = code.querySelectorAll('.code pre span.line');
        var text;
        if (lines.length > 0) {
            text = Array.from(lines).map(function(l) { return l.textContent; }).join('\n');
        } else {
            // Fallback: get all code text
            var codeEl = code.querySelector('pre code') || code.querySelector('pre');
            text = codeEl ? codeEl.textContent : code.textContent;
        }
        navigator.clipboard.writeText(text);
        this.textContent = 'Copied!';
        var btn = this;
        setTimeout(function() { btn.textContent = 'Copy code'; }, 2000);
    });
</script>

</body>
</html>
{% endraw %}