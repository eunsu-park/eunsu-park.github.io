{% raw %}
<!DOCTYPE html>
<html lang="en" data-theme="light">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>README.md - Examples</title>
    <link rel="stylesheet" href="/study/static/css/style.css">
    <link rel="stylesheet" href="/study/static/css/highlight.css">
    <!-- KaTeX for LaTeX math rendering -->
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.11/dist/katex.min.css">
    <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.11/dist/katex.min.js"></script>
    <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.11/dist/contrib/auto-render.min.js"
        onload="renderMathInElement(document.body, {
            delimiters: [
                {left: '$$', right: '$$', display: true},
                {left: '$', right: '$', display: false}
            ],
            throwOnError: false
        });"></script>
    
</head>
<body>
    <div class="app-container">
        <!-- Sidebar -->
        <aside class="sidebar" id="sidebar">
            <div class="sidebar-header">
                <a href="/study/en/" class="logo">Study Materials</a>
                <button class="sidebar-toggle" id="sidebar-toggle" aria-label="Toggle sidebar">
                    <span></span>
                </button>
            </div>

            <nav class="sidebar-nav">
                <a href="/study/en/" class="nav-item ">
                    <span class="nav-icon">üè†</span>
                    <span class="nav-text">Home</span>
                </a>
                <a href="/study/examples/" class="nav-item active">
                    <span class="nav-icon">üíª</span>
                    <span class="nav-text">Examples</span>
                </a>
            </nav>

            <div class="sidebar-search">
                <form action="/study/en/search.html" method="get" id="search-form">
                    <input type="search" name="q" placeholder="Search..." id="search-sidebar-input">
                </form>
            </div>

            <!-- Language Selector -->
            <div class="sidebar-lang">
                <select id="lang-select" class="lang-selector" onchange="switchLanguage(this.value)">
                    
                    <option value="en" selected>
                        English
                    </option>
                    
                    <option value="ko" >
                        ÌïúÍµ≠Ïñ¥
                    </option>
                    
                </select>
            </div>

            <div class="sidebar-footer">
                <button class="theme-toggle" id="theme-toggle" aria-label="Toggle theme">
                    <span class="theme-icon light">‚òÄÔ∏è</span>
                    <span class="theme-icon dark">üåô</span>
                </button>
            </div>
        </aside>

        <!-- Main Content -->
        <main class="main-content">
            <header class="main-header">
                <button class="menu-toggle" id="menu-toggle" aria-label="Open menu">
                    <span></span>
                </button>
                
<nav class="breadcrumb">
    <a href="/study/examples/">Examples</a>
    <span class="separator">/</span>
    <a href="/study/examples/Math_for_AI/">Math for AI</a>
    <span class="separator">/</span>
    <span class="current">README.md</span>
</nav>

            </header>

            <div class="content">
                
<article class="example-article">
    <header class="example-header">
        <h1>README.md</h1>
        <div class="example-actions">
            <a href="README.md" download class="btn">Download</a>
            <button class="btn" id="copy-code-btn">Copy code</button>
        </div>
    </header>

    <div class="example-meta">
        <span>markdown</span>
        <span>311 lines</span>
        <span>9.7 KB</span>
    </div>

    <div class="example-code markdown-body">
        <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span class="normal">  1</span>
<span class="normal">  2</span>
<span class="normal">  3</span>
<span class="normal">  4</span>
<span class="normal">  5</span>
<span class="normal">  6</span>
<span class="normal">  7</span>
<span class="normal">  8</span>
<span class="normal">  9</span>
<span class="normal"> 10</span>
<span class="normal"> 11</span>
<span class="normal"> 12</span>
<span class="normal"> 13</span>
<span class="normal"> 14</span>
<span class="normal"> 15</span>
<span class="normal"> 16</span>
<span class="normal"> 17</span>
<span class="normal"> 18</span>
<span class="normal"> 19</span>
<span class="normal"> 20</span>
<span class="normal"> 21</span>
<span class="normal"> 22</span>
<span class="normal"> 23</span>
<span class="normal"> 24</span>
<span class="normal"> 25</span>
<span class="normal"> 26</span>
<span class="normal"> 27</span>
<span class="normal"> 28</span>
<span class="normal"> 29</span>
<span class="normal"> 30</span>
<span class="normal"> 31</span>
<span class="normal"> 32</span>
<span class="normal"> 33</span>
<span class="normal"> 34</span>
<span class="normal"> 35</span>
<span class="normal"> 36</span>
<span class="normal"> 37</span>
<span class="normal"> 38</span>
<span class="normal"> 39</span>
<span class="normal"> 40</span>
<span class="normal"> 41</span>
<span class="normal"> 42</span>
<span class="normal"> 43</span>
<span class="normal"> 44</span>
<span class="normal"> 45</span>
<span class="normal"> 46</span>
<span class="normal"> 47</span>
<span class="normal"> 48</span>
<span class="normal"> 49</span>
<span class="normal"> 50</span>
<span class="normal"> 51</span>
<span class="normal"> 52</span>
<span class="normal"> 53</span>
<span class="normal"> 54</span>
<span class="normal"> 55</span>
<span class="normal"> 56</span>
<span class="normal"> 57</span>
<span class="normal"> 58</span>
<span class="normal"> 59</span>
<span class="normal"> 60</span>
<span class="normal"> 61</span>
<span class="normal"> 62</span>
<span class="normal"> 63</span>
<span class="normal"> 64</span>
<span class="normal"> 65</span>
<span class="normal"> 66</span>
<span class="normal"> 67</span>
<span class="normal"> 68</span>
<span class="normal"> 69</span>
<span class="normal"> 70</span>
<span class="normal"> 71</span>
<span class="normal"> 72</span>
<span class="normal"> 73</span>
<span class="normal"> 74</span>
<span class="normal"> 75</span>
<span class="normal"> 76</span>
<span class="normal"> 77</span>
<span class="normal"> 78</span>
<span class="normal"> 79</span>
<span class="normal"> 80</span>
<span class="normal"> 81</span>
<span class="normal"> 82</span>
<span class="normal"> 83</span>
<span class="normal"> 84</span>
<span class="normal"> 85</span>
<span class="normal"> 86</span>
<span class="normal"> 87</span>
<span class="normal"> 88</span>
<span class="normal"> 89</span>
<span class="normal"> 90</span>
<span class="normal"> 91</span>
<span class="normal"> 92</span>
<span class="normal"> 93</span>
<span class="normal"> 94</span>
<span class="normal"> 95</span>
<span class="normal"> 96</span>
<span class="normal"> 97</span>
<span class="normal"> 98</span>
<span class="normal"> 99</span>
<span class="normal">100</span>
<span class="normal">101</span>
<span class="normal">102</span>
<span class="normal">103</span>
<span class="normal">104</span>
<span class="normal">105</span>
<span class="normal">106</span>
<span class="normal">107</span>
<span class="normal">108</span>
<span class="normal">109</span>
<span class="normal">110</span>
<span class="normal">111</span>
<span class="normal">112</span>
<span class="normal">113</span>
<span class="normal">114</span>
<span class="normal">115</span>
<span class="normal">116</span>
<span class="normal">117</span>
<span class="normal">118</span>
<span class="normal">119</span>
<span class="normal">120</span>
<span class="normal">121</span>
<span class="normal">122</span>
<span class="normal">123</span>
<span class="normal">124</span>
<span class="normal">125</span>
<span class="normal">126</span>
<span class="normal">127</span>
<span class="normal">128</span>
<span class="normal">129</span>
<span class="normal">130</span>
<span class="normal">131</span>
<span class="normal">132</span>
<span class="normal">133</span>
<span class="normal">134</span>
<span class="normal">135</span>
<span class="normal">136</span>
<span class="normal">137</span>
<span class="normal">138</span>
<span class="normal">139</span>
<span class="normal">140</span>
<span class="normal">141</span>
<span class="normal">142</span>
<span class="normal">143</span>
<span class="normal">144</span>
<span class="normal">145</span>
<span class="normal">146</span>
<span class="normal">147</span>
<span class="normal">148</span>
<span class="normal">149</span>
<span class="normal">150</span>
<span class="normal">151</span>
<span class="normal">152</span>
<span class="normal">153</span>
<span class="normal">154</span>
<span class="normal">155</span>
<span class="normal">156</span>
<span class="normal">157</span>
<span class="normal">158</span>
<span class="normal">159</span>
<span class="normal">160</span>
<span class="normal">161</span>
<span class="normal">162</span>
<span class="normal">163</span>
<span class="normal">164</span>
<span class="normal">165</span>
<span class="normal">166</span>
<span class="normal">167</span>
<span class="normal">168</span>
<span class="normal">169</span>
<span class="normal">170</span>
<span class="normal">171</span>
<span class="normal">172</span>
<span class="normal">173</span>
<span class="normal">174</span>
<span class="normal">175</span>
<span class="normal">176</span>
<span class="normal">177</span>
<span class="normal">178</span>
<span class="normal">179</span>
<span class="normal">180</span>
<span class="normal">181</span>
<span class="normal">182</span>
<span class="normal">183</span>
<span class="normal">184</span>
<span class="normal">185</span>
<span class="normal">186</span>
<span class="normal">187</span>
<span class="normal">188</span>
<span class="normal">189</span>
<span class="normal">190</span>
<span class="normal">191</span>
<span class="normal">192</span>
<span class="normal">193</span>
<span class="normal">194</span>
<span class="normal">195</span>
<span class="normal">196</span>
<span class="normal">197</span>
<span class="normal">198</span>
<span class="normal">199</span>
<span class="normal">200</span>
<span class="normal">201</span>
<span class="normal">202</span>
<span class="normal">203</span>
<span class="normal">204</span>
<span class="normal">205</span>
<span class="normal">206</span>
<span class="normal">207</span>
<span class="normal">208</span>
<span class="normal">209</span>
<span class="normal">210</span>
<span class="normal">211</span>
<span class="normal">212</span>
<span class="normal">213</span>
<span class="normal">214</span>
<span class="normal">215</span>
<span class="normal">216</span>
<span class="normal">217</span>
<span class="normal">218</span>
<span class="normal">219</span>
<span class="normal">220</span>
<span class="normal">221</span>
<span class="normal">222</span>
<span class="normal">223</span>
<span class="normal">224</span>
<span class="normal">225</span>
<span class="normal">226</span>
<span class="normal">227</span>
<span class="normal">228</span>
<span class="normal">229</span>
<span class="normal">230</span>
<span class="normal">231</span>
<span class="normal">232</span>
<span class="normal">233</span>
<span class="normal">234</span>
<span class="normal">235</span>
<span class="normal">236</span>
<span class="normal">237</span>
<span class="normal">238</span>
<span class="normal">239</span>
<span class="normal">240</span>
<span class="normal">241</span>
<span class="normal">242</span>
<span class="normal">243</span>
<span class="normal">244</span>
<span class="normal">245</span>
<span class="normal">246</span>
<span class="normal">247</span>
<span class="normal">248</span>
<span class="normal">249</span>
<span class="normal">250</span>
<span class="normal">251</span>
<span class="normal">252</span>
<span class="normal">253</span>
<span class="normal">254</span>
<span class="normal">255</span>
<span class="normal">256</span>
<span class="normal">257</span>
<span class="normal">258</span>
<span class="normal">259</span>
<span class="normal">260</span>
<span class="normal">261</span>
<span class="normal">262</span>
<span class="normal">263</span>
<span class="normal">264</span>
<span class="normal">265</span>
<span class="normal">266</span>
<span class="normal">267</span>
<span class="normal">268</span>
<span class="normal">269</span>
<span class="normal">270</span>
<span class="normal">271</span>
<span class="normal">272</span>
<span class="normal">273</span>
<span class="normal">274</span>
<span class="normal">275</span>
<span class="normal">276</span>
<span class="normal">277</span>
<span class="normal">278</span>
<span class="normal">279</span>
<span class="normal">280</span>
<span class="normal">281</span>
<span class="normal">282</span>
<span class="normal">283</span>
<span class="normal">284</span>
<span class="normal">285</span>
<span class="normal">286</span>
<span class="normal">287</span>
<span class="normal">288</span>
<span class="normal">289</span>
<span class="normal">290</span>
<span class="normal">291</span>
<span class="normal">292</span>
<span class="normal">293</span>
<span class="normal">294</span>
<span class="normal">295</span>
<span class="normal">296</span>
<span class="normal">297</span>
<span class="normal">298</span>
<span class="normal">299</span>
<span class="normal">300</span>
<span class="normal">301</span>
<span class="normal">302</span>
<span class="normal">303</span>
<span class="normal">304</span>
<span class="normal">305</span>
<span class="normal">306</span>
<span class="normal">307</span>
<span class="normal">308</span>
<span class="normal">309</span>
<span class="normal">310</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="gh"># Math for AI - Example Code</span>

This directory contains Python examples demonstrating mathematical concepts used in AI/ML.

<span class="gu">## Files</span>

<span class="gu">### 01_vector_matrix_ops.py (283 lines)</span>
<span class="gs">**Linear Algebra Fundamentals**</span>

Demonstrates:
<span class="k">-</span><span class="w"> </span>Vector space operations: basis, span, linear independence
<span class="k">-</span><span class="w"> </span>Matrix operations: multiplication, transpose, inverse
<span class="k">-</span><span class="w"> </span>Rank computation and properties
<span class="k">-</span><span class="w"> </span>ML application: feature matrices and weight matrices
<span class="k">-</span><span class="w"> </span>Visualization of vector addition and linear transformations

<span class="gs">**Key concepts:**</span>
<span class="k">-</span><span class="w"> </span>Standard basis vectors in R^n
<span class="k">-</span><span class="w"> </span>Linear independence checking via rank
<span class="k">-</span><span class="w"> </span>Matrix multiplication and properties
<span class="k">-</span><span class="w"> </span>Feature vectors as matrix rows in ML

<span class="gs">**Output:**</span> <span class="sb">`vector_ops.png`</span> - Vector operations visualization

---

<span class="gu">### 02_svd_pca.py (302 lines)</span>
<span class="gs">**SVD and Principal Component Analysis**</span>

Demonstrates:
<span class="k">-</span><span class="w"> </span>Singular Value Decomposition (SVD) with NumPy
<span class="k">-</span><span class="w"> </span>Low-rank matrix approximation
<span class="k">-</span><span class="w"> </span>PCA implementation from scratch (centering ‚Üí covariance ‚Üí eigen)
<span class="k">-</span><span class="w"> </span>Comparison with sklearn PCA
<span class="k">-</span><span class="w"> </span>Application to Iris dataset
<span class="k">-</span><span class="w"> </span>Explained variance visualization

<span class="gs">**Key concepts:**</span>
<span class="k">-</span><span class="w"> </span>SVD decomposition: A = U @ S @ V^T
<span class="k">-</span><span class="w"> </span>Relationship between SVD and PCA
<span class="k">-</span><span class="w"> </span>Principal components as eigenvectors
<span class="k">-</span><span class="w"> </span>Dimensionality reduction in practice

<span class="gs">**Output:**</span> <span class="sb">`pca_visualization.png`</span> - PCA results on Iris dataset

---

<span class="gu">### 03_matrix_calculus_autograd.py (408 lines)</span>
<span class="gs">**Matrix Calculus and Automatic Differentiation**</span>

Demonstrates:
<span class="k">-</span><span class="w"> </span>Manual gradient computation for scalar and vector functions
<span class="k">-</span><span class="w"> </span>Jacobian matrix computation (vector ‚Üí vector)
<span class="k">-</span><span class="w"> </span>Hessian matrix computation (second derivatives)
<span class="k">-</span><span class="w"> </span>PyTorch autograd comparison
<span class="k">-</span><span class="w"> </span>Numerical gradient checking
<span class="k">-</span><span class="w"> </span>MSE loss gradients for linear regression

<span class="gs">**Key concepts:**</span>
<span class="k">-</span><span class="w"> </span>Partial derivatives and gradient vectors
<span class="k">-</span><span class="w"> </span>Jacobian for vector-valued functions
<span class="k">-</span><span class="w"> </span>Hessian for convexity analysis
<span class="k">-</span><span class="w"> </span>Automatic differentiation with PyTorch
<span class="k">-</span><span class="w"> </span>Gradient descent optimization

<span class="gs">**Output:**</span> <span class="sb">`gradient_descent.png`</span> - Gradient descent trajectory visualization

---

<span class="gu">### 04_norms_regularization.py (400 lines)</span>
<span class="gs">**Norms, Distances, and Regularization**</span>

Demonstrates:
<span class="k">-</span><span class="w"> </span>Lp norms (L1, L2, L‚àû) and their properties
<span class="k">-</span><span class="w"> </span>Distance metrics: Euclidean, Manhattan, Cosine, Mahalanobis
<span class="k">-</span><span class="w"> </span>L1 vs L2 regularization in linear regression
<span class="k">-</span><span class="w"> </span>Sparsity-inducing property of L1
<span class="k">-</span><span class="w"> </span>Unit ball visualization for different norms

<span class="gs">**Key concepts:**</span>
<span class="k">-</span><span class="w"> </span>Norm properties: non-negativity, homogeneity, triangle inequality
<span class="k">-</span><span class="w"> </span>L1 (Lasso) produces sparse solutions
<span class="k">-</span><span class="w"> </span>L2 (Ridge) shrinks coefficients smoothly
<span class="k">-</span><span class="w"> </span>Feature selection vs feature shrinkage
<span class="k">-</span><span class="w"> </span>Regularization paths

<span class="gs">**Output:**</span>
<span class="k">-</span><span class="w"> </span><span class="sb">`unit_balls.png`</span> - Unit ball shapes for L1, L2, L‚àû
<span class="k">-</span><span class="w"> </span><span class="sb">`regularization_comparison.png`</span> - L1 vs L2 effects

---

<span class="gu">### 05_gradient_descent.py (271 lines)</span>
<span class="gs">**Gradient Descent Optimization Algorithms**</span>

Demonstrates:
<span class="k">-</span><span class="w"> </span>Basic Gradient Descent (GD) optimizer
<span class="k">-</span><span class="w"> </span>Stochastic Gradient Descent (SGD)
<span class="k">-</span><span class="w"> </span>Momentum-based optimization
<span class="k">-</span><span class="w"> </span>Adam optimizer from scratch
<span class="k">-</span><span class="w"> </span>Optimization of Rosenbrock function (non-convex with narrow valley)

<span class="gs">**Key concepts:**</span>
<span class="k">-</span><span class="w"> </span>Vanilla gradient descent updates: Œ∏ = Œ∏ - lr * ‚àáŒ∏
<span class="k">-</span><span class="w"> </span>Momentum accumulation for acceleration
<span class="k">-</span><span class="w"> </span>Adaptive learning rates with Adam
<span class="k">-</span><span class="w"> </span>Comparison of convergence behavior across optimizers

<span class="gs">**Output:**</span> <span class="sb">`gradient_optimizers.png`</span> - Optimization trajectories for different algorithms

---

<span class="gu">### 06_optimization_constrained.py (320 lines)</span>
<span class="gs">**Constrained and Unconstrained Optimization**</span>

Demonstrates:
<span class="k">-</span><span class="w"> </span>Unconstrained optimization with scipy.optimize methods (BFGS, CG, Newton-CG, L-BFGS-B)
<span class="k">-</span><span class="w"> </span>Lagrange multipliers for equality constraints
<span class="k">-</span><span class="w"> </span>KKT conditions for inequality constraints
<span class="k">-</span><span class="w"> </span>Convex vs non-convex optimization comparison

<span class="gs">**Key concepts:**</span>
<span class="k">-</span><span class="w"> </span>Analytical vs numerical optimization
<span class="k">-</span><span class="w"> </span>Lagrangian formulation for constraints
<span class="k">-</span><span class="w"> </span>First-order (gradient) and second-order (Hessian) methods
<span class="k">-</span><span class="w"> </span>Constraint handling in optimization problems

<span class="gs">**Output:**</span> <span class="sb">`constrained_optimization.png`</span> - Constraint visualization and optimization paths

---

<span class="gu">### 07_probability_distributions.py (367 lines)</span>
<span class="gs">**Probability Distributions and Statistical Inference**</span>

Demonstrates:
<span class="k">-</span><span class="w"> </span>Common probability distributions: Gaussian, Bernoulli, Poisson, Exponential
<span class="k">-</span><span class="w"> </span>Maximum Likelihood Estimation (MLE) for Gaussian parameters
<span class="k">-</span><span class="w"> </span>Maximum A Posteriori (MAP) estimation with Gaussian prior
<span class="k">-</span><span class="w"> </span>Bayesian update visualization

<span class="gs">**Key concepts:**</span>
<span class="k">-</span><span class="w"> </span>PDF/PMF properties for different distributions
<span class="k">-</span><span class="w"> </span>MLE: argmax_Œ∏ P(data | Œ∏)
<span class="k">-</span><span class="w"> </span>MAP: argmax_Œ∏ P(Œ∏ | data) = argmax_Œ∏ P(data | Œ∏) P(Œ∏)
<span class="k">-</span><span class="w"> </span>Prior √ó Likelihood = Posterior (Bayes&#39; rule)

<span class="gs">**Output:**</span> <span class="sb">`probability_distributions.png`</span> - Distribution PDFs and Bayesian inference

---

<span class="gu">### 08_information_theory.py (462 lines)</span>
<span class="gs">**Information Theory for Machine Learning**</span>

Demonstrates:
<span class="k">-</span><span class="w"> </span>Entropy as measure of uncertainty: H(X) = -Œ£ p(x) log p(x)
<span class="k">-</span><span class="w"> </span>Cross-entropy and KL divergence
<span class="k">-</span><span class="w"> </span>Mutual information between variables
<span class="k">-</span><span class="w"> </span>Connection to ML loss functions (cross-entropy loss)
<span class="k">-</span><span class="w"> </span>ELBO (Evidence Lower Bound) visualization for VAEs

<span class="gs">**Key concepts:**</span>
<span class="k">-</span><span class="w"> </span>Maximum entropy for uniform distributions
<span class="k">-</span><span class="w"> </span>KL divergence as measure of distribution difference
<span class="k">-</span><span class="w"> </span>Cross-entropy = Entropy + KL divergence
<span class="k">-</span><span class="w"> </span>ELBO = log p(x) - KL(q||p) used in variational inference

<span class="gs">**Output:**</span> <span class="sb">`information_theory.png`</span> - Entropy, KL divergence, and ELBO visualization

---

<span class="gu">### 09_mcmc_sampling.py (308 lines)</span>
<span class="gs">**MCMC Sampling and Advanced Sampling Techniques**</span>

Demonstrates:
<span class="k">-</span><span class="w"> </span>Rejection sampling from target distribution
<span class="k">-</span><span class="w"> </span>Importance sampling for expectation estimation
<span class="k">-</span><span class="w"> </span>Metropolis-Hastings MCMC algorithm
<span class="k">-</span><span class="w"> </span>Reparameterization trick (VAE-style)

<span class="gs">**Key concepts:**</span>
<span class="k">-</span><span class="w"> </span>Rejection sampling: accept/reject based on proposal distribution
<span class="k">-</span><span class="w"> </span>Importance sampling: weighted samples for expectation
<span class="k">-</span><span class="w"> </span>MCMC: Markov chain converging to target distribution
<span class="k">-</span><span class="w"> </span>Reparameterization: z = Œº + œÉ * Œµ for gradient flow in VAEs

<span class="gs">**Output:**</span> <span class="sb">`mcmc_sampling.png`</span> - Sampling method comparisons and MCMC convergence

---

<span class="gu">### 10_tensor_ops_einsum.py (298 lines)</span>
<span class="gs">**Tensor Operations and Einstein Summation**</span>

Demonstrates:
<span class="k">-</span><span class="w"> </span>Tensor creation and manipulation in NumPy and PyTorch
<span class="k">-</span><span class="w"> </span>Einstein summation notation (einsum) for efficient operations
<span class="k">-</span><span class="w"> </span>Broadcasting rules and examples
<span class="k">-</span><span class="w"> </span>Numerical stability techniques (log-sum-exp, softmax)

<span class="gs">**Key concepts:**</span>
<span class="k">-</span><span class="w"> </span>einsum notation: implicit summation over repeated indices
<span class="k">-</span><span class="w"> </span>Common operations: matrix multiply, batch operations, trace, transpose
<span class="k">-</span><span class="w"> </span>Broadcasting: automatic shape alignment for element-wise ops
<span class="k">-</span><span class="w"> </span>Numerical stability: avoid overflow/underflow in exp/log

<span class="gs">**Output:**</span> Console output demonstrating einsum equivalences and timing comparisons

---

<span class="gu">### 11_graph_spectral.py (372 lines)</span>
<span class="gs">**Graph Theory and Spectral Graph Theory**</span>

Demonstrates:
<span class="k">-</span><span class="w"> </span>Graph matrix construction: adjacency, degree, Laplacian
<span class="k">-</span><span class="w"> </span>Spectral decomposition of graph Laplacian
<span class="k">-</span><span class="w"> </span>Spectral clustering algorithm
<span class="k">-</span><span class="w"> </span>Simple Graph Neural Network (GNN) message passing
<span class="k">-</span><span class="w"> </span>PageRank computation

<span class="gs">**Key concepts:**</span>
<span class="k">-</span><span class="w"> </span>Laplacian eigenvalues/eigenvectors encode graph structure
<span class="k">-</span><span class="w"> </span>Normalized Laplacian: L_norm = I - D^(-1/2) A D^(-1/2)
<span class="k">-</span><span class="w"> </span>Spectral clustering uses eigenvectors for community detection
<span class="k">-</span><span class="w"> </span>GNN message passing aggregates neighbor features

<span class="gs">**Output:**</span> <span class="sb">`graph_spectral.png`</span> - Graph visualization, spectral clustering, and PageRank

---

<span class="gu">### 12_attention_math.py (419 lines)</span>
<span class="gs">**Attention Mechanism Mathematics**</span>

Demonstrates:
<span class="k">-</span><span class="w"> </span>Scaled dot-product attention from scratch
<span class="k">-</span><span class="w"> </span>Multi-head attention implementation
<span class="k">-</span><span class="w"> </span>Positional encoding (sinusoidal)
<span class="k">-</span><span class="w"> </span>Attention weight visualization
<span class="k">-</span><span class="w"> </span>Comparison with PyTorch nn.MultiheadAttention

<span class="gs">**Key concepts:**</span>
<span class="k">-</span><span class="w"> </span>Attention formula: softmax(Q @ K^T / sqrt(d_k)) @ V
<span class="k">-</span><span class="w"> </span>Scaling factor sqrt(d_k) prevents softmax saturation
<span class="k">-</span><span class="w"> </span>Multi-head attention: parallel attention with different projections
<span class="k">-</span><span class="w"> </span>Positional encoding adds sequence order information to embeddings

<span class="gs">**Output:**</span> <span class="sb">`attention_weights.png`</span> - Heatmap visualization of attention weights

---

<span class="gu">## Running the Examples</span>

Each file is standalone and can be run independently:

<span class="sb">```bash</span>
python<span class="w"> </span>01_vector_matrix_ops.py
python<span class="w"> </span>02_svd_pca.py
<span class="c1"># ... through 12_attention_math.py</span>
<span class="sb">```</span>

<span class="gu">## Dependencies</span>

All examples require:
<span class="k">-</span><span class="w"> </span>numpy
<span class="k">-</span><span class="w"> </span>matplotlib
<span class="k">-</span><span class="w"> </span>torch (PyTorch)
<span class="k">-</span><span class="w"> </span>sklearn (scikit-learn)
<span class="k">-</span><span class="w"> </span>scipy

Install with:
<span class="sb">```bash</span>
pip<span class="w"> </span>install<span class="w"> </span>numpy<span class="w"> </span>matplotlib<span class="w"> </span>torch<span class="w"> </span>scikit-learn<span class="w"> </span>scipy
<span class="sb">```</span>

<span class="gu">## Learning Path</span>

<span class="gs">**Recommended order:**</span>
<span class="k">1.</span> <span class="sb">`01_vector_matrix_ops.py`</span> - Linear algebra fundamentals
<span class="k">2.</span> <span class="sb">`02_svd_pca.py`</span> - Matrix factorization and dimensionality reduction
<span class="k">3.</span> <span class="sb">`03_matrix_calculus_autograd.py`</span> - Gradients and automatic differentiation
<span class="k">4.</span> <span class="sb">`04_norms_regularization.py`</span> - Norms and regularization
<span class="k">5.</span> <span class="sb">`05_gradient_descent.py`</span> - Optimization algorithms
<span class="k">6.</span> <span class="sb">`06_optimization_constrained.py`</span> - Constrained optimization
<span class="k">7.</span> <span class="sb">`07_probability_distributions.py`</span> - Probability and inference
<span class="k">8.</span> <span class="sb">`08_information_theory.py`</span> - Information theory for ML
<span class="k">9.</span> <span class="sb">`09_mcmc_sampling.py`</span> - Sampling techniques
<span class="k">10.</span> <span class="sb">`10_tensor_ops_einsum.py`</span> - Tensor operations and einsum
<span class="k">11.</span> <span class="sb">`11_graph_spectral.py`</span> - Graph theory and spectral methods
<span class="k">12.</span> <span class="sb">`12_attention_math.py`</span> - Attention mechanism mathematics

<span class="gu">## Output Files</span>

Running the scripts will generate PNG visualizations:
<span class="k">-</span><span class="w"> </span><span class="sb">`vector_ops.png`</span> - Vector addition and transformations
<span class="k">-</span><span class="w"> </span><span class="sb">`pca_visualization.png`</span> - PCA projection and explained variance
<span class="k">-</span><span class="w"> </span><span class="sb">`gradient_descent.png`</span> - Optimization trajectory
<span class="k">-</span><span class="w"> </span><span class="sb">`unit_balls.png`</span> - Norm visualizations
<span class="k">-</span><span class="w"> </span><span class="sb">`regularization_comparison.png`</span> - Regularization effects
<span class="k">-</span><span class="w"> </span><span class="sb">`gradient_optimizers.png`</span> - Optimizer comparison
<span class="k">-</span><span class="w"> </span><span class="sb">`constrained_optimization.png`</span> - Constrained optimization
<span class="k">-</span><span class="w"> </span><span class="sb">`probability_distributions.png`</span> - Distribution PDFs
<span class="k">-</span><span class="w"> </span><span class="sb">`information_theory.png`</span> - Entropy and KL divergence
<span class="k">-</span><span class="w"> </span><span class="sb">`mcmc_sampling.png`</span> - Sampling methods
<span class="k">-</span><span class="w"> </span><span class="sb">`graph_spectral.png`</span> - Graph spectral analysis
<span class="k">-</span><span class="w"> </span><span class="sb">`attention_weights.png`</span> - Attention weight heatmaps

<span class="gu">## Notes</span>

<span class="k">-</span><span class="w"> </span>All examples include extensive comments explaining mathematical concepts
<span class="k">-</span><span class="w"> </span>Print statements show intermediate results for learning
<span class="k">-</span><span class="w"> </span>Visualizations are automatically saved to the current directory
<span class="k">-</span><span class="w"> </span>Each file has a <span class="sb">`if __name__ == &quot;__main__&quot;:`</span> block for modularity
</code></pre></div></td></tr></table></div>

    </div>
</article>

            </div>
        </main>
    </div>

    <script src="/study/static/js/app.js"></script>
    <script>
        function switchLanguage(newLang) {
            const path = window.location.pathname;
            const base = '/study';
            const currentLang = 'en';
            const newPath = path.replace(base + '/' + currentLang + '/', base + '/' + newLang + '/');
            window.location.href = newPath + window.location.search;
        }
    </script>
    
<script>
    document.getElementById('copy-code-btn').addEventListener('click', function() {
        var code = document.querySelector('.example-code .highlight');
        // Extract just the code text, not line numbers
        var lines = code.querySelectorAll('.code pre span.line');
        var text;
        if (lines.length > 0) {
            text = Array.from(lines).map(function(l) { return l.textContent; }).join('\n');
        } else {
            // Fallback: get all code text
            var codeEl = code.querySelector('pre code') || code.querySelector('pre');
            text = codeEl ? codeEl.textContent : code.textContent;
        }
        navigator.clipboard.writeText(text);
        this.textContent = 'Copied!';
        var btn = this;
        setTimeout(function() { btn.textContent = 'Copy code'; }, 2000);
    });
</script>

</body>
</html>
{% endraw %}