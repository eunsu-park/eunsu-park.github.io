{% raw %}
<!DOCTYPE html>
<html lang="ko" data-theme="light">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>25. ì˜µí‹°ë§ˆì´ì €(Optimizers) - Study Materials</title>
    <link rel="stylesheet" href="/study/static/css/style.css">
    <link rel="stylesheet" href="/study/static/css/highlight.css">
    <!-- KaTeX for LaTeX math rendering -->
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.11/dist/katex.min.css">
    <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.11/dist/katex.min.js"></script>
    <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.11/dist/contrib/auto-render.min.js"
        onload="renderMathInElement(document.body, {
            delimiters: [
                {left: '$$', right: '$$', display: true},
                {left: '$', right: '$', display: false}
            ],
            throwOnError: false
        });"></script>
    
</head>
<body>
    <div class="app-container">
        <!-- Sidebar -->
        <aside class="sidebar" id="sidebar">
            <div class="sidebar-header">
                <a href="/study/ko/" class="logo">Study Materials</a>
                <button class="sidebar-toggle" id="sidebar-toggle" aria-label="Toggle sidebar">
                    <span></span>
                </button>
            </div>

            <nav class="sidebar-nav">
                <a href="/study/ko/" class="nav-item ">
                    <span class="nav-icon">ğŸ </span>
                    <span class="nav-text">Home</span>
                </a>
                <a href="/study/examples/" class="nav-item ">
                    <span class="nav-icon">ğŸ’»</span>
                    <span class="nav-text">Examples</span>
                </a>
            </nav>

            <div class="sidebar-search">
                <form action="/study/ko/search.html" method="get" id="search-form">
                    <input type="search" name="q" placeholder="Search..." id="search-sidebar-input">
                </form>
            </div>

            <!-- Language Selector -->
            <div class="sidebar-lang">
                <select id="lang-select" class="lang-selector" onchange="switchLanguage(this.value)">
                    
                    <option value="en" >
                        English
                    </option>
                    
                    <option value="ko" selected>
                        í•œêµ­ì–´
                    </option>
                    
                </select>
            </div>

            <div class="sidebar-footer">
                <button class="theme-toggle" id="theme-toggle" aria-label="Toggle theme">
                    <span class="theme-icon light">â˜€ï¸</span>
                    <span class="theme-icon dark">ğŸŒ™</span>
                </button>
            </div>
        </aside>

        <!-- Main Content -->
        <main class="main-content">
            <header class="main-header">
                <button class="menu-toggle" id="menu-toggle" aria-label="Open menu">
                    <span></span>
                </button>
                
<nav class="breadcrumb">
    <a href="/study/ko/">Topics</a>
    <span class="separator">/</span>
    <a href="/study/ko/Deep_Learning/">Deep Learning</a>
    <span class="separator">/</span>
    <span class="current">25. ì˜µí‹°ë§ˆì´ì €(Optimizers)</span>
</nav>

            </header>

            <div class="content">
                
<article class="lesson-article">
    <header class="lesson-header">
        <h1>25. ì˜µí‹°ë§ˆì´ì €(Optimizers)</h1>
    </header>

    
<div class="lesson-toolbar lesson-toolbar--top">
    <div class="toolbar-nav toolbar-nav--prev">
        
        <a href="/study/ko/Deep_Learning/24_Loss_Functions.html" class="nav-prev">
            <span class="nav-label">Previous</span>
            <span class="nav-title">24. ì†ì‹¤ í•¨ìˆ˜(Loss Functions)</span>
        </a>
        
    </div>
    <div class="toolbar-actions">
        <button class="btn btn-copy-link" data-action="copy-link" title="Copy link">
            <span class="icon">ğŸ”—</span>
            <span class="text">Copy link</span>
        </button>
        <a href="/study/ko/Deep_Learning/" class="btn btn-topic-link" title="Back to topic list">
            <span class="icon">ğŸ“‹</span>
            <span class="text">Topic list</span>
        </a>
    </div>
    <div class="toolbar-nav toolbar-nav--next">
        
        <a href="/study/ko/Deep_Learning/26_Normalization_Layers.html" class="nav-next">
            <span class="nav-label">Next</span>
            <span class="nav-title">26. ì •ê·œí™” ë ˆì´ì–´(Normalization Layers)</span>
        </a>
        
    </div>
</div>


    
    <nav class="toc" id="toc">
        <h2>Table of Contents</h2>
        <div class="toc">
<ul>
<li><a href="#_1">í•™ìŠµ ëª©í‘œ</a></li>
<li><a href="#1">1. ê²½ì‚¬ í•˜ê°•ë²• ê¸°ì´ˆ</a><ul>
<li><a href="#11">1.1 ê²½ì‚¬ í•˜ê°•ë²•ì˜ ë³€í˜•</a></li>
<li><a href="#12">1.2 ìµœì í™” í™˜ê²½</a></li>
</ul>
</li>
<li><a href="#2">2. ê³ ì „ì  ì˜µí‹°ë§ˆì´ì €</a><ul>
<li><a href="#21-sgd">2.1 í™•ë¥ ì  ê²½ì‚¬ í•˜ê°•ë²•(SGD)</a></li>
<li><a href="#22-sgd">2.2 ëª¨ë©˜í…€ì„ ê°€ì§„ SGD</a></li>
<li><a href="#23-nag">2.3 ë„¤ìŠ¤í…Œë¡œí”„ ê°€ì† ê²½ì‚¬ë²•(NAG)</a></li>
</ul>
</li>
<li><a href="#3">3. ì ì‘ì  í•™ìŠµë¥  ë°©ë²•</a><ul>
<li><a href="#31-adagrad">3.1 Adagrad</a></li>
<li><a href="#32-rmsprop">3.2 RMSprop</a></li>
<li><a href="#33-adam-adaptive-moment-estimation">3.3 Adam (Adaptive Moment Estimation)</a></li>
<li><a href="#34-adamw-adam">3.4 AdamW (ë¶„ë¦¬ëœ ê°€ì¤‘ì¹˜ ê°ì‡ ë¥¼ ê°€ì§„ Adam)</a></li>
</ul>
</li>
<li><a href="#4">4. í˜„ëŒ€ì  ì˜µí‹°ë§ˆì´ì €</a><ul>
<li><a href="#41-lamb-layer-wise-adaptive-moments-optimizer-for-batch-training">4.1 LAMB (Layer-wise Adaptive Moments optimizer for Batch training)</a></li>
<li><a href="#42-adafactor">4.2 Adafactor</a></li>
<li><a href="#43-lion-evolved-sign-momentum">4.3 Lion (Evolved Sign Momentum)</a></li>
<li><a href="#44-8-bit-adam-bitsandbytes">4.4 8-bit Adam (bitsandbytes)</a></li>
<li><a href="#45-sophia-second-order-clipped-stochastic-optimization">4.5 Sophia (Second-order Clipped Stochastic Optimization)</a></li>
<li><a href="#46">4.6 ì˜µí‹°ë§ˆì´ì € ë¹„êµ í‘œ</a></li>
</ul>
</li>
<li><a href="#5">5. í•™ìŠµë¥  ìŠ¤ì¼€ì¤„ëŸ¬</a><ul>
<li><a href="#51">5.1 ìŠ¤í… ê¸°ë°˜ ìŠ¤ì¼€ì¤„ëŸ¬</a></li>
<li><a href="#52">5.2 ì§€ìˆ˜ ê°ì†Œ</a></li>
<li><a href="#53">5.3 ì½”ì‚¬ì¸ ì–´ë‹ë§</a></li>
<li><a href="#54-onecyclelr-super-convergence">5.4 OneCycleLR (Super-convergence)</a></li>
<li><a href="#55">5.5 ì„ í˜• ì›Œë°ì—… + ì½”ì‚¬ì¸ ê°ì†Œ (íŠ¸ëœìŠ¤í¬ë¨¸ í‘œì¤€)</a></li>
<li><a href="#56-reducelronplateau">5.6 ReduceLROnPlateau</a></li>
<li><a href="#57">5.7 ì»¤ìŠ¤í…€ ìŠ¤ì¼€ì¤„ëŸ¬</a></li>
<li><a href="#58">5.8 ìŠ¤ì¼€ì¤„ëŸ¬ ì‹œê°í™” ëª¨ìŒ</a></li>
</ul>
</li>
<li><a href="#6">6. ì‹¤ìš©ì  ê¸°ë²•</a><ul>
<li><a href="#61-lr-range-test">6.1 í•™ìŠµë¥  íƒìƒ‰ (LR Range Test)</a></li>
<li><a href="#62">6.2 ê·¸ë˜ë””ì–¸íŠ¸ í´ë¦¬í•‘</a></li>
<li><a href="#63">6.3 ê·¸ë˜ë””ì–¸íŠ¸ ì¶•ì </a></li>
<li><a href="#64">6.4 í˜¼í•© ì •ë°€ë„ í•™ìŠµ</a></li>
<li><a href="#65">6.5 íŒŒë¼ë¯¸í„° ê·¸ë£¹ë³„ í•™ìŠµë¥ </a></li>
</ul>
</li>
<li><a href="#7">7. ì˜¬ë°”ë¥¸ ì˜µí‹°ë§ˆì´ì € ì„ íƒí•˜ê¸°</a><ul>
<li><a href="#71">7.1 ê²°ì • ê°€ì´ë“œ</a></li>
<li><a href="#72-sgd-vs-adam">7.2 SGD vs Adam ë…¼ìŸ</a></li>
<li><a href="#73">7.3 ì¼ë°˜ì ì¸ ë ˆì‹œí”¼</a></li>
<li><a href="#74">7.4 ìµœì í™” ë¬¸ì œ ë””ë²„ê¹…</a></li>
</ul>
</li>
<li><a href="#_2">ì—°ìŠµ ë¬¸ì œ</a><ul>
<li><a href="#1_1">ì—°ìŠµ ë¬¸ì œ 1: ì˜µí‹°ë§ˆì´ì € êµ¬í˜„ ë° ë¹„êµ</a></li>
<li><a href="#2_1">ì—°ìŠµ ë¬¸ì œ 2: ìŠ¤ì¼€ì¤„ëŸ¬ ì ˆì œ ì—°êµ¬</a></li>
<li><a href="#3_1">ì—°ìŠµ ë¬¸ì œ 3: ëŒ€ê·œëª¨ ë°°ì¹˜ í•™ìŠµ ì‹œë®¬ë ˆì´ì…˜</a></li>
</ul>
</li>
<li><a href="#_3">ì°¸ê³  ìë£Œ</a></li>
</ul>
</div>

    </nav>
    

    <div class="lesson-content markdown-body">
        <p><a href="./24_Loss_Functions.md">ì´ì „: ì†ì‹¤ í•¨ìˆ˜</a> | <a href="./26_Normalization_Layers.md">ë‹¤ìŒ: ì •ê·œí™” ë ˆì´ì–´</a></p>
<hr />
<h1 id="25-optimizers">25. ì˜µí‹°ë§ˆì´ì €(Optimizers)<a class="header-link" href="#25-optimizers" title="Permanent link">&para;</a></h1>
<h2 id="_1">í•™ìŠµ ëª©í‘œ<a class="header-link" href="#_1" title="Permanent link">&para;</a></h2>
<ul>
<li>ê²½ì‚¬ í•˜ê°•ë²•ì˜ ë³€í˜•ë“¤ê³¼ ë”¥ëŸ¬ë‹ì˜ ìµœì í™” í™˜ê²½ ì´í•´í•˜ê¸°</li>
<li>ê³ ì „ì  ì˜µí‹°ë§ˆì´ì €(SGD, Momentum, Nesterov)ì™€ ì ì‘ì  ë°©ë²•(Adagrad, RMSprop, Adam, AdamW) ë§ˆìŠ¤í„°í•˜ê¸°</li>
<li>ëŒ€ê·œëª¨ í•™ìŠµì„ ìœ„í•œ í˜„ëŒ€ì  ì˜µí‹°ë§ˆì´ì €(LAMB, Adafactor, Lion, 8-bit Adam) íƒêµ¬í•˜ê¸°</li>
<li>í•™ìŠµë¥  ìŠ¤ì¼€ì¤„ëŸ¬ êµ¬í˜„í•˜ê¸°(ì½”ì‚¬ì¸ ì–´ë‹ë§, OneCycleLR, ì›Œë°ì—… ì „ëµ)</li>
<li>ì‹¤ìš©ì ì¸ ìµœì í™” ê¸°ë²• ì ìš©í•˜ê¸°(ê·¸ë˜ë””ì–¸íŠ¸ í´ë¦¬í•‘, ì¶•ì , íŒŒë¼ë¯¸í„° ê·¸ë£¹ë³„ ì„¤ì •)</li>
<li>ë‹¤ì–‘í•œ ì•„í‚¤í…ì²˜ì™€ ê³¼ì œì— ì í•©í•œ ì˜µí‹°ë§ˆì´ì €ì™€ í•˜ì´í¼íŒŒë¼ë¯¸í„° ì„ íƒí•˜ê¸°</li>
</ul>
<p><strong>ë‚œì´ë„</strong>: â­â­â­</p>
<hr />
<h2 id="1">1. ê²½ì‚¬ í•˜ê°•ë²• ê¸°ì´ˆ<a class="header-link" href="#1" title="Permanent link">&para;</a></h2>
<h3 id="11">1.1 ê²½ì‚¬ í•˜ê°•ë²•ì˜ ë³€í˜•<a class="header-link" href="#11" title="Permanent link">&para;</a></h3>
<p><strong>ë°°ì¹˜ ê²½ì‚¬ í•˜ê°•ë²•(Batch Gradient Descent)</strong>ì€ ì „ì²´ í•™ìŠµ ë°ì´í„°ì…‹ì„ ì‚¬ìš©í•˜ì—¬ ê·¸ë˜ë””ì–¸íŠ¸ë¥¼ ê³„ì‚°í•©ë‹ˆë‹¤:</p>
<p>$$
\theta_{t+1} = \theta_t - \eta \nabla_\theta \mathcal{L}(\theta_t)
$$</p>
<p>ì—¬ê¸°ì„œ $\mathcal{L}$ì€ ëª¨ë“  í•™ìŠµ ì˜ˆì œì— ëŒ€í•œ í‰ê·  ì†ì‹¤ì…ë‹ˆë‹¤.</p>
<p><strong>í™•ë¥ ì  ê²½ì‚¬ í•˜ê°•ë²•(Stochastic Gradient Descent, SGD)</strong>ì€ í•˜ë‚˜ì˜ ë¬´ì‘ìœ„ ì˜ˆì œë¥¼ ì‚¬ìš©í•©ë‹ˆë‹¤:</p>
<p>$$
\theta_{t+1} = \theta_t - \eta \nabla_\theta \ell(x_i, y_i; \theta_t)
$$</p>
<p><strong>ë¯¸ë‹ˆë°°ì¹˜ ê²½ì‚¬ í•˜ê°•ë²•(Mini-batch Gradient Descent)</strong>ì€ ì‹¤ìš©ì ì¸ ì¤‘ê°„ ì§€ì ì…ë‹ˆë‹¤:</p>
<p>$$
\theta_{t+1} = \theta_t - \eta \nabla_\theta \frac{1}{|\mathcal{B}|} \sum_{(x,y) \in \mathcal{B}} \ell(x, y; \theta_t)
$$</p>
<div class="highlight"><pre><span></span><code><span class="kn">import</span><span class="w"> </span><span class="nn">torch</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">torch.nn</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">nn</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">torch.optim</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">optim</span>

<span class="c1"># Example: Different GD variants</span>
<span class="k">def</span><span class="w"> </span><span class="nf">train_epoch_batch_gd</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">train_loader</span><span class="p">,</span> <span class="n">criterion</span><span class="p">,</span> <span class="n">optimizer</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Batch GD: accumulate gradients over entire epoch&quot;&quot;&quot;</span>
    <span class="n">model</span><span class="o">.</span><span class="n">train</span><span class="p">()</span>
    <span class="n">optimizer</span><span class="o">.</span><span class="n">zero_grad</span><span class="p">()</span>
    <span class="n">total_loss</span> <span class="o">=</span> <span class="mi">0</span>

    <span class="k">for</span> <span class="n">X</span><span class="p">,</span> <span class="n">y</span> <span class="ow">in</span> <span class="n">train_loader</span><span class="p">:</span>
        <span class="n">output</span> <span class="o">=</span> <span class="n">model</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>
        <span class="n">loss</span> <span class="o">=</span> <span class="n">criterion</span><span class="p">(</span><span class="n">output</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>
        <span class="n">loss</span><span class="o">.</span><span class="n">backward</span><span class="p">()</span>  <span class="c1"># Accumulate gradients</span>
        <span class="n">total_loss</span> <span class="o">+=</span> <span class="n">loss</span><span class="o">.</span><span class="n">item</span><span class="p">()</span>

    <span class="n">optimizer</span><span class="o">.</span><span class="n">step</span><span class="p">()</span>  <span class="c1"># Single update after full pass</span>
    <span class="k">return</span> <span class="n">total_loss</span> <span class="o">/</span> <span class="nb">len</span><span class="p">(</span><span class="n">train_loader</span><span class="p">)</span>

<span class="k">def</span><span class="w"> </span><span class="nf">train_epoch_sgd</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">train_loader</span><span class="p">,</span> <span class="n">criterion</span><span class="p">,</span> <span class="n">optimizer</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;SGD: update after each batch&quot;&quot;&quot;</span>
    <span class="n">model</span><span class="o">.</span><span class="n">train</span><span class="p">()</span>
    <span class="n">total_loss</span> <span class="o">=</span> <span class="mi">0</span>

    <span class="k">for</span> <span class="n">X</span><span class="p">,</span> <span class="n">y</span> <span class="ow">in</span> <span class="n">train_loader</span><span class="p">:</span>
        <span class="n">optimizer</span><span class="o">.</span><span class="n">zero_grad</span><span class="p">()</span>
        <span class="n">output</span> <span class="o">=</span> <span class="n">model</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>
        <span class="n">loss</span> <span class="o">=</span> <span class="n">criterion</span><span class="p">(</span><span class="n">output</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>
        <span class="n">loss</span><span class="o">.</span><span class="n">backward</span><span class="p">()</span>
        <span class="n">optimizer</span><span class="o">.</span><span class="n">step</span><span class="p">()</span>  <span class="c1"># Update after each batch</span>
        <span class="n">total_loss</span> <span class="o">+=</span> <span class="n">loss</span><span class="o">.</span><span class="n">item</span><span class="p">()</span>

    <span class="k">return</span> <span class="n">total_loss</span> <span class="o">/</span> <span class="nb">len</span><span class="p">(</span><span class="n">train_loader</span><span class="p">)</span>

<span class="k">def</span><span class="w"> </span><span class="nf">train_epoch_mini_batch</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">train_loader</span><span class="p">,</span> <span class="n">criterion</span><span class="p">,</span> <span class="n">optimizer</span><span class="p">,</span> <span class="n">accumulation_steps</span><span class="o">=</span><span class="mi">4</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Mini-batch with gradient accumulation&quot;&quot;&quot;</span>
    <span class="n">model</span><span class="o">.</span><span class="n">train</span><span class="p">()</span>
    <span class="n">optimizer</span><span class="o">.</span><span class="n">zero_grad</span><span class="p">()</span>
    <span class="n">total_loss</span> <span class="o">=</span> <span class="mi">0</span>

    <span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">train_loader</span><span class="p">):</span>
        <span class="n">output</span> <span class="o">=</span> <span class="n">model</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>
        <span class="n">loss</span> <span class="o">=</span> <span class="n">criterion</span><span class="p">(</span><span class="n">output</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span> <span class="o">/</span> <span class="n">accumulation_steps</span>
        <span class="n">loss</span><span class="o">.</span><span class="n">backward</span><span class="p">()</span>

        <span class="k">if</span> <span class="p">(</span><span class="n">i</span> <span class="o">+</span> <span class="mi">1</span><span class="p">)</span> <span class="o">%</span> <span class="n">accumulation_steps</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
            <span class="n">optimizer</span><span class="o">.</span><span class="n">step</span><span class="p">()</span>
            <span class="n">optimizer</span><span class="o">.</span><span class="n">zero_grad</span><span class="p">()</span>

        <span class="n">total_loss</span> <span class="o">+=</span> <span class="n">loss</span><span class="o">.</span><span class="n">item</span><span class="p">()</span> <span class="o">*</span> <span class="n">accumulation_steps</span>

    <span class="k">return</span> <span class="n">total_loss</span> <span class="o">/</span> <span class="nb">len</span><span class="p">(</span><span class="n">train_loader</span><span class="p">)</span>
</code></pre></div>

<h3 id="12">1.2 ìµœì í™” í™˜ê²½<a class="header-link" href="#12" title="Permanent link">&para;</a></h3>
<p>ì‹¬ì¸µ ì‹ ê²½ë§ì€ ë‹¤ìŒê³¼ ê°™ì€ íŠ¹ì„±ì„ ê°€ì§„ ë§¤ìš° ë¹„ë³¼ë¡(non-convex)í•œ ì†ì‹¤ í‘œë©´ì„ ë§Œë“­ë‹ˆë‹¤:</p>
<ul>
<li><strong>êµ­ì†Œ ìµœì†Ÿê°’(Local minima)</strong>: ê·¸ë˜ë””ì–¸íŠ¸ëŠ” 0ì´ì§€ë§Œ ì „ì—­ ìµœì†Ÿê°’ì´ ì•„ë‹Œ ì§€ì </li>
<li><strong>ì•ˆì¥ì (Saddle points)</strong>: ê·¸ë˜ë””ì–¸íŠ¸ëŠ” 0ì´ì§€ë§Œ ìµœì†Ÿê°’ì´ ì•„ë‹Œ ì§€ì  (ê³ ì°¨ì›ì—ì„œ ë§¤ìš° í”í•¨)</li>
<li><strong>í‰íƒ„ ì§€ì—­(Plateaus)</strong>: ê·¸ë˜ë””ì–¸íŠ¸ê°€ 0ì— ê°€ê¹Œìš´ í‰í‰í•œ ì˜ì—­</li>
<li><strong>í˜‘ê³¡(Ravines)</strong>: ì¼ë¶€ ë°©í–¥ìœ¼ë¡œëŠ” ê°€íŒŒë¥´ê³  ë‹¤ë¥¸ ë°©í–¥ìœ¼ë¡œëŠ” í‰í‰í•œ ì§€ì—­</li>
</ul>
<div class="highlight"><pre><span></span><code><span class="n">Loss</span><span class="w"> </span><span class="n">Surface</span><span class="w"> </span><span class="n">Visualization</span><span class="o">:</span>

<span class="w">           </span><span class="n">Global</span><span class="w"> </span><span class="n">Minimum</span>
<span class="w">                </span><span class="o">|</span>
<span class="w">    </span>â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
<span class="w">    </span><span class="o">|</span><span class="w">                        </span><span class="o">|</span>
<span class="w">    </span><span class="o">|</span><span class="w">  </span>â•±â•²<span class="w">      </span>â•±â•²<span class="w">      </span>â•±â•²<span class="w">   </span><span class="o">|</span><span class="w">  </span><span class="o">&lt;-</span><span class="w"> </span><span class="n">Local</span><span class="w"> </span><span class="n">Minima</span>
<span class="w">    </span><span class="o">|</span><span class="w"> </span>â•±<span class="w">  </span>â•²<span class="w">    </span>â•±<span class="w">  </span>â•²<span class="w">    </span>â•±<span class="w">  </span>â•²<span class="w">  </span><span class="o">|</span>
<span class="w">    </span><span class="o">|</span>â•±<span class="w">    </span>â•²__â•±<span class="w">    </span>â•²__â•±<span class="w">    </span>â•²<span class="w"> </span><span class="o">|</span>
<span class="w">    </span><span class="o">|</span><span class="w">      </span>â–²<span class="w">       </span>â–²<span class="w">        </span>â–¼<span class="o">|</span><span class="w">  </span><span class="o">&lt;-</span><span class="w"> </span><span class="n">Saddle</span><span class="w"> </span><span class="n">Point</span>
<span class="w">    </span><span class="o">|</span><span class="w">  </span><span class="n">Plateau</span><span class="w">  </span><span class="n">Local</span><span class="w"> </span><span class="n">Min</span><span class="w">    </span><span class="o">|</span>
<span class="w">    </span>â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

<span class="n">Gradient</span><span class="w"> </span><span class="n">Behavior</span><span class="o">:</span>
<span class="o">-</span><span class="w"> </span><span class="n">At</span><span class="w"> </span><span class="n">local</span><span class="w"> </span><span class="n">minima</span><span class="o">:</span><span class="w"> </span>âˆ‡<span class="n">L</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="m">0</span><span class="p">,</span><span class="w"> </span><span class="n">all</span><span class="w"> </span><span class="n">eigenvalues</span><span class="w"> </span><span class="o">&gt;</span><span class="w"> </span><span class="m">0</span>
<span class="o">-</span><span class="w"> </span><span class="n">At</span><span class="w"> </span><span class="n">saddle</span><span class="w"> </span><span class="n">points</span><span class="o">:</span><span class="w"> </span>âˆ‡<span class="n">L</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="m">0</span><span class="p">,</span><span class="w"> </span><span class="n">some</span><span class="w"> </span><span class="n">eigenvalues</span><span class="w"> </span><span class="o">&lt;</span><span class="w"> </span><span class="m">0</span>
<span class="o">-</span><span class="w"> </span><span class="n">On</span><span class="w"> </span><span class="n">plateaus</span><span class="o">:</span><span class="w"> </span>âˆ‡<span class="n">L</span><span class="w"> </span>â‰ˆ<span class="w"> </span><span class="m">0</span><span class="p">,</span><span class="w"> </span><span class="n">very</span><span class="w"> </span><span class="n">slow</span><span class="w"> </span><span class="n">progress</span>
</code></pre></div>

<p><strong>ë¹„ë³¼ë¡ì„±ì—ë„ ë¶ˆêµ¬í•˜ê³  ìµœì í™”ê°€ ì‘ë™í•˜ëŠ” ì´ìœ :</strong></p>
<ol>
<li>ê³ ì°¨ì› ê³µê°„ì—ëŠ” êµ­ì†Œ ìµœì†Ÿê°’ë³´ë‹¤ ì•ˆì¥ì ì´ ê¸°í•˜ê¸‰ìˆ˜ì ìœ¼ë¡œ ë§ìŒ</li>
<li>êµ­ì†Œ ìµœì†Ÿê°’ì˜ ì†ì‹¤ ê°’ì´ ì¢…ì¢… ì „ì—­ ìµœì†Ÿê°’ê³¼ ë¹„ìŠ·í•¨</li>
<li>í˜„ëŒ€ ì˜µí‹°ë§ˆì´ì €(ëª¨ë©˜í…€ í¬í•¨)ëŠ” ì•ˆì¥ì ì„ ë²—ì–´ë‚  ìˆ˜ ìˆìŒ</li>
<li>ê³¼ì‰ ë§¤ê°œë³€ìˆ˜í™”(overparameterization)ê°€ ë§ì€ ì¢‹ì€ í•´ë¥¼ ë§Œë“¦</li>
</ol>
<div class="highlight"><pre><span></span><code><span class="kn">import</span><span class="w"> </span><span class="nn">numpy</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">np</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">matplotlib.pyplot</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">plt</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">mpl_toolkits.mplot3d</span><span class="w"> </span><span class="kn">import</span> <span class="n">Axes3D</span>

<span class="k">def</span><span class="w"> </span><span class="nf">visualize_loss_surface</span><span class="p">():</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Visualize a toy non-convex loss surface&quot;&quot;&quot;</span>
    <span class="n">x</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="o">-</span><span class="mi">3</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">100</span><span class="p">)</span>
    <span class="n">y</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="o">-</span><span class="mi">3</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">100</span><span class="p">)</span>
    <span class="n">X</span><span class="p">,</span> <span class="n">Y</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">meshgrid</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>

    <span class="c1"># Non-convex function with multiple minima</span>
    <span class="n">Z</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">sin</span><span class="p">(</span><span class="n">X</span><span class="o">**</span><span class="mi">2</span> <span class="o">+</span> <span class="n">Y</span><span class="o">**</span><span class="mi">2</span><span class="p">)</span> <span class="o">/</span> <span class="p">(</span><span class="n">X</span><span class="o">**</span><span class="mi">2</span> <span class="o">+</span> <span class="n">Y</span><span class="o">**</span><span class="mi">2</span> <span class="o">+</span> <span class="mi">1</span><span class="p">)</span> <span class="o">+</span> <span class="mf">0.1</span> <span class="o">*</span> <span class="p">(</span><span class="n">X</span><span class="o">**</span><span class="mi">2</span> <span class="o">+</span> <span class="n">Y</span><span class="o">**</span><span class="mi">2</span><span class="p">)</span>

    <span class="n">fig</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">12</span><span class="p">,</span> <span class="mi">5</span><span class="p">))</span>

    <span class="c1"># 3D surface</span>
    <span class="n">ax1</span> <span class="o">=</span> <span class="n">fig</span><span class="o">.</span><span class="n">add_subplot</span><span class="p">(</span><span class="mi">121</span><span class="p">,</span> <span class="n">projection</span><span class="o">=</span><span class="s1">&#39;3d&#39;</span><span class="p">)</span>
    <span class="n">ax1</span><span class="o">.</span><span class="n">plot_surface</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">Y</span><span class="p">,</span> <span class="n">Z</span><span class="p">,</span> <span class="n">cmap</span><span class="o">=</span><span class="s1">&#39;viridis&#39;</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.8</span><span class="p">)</span>
    <span class="n">ax1</span><span class="o">.</span><span class="n">set_title</span><span class="p">(</span><span class="s1">&#39;Loss Surface (3D)&#39;</span><span class="p">)</span>
    <span class="n">ax1</span><span class="o">.</span><span class="n">set_xlabel</span><span class="p">(</span><span class="s1">&#39;Î¸â‚&#39;</span><span class="p">)</span>
    <span class="n">ax1</span><span class="o">.</span><span class="n">set_ylabel</span><span class="p">(</span><span class="s1">&#39;Î¸â‚‚&#39;</span><span class="p">)</span>
    <span class="n">ax1</span><span class="o">.</span><span class="n">set_zlabel</span><span class="p">(</span><span class="s1">&#39;Loss&#39;</span><span class="p">)</span>

    <span class="c1"># Contour plot</span>
    <span class="n">ax2</span> <span class="o">=</span> <span class="n">fig</span><span class="o">.</span><span class="n">add_subplot</span><span class="p">(</span><span class="mi">122</span><span class="p">)</span>
    <span class="n">contour</span> <span class="o">=</span> <span class="n">ax2</span><span class="o">.</span><span class="n">contour</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">Y</span><span class="p">,</span> <span class="n">Z</span><span class="p">,</span> <span class="n">levels</span><span class="o">=</span><span class="mi">20</span><span class="p">,</span> <span class="n">cmap</span><span class="o">=</span><span class="s1">&#39;viridis&#39;</span><span class="p">)</span>
    <span class="n">ax2</span><span class="o">.</span><span class="n">clabel</span><span class="p">(</span><span class="n">contour</span><span class="p">,</span> <span class="n">inline</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">8</span><span class="p">)</span>
    <span class="n">ax2</span><span class="o">.</span><span class="n">set_title</span><span class="p">(</span><span class="s1">&#39;Loss Surface (Contour)&#39;</span><span class="p">)</span>
    <span class="n">ax2</span><span class="o">.</span><span class="n">set_xlabel</span><span class="p">(</span><span class="s1">&#39;Î¸â‚&#39;</span><span class="p">)</span>
    <span class="n">ax2</span><span class="o">.</span><span class="n">set_ylabel</span><span class="p">(</span><span class="s1">&#39;Î¸â‚‚&#39;</span><span class="p">)</span>

    <span class="n">plt</span><span class="o">.</span><span class="n">tight_layout</span><span class="p">()</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">savefig</span><span class="p">(</span><span class="s1">&#39;loss_surface.png&#39;</span><span class="p">,</span> <span class="n">dpi</span><span class="o">=</span><span class="mi">150</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">close</span><span class="p">()</span>

<span class="c1"># Simulate optimization trajectory</span>
<span class="k">def</span><span class="w"> </span><span class="nf">optimize_trajectory</span><span class="p">(</span><span class="n">optimizer_fn</span><span class="p">,</span> <span class="n">steps</span><span class="o">=</span><span class="mi">100</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Track optimizer path on loss surface&quot;&quot;&quot;</span>
    <span class="n">trajectory</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="n">theta</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([</span><span class="mf">2.5</span><span class="p">,</span> <span class="mf">2.5</span><span class="p">],</span> <span class="n">requires_grad</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
    <span class="n">optimizer</span> <span class="o">=</span> <span class="n">optimizer_fn</span><span class="p">([</span><span class="n">theta</span><span class="p">])</span>

    <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">steps</span><span class="p">):</span>
        <span class="n">optimizer</span><span class="o">.</span><span class="n">zero_grad</span><span class="p">()</span>

        <span class="c1"># Loss function (same as above)</span>
        <span class="n">loss</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">sin</span><span class="p">(</span><span class="n">theta</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">**</span><span class="mi">2</span> <span class="o">+</span> <span class="n">theta</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">**</span><span class="mi">2</span><span class="p">)</span> <span class="o">/</span> <span class="p">(</span><span class="n">theta</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">**</span><span class="mi">2</span> <span class="o">+</span> <span class="n">theta</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">**</span><span class="mi">2</span> <span class="o">+</span> <span class="mi">1</span><span class="p">)</span> <span class="o">+</span> \
               <span class="mf">0.1</span> <span class="o">*</span> <span class="p">(</span><span class="n">theta</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">**</span><span class="mi">2</span> <span class="o">+</span> <span class="n">theta</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">**</span><span class="mi">2</span><span class="p">)</span>
        <span class="n">loss</span><span class="o">.</span><span class="n">backward</span><span class="p">()</span>

        <span class="n">trajectory</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">theta</span><span class="o">.</span><span class="n">detach</span><span class="p">()</span><span class="o">.</span><span class="n">clone</span><span class="p">()</span><span class="o">.</span><span class="n">numpy</span><span class="p">())</span>
        <span class="n">optimizer</span><span class="o">.</span><span class="n">step</span><span class="p">()</span>

    <span class="k">return</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">trajectory</span><span class="p">)</span>

<span class="c1"># Compare trajectories</span>
<span class="n">sgd_traj</span> <span class="o">=</span> <span class="n">optimize_trajectory</span><span class="p">(</span><span class="k">lambda</span> <span class="n">p</span><span class="p">:</span> <span class="n">optim</span><span class="o">.</span><span class="n">SGD</span><span class="p">(</span><span class="n">p</span><span class="p">,</span> <span class="n">lr</span><span class="o">=</span><span class="mf">0.1</span><span class="p">))</span>
<span class="n">momentum_traj</span> <span class="o">=</span> <span class="n">optimize_trajectory</span><span class="p">(</span><span class="k">lambda</span> <span class="n">p</span><span class="p">:</span> <span class="n">optim</span><span class="o">.</span><span class="n">SGD</span><span class="p">(</span><span class="n">p</span><span class="p">,</span> <span class="n">lr</span><span class="o">=</span><span class="mf">0.1</span><span class="p">,</span> <span class="n">momentum</span><span class="o">=</span><span class="mf">0.9</span><span class="p">))</span>
<span class="n">adam_traj</span> <span class="o">=</span> <span class="n">optimize_trajectory</span><span class="p">(</span><span class="k">lambda</span> <span class="n">p</span><span class="p">:</span> <span class="n">optim</span><span class="o">.</span><span class="n">Adam</span><span class="p">(</span><span class="n">p</span><span class="p">,</span> <span class="n">lr</span><span class="o">=</span><span class="mf">0.1</span><span class="p">))</span>
</code></pre></div>

<hr />
<h2 id="2">2. ê³ ì „ì  ì˜µí‹°ë§ˆì´ì €<a class="header-link" href="#2" title="Permanent link">&para;</a></h2>
<h3 id="21-sgd">2.1 í™•ë¥ ì  ê²½ì‚¬ í•˜ê°•ë²•(SGD)<a class="header-link" href="#21-sgd" title="Permanent link">&para;</a></h3>
<p>ê¸°ë³¸ SGD ì—…ë°ì´íŠ¸ ê·œì¹™:</p>
<p>$$
\theta_{t+1} = \theta_t - \eta \nabla_\theta \mathcal{L}(\theta_t)
$$</p>
<p><strong>ìˆœìˆ˜ SGDì˜ ë¬¸ì œì :</strong>
- í˜‘ê³¡ì—ì„œ ì§„ë™ (ì¼ë¶€ ë°©í–¥ìœ¼ë¡œ ê°€íŒŒë¦„)
- í‰íƒ„í•œ ì§€ì—­ì—ì„œ ëŠë¦° ìˆ˜ë ´
- ì•ˆì¥ì ì„ ë²—ì–´ë‚˜ê¸° ì–´ë ¤ì›€
- ëª¨ë“  íŒŒë¼ë¯¸í„°ì— ë™ì¼í•œ í•™ìŠµë¥  ì‚¬ìš©</p>
<h3 id="22-sgd">2.2 ëª¨ë©˜í…€ì„ ê°€ì§„ SGD<a class="header-link" href="#22-sgd" title="Permanent link">&para;</a></h3>
<p>ëª¨ë©˜í…€(Momentum)ì€ ì¼ê´€ëœ ë°©í–¥ìœ¼ë¡œ ê°€ì†í•˜ê³  ì§„ë™ì„ ê°ì‡ í•©ë‹ˆë‹¤:</p>
<p>$$
\begin{align}
v_t &= \beta v_{t-1} + \nabla_\theta \mathcal{L}(\theta_t) \\
\theta_{t+1} &= \theta_t - \eta v_t
\end{align}
$$</p>
<p>ì—¬ê¸°ì„œ $\beta \in [0, 1)$ëŠ” ëª¨ë©˜í…€ ê³„ìˆ˜ì…ë‹ˆë‹¤ (ì¼ë°˜ì ìœ¼ë¡œ 0.9).</p>
<p><strong>ë¬¼ë¦¬ì  ë¹„ìœ </strong>: ì–¸ë•ì„ êµ´ëŸ¬ë‚´ë ¤ê°€ëŠ” ê³µì´ ì†ë„ë¥¼ ì¶•ì í•©ë‹ˆë‹¤.</p>
<div class="highlight"><pre><span></span><code><span class="n">Momentum</span><span class="w"> </span><span class="n">Effect</span><span class="o">:</span>

<span class="n">Without</span><span class="w"> </span><span class="n">Momentum</span><span class="o">:</span><span class="w">        </span><span class="n">With</span><span class="w"> </span><span class="n">Momentum</span><span class="o">:</span>
<span class="w">     </span>â†“<span class="w">                       </span>â†“
<span class="w">   </span>â†™<span class="w"> </span>â†˜<span class="w">                    </span>â†™<span class="w">  </span>â†˜
<span class="w">  </span>â†™<span class="w">   </span>â†˜<span class="w">                 </span>â†™<span class="w">     </span>â†˜
<span class="w"> </span>â†™<span class="w">  </span>â†“<span class="w">  </span>â†˜<span class="w">              </span>â†™<span class="w">    </span>â†“<span class="w">   </span>â†˜
â†™<span class="w">   </span>â†“<span class="w">   </span>â†˜<span class="w">           </span>â†™<span class="w">      </span>â†“<span class="w">    </span>â†˜<span class="w">  </span><span class="o">&lt;-</span><span class="w"> </span><span class="n">Smoother</span><span class="p">,</span><span class="w"> </span><span class="n">faster</span>
<span class="w">   </span><span class="n">zigzag</span><span class="w">            </span><span class="n">straighter</span><span class="w"> </span><span class="n">path</span>
</code></pre></div>

<p><strong>ìˆ˜ë™ êµ¬í˜„:</strong></p>
<div class="highlight"><pre><span></span><code><span class="k">class</span><span class="w"> </span><span class="nc">SGDMomentum</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Manual implementation of SGD with momentum&quot;&quot;&quot;</span>
    <span class="k">def</span><span class="w"> </span><span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">params</span><span class="p">,</span> <span class="n">lr</span><span class="o">=</span><span class="mf">0.01</span><span class="p">,</span> <span class="n">momentum</span><span class="o">=</span><span class="mf">0.9</span><span class="p">,</span> <span class="n">weight_decay</span><span class="o">=</span><span class="mi">0</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">params</span> <span class="o">=</span> <span class="nb">list</span><span class="p">(</span><span class="n">params</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">lr</span> <span class="o">=</span> <span class="n">lr</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">momentum</span> <span class="o">=</span> <span class="n">momentum</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">weight_decay</span> <span class="o">=</span> <span class="n">weight_decay</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">velocities</span> <span class="o">=</span> <span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">zeros_like</span><span class="p">(</span><span class="n">p</span><span class="p">)</span> <span class="k">for</span> <span class="n">p</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">params</span><span class="p">]</span>

    <span class="k">def</span><span class="w"> </span><span class="nf">zero_grad</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="k">for</span> <span class="n">p</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">params</span><span class="p">:</span>
            <span class="k">if</span> <span class="n">p</span><span class="o">.</span><span class="n">grad</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
                <span class="n">p</span><span class="o">.</span><span class="n">grad</span><span class="o">.</span><span class="n">zero_</span><span class="p">()</span>

    <span class="k">def</span><span class="w"> </span><span class="nf">step</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="k">with</span> <span class="n">torch</span><span class="o">.</span><span class="n">no_grad</span><span class="p">():</span>
            <span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">p</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">params</span><span class="p">):</span>
                <span class="k">if</span> <span class="n">p</span><span class="o">.</span><span class="n">grad</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
                    <span class="k">continue</span>

                <span class="c1"># Gradient with weight decay</span>
                <span class="n">grad</span> <span class="o">=</span> <span class="n">p</span><span class="o">.</span><span class="n">grad</span>
                <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">weight_decay</span> <span class="o">!=</span> <span class="mi">0</span><span class="p">:</span>
                    <span class="n">grad</span> <span class="o">=</span> <span class="n">grad</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">p</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">weight_decay</span><span class="p">)</span>

                <span class="c1"># Momentum update</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">velocities</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">momentum</span> <span class="o">*</span> <span class="bp">self</span><span class="o">.</span><span class="n">velocities</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">+</span> <span class="n">grad</span>
                <span class="n">p</span><span class="o">.</span><span class="n">add_</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">velocities</span><span class="p">[</span><span class="n">i</span><span class="p">],</span> <span class="n">alpha</span><span class="o">=-</span><span class="bp">self</span><span class="o">.</span><span class="n">lr</span><span class="p">)</span>

<span class="c1"># Test implementation</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="mi">10</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
<span class="n">criterion</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">MSELoss</span><span class="p">()</span>

<span class="c1"># Custom optimizer</span>
<span class="n">optimizer</span> <span class="o">=</span> <span class="n">SGDMomentum</span><span class="p">(</span><span class="n">model</span><span class="o">.</span><span class="n">parameters</span><span class="p">(),</span> <span class="n">lr</span><span class="o">=</span><span class="mf">0.01</span><span class="p">,</span> <span class="n">momentum</span><span class="o">=</span><span class="mf">0.9</span><span class="p">)</span>

<span class="c1"># PyTorch equivalent</span>
<span class="n">optimizer_torch</span> <span class="o">=</span> <span class="n">optim</span><span class="o">.</span><span class="n">SGD</span><span class="p">(</span><span class="n">model</span><span class="o">.</span><span class="n">parameters</span><span class="p">(),</span> <span class="n">lr</span><span class="o">=</span><span class="mf">0.01</span><span class="p">,</span> <span class="n">momentum</span><span class="o">=</span><span class="mf">0.9</span><span class="p">)</span>

<span class="c1"># Both should produce identical results</span>
<span class="n">X</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">32</span><span class="p">,</span> <span class="mi">10</span><span class="p">)</span>
<span class="n">y</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">32</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>

<span class="c1"># Custom</span>
<span class="n">optimizer</span><span class="o">.</span><span class="n">zero_grad</span><span class="p">()</span>
<span class="n">loss</span> <span class="o">=</span> <span class="n">criterion</span><span class="p">(</span><span class="n">model</span><span class="p">(</span><span class="n">X</span><span class="p">),</span> <span class="n">y</span><span class="p">)</span>
<span class="n">loss</span><span class="o">.</span><span class="n">backward</span><span class="p">()</span>
<span class="n">optimizer</span><span class="o">.</span><span class="n">step</span><span class="p">()</span>

<span class="c1"># Check velocity is being maintained</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Velocity norm: </span><span class="si">{</span><span class="n">torch</span><span class="o">.</span><span class="n">norm</span><span class="p">(</span><span class="n">optimizer</span><span class="o">.</span><span class="n">velocities</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span><span class="si">:</span><span class="s2">.6f</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
</code></pre></div>

<h3 id="23-nag">2.3 ë„¤ìŠ¤í…Œë¡œí”„ ê°€ì† ê²½ì‚¬ë²•(NAG)<a class="header-link" href="#23-nag" title="Permanent link">&para;</a></h3>
<p>ë„¤ìŠ¤í…Œë¡œí”„ ëª¨ë©˜í…€(Nesterov momentum)ì€ ê·¸ë˜ë””ì–¸íŠ¸ë¥¼ ê³„ì‚°í•˜ê¸° ì „ì— "ë¯¸ë¦¬ ë‚´ë‹¤ë´…ë‹ˆë‹¤":</p>
<p>$$
\begin{align}
v_t &= \beta v_{t-1} + \nabla_\theta \mathcal{L}(\theta_t - \beta v_{t-1}) \\
\theta_{t+1} &= \theta_t - \eta v_t
\end{align}
$$</p>
<p><strong>ì§ê´€</strong>: í˜„ì¬ ìœ„ì¹˜ê°€ ì•„ë‹Œ "ë¯¸ë¦¬ ë³´ëŠ”" ìœ„ì¹˜ì—ì„œ ê·¸ë˜ë””ì–¸íŠ¸ë¥¼ ê³„ì‚°í•©ë‹ˆë‹¤.</p>
<div class="highlight"><pre><span></span><code><span class="c1"># PyTorch SGD with Nesterov momentum</span>
<span class="n">optimizer_sgd</span> <span class="o">=</span> <span class="n">optim</span><span class="o">.</span><span class="n">SGD</span><span class="p">(</span><span class="n">model</span><span class="o">.</span><span class="n">parameters</span><span class="p">(),</span> <span class="n">lr</span><span class="o">=</span><span class="mf">0.01</span><span class="p">,</span> <span class="n">momentum</span><span class="o">=</span><span class="mf">0.9</span><span class="p">)</span>
<span class="n">optimizer_nesterov</span> <span class="o">=</span> <span class="n">optim</span><span class="o">.</span><span class="n">SGD</span><span class="p">(</span><span class="n">model</span><span class="o">.</span><span class="n">parameters</span><span class="p">(),</span> <span class="n">lr</span><span class="o">=</span><span class="mf">0.01</span><span class="p">,</span> <span class="n">momentum</span><span class="o">=</span><span class="mf">0.9</span><span class="p">,</span> <span class="n">nesterov</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>

<span class="c1"># Comparison on a simple problem</span>
<span class="k">def</span><span class="w"> </span><span class="nf">compare_sgd_variants</span><span class="p">():</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Compare vanilla SGD, momentum, and Nesterov&quot;&quot;&quot;</span>
    <span class="n">torch</span><span class="o">.</span><span class="n">manual_seed</span><span class="p">(</span><span class="mi">42</span><span class="p">)</span>

    <span class="c1"># Simple 2D problem</span>
    <span class="n">target</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([</span><span class="mf">1.0</span><span class="p">,</span> <span class="mf">1.0</span><span class="p">])</span>

    <span class="n">results</span> <span class="o">=</span> <span class="p">{}</span>
    <span class="k">for</span> <span class="n">name</span><span class="p">,</span> <span class="n">opt_fn</span> <span class="ow">in</span> <span class="p">[</span>
        <span class="p">(</span><span class="s1">&#39;SGD&#39;</span><span class="p">,</span> <span class="k">lambda</span> <span class="n">p</span><span class="p">:</span> <span class="n">optim</span><span class="o">.</span><span class="n">SGD</span><span class="p">(</span><span class="n">p</span><span class="p">,</span> <span class="n">lr</span><span class="o">=</span><span class="mf">0.1</span><span class="p">)),</span>
        <span class="p">(</span><span class="s1">&#39;Momentum&#39;</span><span class="p">,</span> <span class="k">lambda</span> <span class="n">p</span><span class="p">:</span> <span class="n">optim</span><span class="o">.</span><span class="n">SGD</span><span class="p">(</span><span class="n">p</span><span class="p">,</span> <span class="n">lr</span><span class="o">=</span><span class="mf">0.1</span><span class="p">,</span> <span class="n">momentum</span><span class="o">=</span><span class="mf">0.9</span><span class="p">)),</span>
        <span class="p">(</span><span class="s1">&#39;Nesterov&#39;</span><span class="p">,</span> <span class="k">lambda</span> <span class="n">p</span><span class="p">:</span> <span class="n">optim</span><span class="o">.</span><span class="n">SGD</span><span class="p">(</span><span class="n">p</span><span class="p">,</span> <span class="n">lr</span><span class="o">=</span><span class="mf">0.1</span><span class="p">,</span> <span class="n">momentum</span><span class="o">=</span><span class="mf">0.9</span><span class="p">,</span> <span class="n">nesterov</span><span class="o">=</span><span class="kc">True</span><span class="p">))</span>
    <span class="p">]:</span>
        <span class="n">theta</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([</span><span class="mf">0.0</span><span class="p">,</span> <span class="mf">0.0</span><span class="p">],</span> <span class="n">requires_grad</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
        <span class="n">optimizer</span> <span class="o">=</span> <span class="n">opt_fn</span><span class="p">([</span><span class="n">theta</span><span class="p">])</span>
        <span class="n">losses</span> <span class="o">=</span> <span class="p">[]</span>

        <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">50</span><span class="p">):</span>
            <span class="n">optimizer</span><span class="o">.</span><span class="n">zero_grad</span><span class="p">()</span>
            <span class="n">loss</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">sum</span><span class="p">((</span><span class="n">theta</span> <span class="o">-</span> <span class="n">target</span><span class="p">)</span><span class="o">**</span><span class="mi">2</span><span class="p">)</span>
            <span class="n">loss</span><span class="o">.</span><span class="n">backward</span><span class="p">()</span>
            <span class="n">optimizer</span><span class="o">.</span><span class="n">step</span><span class="p">()</span>
            <span class="n">losses</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">loss</span><span class="o">.</span><span class="n">item</span><span class="p">())</span>

        <span class="n">results</span><span class="p">[</span><span class="n">name</span><span class="p">]</span> <span class="o">=</span> <span class="n">losses</span>

    <span class="c1"># Plot convergence</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">10</span><span class="p">,</span> <span class="mi">6</span><span class="p">))</span>
    <span class="k">for</span> <span class="n">name</span><span class="p">,</span> <span class="n">losses</span> <span class="ow">in</span> <span class="n">results</span><span class="o">.</span><span class="n">items</span><span class="p">():</span>
        <span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">losses</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="n">name</span><span class="p">,</span> <span class="n">linewidth</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s1">&#39;Iteration&#39;</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s1">&#39;Loss&#39;</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s1">&#39;SGD Variants Convergence Comparison&#39;</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">legend</span><span class="p">()</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">yscale</span><span class="p">(</span><span class="s1">&#39;log&#39;</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">grid</span><span class="p">(</span><span class="kc">True</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.3</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">savefig</span><span class="p">(</span><span class="s1">&#39;sgd_comparison.png&#39;</span><span class="p">,</span> <span class="n">dpi</span><span class="o">=</span><span class="mi">150</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">close</span><span class="p">()</span>

<span class="n">compare_sgd_variants</span><span class="p">()</span>
</code></pre></div>

<hr />
<h2 id="3">3. ì ì‘ì  í•™ìŠµë¥  ë°©ë²•<a class="header-link" href="#3" title="Permanent link">&para;</a></h2>
<h3 id="31-adagrad">3.1 Adagrad<a class="header-link" href="#31-adagrad" title="Permanent link">&para;</a></h3>
<p>AdagradëŠ” ê³¼ê±° ê·¸ë˜ë””ì–¸íŠ¸ì— ê¸°ë°˜í•˜ì—¬ íŒŒë¼ë¯¸í„°ë³„ë¡œ í•™ìŠµë¥ ì„ ì¡°ì •í•©ë‹ˆë‹¤:</p>
<p>$$
\begin{align}
g_t &= \nabla_\theta \mathcal{L}(\theta_t) \\
G_t &= G_{t-1} + g_t \odot g_t \\
\theta_{t+1} &= \theta_t - \frac{\eta}{\sqrt{G_t + \epsilon}} \odot g_t
\end{align}
$$</p>
<p>ì—¬ê¸°ì„œ $G_t$ëŠ” ì œê³±ëœ ê·¸ë˜ë””ì–¸íŠ¸ë¥¼ ìš”ì†Œë³„ë¡œ ì¶•ì í•©ë‹ˆë‹¤.</p>
<p><strong>ì¥ì :</strong>
- ìë™ í•™ìŠµë¥  ì–´ë‹ë§
- ë“œë¬¸ íŠ¹ì„±ì— ëŒ€í•´ ë” í° ì—…ë°ì´íŠ¸
- í¬ì†Œ ë°ì´í„°ì— ì˜ ì‘ë™</p>
<p><strong>ë‹¨ì :</strong>
- í•™ìŠµë¥ ì´ ë‹¨ì¡° ê°ì†Œí•¨
- ë„ˆë¬´ ì¼ì° í•™ìŠµì„ ë©ˆì¶œ ìˆ˜ ìˆìŒ</p>
<div class="highlight"><pre><span></span><code><span class="c1"># PyTorch Adagrad</span>
<span class="n">optimizer</span> <span class="o">=</span> <span class="n">optim</span><span class="o">.</span><span class="n">Adagrad</span><span class="p">(</span><span class="n">model</span><span class="o">.</span><span class="n">parameters</span><span class="p">(),</span> <span class="n">lr</span><span class="o">=</span><span class="mf">0.01</span><span class="p">,</span> <span class="n">eps</span><span class="o">=</span><span class="mf">1e-8</span><span class="p">)</span>

<span class="c1"># Adagrad is useful for NLP with sparse features</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">torch.nn.functional</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">F</span>

<span class="k">class</span><span class="w"> </span><span class="nc">WordEmbeddingModel</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="k">def</span><span class="w"> </span><span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">vocab_size</span><span class="p">,</span> <span class="n">embed_dim</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">embedding</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Embedding</span><span class="p">(</span><span class="n">vocab_size</span><span class="p">,</span> <span class="n">embed_dim</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">fc</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">embed_dim</span><span class="p">,</span> <span class="n">vocab_size</span><span class="p">)</span>

    <span class="k">def</span><span class="w"> </span><span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
        <span class="n">embedded</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">embedding</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">fc</span><span class="p">(</span><span class="n">embedded</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">))</span>

<span class="n">model</span> <span class="o">=</span> <span class="n">WordEmbeddingModel</span><span class="p">(</span><span class="n">vocab_size</span><span class="o">=</span><span class="mi">10000</span><span class="p">,</span> <span class="n">embed_dim</span><span class="o">=</span><span class="mi">128</span><span class="p">)</span>
<span class="n">optimizer</span> <span class="o">=</span> <span class="n">optim</span><span class="o">.</span><span class="n">Adagrad</span><span class="p">(</span><span class="n">model</span><span class="o">.</span><span class="n">parameters</span><span class="p">(),</span> <span class="n">lr</span><span class="o">=</span><span class="mf">0.01</span><span class="p">)</span>  <span class="c1"># Good for sparse embeddings</span>
</code></pre></div>

<h3 id="32-rmsprop">3.2 RMSprop<a class="header-link" href="#32-rmsprop" title="Permanent link">&para;</a></h3>
<p>RMSpropì€ ì§€ìˆ˜ ì´ë™ í‰ê· ì„ ì‚¬ìš©í•˜ì—¬ Adagradì˜ ê³µê²©ì ì¸ í•™ìŠµë¥  ê°ì†Œë¥¼ ìˆ˜ì •í•©ë‹ˆë‹¤:</p>
<p>$$
\begin{align}
g_t &= \nabla_\theta \mathcal{L}(\theta_t) \\
E[g^2]_t &= \beta E[g^2]_{t-1} + (1-\beta) g_t \odot g_t \\
\theta_{t+1} &= \theta_t - \frac{\eta}{\sqrt{E[g^2]_t + \epsilon}} \odot g_t
\end{align}
$$</p>
<p>ì—¬ê¸°ì„œ $\beta \in [0, 1)$ (ì¼ë°˜ì ìœ¼ë¡œ 0.9).</p>
<div class="highlight"><pre><span></span><code><span class="c1"># PyTorch RMSprop</span>
<span class="n">optimizer</span> <span class="o">=</span> <span class="n">optim</span><span class="o">.</span><span class="n">RMSprop</span><span class="p">(</span><span class="n">model</span><span class="o">.</span><span class="n">parameters</span><span class="p">(),</span> <span class="n">lr</span><span class="o">=</span><span class="mf">0.001</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.9</span><span class="p">,</span> <span class="n">eps</span><span class="o">=</span><span class="mf">1e-8</span><span class="p">)</span>

<span class="c1"># RMSprop is popular for RNNs</span>
<span class="k">class</span><span class="w"> </span><span class="nc">SimpleRNN</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="k">def</span><span class="w"> </span><span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">input_size</span><span class="p">,</span> <span class="n">hidden_size</span><span class="p">,</span> <span class="n">output_size</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">rnn</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">RNN</span><span class="p">(</span><span class="n">input_size</span><span class="p">,</span> <span class="n">hidden_size</span><span class="p">,</span> <span class="n">batch_first</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">fc</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">hidden_size</span><span class="p">,</span> <span class="n">output_size</span><span class="p">)</span>

    <span class="k">def</span><span class="w"> </span><span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
        <span class="n">out</span><span class="p">,</span> <span class="n">_</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">rnn</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">fc</span><span class="p">(</span><span class="n">out</span><span class="p">[:,</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="p">:])</span>

<span class="n">rnn_model</span> <span class="o">=</span> <span class="n">SimpleRNN</span><span class="p">(</span><span class="n">input_size</span><span class="o">=</span><span class="mi">10</span><span class="p">,</span> <span class="n">hidden_size</span><span class="o">=</span><span class="mi">64</span><span class="p">,</span> <span class="n">output_size</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
<span class="n">optimizer</span> <span class="o">=</span> <span class="n">optim</span><span class="o">.</span><span class="n">RMSprop</span><span class="p">(</span><span class="n">rnn_model</span><span class="o">.</span><span class="n">parameters</span><span class="p">(),</span> <span class="n">lr</span><span class="o">=</span><span class="mf">0.001</span><span class="p">)</span>  <span class="c1"># Good for RNNs</span>
</code></pre></div>

<h3 id="33-adam-adaptive-moment-estimation">3.3 Adam (Adaptive Moment Estimation)<a class="header-link" href="#33-adam-adaptive-moment-estimation" title="Permanent link">&para;</a></h3>
<p>Adamì€ ëª¨ë©˜í…€ê³¼ RMSpropì„ ê²°í•©í•˜ì—¬ 1ì°¨ ë° 2ì°¨ ëª¨ë©˜íŠ¸ ì¶”ì •ì¹˜ë¥¼ ëª¨ë‘ ìœ ì§€í•©ë‹ˆë‹¤:</p>
<p>$$
\begin{align}
g_t &= \nabla_\theta \mathcal{L}(\theta_t) \\
m_t &= \beta_1 m_{t-1} + (1-\beta_1) g_t \quad \text{(1ì°¨ ëª¨ë©˜íŠ¸)} \\
v_t &= \beta_2 v_{t-1} + (1-\beta_2) g_t \odot g_t \quad \text{(2ì°¨ ëª¨ë©˜íŠ¸)} \\
\hat{m}_t &= \frac{m_t}{1-\beta_1^t} \quad \text{(í¸í–¥ ë³´ì •)} \\
\hat{v}_t &= \frac{v_t}{1-\beta_2^t} \quad \text{(í¸í–¥ ë³´ì •)} \\
\theta_{t+1} &= \theta_t - \frac{\eta}{\sqrt{\hat{v}_t} + \epsilon} \hat{m}_t
\end{align}
$$</p>
<p><strong>ê¸°ë³¸ í•˜ì´í¼íŒŒë¼ë¯¸í„°</strong> (ì‹¤ì œë¡œ ì˜ ì‘ë™í•¨):
- $\beta_1 = 0.9$ (ëª¨ë©˜í…€)
- $\beta_2 = 0.999$ (RMSprop ê°ì‡ )
- $\epsilon = 10^{-8}$</p>
<p><strong>ìˆ˜ë™ êµ¬í˜„:</strong></p>
<div class="highlight"><pre><span></span><code><span class="k">class</span><span class="w"> </span><span class="nc">AdamOptimizer</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Manual implementation of Adam optimizer&quot;&quot;&quot;</span>
    <span class="k">def</span><span class="w"> </span><span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">params</span><span class="p">,</span> <span class="n">lr</span><span class="o">=</span><span class="mf">0.001</span><span class="p">,</span> <span class="n">betas</span><span class="o">=</span><span class="p">(</span><span class="mf">0.9</span><span class="p">,</span> <span class="mf">0.999</span><span class="p">),</span> <span class="n">eps</span><span class="o">=</span><span class="mf">1e-8</span><span class="p">,</span> <span class="n">weight_decay</span><span class="o">=</span><span class="mi">0</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">params</span> <span class="o">=</span> <span class="nb">list</span><span class="p">(</span><span class="n">params</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">lr</span> <span class="o">=</span> <span class="n">lr</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">beta1</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">beta2</span> <span class="o">=</span> <span class="n">betas</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">eps</span> <span class="o">=</span> <span class="n">eps</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">weight_decay</span> <span class="o">=</span> <span class="n">weight_decay</span>

        <span class="c1"># Initialize moment estimates</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">m</span> <span class="o">=</span> <span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">zeros_like</span><span class="p">(</span><span class="n">p</span><span class="p">)</span> <span class="k">for</span> <span class="n">p</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">params</span><span class="p">]</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">v</span> <span class="o">=</span> <span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">zeros_like</span><span class="p">(</span><span class="n">p</span><span class="p">)</span> <span class="k">for</span> <span class="n">p</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">params</span><span class="p">]</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">t</span> <span class="o">=</span> <span class="mi">0</span>

    <span class="k">def</span><span class="w"> </span><span class="nf">zero_grad</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="k">for</span> <span class="n">p</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">params</span><span class="p">:</span>
            <span class="k">if</span> <span class="n">p</span><span class="o">.</span><span class="n">grad</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
                <span class="n">p</span><span class="o">.</span><span class="n">grad</span><span class="o">.</span><span class="n">zero_</span><span class="p">()</span>

    <span class="k">def</span><span class="w"> </span><span class="nf">step</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">t</span> <span class="o">+=</span> <span class="mi">1</span>

        <span class="k">with</span> <span class="n">torch</span><span class="o">.</span><span class="n">no_grad</span><span class="p">():</span>
            <span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">p</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">params</span><span class="p">):</span>
                <span class="k">if</span> <span class="n">p</span><span class="o">.</span><span class="n">grad</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
                    <span class="k">continue</span>

                <span class="n">grad</span> <span class="o">=</span> <span class="n">p</span><span class="o">.</span><span class="n">grad</span>

                <span class="c1"># Weight decay (L2 regularization)</span>
                <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">weight_decay</span> <span class="o">!=</span> <span class="mi">0</span><span class="p">:</span>
                    <span class="n">grad</span> <span class="o">=</span> <span class="n">grad</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">p</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">weight_decay</span><span class="p">)</span>

                <span class="c1"># Update biased first moment estimate</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">m</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">beta1</span> <span class="o">*</span> <span class="bp">self</span><span class="o">.</span><span class="n">m</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">+</span> <span class="p">(</span><span class="mi">1</span> <span class="o">-</span> <span class="bp">self</span><span class="o">.</span><span class="n">beta1</span><span class="p">)</span> <span class="o">*</span> <span class="n">grad</span>

                <span class="c1"># Update biased second raw moment estimate</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">v</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">beta2</span> <span class="o">*</span> <span class="bp">self</span><span class="o">.</span><span class="n">v</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">+</span> <span class="p">(</span><span class="mi">1</span> <span class="o">-</span> <span class="bp">self</span><span class="o">.</span><span class="n">beta2</span><span class="p">)</span> <span class="o">*</span> <span class="n">grad</span><span class="o">.</span><span class="n">pow</span><span class="p">(</span><span class="mi">2</span><span class="p">)</span>

                <span class="c1"># Compute bias-corrected first moment estimate</span>
                <span class="n">m_hat</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">m</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">/</span> <span class="p">(</span><span class="mi">1</span> <span class="o">-</span> <span class="bp">self</span><span class="o">.</span><span class="n">beta1</span> <span class="o">**</span> <span class="bp">self</span><span class="o">.</span><span class="n">t</span><span class="p">)</span>

                <span class="c1"># Compute bias-corrected second raw moment estimate</span>
                <span class="n">v_hat</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">v</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">/</span> <span class="p">(</span><span class="mi">1</span> <span class="o">-</span> <span class="bp">self</span><span class="o">.</span><span class="n">beta2</span> <span class="o">**</span> <span class="bp">self</span><span class="o">.</span><span class="n">t</span><span class="p">)</span>

                <span class="c1"># Update parameters</span>
                <span class="n">p</span><span class="o">.</span><span class="n">add_</span><span class="p">(</span><span class="n">m_hat</span> <span class="o">/</span> <span class="p">(</span><span class="n">v_hat</span><span class="o">.</span><span class="n">sqrt</span><span class="p">()</span> <span class="o">+</span> <span class="bp">self</span><span class="o">.</span><span class="n">eps</span><span class="p">),</span> <span class="n">alpha</span><span class="o">=-</span><span class="bp">self</span><span class="o">.</span><span class="n">lr</span><span class="p">)</span>

<span class="c1"># Verify against PyTorch implementation</span>
<span class="k">def</span><span class="w"> </span><span class="nf">test_adam_implementation</span><span class="p">():</span>
    <span class="n">torch</span><span class="o">.</span><span class="n">manual_seed</span><span class="p">(</span><span class="mi">42</span><span class="p">)</span>

    <span class="c1"># Create two identical models</span>
    <span class="n">model1</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Sequential</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="mi">10</span><span class="p">,</span> <span class="mi">5</span><span class="p">),</span> <span class="n">nn</span><span class="o">.</span><span class="n">ReLU</span><span class="p">(),</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="mi">5</span><span class="p">,</span> <span class="mi">1</span><span class="p">))</span>
    <span class="n">model2</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Sequential</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="mi">10</span><span class="p">,</span> <span class="mi">5</span><span class="p">),</span> <span class="n">nn</span><span class="o">.</span><span class="n">ReLU</span><span class="p">(),</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="mi">5</span><span class="p">,</span> <span class="mi">1</span><span class="p">))</span>
    <span class="n">model2</span><span class="o">.</span><span class="n">load_state_dict</span><span class="p">(</span><span class="n">model1</span><span class="o">.</span><span class="n">state_dict</span><span class="p">())</span>

    <span class="c1"># Use both optimizers</span>
    <span class="n">opt_custom</span> <span class="o">=</span> <span class="n">AdamOptimizer</span><span class="p">(</span><span class="n">model1</span><span class="o">.</span><span class="n">parameters</span><span class="p">(),</span> <span class="n">lr</span><span class="o">=</span><span class="mf">0.001</span><span class="p">)</span>
    <span class="n">opt_torch</span> <span class="o">=</span> <span class="n">optim</span><span class="o">.</span><span class="n">Adam</span><span class="p">(</span><span class="n">model2</span><span class="o">.</span><span class="n">parameters</span><span class="p">(),</span> <span class="n">lr</span><span class="o">=</span><span class="mf">0.001</span><span class="p">)</span>

    <span class="n">criterion</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">MSELoss</span><span class="p">()</span>

    <span class="c1"># Run a few steps</span>
    <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">10</span><span class="p">):</span>
        <span class="n">X</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">32</span><span class="p">,</span> <span class="mi">10</span><span class="p">)</span>
        <span class="n">y</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">32</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>

        <span class="c1"># Custom Adam</span>
        <span class="n">opt_custom</span><span class="o">.</span><span class="n">zero_grad</span><span class="p">()</span>
        <span class="n">loss1</span> <span class="o">=</span> <span class="n">criterion</span><span class="p">(</span><span class="n">model1</span><span class="p">(</span><span class="n">X</span><span class="p">),</span> <span class="n">y</span><span class="p">)</span>
        <span class="n">loss1</span><span class="o">.</span><span class="n">backward</span><span class="p">()</span>
        <span class="n">opt_custom</span><span class="o">.</span><span class="n">step</span><span class="p">()</span>

        <span class="c1"># PyTorch Adam</span>
        <span class="n">opt_torch</span><span class="o">.</span><span class="n">zero_grad</span><span class="p">()</span>
        <span class="n">loss2</span> <span class="o">=</span> <span class="n">criterion</span><span class="p">(</span><span class="n">model2</span><span class="p">(</span><span class="n">X</span><span class="p">),</span> <span class="n">y</span><span class="p">)</span>
        <span class="n">loss2</span><span class="o">.</span><span class="n">backward</span><span class="p">()</span>
        <span class="n">opt_torch</span><span class="o">.</span><span class="n">step</span><span class="p">()</span>

    <span class="c1"># Check parameters are close (may have small numerical differences)</span>
    <span class="k">for</span> <span class="n">p1</span><span class="p">,</span> <span class="n">p2</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">(</span><span class="n">model1</span><span class="o">.</span><span class="n">parameters</span><span class="p">(),</span> <span class="n">model2</span><span class="o">.</span><span class="n">parameters</span><span class="p">()):</span>
        <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Param diff: </span><span class="si">{</span><span class="n">torch</span><span class="o">.</span><span class="n">max</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">abs</span><span class="p">(</span><span class="n">p1</span><span class="w"> </span><span class="o">-</span><span class="w"> </span><span class="n">p2</span><span class="p">))</span><span class="o">.</span><span class="n">item</span><span class="p">()</span><span class="si">:</span><span class="s2">.2e</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>

<span class="n">test_adam_implementation</span><span class="p">()</span>

<span class="c1"># PyTorch Adam</span>
<span class="n">optimizer</span> <span class="o">=</span> <span class="n">optim</span><span class="o">.</span><span class="n">Adam</span><span class="p">(</span><span class="n">model</span><span class="o">.</span><span class="n">parameters</span><span class="p">(),</span> <span class="n">lr</span><span class="o">=</span><span class="mf">0.001</span><span class="p">,</span> <span class="n">betas</span><span class="o">=</span><span class="p">(</span><span class="mf">0.9</span><span class="p">,</span> <span class="mf">0.999</span><span class="p">),</span> <span class="n">eps</span><span class="o">=</span><span class="mf">1e-8</span><span class="p">)</span>
</code></pre></div>

<h3 id="34-adamw-adam">3.4 AdamW (ë¶„ë¦¬ëœ ê°€ì¤‘ì¹˜ ê°ì‡ ë¥¼ ê°€ì§„ Adam)<a class="header-link" href="#34-adamw-adam" title="Permanent link">&para;</a></h3>
<p><strong>í•µì‹¬ í†µì°°</strong>: L2 ì •ê·œí™”ì™€ ê°€ì¤‘ì¹˜ ê°ì‡ ëŠ” SGDì—ì„œëŠ” ë™ë“±í•˜ì§€ë§Œ Adamì—ì„œëŠ” ê·¸ë ‡ì§€ ì•ŠìŠµë‹ˆë‹¤!</p>
<p><strong>L2 ì •ê·œí™”</strong> (ì „í†µì ):
$$
\mathcal{L}_{\text{total}} = \mathcal{L} + \frac{\lambda}{2} ||\theta||^2
$$</p>
<p><strong>ê°€ì¤‘ì¹˜ ê°ì‡ </strong> (ë¶„ë¦¬):
$$
\theta_{t+1} = (1 - \eta \lambda) \theta_t - \frac{\eta}{\sqrt{\hat{v}_t} + \epsilon} \hat{m}_t
$$</p>
<p>AdamWëŠ” ê·¸ë˜ë””ì–¸íŠ¸ë¥¼ í†µí•´ì„œê°€ ì•„ë‹ˆë¼ íŒŒë¼ë¯¸í„°ì— ì§ì ‘ ê°€ì¤‘ì¹˜ ê°ì‡ ë¥¼ ì ìš©í•©ë‹ˆë‹¤:</p>
<div class="highlight"><pre><span></span><code><span class="c1"># Adam with L2 regularization (traditional)</span>
<span class="n">optimizer_adam</span> <span class="o">=</span> <span class="n">optim</span><span class="o">.</span><span class="n">Adam</span><span class="p">(</span><span class="n">model</span><span class="o">.</span><span class="n">parameters</span><span class="p">(),</span> <span class="n">lr</span><span class="o">=</span><span class="mf">0.001</span><span class="p">,</span> <span class="n">weight_decay</span><span class="o">=</span><span class="mf">0.01</span><span class="p">)</span>

<span class="c1"># AdamW with decoupled weight decay (better!)</span>
<span class="n">optimizer_adamw</span> <span class="o">=</span> <span class="n">optim</span><span class="o">.</span><span class="n">AdamW</span><span class="p">(</span><span class="n">model</span><span class="o">.</span><span class="n">parameters</span><span class="p">(),</span> <span class="n">lr</span><span class="o">=</span><span class="mf">0.001</span><span class="p">,</span> <span class="n">weight_decay</span><span class="o">=</span><span class="mf">0.01</span><span class="p">)</span>

<span class="c1"># Visualize the difference</span>
<span class="k">def</span><span class="w"> </span><span class="nf">compare_adam_adamw</span><span class="p">():</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Show difference between Adam and AdamW&quot;&quot;&quot;</span>
    <span class="n">torch</span><span class="o">.</span><span class="n">manual_seed</span><span class="p">(</span><span class="mi">42</span><span class="p">)</span>

    <span class="c1"># Simple overparameterized model</span>
    <span class="n">model_adam</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Sequential</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="mi">10</span><span class="p">,</span> <span class="mi">100</span><span class="p">),</span> <span class="n">nn</span><span class="o">.</span><span class="n">ReLU</span><span class="p">(),</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="mi">100</span><span class="p">,</span> <span class="mi">1</span><span class="p">))</span>
    <span class="n">model_adamw</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Sequential</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="mi">10</span><span class="p">,</span> <span class="mi">100</span><span class="p">),</span> <span class="n">nn</span><span class="o">.</span><span class="n">ReLU</span><span class="p">(),</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="mi">100</span><span class="p">,</span> <span class="mi">1</span><span class="p">))</span>
    <span class="n">model_adamw</span><span class="o">.</span><span class="n">load_state_dict</span><span class="p">(</span><span class="n">model_adam</span><span class="o">.</span><span class="n">state_dict</span><span class="p">())</span>

    <span class="n">opt_adam</span> <span class="o">=</span> <span class="n">optim</span><span class="o">.</span><span class="n">Adam</span><span class="p">(</span><span class="n">model_adam</span><span class="o">.</span><span class="n">parameters</span><span class="p">(),</span> <span class="n">lr</span><span class="o">=</span><span class="mf">0.001</span><span class="p">,</span> <span class="n">weight_decay</span><span class="o">=</span><span class="mf">0.01</span><span class="p">)</span>
    <span class="n">opt_adamw</span> <span class="o">=</span> <span class="n">optim</span><span class="o">.</span><span class="n">AdamW</span><span class="p">(</span><span class="n">model_adamw</span><span class="o">.</span><span class="n">parameters</span><span class="p">(),</span> <span class="n">lr</span><span class="o">=</span><span class="mf">0.001</span><span class="p">,</span> <span class="n">weight_decay</span><span class="o">=</span><span class="mf">0.01</span><span class="p">)</span>

    <span class="n">criterion</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">MSELoss</span><span class="p">()</span>

    <span class="c1"># Track weight norms</span>
    <span class="n">adam_norms</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="n">adamw_norms</span> <span class="o">=</span> <span class="p">[]</span>

    <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">100</span><span class="p">):</span>
        <span class="n">X</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">32</span><span class="p">,</span> <span class="mi">10</span><span class="p">)</span>
        <span class="n">y</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">32</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>

        <span class="c1"># Adam</span>
        <span class="n">opt_adam</span><span class="o">.</span><span class="n">zero_grad</span><span class="p">()</span>
        <span class="n">loss</span> <span class="o">=</span> <span class="n">criterion</span><span class="p">(</span><span class="n">model_adam</span><span class="p">(</span><span class="n">X</span><span class="p">),</span> <span class="n">y</span><span class="p">)</span>
        <span class="n">loss</span><span class="o">.</span><span class="n">backward</span><span class="p">()</span>
        <span class="n">opt_adam</span><span class="o">.</span><span class="n">step</span><span class="p">()</span>

        <span class="c1"># AdamW</span>
        <span class="n">opt_adamw</span><span class="o">.</span><span class="n">zero_grad</span><span class="p">()</span>
        <span class="n">loss</span> <span class="o">=</span> <span class="n">criterion</span><span class="p">(</span><span class="n">model_adamw</span><span class="p">(</span><span class="n">X</span><span class="p">),</span> <span class="n">y</span><span class="p">)</span>
        <span class="n">loss</span><span class="o">.</span><span class="n">backward</span><span class="p">()</span>
        <span class="n">opt_adamw</span><span class="o">.</span><span class="n">step</span><span class="p">()</span>

        <span class="c1"># Record weight norms</span>
        <span class="n">adam_norms</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">norm</span><span class="p">(</span><span class="n">model_adam</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">weight</span><span class="p">)</span><span class="o">.</span><span class="n">item</span><span class="p">())</span>
        <span class="n">adamw_norms</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">norm</span><span class="p">(</span><span class="n">model_adamw</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">weight</span><span class="p">)</span><span class="o">.</span><span class="n">item</span><span class="p">())</span>

    <span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">10</span><span class="p">,</span> <span class="mi">6</span><span class="p">))</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">adam_norms</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s1">&#39;Adam (L2 reg)&#39;</span><span class="p">,</span> <span class="n">linewidth</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">adamw_norms</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s1">&#39;AdamW (decoupled)&#39;</span><span class="p">,</span> <span class="n">linewidth</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s1">&#39;Iteration&#39;</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s1">&#39;Weight Norm&#39;</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s1">&#39;Adam vs AdamW: Weight Regularization&#39;</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">legend</span><span class="p">()</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">grid</span><span class="p">(</span><span class="kc">True</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.3</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">savefig</span><span class="p">(</span><span class="s1">&#39;adam_adamw_comparison.png&#39;</span><span class="p">,</span> <span class="n">dpi</span><span class="o">=</span><span class="mi">150</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">close</span><span class="p">()</span>

<span class="n">compare_adam_adamw</span><span class="p">()</span>
</code></pre></div>

<p><strong>AdamWë¥¼ ì‚¬ìš©í•  ë•Œ:</strong>
- ê°€ì¤‘ì¹˜ ê°ì‡ ë¥¼ ì‚¬ìš©í•  ë•ŒëŠ” ê±°ì˜ í•­ìƒ Adamë³´ë‹¤ AdamWë¥¼ ì„ í˜¸
- íŠ¸ëœìŠ¤í¬ë¨¸ ë° ëŒ€ê·œëª¨ ëª¨ë¸ì— íŠ¹íˆ ì¤‘ìš”
- BERT, GPT ë° ëŒ€ë¶€ë¶„ì˜ í˜„ëŒ€ NLP ëª¨ë¸ì˜ ê¸°ë³¸ ì˜µí‹°ë§ˆì´ì €</p>
<hr />
<h2 id="4">4. í˜„ëŒ€ì  ì˜µí‹°ë§ˆì´ì €<a class="header-link" href="#4" title="Permanent link">&para;</a></h2>
<h3 id="41-lamb-layer-wise-adaptive-moments-optimizer-for-batch-training">4.1 LAMB (Layer-wise Adaptive Moments optimizer for Batch training)<a class="header-link" href="#41-lamb-layer-wise-adaptive-moments-optimizer-for-batch-training" title="Permanent link">&para;</a></h3>
<p>LAMBëŠ” ë ˆì´ì–´ë³„ ì ì‘ì„ ì‚¬ìš©í•˜ì—¬ ëŒ€ê·œëª¨ ë°°ì¹˜ í•™ìŠµì„ ê°€ëŠ¥í•˜ê²Œ í•©ë‹ˆë‹¤:</p>
<p>$$
r_t = \frac{||\theta_t||}{||\hat{m}_t / (\sqrt{\hat{v}_t} + \epsilon)||} \cdot \frac{\hat{m}_t}{\sqrt{\hat{v}_t} + \epsilon}
$$</p>
<p>$$
\theta_{t+1} = \theta_t - \eta r_t
$$</p>
<p><strong>ì‚¬ìš© ì‚¬ë¡€</strong>: ë°°ì¹˜ í¬ê¸° 32Kë¡œ BERT í•™ìŠµ (Adamì€ 256).</p>
<div class="highlight"><pre><span></span><code><span class="c1"># LAMB is not in PyTorch by default, but available in apex or standalone</span>
<span class="c1"># pip install pytorch-lamb</span>

<span class="k">try</span><span class="p">:</span>
    <span class="kn">from</span><span class="w"> </span><span class="nn">pytorch_lamb</span><span class="w"> </span><span class="kn">import</span> <span class="n">Lamb</span>

    <span class="c1"># LAMB optimizer for large-batch training</span>
    <span class="n">optimizer</span> <span class="o">=</span> <span class="n">Lamb</span><span class="p">(</span>
        <span class="n">model</span><span class="o">.</span><span class="n">parameters</span><span class="p">(),</span>
        <span class="n">lr</span><span class="o">=</span><span class="mf">0.001</span><span class="p">,</span>
        <span class="n">betas</span><span class="o">=</span><span class="p">(</span><span class="mf">0.9</span><span class="p">,</span> <span class="mf">0.999</span><span class="p">),</span>
        <span class="n">eps</span><span class="o">=</span><span class="mf">1e-6</span><span class="p">,</span>
        <span class="n">weight_decay</span><span class="o">=</span><span class="mf">0.01</span><span class="p">,</span>
        <span class="n">bias_correction</span><span class="o">=</span><span class="kc">True</span>
    <span class="p">)</span>

    <span class="c1"># Typical use: scale learning rate with batch size</span>
    <span class="c1"># LR = base_lr * sqrt(batch_size / base_batch_size)</span>
    <span class="n">base_lr</span> <span class="o">=</span> <span class="mf">0.001</span>
    <span class="n">base_batch_size</span> <span class="o">=</span> <span class="mi">256</span>
    <span class="n">large_batch_size</span> <span class="o">=</span> <span class="mi">8192</span>

    <span class="n">scaled_lr</span> <span class="o">=</span> <span class="n">base_lr</span> <span class="o">*</span> <span class="p">(</span><span class="n">large_batch_size</span> <span class="o">/</span> <span class="n">base_batch_size</span><span class="p">)</span> <span class="o">**</span> <span class="mf">0.5</span>
    <span class="n">optimizer</span> <span class="o">=</span> <span class="n">Lamb</span><span class="p">(</span><span class="n">model</span><span class="o">.</span><span class="n">parameters</span><span class="p">(),</span> <span class="n">lr</span><span class="o">=</span><span class="n">scaled_lr</span><span class="p">,</span> <span class="n">weight_decay</span><span class="o">=</span><span class="mf">0.01</span><span class="p">)</span>

<span class="k">except</span> <span class="ne">ImportError</span><span class="p">:</span>
    <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;LAMB not installed. Install with: pip install pytorch-lamb&quot;</span><span class="p">)</span>

<span class="c1"># LARS (Layer-wise Adaptive Rate Scaling) - similar idea for CNNs</span>
<span class="c1"># Used for ImageNet training with large batches</span>
</code></pre></div>

<h3 id="42-adafactor">4.2 Adafactor<a class="header-link" href="#42-adafactor" title="Permanent link">&para;</a></h3>
<p>AdafactorëŠ” ì „ì²´ 2ì°¨ ëª¨ë©˜íŠ¸ë¥¼ ì €ì¥í•˜ì§€ ì•ŠìŒìœ¼ë¡œì¨ ë©”ëª¨ë¦¬ë¥¼ ì¤„ì…ë‹ˆë‹¤:</p>
<p><strong>í•µì‹¬ ì•„ì´ë””ì–´</strong>: 2ì°¨ ëª¨ë©˜íŠ¸ í–‰ë ¬ì„ ì¸ìˆ˜ë¶„í•´í•˜ì—¬ ë©”ëª¨ë¦¬ë¥¼ O(nm)ì—ì„œ O(n+m)ë¡œ ì¤„ì…ë‹ˆë‹¤.</p>
<div class="highlight"><pre><span></span><code><span class="c1"># PyTorch doesn&#39;t have built-in Adafactor, but transformers library does</span>
<span class="k">try</span><span class="p">:</span>
    <span class="kn">from</span><span class="w"> </span><span class="nn">transformers</span><span class="w"> </span><span class="kn">import</span> <span class="n">Adafactor</span>

    <span class="c1"># Adafactor: memory-efficient for large models</span>
    <span class="n">optimizer</span> <span class="o">=</span> <span class="n">Adafactor</span><span class="p">(</span>
        <span class="n">model</span><span class="o">.</span><span class="n">parameters</span><span class="p">(),</span>
        <span class="n">lr</span><span class="o">=</span><span class="mf">1e-3</span><span class="p">,</span>
        <span class="n">eps</span><span class="o">=</span><span class="p">(</span><span class="mf">1e-30</span><span class="p">,</span> <span class="mf">1e-3</span><span class="p">),</span>
        <span class="n">clip_threshold</span><span class="o">=</span><span class="mf">1.0</span><span class="p">,</span>
        <span class="n">decay_rate</span><span class="o">=-</span><span class="mf">0.8</span><span class="p">,</span>
        <span class="n">beta1</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
        <span class="n">weight_decay</span><span class="o">=</span><span class="mf">0.0</span><span class="p">,</span>
        <span class="n">relative_step</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
        <span class="n">scale_parameter</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
        <span class="n">warmup_init</span><span class="o">=</span><span class="kc">False</span>
    <span class="p">)</span>

    <span class="c1"># With relative step (learning rate is automatically computed)</span>
    <span class="n">optimizer</span> <span class="o">=</span> <span class="n">Adafactor</span><span class="p">(</span>
        <span class="n">model</span><span class="o">.</span><span class="n">parameters</span><span class="p">(),</span>
        <span class="n">scale_parameter</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
        <span class="n">relative_step</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
        <span class="n">warmup_init</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
        <span class="n">lr</span><span class="o">=</span><span class="kc">None</span>  <span class="c1"># LR is computed automatically</span>
    <span class="p">)</span>

<span class="k">except</span> <span class="ne">ImportError</span><span class="p">:</span>
    <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Adafactor not available. Install with: pip install transformers&quot;</span><span class="p">)</span>
</code></pre></div>

<p><strong>Adafactorë¥¼ ì‚¬ìš©í•  ë•Œ:</strong>
- ë§¤ìš° í° ëª¨ë¸ í•™ìŠµ (ìˆ˜ì‹­ì–µ ê°œì˜ íŒŒë¼ë¯¸í„°)
- ì œí•œëœ GPU ë©”ëª¨ë¦¬
- T5, UL2 ëª¨ë¸ì€ Adafactorë¡œ í•™ìŠµë¨</p>
<h3 id="43-lion-evolved-sign-momentum">4.3 Lion (Evolved Sign Momentum)<a class="header-link" href="#43-lion-evolved-sign-momentum" title="Permanent link">&para;</a></h3>
<p>Lionì€ ê·¸ë˜ë””ì–¸íŠ¸ì˜ ë¶€í˜¸ë§Œ ì‚¬ìš©í•˜ì—¬ ë©”ëª¨ë¦¬ì™€ ê³„ì‚°ì„ ì¤„ì…ë‹ˆë‹¤:</p>
<p>$$
\begin{align}
c_t &= \beta_1 m_{t-1} + (1-\beta_1) g_t \\
\theta_{t+1} &= \theta_t - \eta \cdot \text{sign}(c_t) \\
m_t &= \beta_2 m_{t-1} + (1-\beta_2) g_t
\end{align}
$$</p>
<p><strong>ì¥ì :</strong>
- 2ë°° ë©”ëª¨ë¦¬ íš¨ìœ¨ì  (ëª¨ë©˜í…€ë§Œ ì €ì¥, 2ì°¨ ëª¨ë©˜íŠ¸ ì—†ìŒ)
- ë” ë¹ ë¥¸ ê³„ì‚° (ì œê³±ê·¼ ì—°ì‚° ì—†ìŒ)
- Adamê³¼ ê²½ìŸí•˜ê±°ë‚˜ ë” ë‚˜ì€ ì„±ëŠ¥</p>
<div class="highlight"><pre><span></span><code><span class="c1"># Lion optimizer (pip install lion-pytorch)</span>
<span class="k">try</span><span class="p">:</span>
    <span class="kn">from</span><span class="w"> </span><span class="nn">lion_pytorch</span><span class="w"> </span><span class="kn">import</span> <span class="n">Lion</span>

    <span class="n">optimizer</span> <span class="o">=</span> <span class="n">Lion</span><span class="p">(</span>
        <span class="n">model</span><span class="o">.</span><span class="n">parameters</span><span class="p">(),</span>
        <span class="n">lr</span><span class="o">=</span><span class="mf">1e-4</span><span class="p">,</span>  <span class="c1"># Use ~3-10x smaller LR than Adam</span>
        <span class="n">betas</span><span class="o">=</span><span class="p">(</span><span class="mf">0.9</span><span class="p">,</span> <span class="mf">0.99</span><span class="p">),</span>
        <span class="n">weight_decay</span><span class="o">=</span><span class="mf">0.01</span>
    <span class="p">)</span>

    <span class="c1"># Typical usage: train vision transformer</span>
    <span class="c1"># Note: Lion needs smaller learning rate than Adam</span>

<span class="k">except</span> <span class="ne">ImportError</span><span class="p">:</span>
    <span class="c1"># Manual implementation (simplified)</span>
    <span class="k">class</span><span class="w"> </span><span class="nc">Lion</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">optim</span><span class="o">.</span><span class="n">Optimizer</span><span class="p">):</span>
        <span class="k">def</span><span class="w"> </span><span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">params</span><span class="p">,</span> <span class="n">lr</span><span class="o">=</span><span class="mf">1e-4</span><span class="p">,</span> <span class="n">betas</span><span class="o">=</span><span class="p">(</span><span class="mf">0.9</span><span class="p">,</span> <span class="mf">0.99</span><span class="p">),</span> <span class="n">weight_decay</span><span class="o">=</span><span class="mf">0.0</span><span class="p">):</span>
            <span class="n">defaults</span> <span class="o">=</span> <span class="nb">dict</span><span class="p">(</span><span class="n">lr</span><span class="o">=</span><span class="n">lr</span><span class="p">,</span> <span class="n">betas</span><span class="o">=</span><span class="n">betas</span><span class="p">,</span> <span class="n">weight_decay</span><span class="o">=</span><span class="n">weight_decay</span><span class="p">)</span>
            <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="n">params</span><span class="p">,</span> <span class="n">defaults</span><span class="p">)</span>

        <span class="k">def</span><span class="w"> </span><span class="nf">step</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
            <span class="k">for</span> <span class="n">group</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">param_groups</span><span class="p">:</span>
                <span class="k">for</span> <span class="n">p</span> <span class="ow">in</span> <span class="n">group</span><span class="p">[</span><span class="s1">&#39;params&#39;</span><span class="p">]:</span>
                    <span class="k">if</span> <span class="n">p</span><span class="o">.</span><span class="n">grad</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
                        <span class="k">continue</span>

                    <span class="n">grad</span> <span class="o">=</span> <span class="n">p</span><span class="o">.</span><span class="n">grad</span>
                    <span class="n">state</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">state</span><span class="p">[</span><span class="n">p</span><span class="p">]</span>

                    <span class="c1"># Initialize momentum</span>
                    <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">state</span><span class="p">)</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
                        <span class="n">state</span><span class="p">[</span><span class="s1">&#39;exp_avg&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">zeros_like</span><span class="p">(</span><span class="n">p</span><span class="p">)</span>

                    <span class="n">exp_avg</span> <span class="o">=</span> <span class="n">state</span><span class="p">[</span><span class="s1">&#39;exp_avg&#39;</span><span class="p">]</span>
                    <span class="n">beta1</span><span class="p">,</span> <span class="n">beta2</span> <span class="o">=</span> <span class="n">group</span><span class="p">[</span><span class="s1">&#39;betas&#39;</span><span class="p">]</span>

                    <span class="c1"># Weight decay</span>
                    <span class="k">if</span> <span class="n">group</span><span class="p">[</span><span class="s1">&#39;weight_decay&#39;</span><span class="p">]</span> <span class="o">!=</span> <span class="mi">0</span><span class="p">:</span>
                        <span class="n">p</span><span class="o">.</span><span class="n">mul_</span><span class="p">(</span><span class="mi">1</span> <span class="o">-</span> <span class="n">group</span><span class="p">[</span><span class="s1">&#39;lr&#39;</span><span class="p">]</span> <span class="o">*</span> <span class="n">group</span><span class="p">[</span><span class="s1">&#39;weight_decay&#39;</span><span class="p">])</span>

                    <span class="c1"># Update (sign of interpolated gradient)</span>
                    <span class="n">update</span> <span class="o">=</span> <span class="n">exp_avg</span><span class="o">.</span><span class="n">mul</span><span class="p">(</span><span class="n">beta1</span><span class="p">)</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">grad</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mi">1</span> <span class="o">-</span> <span class="n">beta1</span><span class="p">)</span><span class="o">.</span><span class="n">sign_</span><span class="p">()</span>
                    <span class="n">p</span><span class="o">.</span><span class="n">add_</span><span class="p">(</span><span class="n">update</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=-</span><span class="n">group</span><span class="p">[</span><span class="s1">&#39;lr&#39;</span><span class="p">])</span>

                    <span class="c1"># Update momentum</span>
                    <span class="n">exp_avg</span><span class="o">.</span><span class="n">mul_</span><span class="p">(</span><span class="n">beta2</span><span class="p">)</span><span class="o">.</span><span class="n">add_</span><span class="p">(</span><span class="n">grad</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mi">1</span> <span class="o">-</span> <span class="n">beta2</span><span class="p">)</span>

    <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Using simplified Lion implementation&quot;</span><span class="p">)</span>
</code></pre></div>

<h3 id="44-8-bit-adam-bitsandbytes">4.4 8-bit Adam (bitsandbytes)<a class="header-link" href="#44-8-bit-adam-bitsandbytes" title="Permanent link">&para;</a></h3>
<p>8-bit Adamì€ ì˜µí‹°ë§ˆì´ì € ìƒíƒœë¥¼ ì–‘ìí™”í•˜ì—¬ ë©”ëª¨ë¦¬ë¥¼ ì¤„ì…ë‹ˆë‹¤:</p>
<div class="highlight"><pre><span></span><code><span class="c1"># 8-bit Adam from bitsandbytes</span>
<span class="c1"># pip install bitsandbytes</span>

<span class="k">try</span><span class="p">:</span>
    <span class="kn">import</span><span class="w"> </span><span class="nn">bitsandbytes</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">bnb</span>

    <span class="c1"># 8-bit Adam: same performance, 75% less memory</span>
    <span class="n">optimizer</span> <span class="o">=</span> <span class="n">bnb</span><span class="o">.</span><span class="n">optim</span><span class="o">.</span><span class="n">Adam8bit</span><span class="p">(</span>
        <span class="n">model</span><span class="o">.</span><span class="n">parameters</span><span class="p">(),</span>
        <span class="n">lr</span><span class="o">=</span><span class="mf">1e-3</span><span class="p">,</span>
        <span class="n">betas</span><span class="o">=</span><span class="p">(</span><span class="mf">0.9</span><span class="p">,</span> <span class="mf">0.999</span><span class="p">),</span>
        <span class="n">eps</span><span class="o">=</span><span class="mf">1e-8</span><span class="p">,</span>
        <span class="n">weight_decay</span><span class="o">=</span><span class="mf">0.01</span>
    <span class="p">)</span>

    <span class="c1"># Also available: AdamW8bit, Lion8bit, etc.</span>
    <span class="n">optimizer</span> <span class="o">=</span> <span class="n">bnb</span><span class="o">.</span><span class="n">optim</span><span class="o">.</span><span class="n">AdamW8bit</span><span class="p">(</span><span class="n">model</span><span class="o">.</span><span class="n">parameters</span><span class="p">(),</span> <span class="n">lr</span><span class="o">=</span><span class="mf">1e-3</span><span class="p">)</span>

    <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Using 8-bit Adam - significant memory savings!&quot;</span><span class="p">)</span>

<span class="k">except</span> <span class="ne">ImportError</span><span class="p">:</span>
    <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;bitsandbytes not installed. Install with: pip install bitsandbytes&quot;</span><span class="p">)</span>
    <span class="n">optimizer</span> <span class="o">=</span> <span class="n">optim</span><span class="o">.</span><span class="n">AdamW</span><span class="p">(</span><span class="n">model</span><span class="o">.</span><span class="n">parameters</span><span class="p">(),</span> <span class="n">lr</span><span class="o">=</span><span class="mf">1e-3</span><span class="p">)</span>
</code></pre></div>

<p><strong>8-bit ì˜µí‹°ë§ˆì´ì €ë¥¼ ì‚¬ìš©í•  ë•Œ:</strong>
- ì œí•œëœ GPU ë©”ëª¨ë¦¬ë¡œ ëŒ€ê·œëª¨ ëª¨ë¸ í•™ìŠµ
- LLM íŒŒì¸íŠœë‹ (ì˜ˆ: ì†Œë¹„ì GPUì—ì„œ LLaMA-7B)
- ìµœì†Œí•œì˜ ì„±ëŠ¥ ì˜í–¥ (~0.1% ì°¨ì´)</p>
<h3 id="45-sophia-second-order-clipped-stochastic-optimization">4.5 Sophia (Second-order Clipped Stochastic Optimization)<a class="header-link" href="#45-sophia-second-order-clipped-stochastic-optimization" title="Permanent link">&para;</a></h3>
<p>SophiaëŠ” ë” ë‚˜ì€ ê³¡ë¥  ì ì‘ì„ ìœ„í•´ í—¤ì‹œì•ˆ ëŒ€ê° ì •ë³´ë¥¼ ì‚¬ìš©í•©ë‹ˆë‹¤:</p>
<p>$$
\theta_{t+1} = \theta_t - \eta \cdot \frac{m_t}{\max\{h_t, \epsilon\}}
$$</p>
<p>ì—¬ê¸°ì„œ $h_t$ëŠ” í—¤ì‹œì•ˆ ëŒ€ê°ì„ ì˜ ì¶”ì •ì¹˜ì…ë‹ˆë‹¤.</p>
<p><strong>ì‚¬ìš© ì‚¬ë¡€</strong>: Adamë³´ë‹¤ 2ë°° ë¹ ë¥´ê²Œ ì–¸ì–´ ëª¨ë¸ í•™ìŠµ.</p>
<div class="highlight"><pre><span></span><code><span class="c1"># Sophia is experimental and requires custom implementation</span>
<span class="c1"># Available at: https://github.com/Liuhong99/Sophia</span>

<span class="c1"># Typical usage for LLM training:</span>
<span class="c1"># optimizer = SophiaG(model.parameters(), lr=2e-4, rho=0.04, weight_decay=0.1)</span>
</code></pre></div>

<h3 id="46">4.6 ì˜µí‹°ë§ˆì´ì € ë¹„êµ í‘œ<a class="header-link" href="#46" title="Permanent link">&para;</a></h3>
<table>
<thead>
<tr>
<th>ì˜µí‹°ë§ˆì´ì €</th>
<th>ë©”ëª¨ë¦¬</th>
<th>ì†ë„</th>
<th>ì‚¬ìš© ì‚¬ë¡€</th>
<th>ì¼ë°˜ì  LR</th>
</tr>
</thead>
<tbody>
<tr>
<td>SGD+Momentum</td>
<td>ë‚®ìŒ</td>
<td>ë¹ ë¦„</td>
<td>CNN, ResNet</td>
<td>0.1 - 0.01</td>
</tr>
<tr>
<td>Adam</td>
<td>ë†’ìŒ</td>
<td>ì¤‘ê°„</td>
<td>ì¼ë°˜, íŠ¸ëœìŠ¤í¬ë¨¸</td>
<td>1e-3 - 1e-4</td>
</tr>
<tr>
<td>AdamW</td>
<td>ë†’ìŒ</td>
<td>ì¤‘ê°„</td>
<td>íŠ¸ëœìŠ¤í¬ë¨¸, íŒŒì¸íŠœë‹</td>
<td>1e-3 - 1e-4</td>
</tr>
<tr>
<td>LAMB</td>
<td>ë†’ìŒ</td>
<td>ì¤‘ê°„</td>
<td>ëŒ€ê·œëª¨ ë°°ì¹˜ í•™ìŠµ</td>
<td>1e-2 - 1e-3</td>
</tr>
<tr>
<td>Adafactor</td>
<td>ì¤‘ê°„</td>
<td>ëŠë¦¼</td>
<td>ë§¤ìš° í° ëª¨ë¸</td>
<td>1e-3 - 1e-2</td>
</tr>
<tr>
<td>Lion</td>
<td>ì¤‘ê°„</td>
<td>ë¹ ë¦„</td>
<td>Vision Transformer</td>
<td>1e-4 - 1e-5</td>
</tr>
<tr>
<td>8-bit Adam</td>
<td>ë‚®ìŒ</td>
<td>ì¤‘ê°„</td>
<td>ì œí•œëœ GPU ë©”ëª¨ë¦¬</td>
<td>1e-3 - 1e-4</td>
</tr>
<tr>
<td>Sophia</td>
<td>ë†’ìŒ</td>
<td>ì¤‘ê°„</td>
<td>LLM ì‚¬ì „í•™ìŠµ</td>
<td>2e-4 - 5e-4</td>
</tr>
</tbody>
</table>
<hr />
<h2 id="5">5. í•™ìŠµë¥  ìŠ¤ì¼€ì¤„ëŸ¬<a class="header-link" href="#5" title="Permanent link">&para;</a></h2>
<h3 id="51">5.1 ìŠ¤í… ê¸°ë°˜ ìŠ¤ì¼€ì¤„ëŸ¬<a class="header-link" href="#51" title="Permanent link">&para;</a></h3>
<p><strong>StepLR</strong>: step_size ì—í­ë§ˆë‹¤ LRì„ gammaë¡œ ê°ì†Œ:</p>
<div class="highlight"><pre><span></span><code><span class="c1"># StepLR: multiply LR by 0.1 every 30 epochs</span>
<span class="n">optimizer</span> <span class="o">=</span> <span class="n">optim</span><span class="o">.</span><span class="n">SGD</span><span class="p">(</span><span class="n">model</span><span class="o">.</span><span class="n">parameters</span><span class="p">(),</span> <span class="n">lr</span><span class="o">=</span><span class="mf">0.1</span><span class="p">)</span>
<span class="n">scheduler</span> <span class="o">=</span> <span class="n">optim</span><span class="o">.</span><span class="n">lr_scheduler</span><span class="o">.</span><span class="n">StepLR</span><span class="p">(</span><span class="n">optimizer</span><span class="p">,</span> <span class="n">step_size</span><span class="o">=</span><span class="mi">30</span><span class="p">,</span> <span class="n">gamma</span><span class="o">=</span><span class="mf">0.1</span><span class="p">)</span>

<span class="c1"># Epoch 0-29: lr=0.1, Epoch 30-59: lr=0.01, Epoch 60+: lr=0.001</span>
<span class="k">for</span> <span class="n">epoch</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">100</span><span class="p">):</span>
    <span class="n">train_one_epoch</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">optimizer</span><span class="p">)</span>
    <span class="n">scheduler</span><span class="o">.</span><span class="n">step</span><span class="p">()</span>  <span class="c1"># Update LR at end of epoch</span>

<span class="c1"># MultiStepLR: decay at specific milestones</span>
<span class="n">scheduler</span> <span class="o">=</span> <span class="n">optim</span><span class="o">.</span><span class="n">lr_scheduler</span><span class="o">.</span><span class="n">MultiStepLR</span><span class="p">(</span><span class="n">optimizer</span><span class="p">,</span> <span class="n">milestones</span><span class="o">=</span><span class="p">[</span><span class="mi">30</span><span class="p">,</span> <span class="mi">80</span><span class="p">],</span> <span class="n">gamma</span><span class="o">=</span><span class="mf">0.1</span><span class="p">)</span>
<span class="c1"># Epoch 0-29: lr=0.1, Epoch 30-79: lr=0.01, Epoch 80+: lr=0.001</span>
</code></pre></div>

<h3 id="52">5.2 ì§€ìˆ˜ ê°ì†Œ<a class="header-link" href="#52" title="Permanent link">&para;</a></h3>
<p><strong>ExponentialLR</strong>: ë§¤ ì—í­ë§ˆë‹¤ LRì„ gammaë¡œ ê³±í•¨:</p>
<p>$$
\eta_t = \eta_0 \cdot \gamma^t
$$</p>
<div class="highlight"><pre><span></span><code><span class="c1"># Exponential decay</span>
<span class="n">optimizer</span> <span class="o">=</span> <span class="n">optim</span><span class="o">.</span><span class="n">Adam</span><span class="p">(</span><span class="n">model</span><span class="o">.</span><span class="n">parameters</span><span class="p">(),</span> <span class="n">lr</span><span class="o">=</span><span class="mf">0.001</span><span class="p">)</span>
<span class="n">scheduler</span> <span class="o">=</span> <span class="n">optim</span><span class="o">.</span><span class="n">lr_scheduler</span><span class="o">.</span><span class="n">ExponentialLR</span><span class="p">(</span><span class="n">optimizer</span><span class="p">,</span> <span class="n">gamma</span><span class="o">=</span><span class="mf">0.95</span><span class="p">)</span>

<span class="c1"># Each epoch: lr *= 0.95</span>
<span class="k">for</span> <span class="n">epoch</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">100</span><span class="p">):</span>
    <span class="n">train_one_epoch</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">optimizer</span><span class="p">)</span>
    <span class="n">scheduler</span><span class="o">.</span><span class="n">step</span><span class="p">()</span>
</code></pre></div>

<h3 id="53">5.3 ì½”ì‚¬ì¸ ì–´ë‹ë§<a class="header-link" href="#53" title="Permanent link">&para;</a></h3>
<p><strong>CosineAnnealingLR</strong>: ì½”ì‚¬ì¸ ê³¡ì„ ì„ ë”°ë¥´ëŠ” ë¶€ë“œëŸ¬ìš´ ê°ì†Œ:</p>
<p>$$
\eta_t = \eta_{\min} + \frac{1}{2}(\eta_{\max} - \eta_{\min})(1 + \cos(\frac{t}{T}\pi))
$$</p>
<div class="highlight"><pre><span></span><code><span class="c1"># Cosine annealing</span>
<span class="n">optimizer</span> <span class="o">=</span> <span class="n">optim</span><span class="o">.</span><span class="n">AdamW</span><span class="p">(</span><span class="n">model</span><span class="o">.</span><span class="n">parameters</span><span class="p">(),</span> <span class="n">lr</span><span class="o">=</span><span class="mf">0.001</span><span class="p">)</span>
<span class="n">scheduler</span> <span class="o">=</span> <span class="n">optim</span><span class="o">.</span><span class="n">lr_scheduler</span><span class="o">.</span><span class="n">CosineAnnealingLR</span><span class="p">(</span>
    <span class="n">optimizer</span><span class="p">,</span>
    <span class="n">T_max</span><span class="o">=</span><span class="mi">100</span><span class="p">,</span>  <span class="c1"># Total number of epochs</span>
    <span class="n">eta_min</span><span class="o">=</span><span class="mf">1e-6</span>  <span class="c1"># Minimum learning rate</span>
<span class="p">)</span>

<span class="c1"># LR smoothly decays from 0.001 to 1e-6 over 100 epochs</span>

<span class="c1"># Cosine annealing with warm restarts</span>
<span class="n">scheduler</span> <span class="o">=</span> <span class="n">optim</span><span class="o">.</span><span class="n">lr_scheduler</span><span class="o">.</span><span class="n">CosineAnnealingWarmRestarts</span><span class="p">(</span>
    <span class="n">optimizer</span><span class="p">,</span>
    <span class="n">T_0</span><span class="o">=</span><span class="mi">10</span><span class="p">,</span>  <span class="c1"># First restart after 10 epochs</span>
    <span class="n">T_mult</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span>  <span class="c1"># Double period after each restart</span>
    <span class="n">eta_min</span><span class="o">=</span><span class="mf">1e-6</span>
<span class="p">)</span>

<span class="c1"># LR schedule: 10 epochs, restart, 20 epochs, restart, 40 epochs, ...</span>
</code></pre></div>

<p><strong>ì½”ì‚¬ì¸ ì–´ë‹ë§ ì‹œê°í™”:</strong></p>
<div class="highlight"><pre><span></span><code><span class="k">def</span><span class="w"> </span><span class="nf">visualize_cosine_schedule</span><span class="p">():</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Visualize cosine annealing schedule&quot;&quot;&quot;</span>
    <span class="kn">import</span><span class="w"> </span><span class="nn">numpy</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">np</span>

    <span class="n">epochs</span> <span class="o">=</span> <span class="mi">100</span>
    <span class="n">lr_max</span> <span class="o">=</span> <span class="mf">0.001</span>
    <span class="n">lr_min</span> <span class="o">=</span> <span class="mf">1e-6</span>

    <span class="c1"># Standard cosine</span>
    <span class="n">lrs_cosine</span> <span class="o">=</span> <span class="p">[</span>
        <span class="n">lr_min</span> <span class="o">+</span> <span class="mf">0.5</span> <span class="o">*</span> <span class="p">(</span><span class="n">lr_max</span> <span class="o">-</span> <span class="n">lr_min</span><span class="p">)</span> <span class="o">*</span> <span class="p">(</span><span class="mi">1</span> <span class="o">+</span> <span class="n">np</span><span class="o">.</span><span class="n">cos</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">pi</span> <span class="o">*</span> <span class="n">t</span> <span class="o">/</span> <span class="n">epochs</span><span class="p">))</span>
        <span class="k">for</span> <span class="n">t</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">epochs</span><span class="p">)</span>
    <span class="p">]</span>

    <span class="c1"># Cosine with warm restarts (T_0=10, T_mult=2)</span>
    <span class="n">lrs_restart</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="n">t</span> <span class="o">=</span> <span class="mi">0</span>
    <span class="n">T_cur</span> <span class="o">=</span> <span class="mi">10</span>
    <span class="k">for</span> <span class="n">epoch</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">epochs</span><span class="p">):</span>
        <span class="n">lr</span> <span class="o">=</span> <span class="n">lr_min</span> <span class="o">+</span> <span class="mf">0.5</span> <span class="o">*</span> <span class="p">(</span><span class="n">lr_max</span> <span class="o">-</span> <span class="n">lr_min</span><span class="p">)</span> <span class="o">*</span> <span class="p">(</span><span class="mi">1</span> <span class="o">+</span> <span class="n">np</span><span class="o">.</span><span class="n">cos</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">pi</span> <span class="o">*</span> <span class="n">t</span> <span class="o">/</span> <span class="n">T_cur</span><span class="p">))</span>
        <span class="n">lrs_restart</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">lr</span><span class="p">)</span>
        <span class="n">t</span> <span class="o">+=</span> <span class="mi">1</span>
        <span class="k">if</span> <span class="n">t</span> <span class="o">&gt;=</span> <span class="n">T_cur</span><span class="p">:</span>  <span class="c1"># Restart</span>
            <span class="n">t</span> <span class="o">=</span> <span class="mi">0</span>
            <span class="n">T_cur</span> <span class="o">*=</span> <span class="mi">2</span>

    <span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">12</span><span class="p">,</span> <span class="mi">6</span><span class="p">))</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">lrs_cosine</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s1">&#39;Cosine Annealing&#39;</span><span class="p">,</span> <span class="n">linewidth</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">lrs_restart</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s1">&#39;Cosine with Warm Restarts&#39;</span><span class="p">,</span> <span class="n">linewidth</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s1">&#39;Epoch&#39;</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s1">&#39;Learning Rate&#39;</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s1">&#39;Cosine Annealing Schedules&#39;</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">legend</span><span class="p">()</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">yscale</span><span class="p">(</span><span class="s1">&#39;log&#39;</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">grid</span><span class="p">(</span><span class="kc">True</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.3</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">savefig</span><span class="p">(</span><span class="s1">&#39;cosine_schedule.png&#39;</span><span class="p">,</span> <span class="n">dpi</span><span class="o">=</span><span class="mi">150</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">close</span><span class="p">()</span>

<span class="n">visualize_cosine_schedule</span><span class="p">()</span>
</code></pre></div>

<h3 id="54-onecyclelr-super-convergence">5.4 OneCycleLR (Super-convergence)<a class="header-link" href="#54-onecyclelr-super-convergence" title="Permanent link">&para;</a></h3>
<p>OneCycleLRì€ ì›Œë°ì—…, í”¼í¬, ê°ì†Œë¥¼ í¬í•¨í•œ ë‹¨ì¼ ì‚¬ì´í´ì„ ì‚¬ìš©í•©ë‹ˆë‹¤:</p>
<div class="highlight"><pre><span></span><code>LR Schedule:
   max_lr â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•±â•²
                    â•±    â•²
                  â•±        â•²
   div_factor  â•±            â•²â”€â”€â”€â”€â”€â”€ final_div
              â†‘               â†‘
           warmup          annealing
</code></pre></div>

<div class="highlight"><pre><span></span><code><span class="c1"># OneCycleLR: state-of-the-art for many tasks</span>
<span class="n">optimizer</span> <span class="o">=</span> <span class="n">optim</span><span class="o">.</span><span class="n">SGD</span><span class="p">(</span><span class="n">model</span><span class="o">.</span><span class="n">parameters</span><span class="p">(),</span> <span class="n">lr</span><span class="o">=</span><span class="mf">0.1</span><span class="p">,</span> <span class="n">momentum</span><span class="o">=</span><span class="mf">0.9</span><span class="p">)</span>
<span class="n">scheduler</span> <span class="o">=</span> <span class="n">optim</span><span class="o">.</span><span class="n">lr_scheduler</span><span class="o">.</span><span class="n">OneCycleLR</span><span class="p">(</span>
    <span class="n">optimizer</span><span class="p">,</span>
    <span class="n">max_lr</span><span class="o">=</span><span class="mf">0.1</span><span class="p">,</span>  <span class="c1"># Peak learning rate</span>
    <span class="n">total_steps</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>  <span class="c1"># Will be computed from epochs and steps_per_epoch</span>
    <span class="n">epochs</span><span class="o">=</span><span class="mi">90</span><span class="p">,</span>
    <span class="n">steps_per_epoch</span><span class="o">=</span><span class="nb">len</span><span class="p">(</span><span class="n">train_loader</span><span class="p">),</span>
    <span class="n">pct_start</span><span class="o">=</span><span class="mf">0.3</span><span class="p">,</span>  <span class="c1"># Warmup: 30% of training</span>
    <span class="n">anneal_strategy</span><span class="o">=</span><span class="s1">&#39;cos&#39;</span><span class="p">,</span>  <span class="c1"># &#39;cos&#39; or &#39;linear&#39;</span>
    <span class="n">div_factor</span><span class="o">=</span><span class="mf">25.0</span><span class="p">,</span>  <span class="c1"># Initial LR = max_lr / div_factor</span>
    <span class="n">final_div_factor</span><span class="o">=</span><span class="mf">10000.0</span>  <span class="c1"># Final LR = max_lr / final_div_factor</span>
<span class="p">)</span>

<span class="c1"># Must call scheduler.step() after EVERY batch</span>
<span class="k">for</span> <span class="n">epoch</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">90</span><span class="p">):</span>
    <span class="k">for</span> <span class="n">batch</span> <span class="ow">in</span> <span class="n">train_loader</span><span class="p">:</span>
        <span class="n">optimizer</span><span class="o">.</span><span class="n">zero_grad</span><span class="p">()</span>
        <span class="n">loss</span> <span class="o">=</span> <span class="n">model</span><span class="p">(</span><span class="n">batch</span><span class="p">)</span>
        <span class="n">loss</span><span class="o">.</span><span class="n">backward</span><span class="p">()</span>
        <span class="n">optimizer</span><span class="o">.</span><span class="n">step</span><span class="p">()</span>
        <span class="n">scheduler</span><span class="o">.</span><span class="n">step</span><span class="p">()</span>  <span class="c1"># Update after each batch!</span>

<span class="c1"># Visualize OneCycleLR</span>
<span class="k">def</span><span class="w"> </span><span class="nf">visualize_onecycle</span><span class="p">():</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Visualize OneCycleLR schedule&quot;&quot;&quot;</span>
    <span class="n">model</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="mi">10</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
    <span class="n">optimizer</span> <span class="o">=</span> <span class="n">optim</span><span class="o">.</span><span class="n">SGD</span><span class="p">(</span><span class="n">model</span><span class="o">.</span><span class="n">parameters</span><span class="p">(),</span> <span class="n">lr</span><span class="o">=</span><span class="mf">0.1</span><span class="p">)</span>

    <span class="n">scheduler</span> <span class="o">=</span> <span class="n">optim</span><span class="o">.</span><span class="n">lr_scheduler</span><span class="o">.</span><span class="n">OneCycleLR</span><span class="p">(</span>
        <span class="n">optimizer</span><span class="p">,</span>
        <span class="n">max_lr</span><span class="o">=</span><span class="mf">0.1</span><span class="p">,</span>
        <span class="n">total_steps</span><span class="o">=</span><span class="mi">1000</span><span class="p">,</span>
        <span class="n">pct_start</span><span class="o">=</span><span class="mf">0.3</span><span class="p">,</span>
        <span class="n">anneal_strategy</span><span class="o">=</span><span class="s1">&#39;cos&#39;</span>
    <span class="p">)</span>

    <span class="n">lrs</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">1000</span><span class="p">):</span>
        <span class="n">lrs</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">optimizer</span><span class="o">.</span><span class="n">param_groups</span><span class="p">[</span><span class="mi">0</span><span class="p">][</span><span class="s1">&#39;lr&#39;</span><span class="p">])</span>
        <span class="n">optimizer</span><span class="o">.</span><span class="n">step</span><span class="p">()</span>
        <span class="n">scheduler</span><span class="o">.</span><span class="n">step</span><span class="p">()</span>

    <span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">12</span><span class="p">,</span> <span class="mi">6</span><span class="p">))</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">lrs</span><span class="p">,</span> <span class="n">linewidth</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s1">&#39;Step&#39;</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s1">&#39;Learning Rate&#39;</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s1">&#39;OneCycleLR Schedule&#39;</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">axvline</span><span class="p">(</span><span class="mi">300</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s1">&#39;r&#39;</span><span class="p">,</span> <span class="n">linestyle</span><span class="o">=</span><span class="s1">&#39;--&#39;</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.5</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s1">&#39;End of warmup&#39;</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">legend</span><span class="p">()</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">grid</span><span class="p">(</span><span class="kc">True</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.3</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">savefig</span><span class="p">(</span><span class="s1">&#39;onecycle_schedule.png&#39;</span><span class="p">,</span> <span class="n">dpi</span><span class="o">=</span><span class="mi">150</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">close</span><span class="p">()</span>

<span class="n">visualize_onecycle</span><span class="p">()</span>
</code></pre></div>

<h3 id="55">5.5 ì„ í˜• ì›Œë°ì—… + ì½”ì‚¬ì¸ ê°ì†Œ (íŠ¸ëœìŠ¤í¬ë¨¸ í‘œì¤€)<a class="header-link" href="#55" title="Permanent link">&para;</a></h3>
<p>ëŒ€ë¶€ë¶„ì˜ íŠ¸ëœìŠ¤í¬ë¨¸ ëª¨ë¸ì€ ì›Œë°ì—… í›„ ì½”ì‚¬ì¸ ê°ì†Œë¥¼ ì‚¬ìš©í•©ë‹ˆë‹¤:</p>
<div class="highlight"><pre><span></span><code><span class="kn">import</span><span class="w"> </span><span class="nn">math</span>

<span class="k">class</span><span class="w"> </span><span class="nc">WarmupCosineSchedule</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">optim</span><span class="o">.</span><span class="n">lr_scheduler</span><span class="o">.</span><span class="n">LambdaLR</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Linear warmup + cosine decay (BERT/GPT standard)&quot;&quot;&quot;</span>
    <span class="k">def</span><span class="w"> </span><span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">optimizer</span><span class="p">,</span> <span class="n">warmup_steps</span><span class="p">,</span> <span class="n">total_steps</span><span class="p">,</span> <span class="n">min_lr_ratio</span><span class="o">=</span><span class="mf">0.0</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">warmup_steps</span> <span class="o">=</span> <span class="n">warmup_steps</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">total_steps</span> <span class="o">=</span> <span class="n">total_steps</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">min_lr_ratio</span> <span class="o">=</span> <span class="n">min_lr_ratio</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="n">optimizer</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">lr_lambda</span><span class="p">)</span>

    <span class="k">def</span><span class="w"> </span><span class="nf">lr_lambda</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">step</span><span class="p">):</span>
        <span class="k">if</span> <span class="n">step</span> <span class="o">&lt;</span> <span class="bp">self</span><span class="o">.</span><span class="n">warmup_steps</span><span class="p">:</span>
            <span class="c1"># Linear warmup</span>
            <span class="k">return</span> <span class="nb">float</span><span class="p">(</span><span class="n">step</span><span class="p">)</span> <span class="o">/</span> <span class="nb">float</span><span class="p">(</span><span class="nb">max</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">warmup_steps</span><span class="p">))</span>

        <span class="c1"># Cosine decay</span>
        <span class="n">progress</span> <span class="o">=</span> <span class="nb">float</span><span class="p">(</span><span class="n">step</span> <span class="o">-</span> <span class="bp">self</span><span class="o">.</span><span class="n">warmup_steps</span><span class="p">)</span> <span class="o">/</span> <span class="nb">float</span><span class="p">(</span><span class="nb">max</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">total_steps</span> <span class="o">-</span> <span class="bp">self</span><span class="o">.</span><span class="n">warmup_steps</span><span class="p">))</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">min_lr_ratio</span> <span class="o">+</span> <span class="p">(</span><span class="mi">1</span> <span class="o">-</span> <span class="bp">self</span><span class="o">.</span><span class="n">min_lr_ratio</span><span class="p">)</span> <span class="o">*</span> <span class="mf">0.5</span> <span class="o">*</span> <span class="p">(</span><span class="mf">1.0</span> <span class="o">+</span> <span class="n">math</span><span class="o">.</span><span class="n">cos</span><span class="p">(</span><span class="n">math</span><span class="o">.</span><span class="n">pi</span> <span class="o">*</span> <span class="n">progress</span><span class="p">))</span>

<span class="c1"># Usage for transformer training</span>
<span class="n">optimizer</span> <span class="o">=</span> <span class="n">optim</span><span class="o">.</span><span class="n">AdamW</span><span class="p">(</span><span class="n">model</span><span class="o">.</span><span class="n">parameters</span><span class="p">(),</span> <span class="n">lr</span><span class="o">=</span><span class="mf">5e-4</span><span class="p">,</span> <span class="n">weight_decay</span><span class="o">=</span><span class="mf">0.01</span><span class="p">)</span>
<span class="n">total_steps</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">train_loader</span><span class="p">)</span> <span class="o">*</span> <span class="n">num_epochs</span>
<span class="n">warmup_steps</span> <span class="o">=</span> <span class="nb">int</span><span class="p">(</span><span class="mf">0.1</span> <span class="o">*</span> <span class="n">total_steps</span><span class="p">)</span>  <span class="c1"># 10% warmup</span>

<span class="n">scheduler</span> <span class="o">=</span> <span class="n">WarmupCosineSchedule</span><span class="p">(</span>
    <span class="n">optimizer</span><span class="p">,</span>
    <span class="n">warmup_steps</span><span class="o">=</span><span class="n">warmup_steps</span><span class="p">,</span>
    <span class="n">total_steps</span><span class="o">=</span><span class="n">total_steps</span><span class="p">,</span>
    <span class="n">min_lr_ratio</span><span class="o">=</span><span class="mf">0.1</span>  <span class="c1"># Decay to 10% of peak LR</span>
<span class="p">)</span>

<span class="c1"># Training loop</span>
<span class="k">for</span> <span class="n">epoch</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">num_epochs</span><span class="p">):</span>
    <span class="k">for</span> <span class="n">batch</span> <span class="ow">in</span> <span class="n">train_loader</span><span class="p">:</span>
        <span class="n">optimizer</span><span class="o">.</span><span class="n">zero_grad</span><span class="p">()</span>
        <span class="n">loss</span> <span class="o">=</span> <span class="n">model</span><span class="p">(</span><span class="n">batch</span><span class="p">)</span>
        <span class="n">loss</span><span class="o">.</span><span class="n">backward</span><span class="p">()</span>
        <span class="n">optimizer</span><span class="o">.</span><span class="n">step</span><span class="p">()</span>
        <span class="n">scheduler</span><span class="o">.</span><span class="n">step</span><span class="p">()</span>  <span class="c1"># Update after each batch</span>

<span class="c1"># Visualize</span>
<span class="k">def</span><span class="w"> </span><span class="nf">visualize_warmup_cosine</span><span class="p">():</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Visualize warmup + cosine schedule&quot;&quot;&quot;</span>
    <span class="n">model</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="mi">10</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
    <span class="n">optimizer</span> <span class="o">=</span> <span class="n">optim</span><span class="o">.</span><span class="n">AdamW</span><span class="p">(</span><span class="n">model</span><span class="o">.</span><span class="n">parameters</span><span class="p">(),</span> <span class="n">lr</span><span class="o">=</span><span class="mf">5e-4</span><span class="p">)</span>

    <span class="n">total_steps</span> <span class="o">=</span> <span class="mi">10000</span>
    <span class="n">warmup_steps</span> <span class="o">=</span> <span class="mi">1000</span>
    <span class="n">scheduler</span> <span class="o">=</span> <span class="n">WarmupCosineSchedule</span><span class="p">(</span><span class="n">optimizer</span><span class="p">,</span> <span class="n">warmup_steps</span><span class="p">,</span> <span class="n">total_steps</span><span class="p">,</span> <span class="n">min_lr_ratio</span><span class="o">=</span><span class="mf">0.1</span><span class="p">)</span>

    <span class="n">lrs</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">total_steps</span><span class="p">):</span>
        <span class="n">lrs</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">optimizer</span><span class="o">.</span><span class="n">param_groups</span><span class="p">[</span><span class="mi">0</span><span class="p">][</span><span class="s1">&#39;lr&#39;</span><span class="p">])</span>
        <span class="n">optimizer</span><span class="o">.</span><span class="n">step</span><span class="p">()</span>
        <span class="n">scheduler</span><span class="o">.</span><span class="n">step</span><span class="p">()</span>

    <span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">12</span><span class="p">,</span> <span class="mi">6</span><span class="p">))</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">lrs</span><span class="p">,</span> <span class="n">linewidth</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s1">&#39;Step&#39;</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s1">&#39;Learning Rate&#39;</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s1">&#39;Linear Warmup + Cosine Decay (Transformer Standard)&#39;</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">axvline</span><span class="p">(</span><span class="n">warmup_steps</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s1">&#39;r&#39;</span><span class="p">,</span> <span class="n">linestyle</span><span class="o">=</span><span class="s1">&#39;--&#39;</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.5</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s1">&#39;End of warmup&#39;</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">legend</span><span class="p">()</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">grid</span><span class="p">(</span><span class="kc">True</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.3</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">savefig</span><span class="p">(</span><span class="s1">&#39;warmup_cosine_schedule.png&#39;</span><span class="p">,</span> <span class="n">dpi</span><span class="o">=</span><span class="mi">150</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">close</span><span class="p">()</span>

<span class="n">visualize_warmup_cosine</span><span class="p">()</span>
</code></pre></div>

<h3 id="56-reducelronplateau">5.6 ReduceLROnPlateau<a class="header-link" href="#56-reducelronplateau" title="Permanent link">&para;</a></h3>
<p>ê²€ì¦ ë©”íŠ¸ë¦­ì´ ì •ì²´ë  ë•Œ LR ê°ì†Œ:</p>
<div class="highlight"><pre><span></span><code><span class="c1"># ReduceLROnPlateau: data-driven LR reduction</span>
<span class="n">optimizer</span> <span class="o">=</span> <span class="n">optim</span><span class="o">.</span><span class="n">Adam</span><span class="p">(</span><span class="n">model</span><span class="o">.</span><span class="n">parameters</span><span class="p">(),</span> <span class="n">lr</span><span class="o">=</span><span class="mf">0.001</span><span class="p">)</span>
<span class="n">scheduler</span> <span class="o">=</span> <span class="n">optim</span><span class="o">.</span><span class="n">lr_scheduler</span><span class="o">.</span><span class="n">ReduceLROnPlateau</span><span class="p">(</span>
    <span class="n">optimizer</span><span class="p">,</span>
    <span class="n">mode</span><span class="o">=</span><span class="s1">&#39;min&#39;</span><span class="p">,</span>  <span class="c1"># &#39;min&#39; for loss, &#39;max&#39; for accuracy</span>
    <span class="n">factor</span><span class="o">=</span><span class="mf">0.5</span><span class="p">,</span>  <span class="c1"># Multiply LR by 0.5</span>
    <span class="n">patience</span><span class="o">=</span><span class="mi">10</span><span class="p">,</span>  <span class="c1"># Wait 10 epochs before reducing</span>
    <span class="n">verbose</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
    <span class="n">min_lr</span><span class="o">=</span><span class="mf">1e-6</span>
<span class="p">)</span>

<span class="k">for</span> <span class="n">epoch</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">100</span><span class="p">):</span>
    <span class="n">train_loss</span> <span class="o">=</span> <span class="n">train_one_epoch</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">optimizer</span><span class="p">)</span>
    <span class="n">val_loss</span> <span class="o">=</span> <span class="n">validate</span><span class="p">(</span><span class="n">model</span><span class="p">)</span>

    <span class="c1"># Step based on validation loss</span>
    <span class="n">scheduler</span><span class="o">.</span><span class="n">step</span><span class="p">(</span><span class="n">val_loss</span><span class="p">)</span>

    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Epoch </span><span class="si">{</span><span class="n">epoch</span><span class="si">}</span><span class="s2">: LR = </span><span class="si">{</span><span class="n">optimizer</span><span class="o">.</span><span class="n">param_groups</span><span class="p">[</span><span class="mi">0</span><span class="p">][</span><span class="s1">&#39;lr&#39;</span><span class="p">]</span><span class="si">:</span><span class="s2">.6f</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
</code></pre></div>

<h3 id="57">5.7 ì»¤ìŠ¤í…€ ìŠ¤ì¼€ì¤„ëŸ¬<a class="header-link" href="#57" title="Permanent link">&para;</a></h3>
<p>LambdaLRì„ ì‚¬ìš©í•œ ì»¤ìŠ¤í…€ ìŠ¤ì¼€ì¤„ êµ¬í˜„:</p>
<div class="highlight"><pre><span></span><code><span class="c1"># Custom schedule: polynomial decay with warmup</span>
<span class="k">def</span><span class="w"> </span><span class="nf">polynomial_decay_with_warmup</span><span class="p">(</span><span class="n">step</span><span class="p">,</span> <span class="n">warmup_steps</span><span class="p">,</span> <span class="n">total_steps</span><span class="p">,</span> <span class="n">power</span><span class="o">=</span><span class="mf">1.0</span><span class="p">):</span>
    <span class="k">if</span> <span class="n">step</span> <span class="o">&lt;</span> <span class="n">warmup_steps</span><span class="p">:</span>
        <span class="k">return</span> <span class="nb">float</span><span class="p">(</span><span class="n">step</span><span class="p">)</span> <span class="o">/</span> <span class="nb">float</span><span class="p">(</span><span class="nb">max</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">warmup_steps</span><span class="p">))</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="n">progress</span> <span class="o">=</span> <span class="p">(</span><span class="n">step</span> <span class="o">-</span> <span class="n">warmup_steps</span><span class="p">)</span> <span class="o">/</span> <span class="p">(</span><span class="n">total_steps</span> <span class="o">-</span> <span class="n">warmup_steps</span><span class="p">)</span>
        <span class="k">return</span> <span class="p">(</span><span class="mi">1</span> <span class="o">-</span> <span class="n">progress</span><span class="p">)</span> <span class="o">**</span> <span class="n">power</span>

<span class="n">optimizer</span> <span class="o">=</span> <span class="n">optim</span><span class="o">.</span><span class="n">AdamW</span><span class="p">(</span><span class="n">model</span><span class="o">.</span><span class="n">parameters</span><span class="p">(),</span> <span class="n">lr</span><span class="o">=</span><span class="mf">5e-4</span><span class="p">)</span>
<span class="n">scheduler</span> <span class="o">=</span> <span class="n">optim</span><span class="o">.</span><span class="n">lr_scheduler</span><span class="o">.</span><span class="n">LambdaLR</span><span class="p">(</span>
    <span class="n">optimizer</span><span class="p">,</span>
    <span class="n">lr_lambda</span><span class="o">=</span><span class="k">lambda</span> <span class="n">step</span><span class="p">:</span> <span class="n">polynomial_decay_with_warmup</span><span class="p">(</span><span class="n">step</span><span class="p">,</span> <span class="mi">1000</span><span class="p">,</span> <span class="mi">10000</span><span class="p">,</span> <span class="n">power</span><span class="o">=</span><span class="mf">2.0</span><span class="p">)</span>
<span class="p">)</span>

<span class="c1"># Or chain multiple schedulers</span>
<span class="n">scheduler1</span> <span class="o">=</span> <span class="n">optim</span><span class="o">.</span><span class="n">lr_scheduler</span><span class="o">.</span><span class="n">LinearLR</span><span class="p">(</span><span class="n">optimizer</span><span class="p">,</span> <span class="n">start_factor</span><span class="o">=</span><span class="mf">0.1</span><span class="p">,</span> <span class="n">total_iters</span><span class="o">=</span><span class="mi">1000</span><span class="p">)</span>
<span class="n">scheduler2</span> <span class="o">=</span> <span class="n">optim</span><span class="o">.</span><span class="n">lr_scheduler</span><span class="o">.</span><span class="n">ExponentialLR</span><span class="p">(</span><span class="n">optimizer</span><span class="p">,</span> <span class="n">gamma</span><span class="o">=</span><span class="mf">0.9</span><span class="p">)</span>
<span class="n">scheduler</span> <span class="o">=</span> <span class="n">optim</span><span class="o">.</span><span class="n">lr_scheduler</span><span class="o">.</span><span class="n">ChainedScheduler</span><span class="p">([</span><span class="n">scheduler1</span><span class="p">,</span> <span class="n">scheduler2</span><span class="p">])</span>
</code></pre></div>

<h3 id="58">5.8 ìŠ¤ì¼€ì¤„ëŸ¬ ì‹œê°í™” ëª¨ìŒ<a class="header-link" href="#58" title="Permanent link">&para;</a></h3>
<div class="highlight"><pre><span></span><code><span class="k">def</span><span class="w"> </span><span class="nf">compare_all_schedulers</span><span class="p">():</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Compare all scheduler types&quot;&quot;&quot;</span>
    <span class="n">model</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="mi">10</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
    <span class="n">total_steps</span> <span class="o">=</span> <span class="mi">1000</span>

    <span class="n">schedulers</span> <span class="o">=</span> <span class="p">{</span>
        <span class="s1">&#39;StepLR&#39;</span><span class="p">:</span> <span class="k">lambda</span> <span class="n">opt</span><span class="p">:</span> <span class="n">optim</span><span class="o">.</span><span class="n">lr_scheduler</span><span class="o">.</span><span class="n">StepLR</span><span class="p">(</span><span class="n">opt</span><span class="p">,</span> <span class="n">step_size</span><span class="o">=</span><span class="mi">300</span><span class="p">,</span> <span class="n">gamma</span><span class="o">=</span><span class="mf">0.1</span><span class="p">),</span>
        <span class="s1">&#39;ExponentialLR&#39;</span><span class="p">:</span> <span class="k">lambda</span> <span class="n">opt</span><span class="p">:</span> <span class="n">optim</span><span class="o">.</span><span class="n">lr_scheduler</span><span class="o">.</span><span class="n">ExponentialLR</span><span class="p">(</span><span class="n">opt</span><span class="p">,</span> <span class="n">gamma</span><span class="o">=</span><span class="mf">0.995</span><span class="p">),</span>
        <span class="s1">&#39;CosineAnnealing&#39;</span><span class="p">:</span> <span class="k">lambda</span> <span class="n">opt</span><span class="p">:</span> <span class="n">optim</span><span class="o">.</span><span class="n">lr_scheduler</span><span class="o">.</span><span class="n">CosineAnnealingLR</span><span class="p">(</span><span class="n">opt</span><span class="p">,</span> <span class="n">T_max</span><span class="o">=</span><span class="n">total_steps</span><span class="p">),</span>
        <span class="s1">&#39;OneCycleLR&#39;</span><span class="p">:</span> <span class="k">lambda</span> <span class="n">opt</span><span class="p">:</span> <span class="n">optim</span><span class="o">.</span><span class="n">lr_scheduler</span><span class="o">.</span><span class="n">OneCycleLR</span><span class="p">(</span><span class="n">opt</span><span class="p">,</span> <span class="n">max_lr</span><span class="o">=</span><span class="mf">0.1</span><span class="p">,</span> <span class="n">total_steps</span><span class="o">=</span><span class="n">total_steps</span><span class="p">),</span>
        <span class="s1">&#39;Warmup+Cosine&#39;</span><span class="p">:</span> <span class="k">lambda</span> <span class="n">opt</span><span class="p">:</span> <span class="n">WarmupCosineSchedule</span><span class="p">(</span><span class="n">opt</span><span class="p">,</span> <span class="n">warmup_steps</span><span class="o">=</span><span class="mi">100</span><span class="p">,</span> <span class="n">total_steps</span><span class="o">=</span><span class="n">total_steps</span><span class="p">),</span>
    <span class="p">}</span>

    <span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">14</span><span class="p">,</span> <span class="mi">8</span><span class="p">))</span>

    <span class="k">for</span> <span class="n">name</span><span class="p">,</span> <span class="n">sched_fn</span> <span class="ow">in</span> <span class="n">schedulers</span><span class="o">.</span><span class="n">items</span><span class="p">():</span>
        <span class="n">optimizer</span> <span class="o">=</span> <span class="n">optim</span><span class="o">.</span><span class="n">SGD</span><span class="p">(</span><span class="n">model</span><span class="o">.</span><span class="n">parameters</span><span class="p">(),</span> <span class="n">lr</span><span class="o">=</span><span class="mf">0.1</span><span class="p">)</span>
        <span class="n">scheduler</span> <span class="o">=</span> <span class="n">sched_fn</span><span class="p">(</span><span class="n">optimizer</span><span class="p">)</span>

        <span class="n">lrs</span> <span class="o">=</span> <span class="p">[]</span>
        <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">total_steps</span><span class="p">):</span>
            <span class="n">lrs</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">optimizer</span><span class="o">.</span><span class="n">param_groups</span><span class="p">[</span><span class="mi">0</span><span class="p">][</span><span class="s1">&#39;lr&#39;</span><span class="p">])</span>
            <span class="n">optimizer</span><span class="o">.</span><span class="n">step</span><span class="p">()</span>
            <span class="n">scheduler</span><span class="o">.</span><span class="n">step</span><span class="p">()</span>

        <span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">lrs</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="n">name</span><span class="p">,</span> <span class="n">linewidth</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>

    <span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s1">&#39;Step&#39;</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s1">&#39;Learning Rate&#39;</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s1">&#39;Learning Rate Scheduler Comparison&#39;</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">legend</span><span class="p">()</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">yscale</span><span class="p">(</span><span class="s1">&#39;log&#39;</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">grid</span><span class="p">(</span><span class="kc">True</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.3</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">savefig</span><span class="p">(</span><span class="s1">&#39;scheduler_comparison.png&#39;</span><span class="p">,</span> <span class="n">dpi</span><span class="o">=</span><span class="mi">150</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">close</span><span class="p">()</span>

<span class="n">compare_all_schedulers</span><span class="p">()</span>
</code></pre></div>

<hr />
<h2 id="6">6. ì‹¤ìš©ì  ê¸°ë²•<a class="header-link" href="#6" title="Permanent link">&para;</a></h2>
<h3 id="61-lr-range-test">6.1 í•™ìŠµë¥  íƒìƒ‰ (LR Range Test)<a class="header-link" href="#61-lr-range-test" title="Permanent link">&para;</a></h3>
<p>í•™ìŠµë¥ ì„ ì ì§„ì ìœ¼ë¡œ ì¦ê°€ì‹œì¼œ ìµœì  í•™ìŠµë¥  ì°¾ê¸°:</p>
<div class="highlight"><pre><span></span><code><span class="k">class</span><span class="w"> </span><span class="nc">LRFinder</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Learning rate range test (Leslie Smith)&quot;&quot;&quot;</span>
    <span class="k">def</span><span class="w"> </span><span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">model</span><span class="p">,</span> <span class="n">optimizer</span><span class="p">,</span> <span class="n">criterion</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="s1">&#39;cuda&#39;</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">model</span> <span class="o">=</span> <span class="n">model</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">optimizer</span> <span class="o">=</span> <span class="n">optimizer</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">criterion</span> <span class="o">=</span> <span class="n">criterion</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">device</span> <span class="o">=</span> <span class="n">device</span>

    <span class="k">def</span><span class="w"> </span><span class="nf">range_test</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">train_loader</span><span class="p">,</span> <span class="n">start_lr</span><span class="o">=</span><span class="mf">1e-7</span><span class="p">,</span> <span class="n">end_lr</span><span class="o">=</span><span class="mi">10</span><span class="p">,</span> <span class="n">num_iter</span><span class="o">=</span><span class="mi">100</span><span class="p">,</span> <span class="n">smooth_f</span><span class="o">=</span><span class="mf">0.05</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Run LR range test&quot;&quot;&quot;</span>
        <span class="c1"># Save initial state</span>
        <span class="n">model_state</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">model</span><span class="o">.</span><span class="n">state_dict</span><span class="p">()</span>
        <span class="n">optim_state</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">optimizer</span><span class="o">.</span><span class="n">state_dict</span><span class="p">()</span>

        <span class="c1"># Update LR</span>
        <span class="n">lrs</span> <span class="o">=</span> <span class="p">[]</span>
        <span class="n">losses</span> <span class="o">=</span> <span class="p">[]</span>
        <span class="n">best_loss</span> <span class="o">=</span> <span class="nb">float</span><span class="p">(</span><span class="s1">&#39;inf&#39;</span><span class="p">)</span>

        <span class="n">lr_mult</span> <span class="o">=</span> <span class="p">(</span><span class="n">end_lr</span> <span class="o">/</span> <span class="n">start_lr</span><span class="p">)</span> <span class="o">**</span> <span class="p">(</span><span class="mi">1</span> <span class="o">/</span> <span class="n">num_iter</span><span class="p">)</span>
        <span class="n">lr</span> <span class="o">=</span> <span class="n">start_lr</span>

        <span class="k">for</span> <span class="n">param_group</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">optimizer</span><span class="o">.</span><span class="n">param_groups</span><span class="p">:</span>
            <span class="n">param_group</span><span class="p">[</span><span class="s1">&#39;lr&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">lr</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">model</span><span class="o">.</span><span class="n">train</span><span class="p">()</span>
        <span class="n">iterator</span> <span class="o">=</span> <span class="nb">iter</span><span class="p">(</span><span class="n">train_loader</span><span class="p">)</span>

        <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">num_iter</span><span class="p">):</span>
            <span class="k">try</span><span class="p">:</span>
                <span class="n">X</span><span class="p">,</span> <span class="n">y</span> <span class="o">=</span> <span class="nb">next</span><span class="p">(</span><span class="n">iterator</span><span class="p">)</span>
            <span class="k">except</span> <span class="ne">StopIteration</span><span class="p">:</span>
                <span class="n">iterator</span> <span class="o">=</span> <span class="nb">iter</span><span class="p">(</span><span class="n">train_loader</span><span class="p">)</span>
                <span class="n">X</span><span class="p">,</span> <span class="n">y</span> <span class="o">=</span> <span class="nb">next</span><span class="p">(</span><span class="n">iterator</span><span class="p">)</span>

            <span class="n">X</span><span class="p">,</span> <span class="n">y</span> <span class="o">=</span> <span class="n">X</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">device</span><span class="p">),</span> <span class="n">y</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">device</span><span class="p">)</span>

            <span class="c1"># Forward pass</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">optimizer</span><span class="o">.</span><span class="n">zero_grad</span><span class="p">()</span>
            <span class="n">output</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">model</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>
            <span class="n">loss</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">criterion</span><span class="p">(</span><span class="n">output</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>

            <span class="c1"># Smooth loss</span>
            <span class="k">if</span> <span class="n">i</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
                <span class="n">avg_loss</span> <span class="o">=</span> <span class="n">loss</span><span class="o">.</span><span class="n">item</span><span class="p">()</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="n">avg_loss</span> <span class="o">=</span> <span class="n">smooth_f</span> <span class="o">*</span> <span class="n">loss</span><span class="o">.</span><span class="n">item</span><span class="p">()</span> <span class="o">+</span> <span class="p">(</span><span class="mi">1</span> <span class="o">-</span> <span class="n">smooth_f</span><span class="p">)</span> <span class="o">*</span> <span class="n">avg_loss</span>

            <span class="c1"># Stop if loss is exploding</span>
            <span class="k">if</span> <span class="n">avg_loss</span> <span class="o">&gt;</span> <span class="mi">4</span> <span class="o">*</span> <span class="n">best_loss</span> <span class="ow">or</span> <span class="n">torch</span><span class="o">.</span><span class="n">isnan</span><span class="p">(</span><span class="n">loss</span><span class="p">):</span>
                <span class="k">break</span>

            <span class="k">if</span> <span class="n">avg_loss</span> <span class="o">&lt;</span> <span class="n">best_loss</span><span class="p">:</span>
                <span class="n">best_loss</span> <span class="o">=</span> <span class="n">avg_loss</span>

            <span class="c1"># Record</span>
            <span class="n">lrs</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">lr</span><span class="p">)</span>
            <span class="n">losses</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">avg_loss</span><span class="p">)</span>

            <span class="c1"># Backward pass</span>
            <span class="n">loss</span><span class="o">.</span><span class="n">backward</span><span class="p">()</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">optimizer</span><span class="o">.</span><span class="n">step</span><span class="p">()</span>

            <span class="c1"># Update LR</span>
            <span class="n">lr</span> <span class="o">*=</span> <span class="n">lr_mult</span>
            <span class="k">for</span> <span class="n">param_group</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">optimizer</span><span class="o">.</span><span class="n">param_groups</span><span class="p">:</span>
                <span class="n">param_group</span><span class="p">[</span><span class="s1">&#39;lr&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">lr</span>

        <span class="c1"># Restore initial state</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">model</span><span class="o">.</span><span class="n">load_state_dict</span><span class="p">(</span><span class="n">model_state</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">optimizer</span><span class="o">.</span><span class="n">load_state_dict</span><span class="p">(</span><span class="n">optim_state</span><span class="p">)</span>

        <span class="k">return</span> <span class="n">lrs</span><span class="p">,</span> <span class="n">losses</span>

    <span class="k">def</span><span class="w"> </span><span class="nf">plot</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">lrs</span><span class="p">,</span> <span class="n">losses</span><span class="p">,</span> <span class="n">skip_start</span><span class="o">=</span><span class="mi">10</span><span class="p">,</span> <span class="n">skip_end</span><span class="o">=</span><span class="mi">5</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Plot LR finder results&quot;&quot;&quot;</span>
        <span class="k">if</span> <span class="n">skip_end</span> <span class="o">&gt;</span> <span class="mi">0</span><span class="p">:</span>
            <span class="n">lrs</span> <span class="o">=</span> <span class="n">lrs</span><span class="p">[</span><span class="n">skip_start</span><span class="p">:</span><span class="o">-</span><span class="n">skip_end</span><span class="p">]</span>
            <span class="n">losses</span> <span class="o">=</span> <span class="n">losses</span><span class="p">[</span><span class="n">skip_start</span><span class="p">:</span><span class="o">-</span><span class="n">skip_end</span><span class="p">]</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">lrs</span> <span class="o">=</span> <span class="n">lrs</span><span class="p">[</span><span class="n">skip_start</span><span class="p">:]</span>
            <span class="n">losses</span> <span class="o">=</span> <span class="n">losses</span><span class="p">[</span><span class="n">skip_start</span><span class="p">:]</span>

        <span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">10</span><span class="p">,</span> <span class="mi">6</span><span class="p">))</span>
        <span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">lrs</span><span class="p">,</span> <span class="n">losses</span><span class="p">,</span> <span class="n">linewidth</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
        <span class="n">plt</span><span class="o">.</span><span class="n">xscale</span><span class="p">(</span><span class="s1">&#39;log&#39;</span><span class="p">)</span>
        <span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s1">&#39;Learning Rate&#39;</span><span class="p">)</span>
        <span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s1">&#39;Loss&#39;</span><span class="p">)</span>
        <span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s1">&#39;LR Range Test&#39;</span><span class="p">)</span>
        <span class="n">plt</span><span class="o">.</span><span class="n">grid</span><span class="p">(</span><span class="kc">True</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.3</span><span class="p">)</span>

        <span class="c1"># Find steepest descent</span>
        <span class="n">min_grad_idx</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">gradient</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">losses</span><span class="p">))</span><span class="o">.</span><span class="n">argmin</span><span class="p">()</span>
        <span class="n">suggested_lr</span> <span class="o">=</span> <span class="n">lrs</span><span class="p">[</span><span class="n">min_grad_idx</span><span class="p">]</span>
        <span class="n">plt</span><span class="o">.</span><span class="n">axvline</span><span class="p">(</span><span class="n">suggested_lr</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s1">&#39;r&#39;</span><span class="p">,</span> <span class="n">linestyle</span><span class="o">=</span><span class="s1">&#39;--&#39;</span><span class="p">,</span>
                    <span class="n">label</span><span class="o">=</span><span class="sa">f</span><span class="s1">&#39;Suggested LR: </span><span class="si">{</span><span class="n">suggested_lr</span><span class="si">:</span><span class="s1">.2e</span><span class="si">}</span><span class="s1">&#39;</span><span class="p">)</span>
        <span class="n">plt</span><span class="o">.</span><span class="n">legend</span><span class="p">()</span>
        <span class="n">plt</span><span class="o">.</span><span class="n">savefig</span><span class="p">(</span><span class="s1">&#39;lr_finder.png&#39;</span><span class="p">,</span> <span class="n">dpi</span><span class="o">=</span><span class="mi">150</span><span class="p">)</span>
        <span class="n">plt</span><span class="o">.</span><span class="n">close</span><span class="p">()</span>

        <span class="k">return</span> <span class="n">suggested_lr</span>

<span class="c1"># Usage</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Sequential</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="mi">10</span><span class="p">,</span> <span class="mi">50</span><span class="p">),</span> <span class="n">nn</span><span class="o">.</span><span class="n">ReLU</span><span class="p">(),</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="mi">50</span><span class="p">,</span> <span class="mi">1</span><span class="p">))</span>
<span class="n">optimizer</span> <span class="o">=</span> <span class="n">optim</span><span class="o">.</span><span class="n">SGD</span><span class="p">(</span><span class="n">model</span><span class="o">.</span><span class="n">parameters</span><span class="p">(),</span> <span class="n">lr</span><span class="o">=</span><span class="mf">0.001</span><span class="p">)</span>
<span class="n">criterion</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">MSELoss</span><span class="p">()</span>

<span class="n">lr_finder</span> <span class="o">=</span> <span class="n">LRFinder</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">optimizer</span><span class="p">,</span> <span class="n">criterion</span><span class="p">)</span>
<span class="n">lrs</span><span class="p">,</span> <span class="n">losses</span> <span class="o">=</span> <span class="n">lr_finder</span><span class="o">.</span><span class="n">range_test</span><span class="p">(</span><span class="n">train_loader</span><span class="p">,</span> <span class="n">start_lr</span><span class="o">=</span><span class="mf">1e-7</span><span class="p">,</span> <span class="n">end_lr</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
<span class="n">suggested_lr</span> <span class="o">=</span> <span class="n">lr_finder</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">lrs</span><span class="p">,</span> <span class="n">losses</span><span class="p">)</span>

<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Suggested learning rate: </span><span class="si">{</span><span class="n">suggested_lr</span><span class="si">:</span><span class="s2">.2e</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
</code></pre></div>

<h3 id="62">6.2 ê·¸ë˜ë””ì–¸íŠ¸ í´ë¦¬í•‘<a class="header-link" href="#62" title="Permanent link">&para;</a></h3>
<p>í´ë¦¬í•‘ìœ¼ë¡œ ê·¸ë˜ë””ì–¸íŠ¸ í­ë°œ ë°©ì§€:</p>
<div class="highlight"><pre><span></span><code><span class="c1"># Gradient clipping by norm (most common)</span>
<span class="n">optimizer</span> <span class="o">=</span> <span class="n">optim</span><span class="o">.</span><span class="n">Adam</span><span class="p">(</span><span class="n">model</span><span class="o">.</span><span class="n">parameters</span><span class="p">(),</span> <span class="n">lr</span><span class="o">=</span><span class="mf">0.001</span><span class="p">)</span>

<span class="k">for</span> <span class="n">epoch</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">num_epochs</span><span class="p">):</span>
    <span class="k">for</span> <span class="n">X</span><span class="p">,</span> <span class="n">y</span> <span class="ow">in</span> <span class="n">train_loader</span><span class="p">:</span>
        <span class="n">optimizer</span><span class="o">.</span><span class="n">zero_grad</span><span class="p">()</span>
        <span class="n">output</span> <span class="o">=</span> <span class="n">model</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>
        <span class="n">loss</span> <span class="o">=</span> <span class="n">criterion</span><span class="p">(</span><span class="n">output</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>
        <span class="n">loss</span><span class="o">.</span><span class="n">backward</span><span class="p">()</span>

        <span class="c1"># Clip gradients to max norm of 1.0</span>
        <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">utils</span><span class="o">.</span><span class="n">clip_grad_norm_</span><span class="p">(</span><span class="n">model</span><span class="o">.</span><span class="n">parameters</span><span class="p">(),</span> <span class="n">max_norm</span><span class="o">=</span><span class="mf">1.0</span><span class="p">)</span>

        <span class="n">optimizer</span><span class="o">.</span><span class="n">step</span><span class="p">()</span>

<span class="c1"># Gradient clipping by value</span>
<span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">utils</span><span class="o">.</span><span class="n">clip_grad_value_</span><span class="p">(</span><span class="n">model</span><span class="o">.</span><span class="n">parameters</span><span class="p">(),</span> <span class="n">clip_value</span><span class="o">=</span><span class="mf">0.5</span><span class="p">)</span>

<span class="c1"># Monitor gradient norms</span>
<span class="k">def</span><span class="w"> </span><span class="nf">get_grad_norm</span><span class="p">(</span><span class="n">model</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Compute total gradient norm&quot;&quot;&quot;</span>
    <span class="n">total_norm</span> <span class="o">=</span> <span class="mi">0</span>
    <span class="k">for</span> <span class="n">p</span> <span class="ow">in</span> <span class="n">model</span><span class="o">.</span><span class="n">parameters</span><span class="p">():</span>
        <span class="k">if</span> <span class="n">p</span><span class="o">.</span><span class="n">grad</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">param_norm</span> <span class="o">=</span> <span class="n">p</span><span class="o">.</span><span class="n">grad</span><span class="o">.</span><span class="n">data</span><span class="o">.</span><span class="n">norm</span><span class="p">(</span><span class="mi">2</span><span class="p">)</span>
            <span class="n">total_norm</span> <span class="o">+=</span> <span class="n">param_norm</span><span class="o">.</span><span class="n">item</span><span class="p">()</span> <span class="o">**</span> <span class="mi">2</span>
    <span class="k">return</span> <span class="n">total_norm</span> <span class="o">**</span> <span class="mf">0.5</span>

<span class="c1"># Training with gradient monitoring</span>
<span class="n">grad_norms</span> <span class="o">=</span> <span class="p">[]</span>
<span class="k">for</span> <span class="n">epoch</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">num_epochs</span><span class="p">):</span>
    <span class="k">for</span> <span class="n">X</span><span class="p">,</span> <span class="n">y</span> <span class="ow">in</span> <span class="n">train_loader</span><span class="p">:</span>
        <span class="n">optimizer</span><span class="o">.</span><span class="n">zero_grad</span><span class="p">()</span>
        <span class="n">loss</span> <span class="o">=</span> <span class="n">criterion</span><span class="p">(</span><span class="n">model</span><span class="p">(</span><span class="n">X</span><span class="p">),</span> <span class="n">y</span><span class="p">)</span>
        <span class="n">loss</span><span class="o">.</span><span class="n">backward</span><span class="p">()</span>

        <span class="c1"># Monitor before clipping</span>
        <span class="n">grad_norm_before</span> <span class="o">=</span> <span class="n">get_grad_norm</span><span class="p">(</span><span class="n">model</span><span class="p">)</span>

        <span class="c1"># Clip</span>
        <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">utils</span><span class="o">.</span><span class="n">clip_grad_norm_</span><span class="p">(</span><span class="n">model</span><span class="o">.</span><span class="n">parameters</span><span class="p">(),</span> <span class="n">max_norm</span><span class="o">=</span><span class="mf">1.0</span><span class="p">)</span>

        <span class="n">grad_norm_after</span> <span class="o">=</span> <span class="n">get_grad_norm</span><span class="p">(</span><span class="n">model</span><span class="p">)</span>
        <span class="n">grad_norms</span><span class="o">.</span><span class="n">append</span><span class="p">((</span><span class="n">grad_norm_before</span><span class="p">,</span> <span class="n">grad_norm_after</span><span class="p">))</span>

        <span class="n">optimizer</span><span class="o">.</span><span class="n">step</span><span class="p">()</span>

<span class="c1"># Plot gradient norms</span>
<span class="n">before</span><span class="p">,</span> <span class="n">after</span> <span class="o">=</span> <span class="nb">zip</span><span class="p">(</span><span class="o">*</span><span class="n">grad_norms</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">12</span><span class="p">,</span> <span class="mi">6</span><span class="p">))</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">before</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s1">&#39;Before clipping&#39;</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.7</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">after</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s1">&#39;After clipping&#39;</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.7</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s1">&#39;Step&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s1">&#39;Gradient Norm&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s1">&#39;Gradient Clipping Effect&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">legend</span><span class="p">()</span>
<span class="n">plt</span><span class="o">.</span><span class="n">grid</span><span class="p">(</span><span class="kc">True</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.3</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">savefig</span><span class="p">(</span><span class="s1">&#39;gradient_clipping.png&#39;</span><span class="p">,</span> <span class="n">dpi</span><span class="o">=</span><span class="mi">150</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">close</span><span class="p">()</span>
</code></pre></div>

<h3 id="63">6.3 ê·¸ë˜ë””ì–¸íŠ¸ ì¶•ì <a class="header-link" href="#63" title="Permanent link">&para;</a></h3>
<p>ì œí•œëœ ë©”ëª¨ë¦¬ë¡œ ë” í° ë°°ì¹˜ í¬ê¸° ì‹œë®¬ë ˆì´ì…˜:</p>
<div class="highlight"><pre><span></span><code><span class="c1"># Gradient accumulation</span>
<span class="n">accumulation_steps</span> <span class="o">=</span> <span class="mi">4</span>  <span class="c1"># Effective batch size = batch_size * accumulation_steps</span>

<span class="n">optimizer</span><span class="o">.</span><span class="n">zero_grad</span><span class="p">()</span>

<span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">train_loader</span><span class="p">):</span>
    <span class="c1"># Forward pass</span>
    <span class="n">output</span> <span class="o">=</span> <span class="n">model</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>
    <span class="n">loss</span> <span class="o">=</span> <span class="n">criterion</span><span class="p">(</span><span class="n">output</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>

    <span class="c1"># Normalize loss to account for accumulation</span>
    <span class="n">loss</span> <span class="o">=</span> <span class="n">loss</span> <span class="o">/</span> <span class="n">accumulation_steps</span>
    <span class="n">loss</span><span class="o">.</span><span class="n">backward</span><span class="p">()</span>

    <span class="c1"># Update only after accumulating gradients</span>
    <span class="k">if</span> <span class="p">(</span><span class="n">i</span> <span class="o">+</span> <span class="mi">1</span><span class="p">)</span> <span class="o">%</span> <span class="n">accumulation_steps</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
        <span class="n">optimizer</span><span class="o">.</span><span class="n">step</span><span class="p">()</span>
        <span class="n">optimizer</span><span class="o">.</span><span class="n">zero_grad</span><span class="p">()</span>

<span class="c1"># Complete implementation with proper handling</span>
<span class="k">def</span><span class="w"> </span><span class="nf">train_with_accumulation</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">train_loader</span><span class="p">,</span> <span class="n">optimizer</span><span class="p">,</span> <span class="n">criterion</span><span class="p">,</span>
                            <span class="n">accumulation_steps</span><span class="o">=</span><span class="mi">4</span><span class="p">,</span> <span class="n">clip_norm</span><span class="o">=</span><span class="mf">1.0</span><span class="p">):</span>
    <span class="n">model</span><span class="o">.</span><span class="n">train</span><span class="p">()</span>
    <span class="n">total_loss</span> <span class="o">=</span> <span class="mi">0</span>
    <span class="n">optimizer</span><span class="o">.</span><span class="n">zero_grad</span><span class="p">()</span>

    <span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">train_loader</span><span class="p">):</span>
        <span class="n">X</span><span class="p">,</span> <span class="n">y</span> <span class="o">=</span> <span class="n">X</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">),</span> <span class="n">y</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>

        <span class="c1"># Forward pass</span>
        <span class="n">output</span> <span class="o">=</span> <span class="n">model</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>
        <span class="n">loss</span> <span class="o">=</span> <span class="n">criterion</span><span class="p">(</span><span class="n">output</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span> <span class="o">/</span> <span class="n">accumulation_steps</span>
        <span class="n">loss</span><span class="o">.</span><span class="n">backward</span><span class="p">()</span>

        <span class="c1"># Update parameters</span>
        <span class="k">if</span> <span class="p">(</span><span class="n">i</span> <span class="o">+</span> <span class="mi">1</span><span class="p">)</span> <span class="o">%</span> <span class="n">accumulation_steps</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
            <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">utils</span><span class="o">.</span><span class="n">clip_grad_norm_</span><span class="p">(</span><span class="n">model</span><span class="o">.</span><span class="n">parameters</span><span class="p">(),</span> <span class="n">clip_norm</span><span class="p">)</span>
            <span class="n">optimizer</span><span class="o">.</span><span class="n">step</span><span class="p">()</span>
            <span class="n">optimizer</span><span class="o">.</span><span class="n">zero_grad</span><span class="p">()</span>

        <span class="n">total_loss</span> <span class="o">+=</span> <span class="n">loss</span><span class="o">.</span><span class="n">item</span><span class="p">()</span> <span class="o">*</span> <span class="n">accumulation_steps</span>

    <span class="c1"># Handle remaining gradients</span>
    <span class="k">if</span> <span class="p">(</span><span class="n">i</span> <span class="o">+</span> <span class="mi">1</span><span class="p">)</span> <span class="o">%</span> <span class="n">accumulation_steps</span> <span class="o">!=</span> <span class="mi">0</span><span class="p">:</span>
        <span class="n">optimizer</span><span class="o">.</span><span class="n">step</span><span class="p">()</span>
        <span class="n">optimizer</span><span class="o">.</span><span class="n">zero_grad</span><span class="p">()</span>

    <span class="k">return</span> <span class="n">total_loss</span> <span class="o">/</span> <span class="nb">len</span><span class="p">(</span><span class="n">train_loader</span><span class="p">)</span>
</code></pre></div>

<h3 id="64">6.4 í˜¼í•© ì •ë°€ë„ í•™ìŠµ<a class="header-link" href="#64" title="Permanent link">&para;</a></h3>
<p>ë” ë¹ ë¥¸ í•™ìŠµì„ ìœ„í•´ ì˜µí‹°ë§ˆì´ì €ì™€ ê²°í•©:</p>
<div class="highlight"><pre><span></span><code><span class="kn">from</span><span class="w"> </span><span class="nn">torch.cuda.amp</span><span class="w"> </span><span class="kn">import</span> <span class="n">GradScaler</span><span class="p">,</span> <span class="n">autocast</span>

<span class="c1"># Mixed precision training setup</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="s1">&#39;cuda&#39;</span><span class="p">)</span>
<span class="n">optimizer</span> <span class="o">=</span> <span class="n">optim</span><span class="o">.</span><span class="n">AdamW</span><span class="p">(</span><span class="n">model</span><span class="o">.</span><span class="n">parameters</span><span class="p">(),</span> <span class="n">lr</span><span class="o">=</span><span class="mf">1e-3</span><span class="p">)</span>
<span class="n">scaler</span> <span class="o">=</span> <span class="n">GradScaler</span><span class="p">()</span>

<span class="k">for</span> <span class="n">epoch</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">num_epochs</span><span class="p">):</span>
    <span class="k">for</span> <span class="n">X</span><span class="p">,</span> <span class="n">y</span> <span class="ow">in</span> <span class="n">train_loader</span><span class="p">:</span>
        <span class="n">X</span><span class="p">,</span> <span class="n">y</span> <span class="o">=</span> <span class="n">X</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="s1">&#39;cuda&#39;</span><span class="p">),</span> <span class="n">y</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="s1">&#39;cuda&#39;</span><span class="p">)</span>

        <span class="n">optimizer</span><span class="o">.</span><span class="n">zero_grad</span><span class="p">()</span>

        <span class="c1"># Forward pass in fp16</span>
        <span class="k">with</span> <span class="n">autocast</span><span class="p">():</span>
            <span class="n">output</span> <span class="o">=</span> <span class="n">model</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>
            <span class="n">loss</span> <span class="o">=</span> <span class="n">criterion</span><span class="p">(</span><span class="n">output</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>

        <span class="c1"># Backward pass with scaling</span>
        <span class="n">scaler</span><span class="o">.</span><span class="n">scale</span><span class="p">(</span><span class="n">loss</span><span class="p">)</span><span class="o">.</span><span class="n">backward</span><span class="p">()</span>

        <span class="c1"># Gradient clipping (unscale first!)</span>
        <span class="n">scaler</span><span class="o">.</span><span class="n">unscale_</span><span class="p">(</span><span class="n">optimizer</span><span class="p">)</span>
        <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">utils</span><span class="o">.</span><span class="n">clip_grad_norm_</span><span class="p">(</span><span class="n">model</span><span class="o">.</span><span class="n">parameters</span><span class="p">(),</span> <span class="n">max_norm</span><span class="o">=</span><span class="mf">1.0</span><span class="p">)</span>

        <span class="c1"># Update with scaled gradients</span>
        <span class="n">scaler</span><span class="o">.</span><span class="n">step</span><span class="p">(</span><span class="n">optimizer</span><span class="p">)</span>
        <span class="n">scaler</span><span class="o">.</span><span class="n">update</span><span class="p">()</span>

<span class="c1"># Mixed precision with gradient accumulation</span>
<span class="n">scaler</span> <span class="o">=</span> <span class="n">GradScaler</span><span class="p">()</span>
<span class="n">accumulation_steps</span> <span class="o">=</span> <span class="mi">4</span>

<span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">train_loader</span><span class="p">):</span>
    <span class="k">with</span> <span class="n">autocast</span><span class="p">():</span>
        <span class="n">output</span> <span class="o">=</span> <span class="n">model</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>
        <span class="n">loss</span> <span class="o">=</span> <span class="n">criterion</span><span class="p">(</span><span class="n">output</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span> <span class="o">/</span> <span class="n">accumulation_steps</span>

    <span class="n">scaler</span><span class="o">.</span><span class="n">scale</span><span class="p">(</span><span class="n">loss</span><span class="p">)</span><span class="o">.</span><span class="n">backward</span><span class="p">()</span>

    <span class="k">if</span> <span class="p">(</span><span class="n">i</span> <span class="o">+</span> <span class="mi">1</span><span class="p">)</span> <span class="o">%</span> <span class="n">accumulation_steps</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
        <span class="n">scaler</span><span class="o">.</span><span class="n">unscale_</span><span class="p">(</span><span class="n">optimizer</span><span class="p">)</span>
        <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">utils</span><span class="o">.</span><span class="n">clip_grad_norm_</span><span class="p">(</span><span class="n">model</span><span class="o">.</span><span class="n">parameters</span><span class="p">(),</span> <span class="mf">1.0</span><span class="p">)</span>
        <span class="n">scaler</span><span class="o">.</span><span class="n">step</span><span class="p">(</span><span class="n">optimizer</span><span class="p">)</span>
        <span class="n">scaler</span><span class="o">.</span><span class="n">update</span><span class="p">()</span>
        <span class="n">optimizer</span><span class="o">.</span><span class="n">zero_grad</span><span class="p">()</span>
</code></pre></div>

<h3 id="65">6.5 íŒŒë¼ë¯¸í„° ê·¸ë£¹ë³„ í•™ìŠµë¥ <a class="header-link" href="#65" title="Permanent link">&para;</a></h3>
<p>ëª¨ë¸ì˜ ë‹¤ë¥¸ ë¶€ë¶„ì— ë‹¤ë¥¸ LR:</p>
<div class="highlight"><pre><span></span><code><span class="c1"># Example: Fine-tuning with different LRs for backbone and head</span>
<span class="k">class</span><span class="w"> </span><span class="nc">TransferModel</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="k">def</span><span class="w"> </span><span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">backbone</span><span class="p">,</span> <span class="n">num_classes</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">backbone</span> <span class="o">=</span> <span class="n">backbone</span>  <span class="c1"># Pretrained</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">head</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="mi">512</span><span class="p">,</span> <span class="n">num_classes</span><span class="p">)</span>  <span class="c1"># New</span>

    <span class="k">def</span><span class="w"> </span><span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
        <span class="n">features</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">backbone</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">head</span><span class="p">(</span><span class="n">features</span><span class="p">)</span>

<span class="c1"># Different learning rates for different parts</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">TransferModel</span><span class="p">(</span><span class="n">pretrained_backbone</span><span class="p">,</span> <span class="n">num_classes</span><span class="o">=</span><span class="mi">10</span><span class="p">)</span>

<span class="n">optimizer</span> <span class="o">=</span> <span class="n">optim</span><span class="o">.</span><span class="n">AdamW</span><span class="p">([</span>
    <span class="p">{</span><span class="s1">&#39;params&#39;</span><span class="p">:</span> <span class="n">model</span><span class="o">.</span><span class="n">backbone</span><span class="o">.</span><span class="n">parameters</span><span class="p">(),</span> <span class="s1">&#39;lr&#39;</span><span class="p">:</span> <span class="mf">1e-5</span><span class="p">},</span>  <span class="c1"># Low LR for backbone</span>
    <span class="p">{</span><span class="s1">&#39;params&#39;</span><span class="p">:</span> <span class="n">model</span><span class="o">.</span><span class="n">head</span><span class="o">.</span><span class="n">parameters</span><span class="p">(),</span> <span class="s1">&#39;lr&#39;</span><span class="p">:</span> <span class="mf">1e-3</span><span class="p">}</span>  <span class="c1"># High LR for head</span>
<span class="p">],</span> <span class="n">weight_decay</span><span class="o">=</span><span class="mf">0.01</span><span class="p">)</span>

<span class="c1"># Can also use different weight decay</span>
<span class="n">optimizer</span> <span class="o">=</span> <span class="n">optim</span><span class="o">.</span><span class="n">AdamW</span><span class="p">([</span>
    <span class="p">{</span><span class="s1">&#39;params&#39;</span><span class="p">:</span> <span class="n">model</span><span class="o">.</span><span class="n">backbone</span><span class="o">.</span><span class="n">parameters</span><span class="p">(),</span> <span class="s1">&#39;lr&#39;</span><span class="p">:</span> <span class="mf">1e-5</span><span class="p">,</span> <span class="s1">&#39;weight_decay&#39;</span><span class="p">:</span> <span class="mf">0.01</span><span class="p">},</span>
    <span class="p">{</span><span class="s1">&#39;params&#39;</span><span class="p">:</span> <span class="n">model</span><span class="o">.</span><span class="n">head</span><span class="o">.</span><span class="n">parameters</span><span class="p">(),</span> <span class="s1">&#39;lr&#39;</span><span class="p">:</span> <span class="mf">1e-3</span><span class="p">,</span> <span class="s1">&#39;weight_decay&#39;</span><span class="p">:</span> <span class="mf">0.0</span><span class="p">}</span>
<span class="p">])</span>

<span class="c1"># Advanced: layer-wise learning rate decay (LLRD)</span>
<span class="k">def</span><span class="w"> </span><span class="nf">get_layer_wise_lr_groups</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">base_lr</span><span class="o">=</span><span class="mf">1e-3</span><span class="p">,</span> <span class="n">decay_rate</span><span class="o">=</span><span class="mf">0.9</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Apply lower LR to earlier layers&quot;&quot;&quot;</span>
    <span class="n">num_layers</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="nb">list</span><span class="p">(</span><span class="n">model</span><span class="o">.</span><span class="n">named_parameters</span><span class="p">()))</span>

    <span class="n">param_groups</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="p">(</span><span class="n">name</span><span class="p">,</span> <span class="n">param</span><span class="p">)</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">model</span><span class="o">.</span><span class="n">named_parameters</span><span class="p">()):</span>
        <span class="c1"># Earlier layers get lower LR</span>
        <span class="n">layer_lr</span> <span class="o">=</span> <span class="n">base_lr</span> <span class="o">*</span> <span class="p">(</span><span class="n">decay_rate</span> <span class="o">**</span> <span class="p">(</span><span class="n">num_layers</span> <span class="o">-</span> <span class="n">i</span> <span class="o">-</span> <span class="mi">1</span><span class="p">))</span>
        <span class="n">param_groups</span><span class="o">.</span><span class="n">append</span><span class="p">({</span><span class="s1">&#39;params&#39;</span><span class="p">:</span> <span class="n">param</span><span class="p">,</span> <span class="s1">&#39;lr&#39;</span><span class="p">:</span> <span class="n">layer_lr</span><span class="p">})</span>

    <span class="k">return</span> <span class="n">param_groups</span>

<span class="c1"># Usage for transformer fine-tuning</span>
<span class="n">param_groups</span> <span class="o">=</span> <span class="n">get_layer_wise_lr_groups</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">base_lr</span><span class="o">=</span><span class="mf">1e-4</span><span class="p">,</span> <span class="n">decay_rate</span><span class="o">=</span><span class="mf">0.95</span><span class="p">)</span>
<span class="n">optimizer</span> <span class="o">=</span> <span class="n">optim</span><span class="o">.</span><span class="n">AdamW</span><span class="p">(</span><span class="n">param_groups</span><span class="p">,</span> <span class="n">weight_decay</span><span class="o">=</span><span class="mf">0.01</span><span class="p">)</span>

<span class="c1"># Discriminative fine-tuning (common in NLP)</span>
<span class="k">def</span><span class="w"> </span><span class="nf">get_discriminative_lr_groups</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">base_lr</span><span class="o">=</span><span class="mf">1e-3</span><span class="p">,</span> <span class="n">lr_mult</span><span class="o">=</span><span class="mf">2.6</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Higher LR for later layers&quot;&quot;&quot;</span>
    <span class="n">layers</span> <span class="o">=</span> <span class="p">[</span><span class="n">model</span><span class="o">.</span><span class="n">layer1</span><span class="p">,</span> <span class="n">model</span><span class="o">.</span><span class="n">layer2</span><span class="p">,</span> <span class="n">model</span><span class="o">.</span><span class="n">layer3</span><span class="p">,</span> <span class="n">model</span><span class="o">.</span><span class="n">head</span><span class="p">]</span>

    <span class="n">param_groups</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">layer</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">layers</span><span class="p">):</span>
        <span class="n">lr</span> <span class="o">=</span> <span class="n">base_lr</span> <span class="o">*</span> <span class="p">(</span><span class="n">lr_mult</span> <span class="o">**</span> <span class="n">i</span><span class="p">)</span>
        <span class="n">param_groups</span><span class="o">.</span><span class="n">append</span><span class="p">({</span><span class="s1">&#39;params&#39;</span><span class="p">:</span> <span class="n">layer</span><span class="o">.</span><span class="n">parameters</span><span class="p">(),</span> <span class="s1">&#39;lr&#39;</span><span class="p">:</span> <span class="n">lr</span><span class="p">})</span>

    <span class="k">return</span> <span class="n">param_groups</span>
</code></pre></div>

<hr />
<h2 id="7">7. ì˜¬ë°”ë¥¸ ì˜µí‹°ë§ˆì´ì € ì„ íƒí•˜ê¸°<a class="header-link" href="#7" title="Permanent link">&para;</a></h2>
<h3 id="71">7.1 ê²°ì • ê°€ì´ë“œ<a class="header-link" href="#71" title="Permanent link">&para;</a></h3>
<div class="highlight"><pre><span></span><code>Optimization Decision Tree:

Are you training from scratch?
â”‚
â”œâ”€ Yes: What architecture?
â”‚  â”œâ”€ CNN (ResNet, EfficientNet)
â”‚  â”‚  â””â”€ SGD + Momentum (0.9) + Cosine Annealing
â”‚  â”‚     LR: 0.1, batch 256
â”‚  â”‚
â”‚  â”œâ”€ Transformer (BERT, GPT, ViT)
â”‚  â”‚  â””â”€ AdamW + Linear Warmup (10%) + Cosine Decay
â”‚  â”‚     LR: 5e-4, batch 256-512, weight_decay 0.01
â”‚  â”‚
â”‚  â”œâ”€ GAN
â”‚  â”‚  â””â”€ Adam (Î²â‚=0.5, Î²â‚‚=0.999) or RMSprop
â”‚  â”‚     LR: 2e-4, no momentum, no warmup
â”‚  â”‚
â”‚  â””â”€ RNN/LSTM
â”‚     â””â”€ AdamW or RMSprop + Gradient Clipping (1.0)
â”‚        LR: 1e-3
â”‚
â””â”€ No: Fine-tuning pretrained model?
   â””â”€ AdamW + Low LR + Per-parameter groups
      Backbone LR: 1e-5, Head LR: 1e-3
      Linear warmup (5-10%) + Cosine decay
</code></pre></div>

<h3 id="72-sgd-vs-adam">7.2 SGD vs Adam ë…¼ìŸ<a class="header-link" href="#72-sgd-vs-adam" title="Permanent link">&para;</a></h3>
<p><strong>SGD + Momentumì„ ì‚¬ìš©í•  ë•Œ:</strong></p>
<p>âœ… ì²˜ìŒë¶€í„° CNN í•™ìŠµ (ResNet, VGG)
âœ… ì¼ë°˜í™”ê°€ ì¤‘ìš”í•  ë•Œ
âœ… ê´‘ë²”ìœ„í•œ í•˜ì´í¼íŒŒë¼ë¯¸í„° ê²€ìƒ‰ì— ì‹œê°„ì„ ë“¤ì¼ ìˆ˜ ìˆì„ ë•Œ
âœ… í° ë°°ì¹˜ í¬ê¸°ë¥¼ ì‚¬ìš©í•  ìˆ˜ ìˆì„ ë•Œ</p>
<p><strong>ì¥ì :</strong>
- ì¢…ì¢… ë” ë‚˜ì€ ìµœì¢… ì •í™•ë„ (ImageNetì—ì„œ 0.5-1%)
- ìƒˆë¡œìš´ ë°ì´í„°ì— ë” ë‚˜ì€ ì¼ë°˜í™”
- í° ë°°ì¹˜ í¬ê¸°ì—ì„œ ë” ì•ˆì •ì </p>
<p><strong>ë‹¨ì :</strong>
- ì‹ ì¤‘í•œ LR íŠœë‹ í•„ìš”
- ë” ê¸´ í•™ìŠµ í•„ìš” (90-200 ì—í­)
- ì´ˆê¸°í™”ì— ë¯¼ê°
- í° ë°°ì¹˜ì—ëŠ” ì›Œë°ì—… í•„ìš”</p>
<p><strong>Adam/AdamWë¥¼ ì‚¬ìš©í•  ë•Œ:</strong></p>
<p>âœ… íŠ¸ëœìŠ¤í¬ë¨¸ í•™ìŠµ
âœ… ë¹ ë¥¸ í”„ë¡œí† íƒ€ì´í•‘
âœ… ì‚¬ì „í•™ìŠµëœ ëª¨ë¸ íŒŒì¸íŠœë‹
âœ… ì œí•œëœ ê³„ì‚°/ì‹œê°„ìœ¼ë¡œ ì‘ì—…í•  ë•Œ
âœ… ì‘ì€ ë°°ì¹˜ í¬ê¸°</p>
<p><strong>ì¥ì :</strong>
- í•˜ì´í¼íŒŒë¼ë¯¸í„°ì— ê°•ê±´ (ê¸°ë³¸ê°’ìœ¼ë¡œ ì‘ë™)
- ë” ë¹ ë¥¸ ìˆ˜ë ´ (ì ì€ ì—í­)
- LR ì„ íƒì— ëœ ë¯¼ê°
- ì ì‘í˜• ë¬¸ì œì— ì¢‹ìŒ (NLP, RL)</p>
<p><strong>ë‹¨ì :</strong>
- ë¹„ì „ì—ì„œ ì•½ê°„ ë‚˜ìœ ì¼ë°˜í™” ê°€ëŠ¥
- ë” ë†’ì€ ë©”ëª¨ë¦¬ ì‚¬ìš©
- ë” ì‰½ê²Œ ê³¼ì í•© ê°€ëŠ¥</p>
<div class="highlight"><pre><span></span><code><span class="c1"># Example: Training ResNet on CIFAR-10</span>

<span class="c1"># SGD approach (better accuracy, more tuning)</span>
<span class="k">def</span><span class="w"> </span><span class="nf">train_resnet_sgd</span><span class="p">():</span>
    <span class="n">model</span> <span class="o">=</span> <span class="n">torchvision</span><span class="o">.</span><span class="n">models</span><span class="o">.</span><span class="n">resnet18</span><span class="p">(</span><span class="n">num_classes</span><span class="o">=</span><span class="mi">10</span><span class="p">)</span>

    <span class="n">optimizer</span> <span class="o">=</span> <span class="n">optim</span><span class="o">.</span><span class="n">SGD</span><span class="p">(</span>
        <span class="n">model</span><span class="o">.</span><span class="n">parameters</span><span class="p">(),</span>
        <span class="n">lr</span><span class="o">=</span><span class="mf">0.1</span><span class="p">,</span>
        <span class="n">momentum</span><span class="o">=</span><span class="mf">0.9</span><span class="p">,</span>
        <span class="n">weight_decay</span><span class="o">=</span><span class="mf">5e-4</span><span class="p">,</span>
        <span class="n">nesterov</span><span class="o">=</span><span class="kc">True</span>
    <span class="p">)</span>

    <span class="n">scheduler</span> <span class="o">=</span> <span class="n">optim</span><span class="o">.</span><span class="n">lr_scheduler</span><span class="o">.</span><span class="n">CosineAnnealingLR</span><span class="p">(</span>
        <span class="n">optimizer</span><span class="p">,</span>
        <span class="n">T_max</span><span class="o">=</span><span class="mi">200</span><span class="p">,</span>
        <span class="n">eta_min</span><span class="o">=</span><span class="mf">1e-4</span>
    <span class="p">)</span>

    <span class="c1"># Train for 200 epochs</span>
    <span class="c1"># Expected accuracy: ~95%</span>

<span class="c1"># AdamW approach (faster convergence, easier tuning)</span>
<span class="k">def</span><span class="w"> </span><span class="nf">train_resnet_adamw</span><span class="p">():</span>
    <span class="n">model</span> <span class="o">=</span> <span class="n">torchvision</span><span class="o">.</span><span class="n">models</span><span class="o">.</span><span class="n">resnet18</span><span class="p">(</span><span class="n">num_classes</span><span class="o">=</span><span class="mi">10</span><span class="p">)</span>

    <span class="n">optimizer</span> <span class="o">=</span> <span class="n">optim</span><span class="o">.</span><span class="n">AdamW</span><span class="p">(</span>
        <span class="n">model</span><span class="o">.</span><span class="n">parameters</span><span class="p">(),</span>
        <span class="n">lr</span><span class="o">=</span><span class="mf">1e-3</span><span class="p">,</span>
        <span class="n">weight_decay</span><span class="o">=</span><span class="mf">0.01</span>
    <span class="p">)</span>

    <span class="c1"># Simple schedule</span>
    <span class="n">scheduler</span> <span class="o">=</span> <span class="n">optim</span><span class="o">.</span><span class="n">lr_scheduler</span><span class="o">.</span><span class="n">CosineAnnealingLR</span><span class="p">(</span><span class="n">optimizer</span><span class="p">,</span> <span class="n">T_max</span><span class="o">=</span><span class="mi">100</span><span class="p">)</span>

    <span class="c1"># Train for 100 epochs</span>
    <span class="c1"># Expected accuracy: ~94% (slightly lower but faster)</span>
</code></pre></div>

<h3 id="73">7.3 ì¼ë°˜ì ì¸ ë ˆì‹œí”¼<a class="header-link" href="#73" title="Permanent link">&para;</a></h3>
<p><strong>ë ˆì‹œí”¼ 1: ImageNet í•™ìŠµ (ResNet-50)</strong></p>
<div class="highlight"><pre><span></span><code><span class="c1"># Standard recipe for ImageNet</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">torchvision</span><span class="o">.</span><span class="n">models</span><span class="o">.</span><span class="n">resnet50</span><span class="p">(</span><span class="n">pretrained</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
<span class="n">optimizer</span> <span class="o">=</span> <span class="n">optim</span><span class="o">.</span><span class="n">SGD</span><span class="p">(</span>
    <span class="n">model</span><span class="o">.</span><span class="n">parameters</span><span class="p">(),</span>
    <span class="n">lr</span><span class="o">=</span><span class="mf">0.1</span><span class="p">,</span>
    <span class="n">momentum</span><span class="o">=</span><span class="mf">0.9</span><span class="p">,</span>
    <span class="n">weight_decay</span><span class="o">=</span><span class="mf">1e-4</span>
<span class="p">)</span>

<span class="c1"># Warmup for 5 epochs, then cosine decay for 90 epochs</span>
<span class="n">warmup_scheduler</span> <span class="o">=</span> <span class="n">optim</span><span class="o">.</span><span class="n">lr_scheduler</span><span class="o">.</span><span class="n">LinearLR</span><span class="p">(</span>
    <span class="n">optimizer</span><span class="p">,</span>
    <span class="n">start_factor</span><span class="o">=</span><span class="mf">0.1</span><span class="p">,</span>
    <span class="n">total_iters</span><span class="o">=</span><span class="mi">5</span> <span class="o">*</span> <span class="nb">len</span><span class="p">(</span><span class="n">train_loader</span><span class="p">)</span>
<span class="p">)</span>
<span class="n">cosine_scheduler</span> <span class="o">=</span> <span class="n">optim</span><span class="o">.</span><span class="n">lr_scheduler</span><span class="o">.</span><span class="n">CosineAnnealingLR</span><span class="p">(</span>
    <span class="n">optimizer</span><span class="p">,</span>
    <span class="n">T_max</span><span class="o">=</span><span class="mi">90</span> <span class="o">*</span> <span class="nb">len</span><span class="p">(</span><span class="n">train_loader</span><span class="p">),</span>
    <span class="n">eta_min</span><span class="o">=</span><span class="mf">1e-5</span>
<span class="p">)</span>
<span class="n">scheduler</span> <span class="o">=</span> <span class="n">optim</span><span class="o">.</span><span class="n">lr_scheduler</span><span class="o">.</span><span class="n">SequentialLR</span><span class="p">(</span>
    <span class="n">optimizer</span><span class="p">,</span>
    <span class="n">schedulers</span><span class="o">=</span><span class="p">[</span><span class="n">warmup_scheduler</span><span class="p">,</span> <span class="n">cosine_scheduler</span><span class="p">],</span>
    <span class="n">milestones</span><span class="o">=</span><span class="p">[</span><span class="mi">5</span> <span class="o">*</span> <span class="nb">len</span><span class="p">(</span><span class="n">train_loader</span><span class="p">)]</span>
<span class="p">)</span>

<span class="c1"># Batch size 256, 90 epochs total</span>
<span class="c1"># Expected Top-1: 76.2%, Top-5: 93.0%</span>
</code></pre></div>

<p><strong>ë ˆì‹œí”¼ 2: BERT ì‚¬ì „í•™ìŠµ</strong></p>
<div class="highlight"><pre><span></span><code><span class="c1"># BERT-base configuration</span>
<span class="n">optimizer</span> <span class="o">=</span> <span class="n">optim</span><span class="o">.</span><span class="n">AdamW</span><span class="p">(</span>
    <span class="n">model</span><span class="o">.</span><span class="n">parameters</span><span class="p">(),</span>
    <span class="n">lr</span><span class="o">=</span><span class="mf">1e-4</span><span class="p">,</span>
    <span class="n">betas</span><span class="o">=</span><span class="p">(</span><span class="mf">0.9</span><span class="p">,</span> <span class="mf">0.999</span><span class="p">),</span>
    <span class="n">eps</span><span class="o">=</span><span class="mf">1e-6</span><span class="p">,</span>
    <span class="n">weight_decay</span><span class="o">=</span><span class="mf">0.01</span>
<span class="p">)</span>

<span class="c1"># 10% linear warmup + 90% linear decay</span>
<span class="n">total_steps</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">train_loader</span><span class="p">)</span> <span class="o">*</span> <span class="n">num_epochs</span>
<span class="n">warmup_steps</span> <span class="o">=</span> <span class="nb">int</span><span class="p">(</span><span class="mf">0.1</span> <span class="o">*</span> <span class="n">total_steps</span><span class="p">)</span>

<span class="n">scheduler</span> <span class="o">=</span> <span class="n">optim</span><span class="o">.</span><span class="n">lr_scheduler</span><span class="o">.</span><span class="n">LambdaLR</span><span class="p">(</span>
    <span class="n">optimizer</span><span class="p">,</span>
    <span class="n">lr_lambda</span><span class="o">=</span><span class="k">lambda</span> <span class="n">step</span><span class="p">:</span> <span class="nb">min</span><span class="p">(</span>
        <span class="n">step</span> <span class="o">/</span> <span class="n">warmup_steps</span><span class="p">,</span>
        <span class="p">(</span><span class="n">total_steps</span> <span class="o">-</span> <span class="n">step</span><span class="p">)</span> <span class="o">/</span> <span class="p">(</span><span class="n">total_steps</span> <span class="o">-</span> <span class="n">warmup_steps</span><span class="p">)</span>
    <span class="p">)</span>
<span class="p">)</span>

<span class="c1"># Batch size 256, gradient accumulation 4 (effective 1024)</span>
<span class="c1"># Gradient clipping: 1.0</span>
</code></pre></div>

<p><strong>ë ˆì‹œí”¼ 3: ì‚¬ì „í•™ìŠµëœ ëª¨ë¸ íŒŒì¸íŠœë‹</strong></p>
<div class="highlight"><pre><span></span><code><span class="c1"># Fine-tuning BERT for classification</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">AutoModel</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="s1">&#39;bert-base-uncased&#39;</span><span class="p">)</span>
<span class="n">classifier</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="mi">768</span><span class="p">,</span> <span class="n">num_classes</span><span class="p">)</span>

<span class="c1"># Different LR for pretrained vs new layers</span>
<span class="n">optimizer</span> <span class="o">=</span> <span class="n">optim</span><span class="o">.</span><span class="n">AdamW</span><span class="p">([</span>
    <span class="p">{</span><span class="s1">&#39;params&#39;</span><span class="p">:</span> <span class="n">model</span><span class="o">.</span><span class="n">parameters</span><span class="p">(),</span> <span class="s1">&#39;lr&#39;</span><span class="p">:</span> <span class="mf">2e-5</span><span class="p">},</span>  <span class="c1"># Very low for pretrained</span>
    <span class="p">{</span><span class="s1">&#39;params&#39;</span><span class="p">:</span> <span class="n">classifier</span><span class="o">.</span><span class="n">parameters</span><span class="p">(),</span> <span class="s1">&#39;lr&#39;</span><span class="p">:</span> <span class="mf">2e-4</span><span class="p">}</span>  <span class="c1"># Higher for new layer</span>
<span class="p">],</span> <span class="n">weight_decay</span><span class="o">=</span><span class="mf">0.01</span><span class="p">)</span>

<span class="c1"># Short warmup + cosine decay</span>
<span class="n">total_steps</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">train_loader</span><span class="p">)</span> <span class="o">*</span> <span class="mi">5</span>  <span class="c1"># 5 epochs</span>
<span class="n">warmup_steps</span> <span class="o">=</span> <span class="nb">int</span><span class="p">(</span><span class="mf">0.1</span> <span class="o">*</span> <span class="n">total_steps</span><span class="p">)</span>

<span class="n">scheduler</span> <span class="o">=</span> <span class="n">WarmupCosineSchedule</span><span class="p">(</span><span class="n">optimizer</span><span class="p">,</span> <span class="n">warmup_steps</span><span class="p">,</span> <span class="n">total_steps</span><span class="p">)</span>

<span class="c1"># Small batch size (16-32), few epochs (3-5)</span>
<span class="c1"># Gradient clipping: 1.0</span>
</code></pre></div>

<p><strong>ë ˆì‹œí”¼ 4: GAN í•™ìŠµ</strong></p>
<div class="highlight"><pre><span></span><code><span class="c1"># DCGAN recipe</span>
<span class="n">generator</span> <span class="o">=</span> <span class="n">Generator</span><span class="p">()</span>
<span class="n">discriminator</span> <span class="o">=</span> <span class="n">Discriminator</span><span class="p">()</span>

<span class="c1"># Adam with Î²â‚=0.5 (less momentum than default)</span>
<span class="n">opt_G</span> <span class="o">=</span> <span class="n">optim</span><span class="o">.</span><span class="n">Adam</span><span class="p">(</span><span class="n">generator</span><span class="o">.</span><span class="n">parameters</span><span class="p">(),</span> <span class="n">lr</span><span class="o">=</span><span class="mf">2e-4</span><span class="p">,</span> <span class="n">betas</span><span class="o">=</span><span class="p">(</span><span class="mf">0.5</span><span class="p">,</span> <span class="mf">0.999</span><span class="p">))</span>
<span class="n">opt_D</span> <span class="o">=</span> <span class="n">optim</span><span class="o">.</span><span class="n">Adam</span><span class="p">(</span><span class="n">discriminator</span><span class="o">.</span><span class="n">parameters</span><span class="p">(),</span> <span class="n">lr</span><span class="o">=</span><span class="mf">2e-4</span><span class="p">,</span> <span class="n">betas</span><span class="o">=</span><span class="p">(</span><span class="mf">0.5</span><span class="p">,</span> <span class="mf">0.999</span><span class="p">))</span>

<span class="c1"># No scheduler, no weight decay</span>
<span class="c1"># Alternate training: 1 D step, 1 G step</span>
<span class="c1"># No gradient clipping usually</span>
</code></pre></div>

<p><strong>ë ˆì‹œí”¼ 5: Vision Transformer (ViT)</strong></p>
<div class="highlight"><pre><span></span><code><span class="c1"># ViT-B/16 on ImageNet</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">torchvision</span><span class="o">.</span><span class="n">models</span><span class="o">.</span><span class="n">vit_b_16</span><span class="p">(</span><span class="n">pretrained</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>

<span class="n">optimizer</span> <span class="o">=</span> <span class="n">optim</span><span class="o">.</span><span class="n">AdamW</span><span class="p">(</span>
    <span class="n">model</span><span class="o">.</span><span class="n">parameters</span><span class="p">(),</span>
    <span class="n">lr</span><span class="o">=</span><span class="mf">3e-3</span><span class="p">,</span>  <span class="c1"># Higher than BERT!</span>
    <span class="n">weight_decay</span><span class="o">=</span><span class="mf">0.3</span><span class="p">,</span>  <span class="c1"># Stronger regularization</span>
    <span class="n">betas</span><span class="o">=</span><span class="p">(</span><span class="mf">0.9</span><span class="p">,</span> <span class="mf">0.999</span><span class="p">)</span>
<span class="p">)</span>

<span class="c1"># Linear warmup + cosine decay</span>
<span class="n">total_steps</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">train_loader</span><span class="p">)</span> <span class="o">*</span> <span class="mi">300</span>  <span class="c1"># 300 epochs</span>
<span class="n">warmup_steps</span> <span class="o">=</span> <span class="nb">int</span><span class="p">(</span><span class="mf">0.05</span> <span class="o">*</span> <span class="n">total_steps</span><span class="p">)</span>  <span class="c1"># 5% warmup</span>

<span class="n">scheduler</span> <span class="o">=</span> <span class="n">WarmupCosineSchedule</span><span class="p">(</span><span class="n">optimizer</span><span class="p">,</span> <span class="n">warmup_steps</span><span class="p">,</span> <span class="n">total_steps</span><span class="p">,</span> <span class="n">min_lr_ratio</span><span class="o">=</span><span class="mf">0.01</span><span class="p">)</span>

<span class="c1"># Large batch (1024-4096), strong augmentation</span>
<span class="c1"># Gradient clipping: 1.0</span>
</code></pre></div>

<h3 id="74">7.4 ìµœì í™” ë¬¸ì œ ë””ë²„ê¹…<a class="header-link" href="#74" title="Permanent link">&para;</a></h3>
<p><strong>ë¬¸ì œ 1: ì†ì‹¤ì´ ê°ì†Œí•˜ì§€ ì•ŠìŒ</strong></p>
<div class="highlight"><pre><span></span><code><span class="c1"># Checklist:</span>
<span class="c1"># 1. Check learning rate (try LR finder)</span>
<span class="c1"># 2. Verify gradients are flowing</span>
<span class="k">for</span> <span class="n">name</span><span class="p">,</span> <span class="n">param</span> <span class="ow">in</span> <span class="n">model</span><span class="o">.</span><span class="n">named_parameters</span><span class="p">():</span>
    <span class="k">if</span> <span class="n">param</span><span class="o">.</span><span class="n">grad</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
        <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="n">name</span><span class="si">}</span><span class="s2">: grad_norm=</span><span class="si">{</span><span class="n">param</span><span class="o">.</span><span class="n">grad</span><span class="o">.</span><span class="n">norm</span><span class="p">()</span><span class="si">:</span><span class="s2">.6f</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="n">name</span><span class="si">}</span><span class="s2">: NO GRADIENT!&quot;</span><span class="p">)</span>  <span class="c1"># Problem!</span>

<span class="c1"># 3. Check for NaN/Inf</span>
<span class="k">if</span> <span class="n">torch</span><span class="o">.</span><span class="n">isnan</span><span class="p">(</span><span class="n">loss</span><span class="p">)</span> <span class="ow">or</span> <span class="n">torch</span><span class="o">.</span><span class="n">isinf</span><span class="p">(</span><span class="n">loss</span><span class="p">):</span>
    <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Loss is NaN/Inf! Reduce learning rate or check data.&quot;</span><span class="p">)</span>

<span class="c1"># 4. Verify data is changing</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Batch variance: </span><span class="si">{</span><span class="n">X</span><span class="o">.</span><span class="n">var</span><span class="p">()</span><span class="o">.</span><span class="n">item</span><span class="p">()</span><span class="si">:</span><span class="s2">.6f</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>  <span class="c1"># Should be non-zero</span>

<span class="c1"># 5. Try simpler optimizer (SGD) to isolate issue</span>
<span class="n">optimizer</span> <span class="o">=</span> <span class="n">optim</span><span class="o">.</span><span class="n">SGD</span><span class="p">(</span><span class="n">model</span><span class="o">.</span><span class="n">parameters</span><span class="p">(),</span> <span class="n">lr</span><span class="o">=</span><span class="mf">0.001</span><span class="p">)</span>
</code></pre></div>

<p><strong>ë¬¸ì œ 2: ì†ì‹¤ í­ë°œ</strong></p>
<div class="highlight"><pre><span></span><code><span class="c1"># Solutions:</span>
<span class="c1"># 1. Gradient clipping</span>
<span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">utils</span><span class="o">.</span><span class="n">clip_grad_norm_</span><span class="p">(</span><span class="n">model</span><span class="o">.</span><span class="n">parameters</span><span class="p">(),</span> <span class="n">max_norm</span><span class="o">=</span><span class="mf">1.0</span><span class="p">)</span>

<span class="c1"># 2. Lower learning rate</span>
<span class="n">optimizer</span> <span class="o">=</span> <span class="n">optim</span><span class="o">.</span><span class="n">Adam</span><span class="p">(</span><span class="n">model</span><span class="o">.</span><span class="n">parameters</span><span class="p">(),</span> <span class="n">lr</span><span class="o">=</span><span class="mf">1e-4</span><span class="p">)</span>  <span class="c1"># Try 10x lower</span>

<span class="c1"># 3. Check for numerical instability</span>
<span class="c1"># Use float32, avoid log(0), div by 0</span>
<span class="n">loss</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="n">output</span> <span class="o">+</span> <span class="mf">1e-8</span><span class="p">)</span>  <span class="c1"># Add epsilon</span>

<span class="c1"># 4. Use mixed precision</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">torch.cuda.amp</span><span class="w"> </span><span class="kn">import</span> <span class="n">autocast</span>
<span class="k">with</span> <span class="n">autocast</span><span class="p">():</span>
    <span class="n">output</span> <span class="o">=</span> <span class="n">model</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>
</code></pre></div>

<p><strong>ë¬¸ì œ 3: í•™ìŠµê³¼ ê²€ì¦ ì„±ëŠ¥ ì°¨ì´</strong></p>
<div class="highlight"><pre><span></span><code><span class="c1"># Solutions:</span>
<span class="c1"># 1. Increase regularization</span>
<span class="n">optimizer</span> <span class="o">=</span> <span class="n">optim</span><span class="o">.</span><span class="n">AdamW</span><span class="p">(</span><span class="n">model</span><span class="o">.</span><span class="n">parameters</span><span class="p">(),</span> <span class="n">lr</span><span class="o">=</span><span class="mf">1e-3</span><span class="p">,</span> <span class="n">weight_decay</span><span class="o">=</span><span class="mf">0.1</span><span class="p">)</span>  <span class="c1"># Higher</span>

<span class="c1"># 2. Add dropout</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Sequential</span><span class="p">(</span>
    <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="mi">100</span><span class="p">,</span> <span class="mi">50</span><span class="p">),</span>
    <span class="n">nn</span><span class="o">.</span><span class="n">ReLU</span><span class="p">(),</span>
    <span class="n">nn</span><span class="o">.</span><span class="n">Dropout</span><span class="p">(</span><span class="mf">0.5</span><span class="p">),</span>  <span class="c1"># Add dropout</span>
    <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="mi">50</span><span class="p">,</span> <span class="mi">10</span><span class="p">)</span>
<span class="p">)</span>

<span class="c1"># 3. Reduce model capacity</span>
<span class="c1"># Use smaller model or fewer layers</span>

<span class="c1"># 4. More data augmentation</span>
<span class="c1"># Stronger augmentation on training data</span>
</code></pre></div>

<p><strong>ë¬¸ì œ 4: ëŠë¦° ìˆ˜ë ´</strong></p>
<div class="highlight"><pre><span></span><code><span class="c1"># Solutions:</span>
<span class="c1"># 1. Increase learning rate (use LR finder)</span>
<span class="c1"># 2. Switch from SGD to Adam</span>
<span class="n">optimizer</span> <span class="o">=</span> <span class="n">optim</span><span class="o">.</span><span class="n">AdamW</span><span class="p">(</span><span class="n">model</span><span class="o">.</span><span class="n">parameters</span><span class="p">(),</span> <span class="n">lr</span><span class="o">=</span><span class="mf">1e-3</span><span class="p">)</span>

<span class="c1"># 3. Add learning rate warmup</span>
<span class="n">scheduler</span> <span class="o">=</span> <span class="n">WarmupCosineSchedule</span><span class="p">(</span><span class="n">optimizer</span><span class="p">,</span> <span class="n">warmup_steps</span><span class="o">=</span><span class="mi">1000</span><span class="p">,</span> <span class="n">total_steps</span><span class="o">=</span><span class="mi">10000</span><span class="p">)</span>

<span class="c1"># 4. Check batch size (try larger)</span>
<span class="c1"># 5. Verify batch normalization is working</span>
<span class="c1"># 6. Check weight initialization</span>
<span class="k">def</span><span class="w"> </span><span class="nf">init_weights</span><span class="p">(</span><span class="n">m</span><span class="p">):</span>
    <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">m</span><span class="p">,</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">):</span>
        <span class="n">nn</span><span class="o">.</span><span class="n">init</span><span class="o">.</span><span class="n">kaiming_normal_</span><span class="p">(</span><span class="n">m</span><span class="o">.</span><span class="n">weight</span><span class="p">)</span>
        <span class="n">m</span><span class="o">.</span><span class="n">bias</span><span class="o">.</span><span class="n">data</span><span class="o">.</span><span class="n">fill_</span><span class="p">(</span><span class="mf">0.01</span><span class="p">)</span>

<span class="n">model</span><span class="o">.</span><span class="n">apply</span><span class="p">(</span><span class="n">init_weights</span><span class="p">)</span>
</code></pre></div>

<hr />
<h2 id="_2">ì—°ìŠµ ë¬¸ì œ<a class="header-link" href="#_2" title="Permanent link">&para;</a></h2>
<h3 id="1_1">ì—°ìŠµ ë¬¸ì œ 1: ì˜µí‹°ë§ˆì´ì € êµ¬í˜„ ë° ë¹„êµ<a class="header-link" href="#1_1" title="Permanent link">&para;</a></h3>
<p>RMSpropì„ ì²˜ìŒë¶€í„° êµ¬í˜„í•˜ê³  2D í† ì´ ë¬¸ì œì—ì„œ SGD ë° Adamê³¼ ê¶¤ì ì„ ë¹„êµí•˜ì„¸ìš”. ë¹„ë³¼ë¡ í‘œë©´(ì˜ˆ: Rosenbrock í•¨ìˆ˜)ì—ì„œ ìµœì í™” ê²½ë¡œë¥¼ ì‹œê°í™”í•˜ì„¸ìš”. ìˆ˜ë ´ ì†ë„(ì†ì‹¤ &lt; 0.01ì— ë„ë‹¬í•˜ëŠ” ë°˜ë³µ íšŸìˆ˜)ë¥¼ ì¸¡ì •í•˜ì„¸ìš”.</p>
<p><strong>ë³´ë„ˆìŠ¤</strong>: RMSprop êµ¬í˜„ì— ë„¤ìŠ¤í…Œë¡œí”„ ëª¨ë©˜í…€ì„ ì¶”ê°€í•˜ì„¸ìš”.</p>
<h3 id="2_1">ì—°ìŠµ ë¬¸ì œ 2: ìŠ¤ì¼€ì¤„ëŸ¬ ì ˆì œ ì—°êµ¬<a class="header-link" href="#2_1" title="Permanent link">&para;</a></h3>
<p>ë‹¤ì„¯ ê°€ì§€ ë‹¤ë¥¸ ìŠ¤ì¼€ì¤„ëŸ¬ë¡œ CIFAR-10ì—ì„œ ResNet-18ì„ í•™ìŠµí•˜ì„¸ìš”: (1) ìŠ¤ì¼€ì¤„ëŸ¬ ì—†ìŒ, (2) StepLR, (3) CosineAnnealingLR, (4) OneCycleLR, (5) Warmup + Cosine. ë™ì¼í•œ ì˜µí‹°ë§ˆì´ì €(SGD, momentum=0.9)ì™€ ì´ˆê¸° LRì„ ì‚¬ìš©í•˜ì„¸ìš”. í•™ìŠµ ê³¡ì„ ì„ ê·¸ë¦¬ê³  ìµœì¢… í…ŒìŠ¤íŠ¸ ì •í™•ë„ë¥¼ ë³´ê³ í•˜ì„¸ìš”. ì–´ë–¤ ìŠ¤ì¼€ì¤„ëŸ¬ê°€ ìµœê³  ì •í™•ë„ë¥¼ ë‹¬ì„±í–ˆë‚˜ìš”? ì–´ë–¤ ê²ƒì´ ê°€ì¥ ë¹¨ë¦¬ ìˆ˜ë ´í–ˆë‚˜ìš”?</p>
<p><strong>ë³´ë„ˆìŠ¤</strong>: LR íƒìƒ‰ê¸°ë¥¼ ì‚¬ìš©í•˜ì—¬ OneCycleLRì˜ ìµœì  ì´ˆê¸° LRì„ ìë™ìœ¼ë¡œ ê²°ì •í•˜ì„¸ìš”.</p>
<h3 id="3_1">ì—°ìŠµ ë¬¸ì œ 3: ëŒ€ê·œëª¨ ë°°ì¹˜ í•™ìŠµ ì‹œë®¬ë ˆì´ì…˜<a class="header-link" href="#3_1" title="Permanent link">&para;</a></h3>
<p>ê·¸ë˜ë””ì–¸íŠ¸ ì¶•ì ì„ ì‚¬ìš©í•˜ì—¬ ëŒ€ê·œëª¨ ë°°ì¹˜ í•™ìŠµì„ ì‹œë®¬ë ˆì´ì…˜í•˜ì„¸ìš”. ì‹¤ì œ ë°°ì¹˜ í¬ê¸° 64ì™€ accumulation_steps=16ì„ ì‚¬ìš©í•˜ì—¬ ìœ íš¨ ë°°ì¹˜ í¬ê¸° 1024ë¡œ ëª¨ë¸ì„ í•™ìŠµí•˜ì„¸ìš”. í•™ìŠµ ì‹œê°„, ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ë° ìµœì¢… ì •í™•ë„ë¥¼ batch_size=1024ë¥¼ ì§ì ‘ ì‚¬ìš©í•œ ê²ƒê³¼ ë¹„êµí•˜ì„¸ìš”(GPU ë©”ëª¨ë¦¬ê°€ í—ˆìš©í•˜ëŠ” ê²½ìš°). LARS ìŠ¤íƒ€ì¼ì˜ ë ˆì´ì–´ë³„ LR ìŠ¤ì¼€ì¼ë§ì„ êµ¬í˜„í•˜ê³  ëŒ€ê·œëª¨ ë°°ì¹˜ì— ë„ì›€ì´ ë˜ëŠ”ì§€ ë³´ì—¬ì£¼ì„¸ìš”.</p>
<p><strong>ë³´ë„ˆìŠ¤</strong>: í˜¼í•© ì •ë°€ë„ í•™ìŠµì„ ì¶”ê°€í•˜ê³  ì†ë„ í–¥ìƒì„ ì¸¡ì •í•˜ì„¸ìš”.</p>
<hr />
<h2 id="_3">ì°¸ê³  ìë£Œ<a class="header-link" href="#_3" title="Permanent link">&para;</a></h2>
<ol>
<li><strong>Ruder, S.</strong> (2016). <em>An overview of gradient descent optimization algorithms</em>. arXiv:1609.04747</li>
<li>
<p>SGD, momentum, Adagrad, RMSprop, Adamì˜ í¬ê´„ì ì¸ ì„œë² ì´</p>
</li>
<li>
<p><strong>Kingma, D. P., &amp; Ba, J.</strong> (2015). <em>Adam: A Method for Stochastic Optimization</em>. ICLR 2015</p>
</li>
<li>
<p>ì›ë³¸ Adam ë…¼ë¬¸</p>
</li>
<li>
<p><strong>Loshchilov, I., &amp; Hutter, F.</strong> (2019). <em>Decoupled Weight Decay Regularization</em>. ICLR 2019</p>
</li>
<li>
<p>AdamW: Adamì—ì„œ ê°€ì¤‘ì¹˜ ê°ì‡  ìˆ˜ì •</p>
</li>
<li>
<p><strong>You, Y., et al.</strong> (2020). <em>Large Batch Optimization for Deep Learning: Training BERT in 76 minutes</em>. ICLR 2020</p>
</li>
<li>
<p>LAMB ì˜µí‹°ë§ˆì´ì €</p>
</li>
<li>
<p><strong>Smith, L. N.</strong> (2018). <em>A disciplined approach to neural network hyper-parameters</em>. arXiv:1803.09820</p>
</li>
<li>
<p>ìˆœí™˜ í•™ìŠµë¥ , LR range test, super-convergence</p>
</li>
<li>
<p><strong>Chen, X., et al.</strong> (2023). <em>Symbolic Discovery of Optimization Algorithms</em>. arXiv:2302.06675</p>
</li>
<li>
<p>Lion ì˜µí‹°ë§ˆì´ì € (Google)</p>
</li>
<li>
<p><strong>Dettmers, T., et al.</strong> (2022). <em>8-bit Optimizers via Block-wise Quantization</em>. ICLR 2022</p>
</li>
<li>
<p>ë©”ëª¨ë¦¬ íš¨ìœ¨ì ì¸ í•™ìŠµì„ ìœ„í•œ 8-bit Adam</p>
</li>
<li>
<p><strong>PyTorch Documentation</strong>: https://pytorch.org/docs/stable/optim.html</p>
</li>
<li>
<p>ê³µì‹ ì˜µí‹°ë§ˆì´ì € ë° ìŠ¤ì¼€ì¤„ëŸ¬ ë¬¸ì„œ</p>
</li>
<li>
<p><strong>Goodfellow, I., et al.</strong> (2016). <em>Deep Learning</em>. MIT Press</p>
</li>
<li>
<p>Chapter 8: Optimization for Training Deep Models</p>
</li>
<li>
<p><strong>Zhang, M., et al.</strong> (2020). <em>Lookahead Optimizer: k steps forward, 1 step back</em>. NeurIPS 2019</p>
<ul>
<li>ëª¨ë“  ì˜µí‹°ë§ˆì´ì €ë¥¼ ìœ„í•œ Lookahead ë˜í¼</li>
</ul>
</li>
</ol>
    </div>

    
<div class="lesson-toolbar lesson-toolbar--bottom">
    <div class="toolbar-nav toolbar-nav--prev">
        
        <a href="/study/ko/Deep_Learning/24_Loss_Functions.html" class="nav-prev">
            <span class="nav-label">Previous</span>
            <span class="nav-title">24. ì†ì‹¤ í•¨ìˆ˜(Loss Functions)</span>
        </a>
        
    </div>
    <div class="toolbar-actions">
        <button class="btn btn-copy-link" data-action="copy-link" title="Copy link">
            <span class="icon">ğŸ”—</span>
            <span class="text">Copy link</span>
        </button>
        <a href="/study/ko/Deep_Learning/" class="btn btn-topic-link" title="Back to topic list">
            <span class="icon">ğŸ“‹</span>
            <span class="text">Topic list</span>
        </a>
    </div>
    <div class="toolbar-nav toolbar-nav--next">
        
        <a href="/study/ko/Deep_Learning/26_Normalization_Layers.html" class="nav-next">
            <span class="nav-label">Next</span>
            <span class="nav-title">26. ì •ê·œí™” ë ˆì´ì–´(Normalization Layers)</span>
        </a>
        
    </div>
</div>


    <div class="toolbar-keyboard-hint">
        <kbd>&larr;</kbd> <kbd>&rarr;</kbd> to navigate between lessons
    </div>
</article>

<!-- Scroll to top floating button -->
<button class="scroll-to-top" id="scroll-to-top" title="Scroll to top" aria-label="Scroll to top">â†‘</button>

            </div>
        </main>
    </div>

    <script src="/study/static/js/app.js"></script>
    <script>
        function switchLanguage(newLang) {
            const path = window.location.pathname;
            const base = '/study';
            const currentLang = 'ko';
            const newPath = path.replace(base + '/' + currentLang + '/', base + '/' + newLang + '/');
            window.location.href = newPath + window.location.search;
        }
    </script>
    
<script>
    document.addEventListener('DOMContentLoaded', function() {
        // Copy link - independent feedback per button
        document.querySelectorAll('[data-action="copy-link"]').forEach(function(btn) {
            btn.addEventListener('click', function() {
                navigator.clipboard.writeText(window.location.href);
                var textEl = this.querySelector('.text');
                var originalText = textEl.textContent;
                textEl.textContent = 'Copied!';
                setTimeout(function() { textEl.textContent = originalText; }, 2000);
            });
        });

        // Add copy buttons to code blocks
        document.querySelectorAll('.lesson-content pre').forEach(function(pre) {
            var copyBtn = document.createElement('button');
            copyBtn.className = 'code-copy-btn';
            copyBtn.textContent = 'Copy';
            copyBtn.addEventListener('click', function() {
                var code = pre.querySelector('code');
                var text = code ? code.textContent : pre.textContent;
                navigator.clipboard.writeText(text);
                copyBtn.textContent = 'Copied!';
                setTimeout(function() { copyBtn.textContent = 'Copy'; }, 2000);
            });
            pre.style.position = 'relative';
            pre.appendChild(copyBtn);
        });

        // Scroll to top button
        var scrollBtn = document.getElementById('scroll-to-top');
        window.addEventListener('scroll', function() {
            scrollBtn.classList.toggle('visible', window.scrollY > 300);
        });
        scrollBtn.addEventListener('click', function() {
            window.scrollTo({ top: 0, behavior: 'smooth' });
        });

        // Keyboard shortcuts: left/right arrows for lesson navigation
        document.addEventListener('keydown', function(e) {
            var tag = document.activeElement.tagName;
            if (tag === 'INPUT' || tag === 'TEXTAREA' || tag === 'SELECT') return;

            if (e.key === 'ArrowLeft') {
                var prevLink = document.querySelector('.nav-prev');
                if (prevLink) prevLink.click();
            } else if (e.key === 'ArrowRight') {
                var nextLink = document.querySelector('.nav-next');
                if (nextLink) nextLink.click();
            }
        });
    });
</script>

</body>
</html>
{% endraw %}