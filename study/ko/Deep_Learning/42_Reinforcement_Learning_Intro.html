{% raw %}
<!DOCTYPE html>
<html lang="ko" data-theme="light">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>42. ê°•í™”í•™ìŠµ ì…ë¬¸ (Reinforcement Learning Introduction) - Study Materials</title>
    <link rel="stylesheet" href="/study/static/css/style.css">
    <link rel="stylesheet" href="/study/static/css/highlight.css">
    <!-- KaTeX for LaTeX math rendering -->
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.11/dist/katex.min.css">
    <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.11/dist/katex.min.js"></script>
    <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.11/dist/contrib/auto-render.min.js"
        onload="renderMathInElement(document.body, {
            delimiters: [
                {left: '$$', right: '$$', display: true},
                {left: '$', right: '$', display: false}
            ],
            throwOnError: false
        });"></script>
    
</head>
<body>
    <div class="app-container">
        <!-- Sidebar -->
        <aside class="sidebar" id="sidebar">
            <div class="sidebar-header">
                <a href="/study/ko/" class="logo">Study Materials</a>
                <button class="sidebar-toggle" id="sidebar-toggle" aria-label="Toggle sidebar">
                    <span></span>
                </button>
            </div>

            <nav class="sidebar-nav">
                <a href="/study/ko/" class="nav-item ">
                    <span class="nav-icon">ğŸ </span>
                    <span class="nav-text">Home</span>
                </a>
                <a href="/study/examples/" class="nav-item ">
                    <span class="nav-icon">ğŸ’»</span>
                    <span class="nav-text">Examples</span>
                </a>
            </nav>

            <div class="sidebar-search">
                <form action="/study/ko/search.html" method="get" id="search-form">
                    <input type="search" name="q" placeholder="Search..." id="search-sidebar-input">
                </form>
            </div>

            <!-- Language Selector -->
            <div class="sidebar-lang">
                <select id="lang-select" class="lang-selector" onchange="switchLanguage(this.value)">
                    
                    <option value="en" >
                        English
                    </option>
                    
                    <option value="ko" selected>
                        í•œêµ­ì–´
                    </option>
                    
                </select>
            </div>

            <div class="sidebar-footer">
                <button class="theme-toggle" id="theme-toggle" aria-label="Toggle theme">
                    <span class="theme-icon light">â˜€ï¸</span>
                    <span class="theme-icon dark">ğŸŒ™</span>
                </button>
            </div>
        </aside>

        <!-- Main Content -->
        <main class="main-content">
            <header class="main-header">
                <button class="menu-toggle" id="menu-toggle" aria-label="Open menu">
                    <span></span>
                </button>
                
<nav class="breadcrumb">
    <a href="/study/ko/">Topics</a>
    <span class="separator">/</span>
    <a href="/study/ko/Deep_Learning/">Deep Learning</a>
    <span class="separator">/</span>
    <span class="current">42. ê°•í™”í•™ìŠµ ì…ë¬¸ (Reinforcement Learning Introduction)</span>
</nav>

            </header>

            <div class="content">
                
<article class="lesson-article">
    <header class="lesson-header">
        <h1>42. ê°•í™”í•™ìŠµ ì…ë¬¸ (Reinforcement Learning Introduction)</h1>
    </header>

    
<div class="lesson-toolbar lesson-toolbar--top">
    <div class="toolbar-nav toolbar-nav--prev">
        
        <a href="/study/ko/Deep_Learning/41_Model_Saving_Deployment.html" class="nav-prev">
            <span class="nav-label">Previous</span>
            <span class="nav-title">41. ëª¨ë¸ ì €ì¥ ë° ë°°í¬</span>
        </a>
        
    </div>
    <div class="toolbar-actions">
        <button class="btn btn-copy-link" data-action="copy-link" title="Copy link">
            <span class="icon">ğŸ”—</span>
            <span class="text">Copy link</span>
        </button>
        <a href="/study/ko/Deep_Learning/" class="btn btn-topic-link" title="Back to topic list">
            <span class="icon">ğŸ“‹</span>
            <span class="text">Topic list</span>
        </a>
    </div>
    <div class="toolbar-nav toolbar-nav--next">
        
    </div>
</div>


    
    <nav class="toc" id="toc">
        <h2>Table of Contents</h2>
        <div class="toc">
<ul>
<li><a href="#_1">í•™ìŠµ ëª©í‘œ</a></li>
<li><a href="#1">1. ê°•í™”í•™ìŠµ ê°œìš”</a><ul>
<li><a href="#_2">ì •ì˜ì™€ íŠ¹ì§•</a></li>
<li><a href="#vs">ì§€ë„í•™ìŠµ vs ê°•í™”í•™ìŠµ</a></li>
<li><a href="#_3">ê°•í™”í•™ìŠµ ì‘ìš© ë¶„ì•¼</a></li>
</ul>
</li>
<li><a href="#2-mdp-markov-decision-process">2. MDP (Markov Decision Process)</a><ul>
<li><a href="#_4">êµ¬ì„± ìš”ì†Œ</a></li>
<li><a href="#markov-property">Markov Property</a></li>
<li><a href="#_5">ìƒí˜¸ì‘ìš© ë£¨í”„</a></li>
</ul>
</li>
<li><a href="#3-value-functions">3. Value Functions</a><ul>
<li><a href="#state-value-function-v">State Value Function (V)</a></li>
<li><a href="#action-value-function-q">Action Value Function (Q)</a></li>
<li><a href="#bellman-equation">Bellman Equation</a></li>
</ul>
</li>
<li><a href="#4-q-learning">4. Q-Learning</a><ul>
<li><a href="#_6">ì•Œê³ ë¦¬ì¦˜ ê°œìš”</a></li>
<li><a href="#q-learning">Q-Learning ì—…ë°ì´íŠ¸</a></li>
<li><a href="#pytorch">PyTorch êµ¬í˜„</a></li>
</ul>
</li>
<li><a href="#5-deep-q-network-dqn">5. Deep Q-Network (DQN)</a><ul>
<li><a href="#_7">í•µì‹¬ ì•„ì´ë””ì–´</a></li>
<li><a href="#dqn">DQN ì•„í‚¤í…ì²˜</a></li>
<li><a href="#experience-replay">Experience Replay</a></li>
<li><a href="#dqn-pytorch">DQN PyTorch êµ¬í˜„</a></li>
<li><a href="#dqn_1">DQN í•™ìŠµ ë£¨í”„</a></li>
</ul>
</li>
<li><a href="#6-policy-gradient">6. Policy Gradient</a><ul>
<li><a href="#_8">ì•„ì´ë””ì–´</a></li>
<li><a href="#policy-gradient-theorem">Policy Gradient Theorem</a></li>
<li><a href="#reinforce">REINFORCE ì•Œê³ ë¦¬ì¦˜</a></li>
<li><a href="#reinforce_1">REINFORCE í•™ìŠµ</a></li>
</ul>
</li>
<li><a href="#7-actor-critic">7. Actor-Critic</a><ul>
<li><a href="#_9">ì•„ì´ë””ì–´</a></li>
<li><a href="#advantage-function">Advantage Function</a></li>
<li><a href="#actor-critic">Actor-Critic êµ¬í˜„</a></li>
</ul>
</li>
<li><a href="#8">8. í™˜ê²½ê³¼ ì‹¤í—˜</a><ul>
<li><a href="#openai-gym">OpenAI Gym ì‚¬ìš©</a></li>
<li><a href="#cartpole">ì‹¤í—˜ ì˜ˆì œ: CartPole</a></li>
</ul>
</li>
<li><a href="#9">9. ì•Œê³ ë¦¬ì¦˜ ë¹„êµ</a><ul>
<li><a href="#_10">ì£¼ìš” ì•Œê³ ë¦¬ì¦˜ íŠ¹ì„±</a></li>
<li><a href="#_11">ì„ íƒ ê°€ì´ë“œ</a></li>
</ul>
</li>
<li><a href="#10">10. ì‹¬í™” ì£¼ì œ ê°œìš”</a><ul>
<li><a href="#double-dqn">Double DQN</a></li>
<li><a href="#dueling-dqn">Dueling DQN</a></li>
<li><a href="#prioritized-experience-replay">Prioritized Experience Replay</a></li>
</ul>
</li>
<li><a href="#_12">ì •ë¦¬</a><ul>
<li><a href="#_13">í•µì‹¬ ê°œë…</a></li>
<li><a href="#_14">ì‹¤ì „ íŒ</a></li>
</ul>
</li>
<li><a href="#_15">ì°¸ê³  ìë£Œ</a></li>
</ul>
</div>

    </nav>
    

    <div class="lesson-content markdown-body">
        <p><a href="./41_Model_Saving_Deployment.md">ì´ì „: ëª¨ë¸ ì €ì¥ ë° ë°°í¬</a></p>
<hr />
<h1 id="42-reinforcement-learning-introduction">42. ê°•í™”í•™ìŠµ ì…ë¬¸ (Reinforcement Learning Introduction)<a class="header-link" href="#42-reinforcement-learning-introduction" title="Permanent link">&para;</a></h1>
<h2 id="_1">í•™ìŠµ ëª©í‘œ<a class="header-link" href="#_1" title="Permanent link">&para;</a></h2>
<ul>
<li>ê°•í™”í•™ìŠµì˜ ê¸°ë³¸ ê°œë…ê³¼ ìš©ì–´ ì´í•´</li>
<li>MDP (Markov Decision Process) í”„ë ˆì„ì›Œí¬</li>
<li>Q-Learningê³¼ Value-based ë°©ë²•</li>
<li>Policy Gradient ê°œìš”</li>
<li>Deep RL ê¸°ì´ˆ (DQN)</li>
<li>PyTorch êµ¬í˜„ ë° ì‹¤ìŠµ</li>
</ul>
<hr />
<h2 id="1">1. ê°•í™”í•™ìŠµ ê°œìš”<a class="header-link" href="#1" title="Permanent link">&para;</a></h2>
<h3 id="_2">ì •ì˜ì™€ íŠ¹ì§•<a class="header-link" href="#_2" title="Permanent link">&para;</a></h3>
<div class="highlight"><pre><span></span><code><span class="err">ê°•í™”í•™ìŠµ</span><span class="o">:</span><span class="w"> </span><span class="err">ì—ì´ì „íŠ¸ê°€</span><span class="w"> </span><span class="err">í™˜ê²½ê³¼</span><span class="w"> </span><span class="err">ìƒí˜¸ì‘ìš©í•˜ë©°</span><span class="w"> </span><span class="err">ë³´ìƒì„</span><span class="w"> </span><span class="err">ìµœëŒ€í™”í•˜ëŠ”</span><span class="w"> </span><span class="err">í–‰ë™ì„</span><span class="w"> </span><span class="err">í•™ìŠµ</span>

<span class="err">íŠ¹ì§•</span><span class="o">:</span>
<span class="mi">1</span><span class="o">.</span><span class="w"> </span><span class="err">ì‹œí–‰ì°©ì˜¤</span><span class="w"> </span><span class="err">í•™ìŠµ</span><span class="w"> </span><span class="o">(</span><span class="n">Trial</span><span class="w"> </span><span class="n">and</span><span class="w"> </span><span class="n">Error</span><span class="o">)</span>
<span class="mi">2</span><span class="o">.</span><span class="w"> </span><span class="err">ì§€ì—°ëœ</span><span class="w"> </span><span class="err">ë³´ìƒ</span><span class="w"> </span><span class="o">(</span><span class="n">Delayed</span><span class="w"> </span><span class="n">Reward</span><span class="o">)</span>
<span class="mi">3</span><span class="o">.</span><span class="w"> </span><span class="err">íƒìƒ‰</span><span class="o">-</span><span class="err">í™œìš©</span><span class="w"> </span><span class="err">ê· í˜•</span><span class="w"> </span><span class="o">(</span><span class="n">Exploration</span><span class="o">-</span><span class="n">Exploitation</span><span class="o">)</span>
<span class="mi">4</span><span class="o">.</span><span class="w"> </span><span class="err">ìˆœì°¨ì </span><span class="w"> </span><span class="err">ì˜ì‚¬ê²°ì •</span><span class="w"> </span><span class="o">(</span><span class="n">Sequential</span><span class="w"> </span><span class="n">Decision</span><span class="w"> </span><span class="n">Making</span><span class="o">)</span>
</code></pre></div>

<h3 id="vs">ì§€ë„í•™ìŠµ vs ê°•í™”í•™ìŠµ<a class="header-link" href="#vs" title="Permanent link">&para;</a></h3>
<div class="highlight"><pre><span></span><code>â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚           Supervised Learning vs Reinforcement Learning      â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                              â”‚
â”‚  Supervised Learning                                         â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”    ì •ë‹µ     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”                        â”‚
â”‚  â”‚ ì…ë ¥ x  â”‚ â”€â”€â”€â”€â”€â”€â”€â”€â”€â†’ â”‚ ë ˆì´ë¸” y â”‚                        â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜             â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                        â”‚
â”‚  ì¦‰ê°ì ì¸ í”¼ë“œë°±, ì •ë‹µ ì œê³µ                                  â”‚
â”‚                                                              â”‚
â”‚  Reinforcement Learning                                      â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”  í–‰ë™   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”  ë³´ìƒ   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”       â”‚
â”‚  â”‚ ìƒíƒœ s  â”‚ â”€â”€â”€â”€â”€â”€â†’ â”‚ í–‰ë™ a  â”‚ â”€â”€â”€â”€â”€â”€â†’ â”‚ ë³´ìƒ r  â”‚       â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜       â”‚
â”‚       â†‘                    â”‚                   â”‚             â”‚
â”‚       â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜             â”‚
â”‚  ì§€ì—°ëœ í”¼ë“œë°±, íƒìƒ‰ í•„ìš”                                    â”‚
â”‚                                                              â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
</code></pre></div>

<h3 id="_3">ê°•í™”í•™ìŠµ ì‘ìš© ë¶„ì•¼<a class="header-link" href="#_3" title="Permanent link">&para;</a></h3>
<div class="highlight"><pre><span></span><code><span class="err">ê²Œì„</span><span class="o">:</span><span class="w"> </span><span class="n">AlphaGo</span><span class="o">,</span><span class="w"> </span><span class="n">Atari</span><span class="o">,</span><span class="w"> </span><span class="n">StarCraft</span><span class="w"> </span><span class="n">II</span>
<span class="err">ë¡œë´‡</span><span class="o">:</span><span class="w"> </span><span class="err">ë¡œë´‡</span><span class="w"> </span><span class="err">ì œì–´</span><span class="o">,</span><span class="w"> </span><span class="err">ììœ¨</span><span class="w"> </span><span class="err">ì£¼í–‰</span>
<span class="err">ê¸ˆìœµ</span><span class="o">:</span><span class="w"> </span><span class="err">í¬íŠ¸í´ë¦¬ì˜¤</span><span class="w"> </span><span class="err">ìµœì í™”</span><span class="o">,</span><span class="w"> </span><span class="err">ì•Œê³ ë¦¬ì¦˜</span><span class="w"> </span><span class="err">íŠ¸ë ˆì´ë”©</span>
<span class="err">ì¶”ì²œ</span><span class="o">:</span><span class="w"> </span><span class="err">ê°œì¸í™”</span><span class="w"> </span><span class="err">ì¶”ì²œ</span><span class="o">,</span><span class="w"> </span><span class="err">ëŒ€í™”</span><span class="w"> </span><span class="err">ì‹œìŠ¤í…œ</span>
<span class="err">ìì›</span><span class="w"> </span><span class="err">ê´€ë¦¬</span><span class="o">:</span><span class="w"> </span><span class="err">ë°ì´í„°ì„¼í„°</span><span class="w"> </span><span class="err">ì¿¨ë§</span><span class="o">,</span><span class="w"> </span><span class="err">ë„¤íŠ¸ì›Œí¬</span><span class="w"> </span><span class="err">ìµœì í™”</span>
</code></pre></div>

<hr />
<h2 id="2-mdp-markov-decision-process">2. MDP (Markov Decision Process)<a class="header-link" href="#2-mdp-markov-decision-process" title="Permanent link">&para;</a></h2>
<h3 id="_4">êµ¬ì„± ìš”ì†Œ<a class="header-link" href="#_4" title="Permanent link">&para;</a></h3>
<div class="highlight"><pre><span></span><code>MDP = (S, A, P, R, Î³)

S: State (ìƒíƒœ ì§‘í•©)
   - ì—ì´ì „íŠ¸ê°€ ê´€ì¸¡í•˜ëŠ” í™˜ê²½ì˜ ìƒíƒœ
   - ì˜ˆ: ê²Œì„ í™”ë©´, ë¡œë´‡ì˜ ìœ„ì¹˜/ì†ë„

A: Action (í–‰ë™ ì§‘í•©)
   - ì—ì´ì „íŠ¸ê°€ ì·¨í•  ìˆ˜ ìˆëŠ” í–‰ë™
   - ì˜ˆ: ìƒí•˜ì¢Œìš° ì´ë™, ëª¨í„° í† í¬

P: Transition Probability (ì „ì´ í™•ë¥ )
   - P(s&#39;|s, a): ìƒíƒœ sì—ì„œ í–‰ë™ aë¥¼ ì·¨í•  ë•Œ s&#39;ë¡œ ì „ì´í•  í™•ë¥ 

R: Reward (ë³´ìƒ í•¨ìˆ˜)
   - R(s, a, s&#39;): ìƒíƒœ ì „ì´ ì‹œ ë°›ëŠ” ë³´ìƒ

Î³: Discount Factor (í• ì¸ìœ¨)
   - ë¯¸ë˜ ë³´ìƒì˜ í˜„ì¬ ê°€ì¹˜ (0 &lt; Î³ â‰¤ 1)
</code></pre></div>

<h3 id="markov-property">Markov Property<a class="header-link" href="#markov-property" title="Permanent link">&para;</a></h3>
<div class="highlight"><pre><span></span><code>ë¯¸ë˜ëŠ” í˜„ì¬ ìƒíƒœì—ë§Œ ì˜ì¡´ (ê³¼ê±° ë¬´ê´€):

P(s_{t+1} | s_t, a_t) = P(s_{t+1} | s_0, a_0, ..., s_t, a_t)

ì˜ë¯¸: í˜„ì¬ ìƒíƒœê°€ ì¶©ë¶„í•œ ì •ë³´ë¥¼ ë‹´ê³  ìˆìŒ
</code></pre></div>

<h3 id="_5">ìƒí˜¸ì‘ìš© ë£¨í”„<a class="header-link" href="#_5" title="Permanent link">&para;</a></h3>
<div class="highlight"><pre><span></span><code><span class="c1"># RL ê¸°ë³¸ ë£¨í”„</span>
<span class="k">def</span><span class="w"> </span><span class="nf">rl_loop</span><span class="p">(</span><span class="n">env</span><span class="p">,</span> <span class="n">agent</span><span class="p">,</span> <span class="n">episodes</span><span class="o">=</span><span class="mi">1000</span><span class="p">):</span>
    <span class="k">for</span> <span class="n">episode</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">episodes</span><span class="p">):</span>
        <span class="n">state</span> <span class="o">=</span> <span class="n">env</span><span class="o">.</span><span class="n">reset</span><span class="p">()</span>
        <span class="n">done</span> <span class="o">=</span> <span class="kc">False</span>
        <span class="n">total_reward</span> <span class="o">=</span> <span class="mi">0</span>

        <span class="k">while</span> <span class="ow">not</span> <span class="n">done</span><span class="p">:</span>
            <span class="c1"># 1. ì—ì´ì „íŠ¸ê°€ í–‰ë™ ì„ íƒ</span>
            <span class="n">action</span> <span class="o">=</span> <span class="n">agent</span><span class="o">.</span><span class="n">select_action</span><span class="p">(</span><span class="n">state</span><span class="p">)</span>

            <span class="c1"># 2. í™˜ê²½ì´ ë‹¤ìŒ ìƒíƒœì™€ ë³´ìƒ ë°˜í™˜</span>
            <span class="n">next_state</span><span class="p">,</span> <span class="n">reward</span><span class="p">,</span> <span class="n">done</span><span class="p">,</span> <span class="n">info</span> <span class="o">=</span> <span class="n">env</span><span class="o">.</span><span class="n">step</span><span class="p">(</span><span class="n">action</span><span class="p">)</span>

            <span class="c1"># 3. ì—ì´ì „íŠ¸ê°€ ê²½í—˜ì—ì„œ í•™ìŠµ</span>
            <span class="n">agent</span><span class="o">.</span><span class="n">learn</span><span class="p">(</span><span class="n">state</span><span class="p">,</span> <span class="n">action</span><span class="p">,</span> <span class="n">reward</span><span class="p">,</span> <span class="n">next_state</span><span class="p">,</span> <span class="n">done</span><span class="p">)</span>

            <span class="c1"># 4. ìƒíƒœ ì—…ë°ì´íŠ¸</span>
            <span class="n">state</span> <span class="o">=</span> <span class="n">next_state</span>
            <span class="n">total_reward</span> <span class="o">+=</span> <span class="n">reward</span>

        <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Episode </span><span class="si">{</span><span class="n">episode</span><span class="si">}</span><span class="s2">: Total Reward = </span><span class="si">{</span><span class="n">total_reward</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
</code></pre></div>

<hr />
<h2 id="3-value-functions">3. Value Functions<a class="header-link" href="#3-value-functions" title="Permanent link">&para;</a></h2>
<h3 id="state-value-function-v">State Value Function (V)<a class="header-link" href="#state-value-function-v" title="Permanent link">&para;</a></h3>
<div class="highlight"><pre><span></span><code>V^Ï€(s) = E[G_t | S_t = s, Ï€]

G_t = R_{t+1} + Î³R_{t+2} + Î³Â²R_{t+3} + ...
    = Î£_{k=0}^âˆ Î³^k R_{t+k+1}

ì˜ë¯¸: ìƒíƒœ sì—ì„œ ì •ì±… Ï€ë¥¼ ë”°ë¥¼ ë•Œ ê¸°ëŒ€ë˜ëŠ” ëˆ„ì  ë³´ìƒ
</code></pre></div>

<h3 id="action-value-function-q">Action Value Function (Q)<a class="header-link" href="#action-value-function-q" title="Permanent link">&para;</a></h3>
<div class="highlight"><pre><span></span><code>Q^Ï€(s, a) = E[G_t | S_t = s, A_t = a, Ï€]

ì˜ë¯¸: ìƒíƒœ sì—ì„œ í–‰ë™ aë¥¼ ì·¨í•˜ê³ , ì´í›„ Ï€ë¥¼ ë”°ë¥¼ ë•Œì˜ ê¸°ëŒ€ ë³´ìƒ
</code></pre></div>

<h3 id="bellman-equation">Bellman Equation<a class="header-link" href="#bellman-equation" title="Permanent link">&para;</a></h3>
<div class="highlight"><pre><span></span><code><span class="c1"># Bellman ë°©ì •ì‹ (í•µì‹¬!)</span>

<span class="c1"># Value Function</span>
<span class="n">V</span><span class="p">(</span><span class="n">s</span><span class="p">)</span> <span class="o">=</span> <span class="n">max_a</span> <span class="p">[</span> <span class="n">R</span><span class="p">(</span><span class="n">s</span><span class="p">,</span> <span class="n">a</span><span class="p">)</span> <span class="o">+</span> <span class="n">Î³</span> <span class="o">*</span> <span class="n">Î£</span> <span class="n">P</span><span class="p">(</span><span class="n">s</span><span class="s1">&#39;|s,a) * V(s&#39;</span><span class="p">)</span> <span class="p">]</span>

<span class="c1"># Q Function</span>
<span class="n">Q</span><span class="p">(</span><span class="n">s</span><span class="p">,</span> <span class="n">a</span><span class="p">)</span> <span class="o">=</span> <span class="n">R</span><span class="p">(</span><span class="n">s</span><span class="p">,</span> <span class="n">a</span><span class="p">)</span> <span class="o">+</span> <span class="n">Î³</span> <span class="o">*</span> <span class="n">Î£</span> <span class="n">P</span><span class="p">(</span><span class="n">s</span><span class="s1">&#39;|s,a) * max_a&#39;</span> <span class="n">Q</span><span class="p">(</span><span class="n">s</span><span class="s1">&#39;, a&#39;</span><span class="p">)</span>

<span class="c1"># ì˜ë¯¸: í˜„ì¬ ê°€ì¹˜ = ì¦‰ê° ë³´ìƒ + í• ì¸ëœ ë¯¸ë˜ ê°€ì¹˜</span>
</code></pre></div>

<hr />
<h2 id="4-q-learning">4. Q-Learning<a class="header-link" href="#4-q-learning" title="Permanent link">&para;</a></h2>
<h3 id="_6">ì•Œê³ ë¦¬ì¦˜ ê°œìš”<a class="header-link" href="#_6" title="Permanent link">&para;</a></h3>
<div class="highlight"><pre><span></span><code>Q-Learning: Model-free, Off-policy ì•Œê³ ë¦¬ì¦˜

íŠ¹ì§•:
1. í™˜ê²½ ëª¨ë¸(P) í•„ìš” ì—†ìŒ
2. ë‹¤ë¥¸ ì •ì±…(Îµ-greedy)ìœ¼ë¡œ ìˆ˜ì§‘í•œ ë°ì´í„°ë¡œ ìµœì  ì •ì±… í•™ìŠµ
3. í…Œì´ë¸” í˜•íƒœë¡œ Q ê°’ ì €ì¥
</code></pre></div>

<h3 id="q-learning">Q-Learning ì—…ë°ì´íŠ¸<a class="header-link" href="#q-learning" title="Permanent link">&para;</a></h3>
<div class="highlight"><pre><span></span><code><span class="c1"># Q-Learning ì—…ë°ì´íŠ¸ ê·œì¹™</span>

<span class="n">Q</span><span class="p">(</span><span class="n">s</span><span class="p">,</span> <span class="n">a</span><span class="p">)</span> <span class="err">â†</span> <span class="n">Q</span><span class="p">(</span><span class="n">s</span><span class="p">,</span> <span class="n">a</span><span class="p">)</span> <span class="o">+</span> <span class="n">Î±</span> <span class="o">*</span> <span class="p">[</span><span class="n">r</span> <span class="o">+</span> <span class="n">Î³</span> <span class="o">*</span> <span class="n">max_a</span><span class="s1">&#39; Q(s&#39;</span><span class="p">,</span> <span class="n">a</span><span class="s1">&#39;) - Q(s, a)]</span>

<span class="c1"># ë¶„í•´:</span>
<span class="c1"># TD Target: r + Î³ * max_a&#39; Q(s&#39;, a&#39;)  (ëª©í‘œ)</span>
<span class="c1"># TD Error: TD Target - Q(s, a)        (ì˜¤ì°¨)</span>
<span class="c1"># Î±: Learning Rate                     (í•™ìŠµë¥ )</span>
</code></pre></div>

<h3 id="pytorch">PyTorch êµ¬í˜„<a class="header-link" href="#pytorch" title="Permanent link">&para;</a></h3>
<div class="highlight"><pre><span></span><code><span class="kn">import</span><span class="w"> </span><span class="nn">numpy</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">np</span>

<span class="k">class</span><span class="w"> </span><span class="nc">QLearningAgent</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Q-Learning Agent (Tabular) (â­â­â­)&quot;&quot;&quot;</span>
    <span class="k">def</span><span class="w"> </span><span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">state_size</span><span class="p">,</span> <span class="n">action_size</span><span class="p">,</span> <span class="n">lr</span><span class="o">=</span><span class="mf">0.1</span><span class="p">,</span> <span class="n">gamma</span><span class="o">=</span><span class="mf">0.99</span><span class="p">,</span> <span class="n">epsilon</span><span class="o">=</span><span class="mf">1.0</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">state_size</span> <span class="o">=</span> <span class="n">state_size</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">action_size</span> <span class="o">=</span> <span class="n">action_size</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">lr</span> <span class="o">=</span> <span class="n">lr</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">gamma</span> <span class="o">=</span> <span class="n">gamma</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">epsilon</span> <span class="o">=</span> <span class="n">epsilon</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">epsilon_min</span> <span class="o">=</span> <span class="mf">0.01</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">epsilon_decay</span> <span class="o">=</span> <span class="mf">0.995</span>

        <span class="c1"># Q-Table ì´ˆê¸°í™”</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">q_table</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">((</span><span class="n">state_size</span><span class="p">,</span> <span class="n">action_size</span><span class="p">))</span>

    <span class="k">def</span><span class="w"> </span><span class="nf">select_action</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">state</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Îµ-greedy í–‰ë™ ì„ íƒ&quot;&quot;&quot;</span>
        <span class="k">if</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">random</span><span class="p">()</span> <span class="o">&lt;</span> <span class="bp">self</span><span class="o">.</span><span class="n">epsilon</span><span class="p">:</span>
            <span class="k">return</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">randint</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">action_size</span><span class="p">)</span>  <span class="c1"># íƒìƒ‰</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="k">return</span> <span class="n">np</span><span class="o">.</span><span class="n">argmax</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">q_table</span><span class="p">[</span><span class="n">state</span><span class="p">])</span>       <span class="c1"># í™œìš©</span>

    <span class="k">def</span><span class="w"> </span><span class="nf">learn</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">state</span><span class="p">,</span> <span class="n">action</span><span class="p">,</span> <span class="n">reward</span><span class="p">,</span> <span class="n">next_state</span><span class="p">,</span> <span class="n">done</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Q-Table ì—…ë°ì´íŠ¸&quot;&quot;&quot;</span>
        <span class="c1"># TD Target</span>
        <span class="k">if</span> <span class="n">done</span><span class="p">:</span>
            <span class="n">td_target</span> <span class="o">=</span> <span class="n">reward</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">td_target</span> <span class="o">=</span> <span class="n">reward</span> <span class="o">+</span> <span class="bp">self</span><span class="o">.</span><span class="n">gamma</span> <span class="o">*</span> <span class="n">np</span><span class="o">.</span><span class="n">max</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">q_table</span><span class="p">[</span><span class="n">next_state</span><span class="p">])</span>

        <span class="c1"># TD Error</span>
        <span class="n">td_error</span> <span class="o">=</span> <span class="n">td_target</span> <span class="o">-</span> <span class="bp">self</span><span class="o">.</span><span class="n">q_table</span><span class="p">[</span><span class="n">state</span><span class="p">,</span> <span class="n">action</span><span class="p">]</span>

        <span class="c1"># Update</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">q_table</span><span class="p">[</span><span class="n">state</span><span class="p">,</span> <span class="n">action</span><span class="p">]</span> <span class="o">+=</span> <span class="bp">self</span><span class="o">.</span><span class="n">lr</span> <span class="o">*</span> <span class="n">td_error</span>

        <span class="c1"># Epsilon Decay</span>
        <span class="k">if</span> <span class="n">done</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">epsilon</span> <span class="o">=</span> <span class="nb">max</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">epsilon_min</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">epsilon</span> <span class="o">*</span> <span class="bp">self</span><span class="o">.</span><span class="n">epsilon_decay</span><span class="p">)</span>

    <span class="k">def</span><span class="w"> </span><span class="nf">get_policy</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;í•™ìŠµëœ ì •ì±… ë°˜í™˜&quot;&quot;&quot;</span>
        <span class="k">return</span> <span class="n">np</span><span class="o">.</span><span class="n">argmax</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">q_table</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
</code></pre></div>

<hr />
<h2 id="5-deep-q-network-dqn">5. Deep Q-Network (DQN)<a class="header-link" href="#5-deep-q-network-dqn" title="Permanent link">&para;</a></h2>
<h3 id="_7">í•µì‹¬ ì•„ì´ë””ì–´<a class="header-link" href="#_7" title="Permanent link">&para;</a></h3>
<div class="highlight"><pre><span></span><code><span class="err">ë¬¸ì œ</span><span class="o">:</span><span class="w"> </span><span class="err">í°</span><span class="w"> </span><span class="err">ìƒíƒœ</span><span class="w"> </span><span class="err">ê³µê°„ì—ì„œ</span><span class="w"> </span><span class="n">Q</span><span class="o">-</span><span class="n">Table</span><span class="w"> </span><span class="err">ë¶ˆê°€ëŠ¥</span>
<span class="err">í•´ê²°</span><span class="o">:</span><span class="w"> </span><span class="err">ì‹ ê²½ë§ìœ¼ë¡œ</span><span class="w"> </span><span class="n">Q</span><span class="o">(</span><span class="n">s</span><span class="o">,</span><span class="w"> </span><span class="n">a</span><span class="o">)</span><span class="w"> </span><span class="err">ê·¼ì‚¬</span>

<span class="n">Q</span><span class="o">(</span><span class="n">s</span><span class="o">,</span><span class="w"> </span><span class="n">a</span><span class="o">;</span><span class="w"> </span><span class="err">Î¸</span><span class="o">)</span><span class="w"> </span><span class="err">â‰ˆ</span><span class="w"> </span><span class="n">Q</span><span class="o">*(</span><span class="n">s</span><span class="o">,</span><span class="w"> </span><span class="n">a</span><span class="o">)</span>

<span class="err">í•µì‹¬</span><span class="w"> </span><span class="err">ê¸°ë²•</span><span class="o">:</span>
<span class="mi">1</span><span class="o">.</span><span class="w"> </span><span class="n">Experience</span><span class="w"> </span><span class="n">Replay</span><span class="o">:</span><span class="w"> </span><span class="err">ê²½í—˜</span><span class="w"> </span><span class="err">ì¬ì‚¬ìš©ìœ¼ë¡œ</span><span class="w"> </span><span class="err">íš¨ìœ¨ì„±</span><span class="w"> </span><span class="err">í–¥ìƒ</span>
<span class="mi">2</span><span class="o">.</span><span class="w"> </span><span class="n">Target</span><span class="w"> </span><span class="n">Network</span><span class="o">:</span><span class="w"> </span><span class="err">í•™ìŠµ</span><span class="w"> </span><span class="err">ì•ˆì •í™”</span>
</code></pre></div>

<h3 id="dqn">DQN ì•„í‚¤í…ì²˜<a class="header-link" href="#dqn" title="Permanent link">&para;</a></h3>
<div class="highlight"><pre><span></span><code>â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                     DQN Architecture                         â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                              â”‚
â”‚  State s                                                     â”‚
â”‚     â”‚                                                        â”‚
â”‚     â–¼                                                        â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                    â”‚
â”‚  â”‚  Neural Network (CNN/MLP)           â”‚                    â”‚
â”‚  â”‚  Input: State s                     â”‚                    â”‚
â”‚  â”‚  Output: Q(s, a) for all actions    â”‚                    â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                    â”‚
â”‚     â”‚                                                        â”‚
â”‚     â–¼                                                        â”‚
â”‚  [Q(s, a_1), Q(s, a_2), ..., Q(s, a_n)]                     â”‚
â”‚     â”‚                                                        â”‚
â”‚     â–¼                                                        â”‚
â”‚  Action = argmax_a Q(s, a)                                  â”‚
â”‚                                                              â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
</code></pre></div>

<h3 id="experience-replay">Experience Replay<a class="header-link" href="#experience-replay" title="Permanent link">&para;</a></h3>
<div class="highlight"><pre><span></span><code><span class="kn">from</span><span class="w"> </span><span class="nn">collections</span><span class="w"> </span><span class="kn">import</span> <span class="n">deque</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">random</span>

<span class="k">class</span><span class="w"> </span><span class="nc">ReplayBuffer</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Experience Replay Buffer (â­â­â­)&quot;&quot;&quot;</span>
    <span class="k">def</span><span class="w"> </span><span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">capacity</span><span class="o">=</span><span class="mi">10000</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">buffer</span> <span class="o">=</span> <span class="n">deque</span><span class="p">(</span><span class="n">maxlen</span><span class="o">=</span><span class="n">capacity</span><span class="p">)</span>

    <span class="k">def</span><span class="w"> </span><span class="nf">push</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">state</span><span class="p">,</span> <span class="n">action</span><span class="p">,</span> <span class="n">reward</span><span class="p">,</span> <span class="n">next_state</span><span class="p">,</span> <span class="n">done</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;ê²½í—˜ ì €ì¥&quot;&quot;&quot;</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">buffer</span><span class="o">.</span><span class="n">append</span><span class="p">((</span><span class="n">state</span><span class="p">,</span> <span class="n">action</span><span class="p">,</span> <span class="n">reward</span><span class="p">,</span> <span class="n">next_state</span><span class="p">,</span> <span class="n">done</span><span class="p">))</span>

    <span class="k">def</span><span class="w"> </span><span class="nf">sample</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">batch_size</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;ëœë¤ ìƒ˜í”Œë§&quot;&quot;&quot;</span>
        <span class="n">batch</span> <span class="o">=</span> <span class="n">random</span><span class="o">.</span><span class="n">sample</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">buffer</span><span class="p">,</span> <span class="n">batch_size</span><span class="p">)</span>
        <span class="n">states</span><span class="p">,</span> <span class="n">actions</span><span class="p">,</span> <span class="n">rewards</span><span class="p">,</span> <span class="n">next_states</span><span class="p">,</span> <span class="n">dones</span> <span class="o">=</span> <span class="nb">zip</span><span class="p">(</span><span class="o">*</span><span class="n">batch</span><span class="p">)</span>
        <span class="k">return</span> <span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">states</span><span class="p">),</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">actions</span><span class="p">),</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">rewards</span><span class="p">),</span>
                <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">next_states</span><span class="p">),</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">dones</span><span class="p">))</span>

    <span class="k">def</span><span class="w"> </span><span class="fm">__len__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="k">return</span> <span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">buffer</span><span class="p">)</span>
</code></pre></div>

<h3 id="dqn-pytorch">DQN PyTorch êµ¬í˜„<a class="header-link" href="#dqn-pytorch" title="Permanent link">&para;</a></h3>
<div class="highlight"><pre><span></span><code><span class="kn">import</span><span class="w"> </span><span class="nn">torch</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">torch.nn</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">nn</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">torch.optim</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">optim</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">copy</span>

<span class="k">class</span><span class="w"> </span><span class="nc">QNetwork</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Q-Network (MLP) (â­â­â­)&quot;&quot;&quot;</span>
    <span class="k">def</span><span class="w"> </span><span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">state_dim</span><span class="p">,</span> <span class="n">action_dim</span><span class="p">,</span> <span class="n">hidden_dim</span><span class="o">=</span><span class="mi">128</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">net</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Sequential</span><span class="p">(</span>
            <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">state_dim</span><span class="p">,</span> <span class="n">hidden_dim</span><span class="p">),</span>
            <span class="n">nn</span><span class="o">.</span><span class="n">ReLU</span><span class="p">(),</span>
            <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">hidden_dim</span><span class="p">,</span> <span class="n">hidden_dim</span><span class="p">),</span>
            <span class="n">nn</span><span class="o">.</span><span class="n">ReLU</span><span class="p">(),</span>
            <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">hidden_dim</span><span class="p">,</span> <span class="n">action_dim</span><span class="p">)</span>
        <span class="p">)</span>

    <span class="k">def</span><span class="w"> </span><span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">net</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>


<span class="k">class</span><span class="w"> </span><span class="nc">DQNAgent</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;DQN Agent (â­â­â­â­)&quot;&quot;&quot;</span>
    <span class="k">def</span><span class="w"> </span><span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">state_dim</span><span class="p">,</span> <span class="n">action_dim</span><span class="p">,</span> <span class="n">lr</span><span class="o">=</span><span class="mf">1e-3</span><span class="p">,</span> <span class="n">gamma</span><span class="o">=</span><span class="mf">0.99</span><span class="p">,</span>
                 <span class="n">epsilon</span><span class="o">=</span><span class="mf">1.0</span><span class="p">,</span> <span class="n">epsilon_min</span><span class="o">=</span><span class="mf">0.01</span><span class="p">,</span> <span class="n">epsilon_decay</span><span class="o">=</span><span class="mf">0.995</span><span class="p">,</span>
                 <span class="n">buffer_size</span><span class="o">=</span><span class="mi">10000</span><span class="p">,</span> <span class="n">batch_size</span><span class="o">=</span><span class="mi">64</span><span class="p">,</span> <span class="n">target_update</span><span class="o">=</span><span class="mi">100</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">state_dim</span> <span class="o">=</span> <span class="n">state_dim</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">action_dim</span> <span class="o">=</span> <span class="n">action_dim</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">gamma</span> <span class="o">=</span> <span class="n">gamma</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">epsilon</span> <span class="o">=</span> <span class="n">epsilon</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">epsilon_min</span> <span class="o">=</span> <span class="n">epsilon_min</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">epsilon_decay</span> <span class="o">=</span> <span class="n">epsilon_decay</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">batch_size</span> <span class="o">=</span> <span class="n">batch_size</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">target_update</span> <span class="o">=</span> <span class="n">target_update</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">learn_step</span> <span class="o">=</span> <span class="mi">0</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">device</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">device</span><span class="p">(</span><span class="s1">&#39;cuda&#39;</span> <span class="k">if</span> <span class="n">torch</span><span class="o">.</span><span class="n">cuda</span><span class="o">.</span><span class="n">is_available</span><span class="p">()</span> <span class="k">else</span> <span class="s1">&#39;cpu&#39;</span><span class="p">)</span>

        <span class="c1"># Q-Networks</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">q_network</span> <span class="o">=</span> <span class="n">QNetwork</span><span class="p">(</span><span class="n">state_dim</span><span class="p">,</span> <span class="n">action_dim</span><span class="p">)</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">device</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">target_network</span> <span class="o">=</span> <span class="n">copy</span><span class="o">.</span><span class="n">deepcopy</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">q_network</span><span class="p">)</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">optimizer</span> <span class="o">=</span> <span class="n">optim</span><span class="o">.</span><span class="n">Adam</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">q_network</span><span class="o">.</span><span class="n">parameters</span><span class="p">(),</span> <span class="n">lr</span><span class="o">=</span><span class="n">lr</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">criterion</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">MSELoss</span><span class="p">()</span>

        <span class="c1"># Replay Buffer</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">replay_buffer</span> <span class="o">=</span> <span class="n">ReplayBuffer</span><span class="p">(</span><span class="n">buffer_size</span><span class="p">)</span>

    <span class="k">def</span><span class="w"> </span><span class="nf">select_action</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">state</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Îµ-greedy í–‰ë™ ì„ íƒ&quot;&quot;&quot;</span>
        <span class="k">if</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">random</span><span class="p">()</span> <span class="o">&lt;</span> <span class="bp">self</span><span class="o">.</span><span class="n">epsilon</span><span class="p">:</span>
            <span class="k">return</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">randint</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">action_dim</span><span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">state</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">FloatTensor</span><span class="p">(</span><span class="n">state</span><span class="p">)</span><span class="o">.</span><span class="n">unsqueeze</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">device</span><span class="p">)</span>
            <span class="k">with</span> <span class="n">torch</span><span class="o">.</span><span class="n">no_grad</span><span class="p">():</span>
                <span class="n">q_values</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">q_network</span><span class="p">(</span><span class="n">state</span><span class="p">)</span>
            <span class="k">return</span> <span class="n">q_values</span><span class="o">.</span><span class="n">argmax</span><span class="p">()</span><span class="o">.</span><span class="n">item</span><span class="p">()</span>

    <span class="k">def</span><span class="w"> </span><span class="nf">store_transition</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">state</span><span class="p">,</span> <span class="n">action</span><span class="p">,</span> <span class="n">reward</span><span class="p">,</span> <span class="n">next_state</span><span class="p">,</span> <span class="n">done</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;ê²½í—˜ ì €ì¥&quot;&quot;&quot;</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">replay_buffer</span><span class="o">.</span><span class="n">push</span><span class="p">(</span><span class="n">state</span><span class="p">,</span> <span class="n">action</span><span class="p">,</span> <span class="n">reward</span><span class="p">,</span> <span class="n">next_state</span><span class="p">,</span> <span class="n">done</span><span class="p">)</span>

    <span class="k">def</span><span class="w"> </span><span class="nf">learn</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;DQN í•™ìŠµ&quot;&quot;&quot;</span>
        <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">replay_buffer</span><span class="p">)</span> <span class="o">&lt;</span> <span class="bp">self</span><span class="o">.</span><span class="n">batch_size</span><span class="p">:</span>
            <span class="k">return</span>

        <span class="c1"># Sample batch</span>
        <span class="n">states</span><span class="p">,</span> <span class="n">actions</span><span class="p">,</span> <span class="n">rewards</span><span class="p">,</span> <span class="n">next_states</span><span class="p">,</span> <span class="n">dones</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">replay_buffer</span><span class="o">.</span><span class="n">sample</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">batch_size</span><span class="p">)</span>

        <span class="n">states</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">FloatTensor</span><span class="p">(</span><span class="n">states</span><span class="p">)</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">device</span><span class="p">)</span>
        <span class="n">actions</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">LongTensor</span><span class="p">(</span><span class="n">actions</span><span class="p">)</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">device</span><span class="p">)</span>
        <span class="n">rewards</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">FloatTensor</span><span class="p">(</span><span class="n">rewards</span><span class="p">)</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">device</span><span class="p">)</span>
        <span class="n">next_states</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">FloatTensor</span><span class="p">(</span><span class="n">next_states</span><span class="p">)</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">device</span><span class="p">)</span>
        <span class="n">dones</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">FloatTensor</span><span class="p">(</span><span class="n">dones</span><span class="p">)</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">device</span><span class="p">)</span>

        <span class="c1"># Current Q values</span>
        <span class="n">current_q</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">q_network</span><span class="p">(</span><span class="n">states</span><span class="p">)</span><span class="o">.</span><span class="n">gather</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">actions</span><span class="o">.</span><span class="n">unsqueeze</span><span class="p">(</span><span class="mi">1</span><span class="p">))</span><span class="o">.</span><span class="n">squeeze</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span>

        <span class="c1"># Target Q values (with target network)</span>
        <span class="k">with</span> <span class="n">torch</span><span class="o">.</span><span class="n">no_grad</span><span class="p">():</span>
            <span class="n">next_q</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">target_network</span><span class="p">(</span><span class="n">next_states</span><span class="p">)</span><span class="o">.</span><span class="n">max</span><span class="p">(</span><span class="mi">1</span><span class="p">)[</span><span class="mi">0</span><span class="p">]</span>
            <span class="n">target_q</span> <span class="o">=</span> <span class="n">rewards</span> <span class="o">+</span> <span class="bp">self</span><span class="o">.</span><span class="n">gamma</span> <span class="o">*</span> <span class="n">next_q</span> <span class="o">*</span> <span class="p">(</span><span class="mi">1</span> <span class="o">-</span> <span class="n">dones</span><span class="p">)</span>

        <span class="c1"># Loss and update</span>
        <span class="n">loss</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">criterion</span><span class="p">(</span><span class="n">current_q</span><span class="p">,</span> <span class="n">target_q</span><span class="p">)</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">optimizer</span><span class="o">.</span><span class="n">zero_grad</span><span class="p">()</span>
        <span class="n">loss</span><span class="o">.</span><span class="n">backward</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">optimizer</span><span class="o">.</span><span class="n">step</span><span class="p">()</span>

        <span class="c1"># Update target network</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">learn_step</span> <span class="o">+=</span> <span class="mi">1</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">learn_step</span> <span class="o">%</span> <span class="bp">self</span><span class="o">.</span><span class="n">target_update</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">target_network</span><span class="o">.</span><span class="n">load_state_dict</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">q_network</span><span class="o">.</span><span class="n">state_dict</span><span class="p">())</span>

        <span class="c1"># Epsilon decay</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">epsilon</span> <span class="o">=</span> <span class="nb">max</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">epsilon_min</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">epsilon</span> <span class="o">*</span> <span class="bp">self</span><span class="o">.</span><span class="n">epsilon_decay</span><span class="p">)</span>

        <span class="k">return</span> <span class="n">loss</span><span class="o">.</span><span class="n">item</span><span class="p">()</span>
</code></pre></div>

<h3 id="dqn_1">DQN í•™ìŠµ ë£¨í”„<a class="header-link" href="#dqn_1" title="Permanent link">&para;</a></h3>
<div class="highlight"><pre><span></span><code><span class="k">def</span><span class="w"> </span><span class="nf">train_dqn</span><span class="p">(</span><span class="n">env</span><span class="p">,</span> <span class="n">agent</span><span class="p">,</span> <span class="n">episodes</span><span class="o">=</span><span class="mi">500</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;DQN Training Loop (â­â­â­)&quot;&quot;&quot;</span>
    <span class="n">rewards_history</span> <span class="o">=</span> <span class="p">[]</span>

    <span class="k">for</span> <span class="n">episode</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">episodes</span><span class="p">):</span>
        <span class="n">state</span> <span class="o">=</span> <span class="n">env</span><span class="o">.</span><span class="n">reset</span><span class="p">()</span>
        <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">state</span><span class="p">,</span> <span class="nb">tuple</span><span class="p">):</span>  <span class="c1"># gym 0.26+</span>
            <span class="n">state</span> <span class="o">=</span> <span class="n">state</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>

        <span class="n">total_reward</span> <span class="o">=</span> <span class="mi">0</span>
        <span class="n">done</span> <span class="o">=</span> <span class="kc">False</span>

        <span class="k">while</span> <span class="ow">not</span> <span class="n">done</span><span class="p">:</span>
            <span class="n">action</span> <span class="o">=</span> <span class="n">agent</span><span class="o">.</span><span class="n">select_action</span><span class="p">(</span><span class="n">state</span><span class="p">)</span>
            <span class="n">result</span> <span class="o">=</span> <span class="n">env</span><span class="o">.</span><span class="n">step</span><span class="p">(</span><span class="n">action</span><span class="p">)</span>
            <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">result</span><span class="p">)</span> <span class="o">==</span> <span class="mi">5</span><span class="p">:</span>  <span class="c1"># gym 0.26+</span>
                <span class="n">next_state</span><span class="p">,</span> <span class="n">reward</span><span class="p">,</span> <span class="n">terminated</span><span class="p">,</span> <span class="n">truncated</span><span class="p">,</span> <span class="n">info</span> <span class="o">=</span> <span class="n">result</span>
                <span class="n">done</span> <span class="o">=</span> <span class="n">terminated</span> <span class="ow">or</span> <span class="n">truncated</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="n">next_state</span><span class="p">,</span> <span class="n">reward</span><span class="p">,</span> <span class="n">done</span><span class="p">,</span> <span class="n">info</span> <span class="o">=</span> <span class="n">result</span>

            <span class="n">agent</span><span class="o">.</span><span class="n">store_transition</span><span class="p">(</span><span class="n">state</span><span class="p">,</span> <span class="n">action</span><span class="p">,</span> <span class="n">reward</span><span class="p">,</span> <span class="n">next_state</span><span class="p">,</span> <span class="n">done</span><span class="p">)</span>
            <span class="n">loss</span> <span class="o">=</span> <span class="n">agent</span><span class="o">.</span><span class="n">learn</span><span class="p">()</span>

            <span class="n">state</span> <span class="o">=</span> <span class="n">next_state</span>
            <span class="n">total_reward</span> <span class="o">+=</span> <span class="n">reward</span>

        <span class="n">rewards_history</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">total_reward</span><span class="p">)</span>

        <span class="k">if</span> <span class="n">episode</span> <span class="o">%</span> <span class="mi">10</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
            <span class="n">avg_reward</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">rewards_history</span><span class="p">[</span><span class="o">-</span><span class="mi">10</span><span class="p">:])</span>
            <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Episode </span><span class="si">{</span><span class="n">episode</span><span class="si">}</span><span class="s2">: Avg Reward = </span><span class="si">{</span><span class="n">avg_reward</span><span class="si">:</span><span class="s2">.2f</span><span class="si">}</span><span class="s2">, Epsilon = </span><span class="si">{</span><span class="n">agent</span><span class="o">.</span><span class="n">epsilon</span><span class="si">:</span><span class="s2">.3f</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>

    <span class="k">return</span> <span class="n">rewards_history</span>
</code></pre></div>

<hr />
<h2 id="6-policy-gradient">6. Policy Gradient<a class="header-link" href="#6-policy-gradient" title="Permanent link">&para;</a></h2>
<h3 id="_8">ì•„ì´ë””ì–´<a class="header-link" href="#_8" title="Permanent link">&para;</a></h3>
<div class="highlight"><pre><span></span><code><span class="nt">Value-based</span><span class="w"> </span><span class="o">(</span><span class="nt">DQN</span><span class="o">):</span><span class="w"> </span><span class="nt">Q</span><span class="w"> </span><span class="nt">í•¨ìˆ˜</span><span class="w"> </span><span class="nt">í•™ìŠµ</span><span class="w"> </span><span class="err">â†’</span><span class="w"> </span><span class="nt">ê°„ì ‘ì ìœ¼ë¡œ</span><span class="w"> </span><span class="nt">ì •ì±…</span><span class="w"> </span><span class="nt">ë„ì¶œ</span>
<span class="nt">Policy-based</span><span class="o">:</span><span class="w"> </span><span class="nt">ì •ì±…ì„</span><span class="w"> </span><span class="nt">ì§ì ‘</span><span class="w"> </span><span class="nt">í•™ìŠµ</span>

<span class="nt">ì •ì±…</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="nt">Ï€_Î¸</span><span class="o">(</span><span class="nt">a</span><span class="o">|</span><span class="nt">s</span><span class="o">)</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="nt">P</span><span class="o">(</span><span class="nt">a</span><span class="o">|</span><span class="nt">s</span><span class="o">;</span><span class="w"> </span><span class="nt">Î¸</span><span class="o">)</span>

<span class="nt">ì¥ì </span><span class="o">:</span>
<span class="nt">1</span><span class="o">.</span><span class="w"> </span><span class="nt">ì—°ì†</span><span class="w"> </span><span class="nt">í–‰ë™</span><span class="w"> </span><span class="nt">ê³µê°„</span><span class="w"> </span><span class="nt">ì²˜ë¦¬</span><span class="w"> </span><span class="nt">ê°€ëŠ¥</span>
<span class="nt">2</span><span class="o">.</span><span class="w"> </span><span class="nt">í™•ë¥ ì </span><span class="w"> </span><span class="nt">ì •ì±…</span><span class="w"> </span><span class="nt">í•™ìŠµ</span><span class="w"> </span><span class="nt">ê°€ëŠ¥</span>
<span class="nt">3</span><span class="o">.</span><span class="w"> </span><span class="nt">ìˆ˜ë ´</span><span class="w"> </span><span class="nt">ë³´ì¥</span><span class="w"> </span><span class="o">(</span><span class="nt">ì§€ì—­</span><span class="w"> </span><span class="nt">ìµœì </span><span class="o">)</span>
</code></pre></div>

<h3 id="policy-gradient-theorem">Policy Gradient Theorem<a class="header-link" href="#policy-gradient-theorem" title="Permanent link">&para;</a></h3>
<div class="highlight"><pre><span></span><code><span class="c1"># ëª©í‘œ: J(Î¸) = E[Î£ R_t] ìµœëŒ€í™”</span>

<span class="c1"># Gradient:</span>
<span class="err">âˆ‡</span><span class="n">_Î¸</span> <span class="n">J</span><span class="p">(</span><span class="n">Î¸</span><span class="p">)</span> <span class="o">=</span> <span class="n">E</span><span class="p">[</span> <span class="n">Î£_t</span> <span class="err">âˆ‡</span><span class="n">_Î¸</span> <span class="n">log</span> <span class="n">Ï€_Î¸</span><span class="p">(</span><span class="n">a_t</span><span class="o">|</span><span class="n">s_t</span><span class="p">)</span> <span class="o">*</span> <span class="n">G_t</span> <span class="p">]</span>

<span class="c1"># G_t: t ì‹œì ë¶€í„°ì˜ ëˆ„ì  ë³´ìƒ (Return)</span>
<span class="c1"># log Ï€_Î¸: ì •ì±…ì˜ ë¡œê·¸ í™•ë¥ </span>
</code></pre></div>

<h3 id="reinforce">REINFORCE ì•Œê³ ë¦¬ì¦˜<a class="header-link" href="#reinforce" title="Permanent link">&para;</a></h3>
<div class="highlight"><pre><span></span><code><span class="k">class</span><span class="w"> </span><span class="nc">PolicyNetwork</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Policy Network (â­â­â­)&quot;&quot;&quot;</span>
    <span class="k">def</span><span class="w"> </span><span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">state_dim</span><span class="p">,</span> <span class="n">action_dim</span><span class="p">,</span> <span class="n">hidden_dim</span><span class="o">=</span><span class="mi">128</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">net</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Sequential</span><span class="p">(</span>
            <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">state_dim</span><span class="p">,</span> <span class="n">hidden_dim</span><span class="p">),</span>
            <span class="n">nn</span><span class="o">.</span><span class="n">ReLU</span><span class="p">(),</span>
            <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">hidden_dim</span><span class="p">,</span> <span class="n">hidden_dim</span><span class="p">),</span>
            <span class="n">nn</span><span class="o">.</span><span class="n">ReLU</span><span class="p">(),</span>
            <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">hidden_dim</span><span class="p">,</span> <span class="n">action_dim</span><span class="p">),</span>
            <span class="n">nn</span><span class="o">.</span><span class="n">Softmax</span><span class="p">(</span><span class="n">dim</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span>
        <span class="p">)</span>

    <span class="k">def</span><span class="w"> </span><span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">net</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>


<span class="k">class</span><span class="w"> </span><span class="nc">REINFORCEAgent</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;REINFORCE (Monte Carlo Policy Gradient) (â­â­â­â­)&quot;&quot;&quot;</span>
    <span class="k">def</span><span class="w"> </span><span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">state_dim</span><span class="p">,</span> <span class="n">action_dim</span><span class="p">,</span> <span class="n">lr</span><span class="o">=</span><span class="mf">1e-3</span><span class="p">,</span> <span class="n">gamma</span><span class="o">=</span><span class="mf">0.99</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">gamma</span> <span class="o">=</span> <span class="n">gamma</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">device</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">device</span><span class="p">(</span><span class="s1">&#39;cuda&#39;</span> <span class="k">if</span> <span class="n">torch</span><span class="o">.</span><span class="n">cuda</span><span class="o">.</span><span class="n">is_available</span><span class="p">()</span> <span class="k">else</span> <span class="s1">&#39;cpu&#39;</span><span class="p">)</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">policy</span> <span class="o">=</span> <span class="n">PolicyNetwork</span><span class="p">(</span><span class="n">state_dim</span><span class="p">,</span> <span class="n">action_dim</span><span class="p">)</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">device</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">optimizer</span> <span class="o">=</span> <span class="n">optim</span><span class="o">.</span><span class="n">Adam</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">policy</span><span class="o">.</span><span class="n">parameters</span><span class="p">(),</span> <span class="n">lr</span><span class="o">=</span><span class="n">lr</span><span class="p">)</span>

        <span class="c1"># Episode buffer</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">log_probs</span> <span class="o">=</span> <span class="p">[]</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">rewards</span> <span class="o">=</span> <span class="p">[]</span>

    <span class="k">def</span><span class="w"> </span><span class="nf">select_action</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">state</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;í™•ë¥ ì  í–‰ë™ ì„ íƒ&quot;&quot;&quot;</span>
        <span class="n">state</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">FloatTensor</span><span class="p">(</span><span class="n">state</span><span class="p">)</span><span class="o">.</span><span class="n">unsqueeze</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">device</span><span class="p">)</span>
        <span class="n">probs</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">policy</span><span class="p">(</span><span class="n">state</span><span class="p">)</span>

        <span class="c1"># Categorical distributionì—ì„œ ìƒ˜í”Œë§</span>
        <span class="n">dist</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">distributions</span><span class="o">.</span><span class="n">Categorical</span><span class="p">(</span><span class="n">probs</span><span class="p">)</span>
        <span class="n">action</span> <span class="o">=</span> <span class="n">dist</span><span class="o">.</span><span class="n">sample</span><span class="p">()</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">log_probs</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">dist</span><span class="o">.</span><span class="n">log_prob</span><span class="p">(</span><span class="n">action</span><span class="p">))</span>

        <span class="k">return</span> <span class="n">action</span><span class="o">.</span><span class="n">item</span><span class="p">()</span>

    <span class="k">def</span><span class="w"> </span><span class="nf">store_reward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">reward</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;ë³´ìƒ ì €ì¥&quot;&quot;&quot;</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">rewards</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">reward</span><span class="p">)</span>

    <span class="k">def</span><span class="w"> </span><span class="nf">learn</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;ì—í”¼ì†Œë“œ ëì— í•™ìŠµ&quot;&quot;&quot;</span>
        <span class="c1"># Returns ê³„ì‚° (ë’¤ì—ì„œë¶€í„°)</span>
        <span class="n">returns</span> <span class="o">=</span> <span class="p">[]</span>
        <span class="n">G</span> <span class="o">=</span> <span class="mi">0</span>
        <span class="k">for</span> <span class="n">r</span> <span class="ow">in</span> <span class="nb">reversed</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">rewards</span><span class="p">):</span>
            <span class="n">G</span> <span class="o">=</span> <span class="n">r</span> <span class="o">+</span> <span class="bp">self</span><span class="o">.</span><span class="n">gamma</span> <span class="o">*</span> <span class="n">G</span>
            <span class="n">returns</span><span class="o">.</span><span class="n">insert</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">G</span><span class="p">)</span>

        <span class="n">returns</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">FloatTensor</span><span class="p">(</span><span class="n">returns</span><span class="p">)</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">device</span><span class="p">)</span>

        <span class="c1"># Normalize (baseline íš¨ê³¼)</span>
        <span class="n">returns</span> <span class="o">=</span> <span class="p">(</span><span class="n">returns</span> <span class="o">-</span> <span class="n">returns</span><span class="o">.</span><span class="n">mean</span><span class="p">())</span> <span class="o">/</span> <span class="p">(</span><span class="n">returns</span><span class="o">.</span><span class="n">std</span><span class="p">()</span> <span class="o">+</span> <span class="mf">1e-8</span><span class="p">)</span>

        <span class="c1"># Policy Gradient ì†ì‹¤</span>
        <span class="n">loss</span> <span class="o">=</span> <span class="mi">0</span>
        <span class="k">for</span> <span class="n">log_prob</span><span class="p">,</span> <span class="n">G</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">log_probs</span><span class="p">,</span> <span class="n">returns</span><span class="p">):</span>
            <span class="n">loss</span> <span class="o">-=</span> <span class="n">log_prob</span> <span class="o">*</span> <span class="n">G</span>  <span class="c1"># ìŒìˆ˜: gradient ascent</span>

        <span class="c1"># Update</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">optimizer</span><span class="o">.</span><span class="n">zero_grad</span><span class="p">()</span>
        <span class="n">loss</span><span class="o">.</span><span class="n">backward</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">optimizer</span><span class="o">.</span><span class="n">step</span><span class="p">()</span>

        <span class="c1"># ë²„í¼ ì´ˆê¸°í™”</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">log_probs</span> <span class="o">=</span> <span class="p">[]</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">rewards</span> <span class="o">=</span> <span class="p">[]</span>

        <span class="k">return</span> <span class="n">loss</span><span class="o">.</span><span class="n">item</span><span class="p">()</span>
</code></pre></div>

<h3 id="reinforce_1">REINFORCE í•™ìŠµ<a class="header-link" href="#reinforce_1" title="Permanent link">&para;</a></h3>
<div class="highlight"><pre><span></span><code><span class="k">def</span><span class="w"> </span><span class="nf">train_reinforce</span><span class="p">(</span><span class="n">env</span><span class="p">,</span> <span class="n">agent</span><span class="p">,</span> <span class="n">episodes</span><span class="o">=</span><span class="mi">1000</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;REINFORCE Training (â­â­â­)&quot;&quot;&quot;</span>
    <span class="n">rewards_history</span> <span class="o">=</span> <span class="p">[]</span>

    <span class="k">for</span> <span class="n">episode</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">episodes</span><span class="p">):</span>
        <span class="n">state</span> <span class="o">=</span> <span class="n">env</span><span class="o">.</span><span class="n">reset</span><span class="p">()</span>
        <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">state</span><span class="p">,</span> <span class="nb">tuple</span><span class="p">):</span>
            <span class="n">state</span> <span class="o">=</span> <span class="n">state</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>

        <span class="n">done</span> <span class="o">=</span> <span class="kc">False</span>
        <span class="n">total_reward</span> <span class="o">=</span> <span class="mi">0</span>

        <span class="k">while</span> <span class="ow">not</span> <span class="n">done</span><span class="p">:</span>
            <span class="n">action</span> <span class="o">=</span> <span class="n">agent</span><span class="o">.</span><span class="n">select_action</span><span class="p">(</span><span class="n">state</span><span class="p">)</span>
            <span class="n">result</span> <span class="o">=</span> <span class="n">env</span><span class="o">.</span><span class="n">step</span><span class="p">(</span><span class="n">action</span><span class="p">)</span>
            <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">result</span><span class="p">)</span> <span class="o">==</span> <span class="mi">5</span><span class="p">:</span>
                <span class="n">next_state</span><span class="p">,</span> <span class="n">reward</span><span class="p">,</span> <span class="n">terminated</span><span class="p">,</span> <span class="n">truncated</span><span class="p">,</span> <span class="n">_</span> <span class="o">=</span> <span class="n">result</span>
                <span class="n">done</span> <span class="o">=</span> <span class="n">terminated</span> <span class="ow">or</span> <span class="n">truncated</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="n">next_state</span><span class="p">,</span> <span class="n">reward</span><span class="p">,</span> <span class="n">done</span><span class="p">,</span> <span class="n">_</span> <span class="o">=</span> <span class="n">result</span>

            <span class="n">agent</span><span class="o">.</span><span class="n">store_reward</span><span class="p">(</span><span class="n">reward</span><span class="p">)</span>

            <span class="n">state</span> <span class="o">=</span> <span class="n">next_state</span>
            <span class="n">total_reward</span> <span class="o">+=</span> <span class="n">reward</span>

        <span class="c1"># Episode ëì— í•™ìŠµ</span>
        <span class="n">agent</span><span class="o">.</span><span class="n">learn</span><span class="p">()</span>
        <span class="n">rewards_history</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">total_reward</span><span class="p">)</span>

        <span class="k">if</span> <span class="n">episode</span> <span class="o">%</span> <span class="mi">50</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
            <span class="n">avg_reward</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">rewards_history</span><span class="p">[</span><span class="o">-</span><span class="mi">50</span><span class="p">:])</span>
            <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Episode </span><span class="si">{</span><span class="n">episode</span><span class="si">}</span><span class="s2">: Avg Reward = </span><span class="si">{</span><span class="n">avg_reward</span><span class="si">:</span><span class="s2">.2f</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>

    <span class="k">return</span> <span class="n">rewards_history</span>
</code></pre></div>

<hr />
<h2 id="7-actor-critic">7. Actor-Critic<a class="header-link" href="#7-actor-critic" title="Permanent link">&para;</a></h2>
<h3 id="_9">ì•„ì´ë””ì–´<a class="header-link" href="#_9" title="Permanent link">&para;</a></h3>
<div class="highlight"><pre><span></span><code>REINFORCE ë¬¸ì œ: ë†’ì€ ë¶„ì‚° (Monte Carlo ì¶”ì •)
í•´ê²°: Criticìœ¼ë¡œ Value ì¶”ì • â†’ ë¶„ì‚° ê°ì†Œ

Actor: ì •ì±… Ï€_Î¸ (í–‰ë™ ê²°ì •)
Critic: Value V_Ï† (ìƒíƒœ í‰ê°€)
</code></pre></div>

<h3 id="advantage-function">Advantage Function<a class="header-link" href="#advantage-function" title="Permanent link">&para;</a></h3>
<div class="highlight"><pre><span></span><code><span class="c1"># Advantage = Q(s,a) - V(s)</span>
<span class="c1"># ì˜ë¯¸: í‰ê·  ëŒ€ë¹„ í•´ë‹¹ í–‰ë™ì´ ì–¼ë§ˆë‚˜ ì¢‹ì€ê°€</span>

<span class="c1"># TD Errorë¡œ ì¶”ì •:</span>
<span class="n">A</span><span class="p">(</span><span class="n">s</span><span class="p">,</span> <span class="n">a</span><span class="p">)</span> <span class="err">â‰ˆ</span> <span class="n">r</span> <span class="o">+</span> <span class="n">Î³V</span><span class="p">(</span><span class="n">s</span><span class="s1">&#39;) - V(s)</span>
</code></pre></div>

<h3 id="actor-critic">Actor-Critic êµ¬í˜„<a class="header-link" href="#actor-critic" title="Permanent link">&para;</a></h3>
<div class="highlight"><pre><span></span><code><span class="k">class</span><span class="w"> </span><span class="nc">ActorCritic</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Actor-Critic Network (â­â­â­â­)&quot;&quot;&quot;</span>
    <span class="k">def</span><span class="w"> </span><span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">state_dim</span><span class="p">,</span> <span class="n">action_dim</span><span class="p">,</span> <span class="n">hidden_dim</span><span class="o">=</span><span class="mi">128</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>

        <span class="c1"># Shared feature extractor</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">shared</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Sequential</span><span class="p">(</span>
            <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">state_dim</span><span class="p">,</span> <span class="n">hidden_dim</span><span class="p">),</span>
            <span class="n">nn</span><span class="o">.</span><span class="n">ReLU</span><span class="p">()</span>
        <span class="p">)</span>

        <span class="c1"># Actor (Policy)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">actor</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Sequential</span><span class="p">(</span>
            <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">hidden_dim</span><span class="p">,</span> <span class="n">action_dim</span><span class="p">),</span>
            <span class="n">nn</span><span class="o">.</span><span class="n">Softmax</span><span class="p">(</span><span class="n">dim</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span>
        <span class="p">)</span>

        <span class="c1"># Critic (Value)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">critic</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">hidden_dim</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>

    <span class="k">def</span><span class="w"> </span><span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
        <span class="n">features</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">shared</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="n">policy</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">actor</span><span class="p">(</span><span class="n">features</span><span class="p">)</span>
        <span class="n">value</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">critic</span><span class="p">(</span><span class="n">features</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">policy</span><span class="p">,</span> <span class="n">value</span>


<span class="k">class</span><span class="w"> </span><span class="nc">A2CAgent</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Advantage Actor-Critic (â­â­â­â­)&quot;&quot;&quot;</span>
    <span class="k">def</span><span class="w"> </span><span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">state_dim</span><span class="p">,</span> <span class="n">action_dim</span><span class="p">,</span> <span class="n">lr</span><span class="o">=</span><span class="mf">1e-3</span><span class="p">,</span> <span class="n">gamma</span><span class="o">=</span><span class="mf">0.99</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">gamma</span> <span class="o">=</span> <span class="n">gamma</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">device</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">device</span><span class="p">(</span><span class="s1">&#39;cuda&#39;</span> <span class="k">if</span> <span class="n">torch</span><span class="o">.</span><span class="n">cuda</span><span class="o">.</span><span class="n">is_available</span><span class="p">()</span> <span class="k">else</span> <span class="s1">&#39;cpu&#39;</span><span class="p">)</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">network</span> <span class="o">=</span> <span class="n">ActorCritic</span><span class="p">(</span><span class="n">state_dim</span><span class="p">,</span> <span class="n">action_dim</span><span class="p">)</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">device</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">optimizer</span> <span class="o">=</span> <span class="n">optim</span><span class="o">.</span><span class="n">Adam</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">network</span><span class="o">.</span><span class="n">parameters</span><span class="p">(),</span> <span class="n">lr</span><span class="o">=</span><span class="n">lr</span><span class="p">)</span>

    <span class="k">def</span><span class="w"> </span><span class="nf">select_action</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">state</span><span class="p">):</span>
        <span class="n">state</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">FloatTensor</span><span class="p">(</span><span class="n">state</span><span class="p">)</span><span class="o">.</span><span class="n">unsqueeze</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">device</span><span class="p">)</span>
        <span class="n">policy</span><span class="p">,</span> <span class="n">_</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">network</span><span class="p">(</span><span class="n">state</span><span class="p">)</span>
        <span class="n">dist</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">distributions</span><span class="o">.</span><span class="n">Categorical</span><span class="p">(</span><span class="n">policy</span><span class="p">)</span>
        <span class="n">action</span> <span class="o">=</span> <span class="n">dist</span><span class="o">.</span><span class="n">sample</span><span class="p">()</span>
        <span class="k">return</span> <span class="n">action</span><span class="o">.</span><span class="n">item</span><span class="p">(),</span> <span class="n">dist</span><span class="o">.</span><span class="n">log_prob</span><span class="p">(</span><span class="n">action</span><span class="p">)</span>

    <span class="k">def</span><span class="w"> </span><span class="nf">learn</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">state</span><span class="p">,</span> <span class="n">action</span><span class="p">,</span> <span class="n">reward</span><span class="p">,</span> <span class="n">next_state</span><span class="p">,</span> <span class="n">done</span><span class="p">,</span> <span class="n">log_prob</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;One-step Actor-Critic Update&quot;&quot;&quot;</span>
        <span class="n">state</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">FloatTensor</span><span class="p">(</span><span class="n">state</span><span class="p">)</span><span class="o">.</span><span class="n">unsqueeze</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">device</span><span class="p">)</span>
        <span class="n">next_state</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">FloatTensor</span><span class="p">(</span><span class="n">next_state</span><span class="p">)</span><span class="o">.</span><span class="n">unsqueeze</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">device</span><span class="p">)</span>
        <span class="n">reward</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">FloatTensor</span><span class="p">([</span><span class="n">reward</span><span class="p">])</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">device</span><span class="p">)</span>
        <span class="n">done</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">FloatTensor</span><span class="p">([</span><span class="n">done</span><span class="p">])</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">device</span><span class="p">)</span>

        <span class="n">_</span><span class="p">,</span> <span class="n">value</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">network</span><span class="p">(</span><span class="n">state</span><span class="p">)</span>
        <span class="n">_</span><span class="p">,</span> <span class="n">next_value</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">network</span><span class="p">(</span><span class="n">next_state</span><span class="p">)</span>

        <span class="c1"># TD Target and Advantage</span>
        <span class="n">td_target</span> <span class="o">=</span> <span class="n">reward</span> <span class="o">+</span> <span class="bp">self</span><span class="o">.</span><span class="n">gamma</span> <span class="o">*</span> <span class="n">next_value</span> <span class="o">*</span> <span class="p">(</span><span class="mi">1</span> <span class="o">-</span> <span class="n">done</span><span class="p">)</span>
        <span class="n">advantage</span> <span class="o">=</span> <span class="n">td_target</span> <span class="o">-</span> <span class="n">value</span>

        <span class="c1"># Actor Loss (policy gradient with advantage)</span>
        <span class="n">actor_loss</span> <span class="o">=</span> <span class="o">-</span><span class="n">log_prob</span> <span class="o">*</span> <span class="n">advantage</span><span class="o">.</span><span class="n">detach</span><span class="p">()</span>

        <span class="c1"># Critic Loss (value function)</span>
        <span class="n">critic_loss</span> <span class="o">=</span> <span class="n">advantage</span><span class="o">.</span><span class="n">pow</span><span class="p">(</span><span class="mi">2</span><span class="p">)</span>

        <span class="c1"># Total Loss</span>
        <span class="n">loss</span> <span class="o">=</span> <span class="n">actor_loss</span> <span class="o">+</span> <span class="mf">0.5</span> <span class="o">*</span> <span class="n">critic_loss</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">optimizer</span><span class="o">.</span><span class="n">zero_grad</span><span class="p">()</span>
        <span class="n">loss</span><span class="o">.</span><span class="n">backward</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">optimizer</span><span class="o">.</span><span class="n">step</span><span class="p">()</span>

        <span class="k">return</span> <span class="n">loss</span><span class="o">.</span><span class="n">item</span><span class="p">()</span>
</code></pre></div>

<hr />
<h2 id="8">8. í™˜ê²½ê³¼ ì‹¤í—˜<a class="header-link" href="#8" title="Permanent link">&para;</a></h2>
<h3 id="openai-gym">OpenAI Gym ì‚¬ìš©<a class="header-link" href="#openai-gym" title="Permanent link">&para;</a></h3>
<div class="highlight"><pre><span></span><code><span class="kn">import</span><span class="w"> </span><span class="nn">gymnasium</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">gym</span>

<span class="c1"># í™˜ê²½ ìƒì„±</span>
<span class="n">env</span> <span class="o">=</span> <span class="n">gym</span><span class="o">.</span><span class="n">make</span><span class="p">(</span><span class="s1">&#39;CartPole-v1&#39;</span><span class="p">)</span>

<span class="c1"># í™˜ê²½ ì •ë³´</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;State space: </span><span class="si">{</span><span class="n">env</span><span class="o">.</span><span class="n">observation_space</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>      <span class="c1"># Box(4,)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Action space: </span><span class="si">{</span><span class="n">env</span><span class="o">.</span><span class="n">action_space</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>          <span class="c1"># Discrete(2)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;State dim: </span><span class="si">{</span><span class="n">env</span><span class="o">.</span><span class="n">observation_space</span><span class="o">.</span><span class="n">shape</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>  <span class="c1"># (4,)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Action dim: </span><span class="si">{</span><span class="n">env</span><span class="o">.</span><span class="n">action_space</span><span class="o">.</span><span class="n">n</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>          <span class="c1"># 2</span>

<span class="c1"># ì—í”¼ì†Œë“œ ì‹¤í–‰</span>
<span class="n">state</span><span class="p">,</span> <span class="n">_</span> <span class="o">=</span> <span class="n">env</span><span class="o">.</span><span class="n">reset</span><span class="p">()</span>
<span class="n">done</span> <span class="o">=</span> <span class="kc">False</span>

<span class="k">while</span> <span class="ow">not</span> <span class="n">done</span><span class="p">:</span>
    <span class="n">action</span> <span class="o">=</span> <span class="n">env</span><span class="o">.</span><span class="n">action_space</span><span class="o">.</span><span class="n">sample</span><span class="p">()</span>  <span class="c1"># ëœë¤ í–‰ë™</span>
    <span class="n">next_state</span><span class="p">,</span> <span class="n">reward</span><span class="p">,</span> <span class="n">terminated</span><span class="p">,</span> <span class="n">truncated</span><span class="p">,</span> <span class="n">info</span> <span class="o">=</span> <span class="n">env</span><span class="o">.</span><span class="n">step</span><span class="p">(</span><span class="n">action</span><span class="p">)</span>
    <span class="n">done</span> <span class="o">=</span> <span class="n">terminated</span> <span class="ow">or</span> <span class="n">truncated</span>
    <span class="n">state</span> <span class="o">=</span> <span class="n">next_state</span>

<span class="n">env</span><span class="o">.</span><span class="n">close</span><span class="p">()</span>
</code></pre></div>

<h3 id="cartpole">ì‹¤í—˜ ì˜ˆì œ: CartPole<a class="header-link" href="#cartpole" title="Permanent link">&para;</a></h3>
<div class="highlight"><pre><span></span><code><span class="k">def</span><span class="w"> </span><span class="nf">run_experiment</span><span class="p">():</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;CartPole ì‹¤í—˜ (â­â­â­)&quot;&quot;&quot;</span>
    <span class="n">env</span> <span class="o">=</span> <span class="n">gym</span><span class="o">.</span><span class="n">make</span><span class="p">(</span><span class="s1">&#39;CartPole-v1&#39;</span><span class="p">)</span>
    <span class="n">state_dim</span> <span class="o">=</span> <span class="n">env</span><span class="o">.</span><span class="n">observation_space</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>  <span class="c1"># 4</span>
    <span class="n">action_dim</span> <span class="o">=</span> <span class="n">env</span><span class="o">.</span><span class="n">action_space</span><span class="o">.</span><span class="n">n</span>              <span class="c1"># 2</span>

    <span class="c1"># DQN ì—ì´ì „íŠ¸</span>
    <span class="n">agent</span> <span class="o">=</span> <span class="n">DQNAgent</span><span class="p">(</span><span class="n">state_dim</span><span class="p">,</span> <span class="n">action_dim</span><span class="p">)</span>

    <span class="c1"># í•™ìŠµ</span>
    <span class="n">rewards</span> <span class="o">=</span> <span class="n">train_dqn</span><span class="p">(</span><span class="n">env</span><span class="p">,</span> <span class="n">agent</span><span class="p">,</span> <span class="n">episodes</span><span class="o">=</span><span class="mi">500</span><span class="p">)</span>

    <span class="c1"># ê²°ê³¼ ì‹œê°í™”</span>
    <span class="kn">import</span><span class="w"> </span><span class="nn">matplotlib.pyplot</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">plt</span>

    <span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">10</span><span class="p">,</span> <span class="mi">5</span><span class="p">))</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">rewards</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s1">&#39;Episode&#39;</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s1">&#39;Reward&#39;</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s1">&#39;DQN on CartPole-v1&#39;</span><span class="p">)</span>

    <span class="c1"># Moving average</span>
    <span class="n">window</span> <span class="o">=</span> <span class="mi">50</span>
    <span class="n">ma</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">convolve</span><span class="p">(</span><span class="n">rewards</span><span class="p">,</span> <span class="n">np</span><span class="o">.</span><span class="n">ones</span><span class="p">(</span><span class="n">window</span><span class="p">)</span><span class="o">/</span><span class="n">window</span><span class="p">,</span> <span class="n">mode</span><span class="o">=</span><span class="s1">&#39;valid&#39;</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="nb">range</span><span class="p">(</span><span class="n">window</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="nb">len</span><span class="p">(</span><span class="n">rewards</span><span class="p">)),</span> <span class="n">ma</span><span class="p">,</span> <span class="s1">&#39;r-&#39;</span><span class="p">,</span> <span class="n">linewidth</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>

    <span class="n">plt</span><span class="o">.</span><span class="n">savefig</span><span class="p">(</span><span class="s1">&#39;cartpole_dqn.png&#39;</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">close</span><span class="p">()</span>

    <span class="n">env</span><span class="o">.</span><span class="n">close</span><span class="p">()</span>
    <span class="k">return</span> <span class="n">rewards</span>
</code></pre></div>

<hr />
<h2 id="9">9. ì•Œê³ ë¦¬ì¦˜ ë¹„êµ<a class="header-link" href="#9" title="Permanent link">&para;</a></h2>
<h3 id="_10">ì£¼ìš” ì•Œê³ ë¦¬ì¦˜ íŠ¹ì„±<a class="header-link" href="#_10" title="Permanent link">&para;</a></h3>
<table>
<thead>
<tr>
<th>ì•Œê³ ë¦¬ì¦˜</th>
<th>ìœ í˜•</th>
<th>On/Off-Policy</th>
<th>íŠ¹ì§•</th>
</tr>
</thead>
<tbody>
<tr>
<td>Q-Learning</td>
<td>Value-based</td>
<td>Off-policy</td>
<td>í…Œì´ë¸”, ê°„ë‹¨</td>
</tr>
<tr>
<td>DQN</td>
<td>Value-based</td>
<td>Off-policy</td>
<td>ì‹ ê²½ë§, ê²½í—˜ ì¬í˜„</td>
</tr>
<tr>
<td>REINFORCE</td>
<td>Policy-based</td>
<td>On-policy</td>
<td>Monte Carlo, ë†’ì€ ë¶„ì‚°</td>
</tr>
<tr>
<td>A2C/A3C</td>
<td>Actor-Critic</td>
<td>On-policy</td>
<td>Advantage, ë³‘ë ¬í™”</td>
</tr>
<tr>
<td>PPO</td>
<td>Actor-Critic</td>
<td>On-policy</td>
<td>ì•ˆì •ì , ì‹¤ìš©ì </td>
</tr>
<tr>
<td>SAC</td>
<td>Actor-Critic</td>
<td>Off-policy</td>
<td>ì—°ì† í–‰ë™, entropy</td>
</tr>
</tbody>
</table>
<h3 id="_11">ì„ íƒ ê°€ì´ë“œ<a class="header-link" href="#_11" title="Permanent link">&para;</a></h3>
<div class="highlight"><pre><span></span><code>ì´ì‚° í–‰ë™ ê³µê°„:
- ê°„ë‹¨í•œ ë¬¸ì œ: DQN
- ë³µì¡í•œ ë¬¸ì œ: PPO

ì—°ì† í–‰ë™ ê³µê°„:
- ì•ˆì •ì : SAC
- ë¹ ë¥¸ í•™ìŠµ: PPO

ìì› ì œí•œ:
- A2C (ë‹¨ì¼ ë¨¸ì‹ )

ëŒ€ê·œëª¨ ë³‘ë ¬:
- A3C, PPO
</code></pre></div>

<hr />
<h2 id="10">10. ì‹¬í™” ì£¼ì œ ê°œìš”<a class="header-link" href="#10" title="Permanent link">&para;</a></h2>
<h3 id="double-dqn">Double DQN<a class="header-link" href="#double-dqn" title="Permanent link">&para;</a></h3>
<div class="highlight"><pre><span></span><code><span class="c1"># DQN ë¬¸ì œ: Q ê°’ ê³¼ëŒ€í‰ê°€</span>
<span class="c1"># í•´ê²°: í–‰ë™ ì„ íƒê³¼ í‰ê°€ë¥¼ ë‹¤ë¥¸ ë„¤íŠ¸ì›Œí¬ë¡œ</span>

<span class="c1"># ê¸°ì¡´ DQN:</span>
<span class="n">target_q</span> <span class="o">=</span> <span class="n">reward</span> <span class="o">+</span> <span class="n">gamma</span> <span class="o">*</span> <span class="n">target_net</span><span class="p">(</span><span class="n">next_state</span><span class="p">)</span><span class="o">.</span><span class="n">max</span><span class="p">()</span>

<span class="c1"># Double DQN:</span>
<span class="n">best_action</span> <span class="o">=</span> <span class="n">q_net</span><span class="p">(</span><span class="n">next_state</span><span class="p">)</span><span class="o">.</span><span class="n">argmax</span><span class="p">()</span>
<span class="n">target_q</span> <span class="o">=</span> <span class="n">reward</span> <span class="o">+</span> <span class="n">gamma</span> <span class="o">*</span> <span class="n">target_net</span><span class="p">(</span><span class="n">next_state</span><span class="p">)[</span><span class="n">best_action</span><span class="p">]</span>
</code></pre></div>

<h3 id="dueling-dqn">Dueling DQN<a class="header-link" href="#dueling-dqn" title="Permanent link">&para;</a></h3>
<div class="highlight"><pre><span></span><code><span class="c1"># Q = V + A (Value + Advantage)</span>
<span class="c1"># ìƒíƒœì˜ ê°€ì¹˜ì™€ í–‰ë™ì˜ ì´ì ì„ ë¶„ë¦¬</span>

<span class="k">class</span><span class="w"> </span><span class="nc">DuelingNetwork</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="k">def</span><span class="w"> </span><span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">state_dim</span><span class="p">,</span> <span class="n">action_dim</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">feature</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Sequential</span><span class="p">(</span>
            <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">state_dim</span><span class="p">,</span> <span class="mi">128</span><span class="p">),</span>
            <span class="n">nn</span><span class="o">.</span><span class="n">ReLU</span><span class="p">()</span>
        <span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">value</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="mi">128</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">advantage</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="mi">128</span><span class="p">,</span> <span class="n">action_dim</span><span class="p">)</span>

    <span class="k">def</span><span class="w"> </span><span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
        <span class="n">f</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">feature</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="n">v</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">value</span><span class="p">(</span><span class="n">f</span><span class="p">)</span>
        <span class="n">a</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">advantage</span><span class="p">(</span><span class="n">f</span><span class="p">)</span>
        <span class="n">q</span> <span class="o">=</span> <span class="n">v</span> <span class="o">+</span> <span class="n">a</span> <span class="o">-</span> <span class="n">a</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">dim</span><span class="o">=-</span><span class="mi">1</span><span class="p">,</span> <span class="n">keepdim</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">q</span>
</code></pre></div>

<h3 id="prioritized-experience-replay">Prioritized Experience Replay<a class="header-link" href="#prioritized-experience-replay" title="Permanent link">&para;</a></h3>
<div class="highlight"><pre><span></span><code><span class="c1"># ì¤‘ìš”í•œ ê²½í—˜(TD Error í°)ì„ ë” ìì£¼ ìƒ˜í”Œë§</span>
<span class="c1"># P(i) âˆ |TD_error_i|^Î±</span>

<span class="c1"># êµ¬í˜„ ì‹œ Sum Tree ìë£Œêµ¬ì¡° ì‚¬ìš©</span>
</code></pre></div>

<hr />
<h2 id="_12">ì •ë¦¬<a class="header-link" href="#_12" title="Permanent link">&para;</a></h2>
<h3 id="_13">í•µì‹¬ ê°œë…<a class="header-link" href="#_13" title="Permanent link">&para;</a></h3>
<ol>
<li><strong>MDP</strong>: ìƒíƒœ, í–‰ë™, ë³´ìƒ, ì „ì´ë¡œ ë¬¸ì œ ì •ì˜</li>
<li><strong>Bellman Equation</strong>: í˜„ì¬ ê°€ì¹˜ = ì¦‰ê° ë³´ìƒ + ë¯¸ë˜ ê°€ì¹˜</li>
<li><strong>Q-Learning</strong>: TDë¡œ Q í•¨ìˆ˜ í•™ìŠµ</li>
<li><strong>DQN</strong>: ì‹ ê²½ë§ + ê²½í—˜ ì¬í˜„ + íƒ€ê²Ÿ ë„¤íŠ¸ì›Œí¬</li>
<li><strong>Policy Gradient</strong>: ì •ì±… ì§ì ‘ ìµœì í™”</li>
<li><strong>Actor-Critic</strong>: Actor + Criticìœ¼ë¡œ ë¶„ì‚° ê°ì†Œ</li>
</ol>
<h3 id="_14">ì‹¤ì „ íŒ<a class="header-link" href="#_14" title="Permanent link">&para;</a></h3>
<div class="highlight"><pre><span></span><code><span class="c1"># 1. ë³´ìƒ ì„¤ê³„ê°€ í•µì‹¬</span>
<span class="c1"># - Sparse reward â†’ í•™ìŠµ ì–´ë ¤ì›€</span>
<span class="c1"># - Shaped reward â†’ í•™ìŠµ ë„ì›€ (but í¸í–¥ ê°€ëŠ¥)</span>

<span class="c1"># 2. í•˜ì´í¼íŒŒë¼ë¯¸í„° íŠœë‹</span>
<span class="c1"># - Learning rate: 1e-4 ~ 1e-3</span>
<span class="c1"># - Gamma: 0.99</span>
<span class="c1"># - Epsilon decay: ì²œì²œíˆ</span>

<span class="c1"># 3. ë””ë²„ê¹…</span>
<span class="c1"># - Reward ê³¡ì„  í™•ì¸</span>
<span class="c1"># - Q ê°’ ë¶„í¬ ëª¨ë‹ˆí„°ë§</span>
<span class="c1"># - í•™ìŠµëœ ì •ì±… ì‹œê°í™”</span>
</code></pre></div>

<hr />
<h2 id="_15">ì°¸ê³  ìë£Œ<a class="header-link" href="#_15" title="Permanent link">&para;</a></h2>
<ul>
<li>Sutton &amp; Barto: http://incompleteideas.net/book/the-book.html</li>
<li>DQN: https://arxiv.org/abs/1312.5602</li>
<li>Policy Gradient: https://proceedings.neurips.cc/paper/1999/file/464d828b85b0bed98e80ade0a5c43b0f-Paper.pdf</li>
<li>OpenAI Spinning Up: https://spinningup.openai.com/</li>
<li>Gymnasium: https://gymnasium.farama.org/</li>
</ul>
    </div>

    
<div class="lesson-toolbar lesson-toolbar--bottom">
    <div class="toolbar-nav toolbar-nav--prev">
        
        <a href="/study/ko/Deep_Learning/41_Model_Saving_Deployment.html" class="nav-prev">
            <span class="nav-label">Previous</span>
            <span class="nav-title">41. ëª¨ë¸ ì €ì¥ ë° ë°°í¬</span>
        </a>
        
    </div>
    <div class="toolbar-actions">
        <button class="btn btn-copy-link" data-action="copy-link" title="Copy link">
            <span class="icon">ğŸ”—</span>
            <span class="text">Copy link</span>
        </button>
        <a href="/study/ko/Deep_Learning/" class="btn btn-topic-link" title="Back to topic list">
            <span class="icon">ğŸ“‹</span>
            <span class="text">Topic list</span>
        </a>
    </div>
    <div class="toolbar-nav toolbar-nav--next">
        
    </div>
</div>


    <div class="toolbar-keyboard-hint">
        <kbd>&larr;</kbd> <kbd>&rarr;</kbd> to navigate between lessons
    </div>
</article>

<!-- Scroll to top floating button -->
<button class="scroll-to-top" id="scroll-to-top" title="Scroll to top" aria-label="Scroll to top">â†‘</button>

            </div>
        </main>
    </div>

    <script src="/study/static/js/app.js"></script>
    <script>
        function switchLanguage(newLang) {
            const path = window.location.pathname;
            const base = '/study';
            const currentLang = 'ko';
            const newPath = path.replace(base + '/' + currentLang + '/', base + '/' + newLang + '/');
            window.location.href = newPath + window.location.search;
        }
    </script>
    
<script>
    document.addEventListener('DOMContentLoaded', function() {
        // Copy link - independent feedback per button
        document.querySelectorAll('[data-action="copy-link"]').forEach(function(btn) {
            btn.addEventListener('click', function() {
                navigator.clipboard.writeText(window.location.href);
                var textEl = this.querySelector('.text');
                var originalText = textEl.textContent;
                textEl.textContent = 'Copied!';
                setTimeout(function() { textEl.textContent = originalText; }, 2000);
            });
        });

        // Add copy buttons to code blocks
        document.querySelectorAll('.lesson-content pre').forEach(function(pre) {
            var copyBtn = document.createElement('button');
            copyBtn.className = 'code-copy-btn';
            copyBtn.textContent = 'Copy';
            copyBtn.addEventListener('click', function() {
                var code = pre.querySelector('code');
                var text = code ? code.textContent : pre.textContent;
                navigator.clipboard.writeText(text);
                copyBtn.textContent = 'Copied!';
                setTimeout(function() { copyBtn.textContent = 'Copy'; }, 2000);
            });
            pre.style.position = 'relative';
            pre.appendChild(copyBtn);
        });

        // Scroll to top button
        var scrollBtn = document.getElementById('scroll-to-top');
        window.addEventListener('scroll', function() {
            scrollBtn.classList.toggle('visible', window.scrollY > 300);
        });
        scrollBtn.addEventListener('click', function() {
            window.scrollTo({ top: 0, behavior: 'smooth' });
        });

        // Keyboard shortcuts: left/right arrows for lesson navigation
        document.addEventListener('keydown', function(e) {
            var tag = document.activeElement.tagName;
            if (tag === 'INPUT' || tag === 'TEXTAREA' || tag === 'SELECT') return;

            if (e.key === 'ArrowLeft') {
                var prevLink = document.querySelector('.nav-prev');
                if (prevLink) prevLink.click();
            } else if (e.key === 'ArrowRight') {
                var nextLink = document.querySelector('.nav-next');
                if (nextLink) nextLink.click();
            }
        });
    });
</script>

</body>
</html>
{% endraw %}