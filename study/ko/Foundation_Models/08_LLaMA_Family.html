{% raw %}
<!DOCTYPE html>
<html lang="ko" data-theme="light">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>LLaMA Family - Study Materials</title>
    <link rel="stylesheet" href="/study/static/css/style.css">
    <link rel="stylesheet" href="/study/static/css/highlight.css">
    <!-- KaTeX for LaTeX math rendering -->
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.11/dist/katex.min.css">
    <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.11/dist/katex.min.js"></script>
    <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.11/dist/contrib/auto-render.min.js"
        onload="renderMathInElement(document.body, {
            delimiters: [
                {left: '$$', right: '$$', display: true},
                {left: '$', right: '$', display: false}
            ],
            throwOnError: false
        });"></script>
    
</head>
<body>
    <div class="app-container">
        <!-- Sidebar -->
        <aside class="sidebar" id="sidebar">
            <div class="sidebar-header">
                <a href="/study/ko/" class="logo">Study Materials</a>
                <button class="sidebar-toggle" id="sidebar-toggle" aria-label="Toggle sidebar">
                    <span></span>
                </button>
            </div>

            <nav class="sidebar-nav">
                <a href="/study/ko/" class="nav-item ">
                    <span class="nav-icon">ğŸ </span>
                    <span class="nav-text">Home</span>
                </a>
                <a href="/study/examples/" class="nav-item ">
                    <span class="nav-icon">ğŸ’»</span>
                    <span class="nav-text">Examples</span>
                </a>
            </nav>

            <div class="sidebar-search">
                <form action="/study/ko/search.html" method="get" id="search-form">
                    <input type="search" name="q" placeholder="Search..." id="search-sidebar-input">
                </form>
            </div>

            <!-- Language Selector -->
            <div class="sidebar-lang">
                <select id="lang-select" class="lang-selector" onchange="switchLanguage(this.value)">
                    
                    <option value="en" >
                        English
                    </option>
                    
                    <option value="ko" selected>
                        í•œêµ­ì–´
                    </option>
                    
                </select>
            </div>

            <div class="sidebar-footer">
                <button class="theme-toggle" id="theme-toggle" aria-label="Toggle theme">
                    <span class="theme-icon light">â˜€ï¸</span>
                    <span class="theme-icon dark">ğŸŒ™</span>
                </button>
            </div>
        </aside>

        <!-- Main Content -->
        <main class="main-content">
            <header class="main-header">
                <button class="menu-toggle" id="menu-toggle" aria-label="Open menu">
                    <span></span>
                </button>
                
<nav class="breadcrumb">
    <a href="/study/ko/">Topics</a>
    <span class="separator">/</span>
    <a href="/study/ko/Foundation_Models/">Foundation Models</a>
    <span class="separator">/</span>
    <span class="current">LLaMA Family</span>
</nav>

            </header>

            <div class="content">
                
<article class="lesson-article">
    <header class="lesson-header">
        <h1>LLaMA Family</h1>
    </header>

    
<div class="lesson-toolbar lesson-toolbar--top">
    <div class="toolbar-nav toolbar-nav--prev">
        
        <a href="/study/ko/Foundation_Models/07_Tokenization_Deep_Dive.html" class="nav-prev">
            <span class="nav-label">Previous</span>
            <span class="nav-title">07. Tokenization ì‹¬í™”</span>
        </a>
        
    </div>
    <div class="toolbar-actions">
        <button class="btn btn-copy-link" data-action="copy-link" title="Copy link">
            <span class="icon">ğŸ”—</span>
            <span class="text">Copy link</span>
        </button>
        <a href="/study/ko/Foundation_Models/" class="btn btn-topic-link" title="Back to topic list">
            <span class="icon">ğŸ“‹</span>
            <span class="text">Topic list</span>
        </a>
    </div>
    <div class="toolbar-nav toolbar-nav--next">
        
        <a href="/study/ko/Foundation_Models/09_Mistral_MoE.html" class="nav-next">
            <span class="nav-label">Next</span>
            <span class="nav-title">Mistral & Mixture of Experts</span>
        </a>
        
    </div>
</div>


    
    <nav class="toc" id="toc">
        <h2>Table of Contents</h2>
        <div class="toc">
<ul>
<li><a href="#_1">í•™ìŠµ ëª©í‘œ</a></li>
<li><a href="#1-llama">1. LLaMA ê°œìš”</a><ul>
<li><a href="#11-llama">1.1 LLaMAì˜ ì˜ì˜</a></li>
<li><a href="#12">1.2 ë²„ì „ ë¹„êµ</a></li>
</ul>
</li>
<li><a href="#2-llama">2. LLaMA ì•„í‚¤í…ì²˜</a><ul>
<li><a href="#21">2.1 í•µì‹¬ êµ¬ì„± ìš”ì†Œ</a></li>
<li><a href="#22">2.2 í•˜ì´í¼íŒŒë¼ë¯¸í„°</a></li>
</ul>
</li>
<li><a href="#3-rope-rotary-position-embedding">3. RoPE (Rotary Position Embedding)</a><ul>
<li><a href="#31">3.1 ê°œë…</a></li>
<li><a href="#32">3.2 ìˆ˜í•™ì  ì´í•´</a></li>
<li><a href="#33-rope">3.3 RoPEì˜ ì¥ì </a></li>
</ul>
</li>
<li><a href="#4-rmsnorm">4. RMSNorm</a><ul>
<li><a href="#41">4.1 ê°œë…</a></li>
<li><a href="#42">4.2 êµ¬í˜„</a></li>
</ul>
</li>
<li><a href="#5-swiglu">5. SwiGLU</a><ul>
<li><a href="#51">5.1 ê°œë…</a></li>
<li><a href="#52">5.2 êµ¬í˜„</a></li>
</ul>
</li>
<li><a href="#6-grouped-query-attention-gqa">6. Grouped Query Attention (GQA)</a><ul>
<li><a href="#61">6.1 ê°œë…</a></li>
<li><a href="#62">6.2 êµ¬í˜„</a></li>
</ul>
</li>
<li><a href="#7-llama">7. LLaMA ì‹¤ìŠµ</a><ul>
<li><a href="#71-huggingface">7.1 HuggingFaceë¡œ ì‚¬ìš©í•˜ê¸°</a></li>
<li><a href="#72">7.2 ì–‘ìí™”ë¡œ íš¨ìœ¨ì  ì‚¬ìš©</a></li>
<li><a href="#73-llama-3">7.3 LLaMA 3 ì‚¬ìš©</a></li>
</ul>
</li>
<li><a href="#8-llama-3132">8. LLaMA 3.1/3.2 ìƒì„¸</a><ul>
<li><a href="#81-llama-31-2024-7">8.1 LLaMA 3.1 (2024ë…„ 7ì›”)</a></li>
<li><a href="#82-llama-32-2024-9">8.2 LLaMA 3.2 (2024ë…„ 9ì›”)</a></li>
</ul>
</li>
<li><a href="#_2">ì •ë¦¬</a><ul>
<li><a href="#llama">LLaMA í•µì‹¬ ê¸°ìˆ </a></li>
<li><a href="#_3">ì‹¤ë¬´ ê¶Œì¥ ì‚¬í•­</a></li>
<li><a href="#_4">ë‹¤ìŒ ë‹¨ê³„</a></li>
</ul>
</li>
<li><a href="#_5">ì°¸ê³  ìë£Œ</a><ul>
<li><a href="#_6">í•µì‹¬ ë…¼ë¬¸</a></li>
<li><a href="#_7">ì½”ë“œ &amp; ìë£Œ</a></li>
</ul>
</li>
</ul>
</div>

    </nav>
    

    <div class="lesson-content markdown-body">
        <h1 id="llama-family">LLaMA Family<a class="header-link" href="#llama-family" title="Permanent link">&para;</a></h1>
<h2 id="_1">í•™ìŠµ ëª©í‘œ<a class="header-link" href="#_1" title="Permanent link">&para;</a></h2>
<ul>
<li>LLaMA 1/2/3ì˜ ì•„í‚¤í…ì²˜ ì§„í™” ì´í•´</li>
<li>RoPE, RMSNorm, SwiGLU ë“± í•µì‹¬ ê¸°ìˆ  ìŠµë“</li>
<li>Grouped Query Attention (GQA) ë©”ì»¤ë‹ˆì¦˜ íŒŒì•…</li>
<li>ì‹¤ë¬´ì—ì„œì˜ LLaMA í™œìš©ë²• í•™ìŠµ</li>
</ul>
<hr />
<h2 id="1-llama">1. LLaMA ê°œìš”<a class="header-link" href="#1-llama" title="Permanent link">&para;</a></h2>
<h3 id="11-llama">1.1 LLaMAì˜ ì˜ì˜<a class="header-link" href="#11-llama" title="Permanent link">&para;</a></h3>
<p><strong>LLaMA</strong>(Large Language Model Meta AI)ëŠ” 2023ë…„ Metaê°€ ê³µê°œí•œ ì˜¤í”ˆì†ŒìŠ¤ LLMìœ¼ë¡œ, Foundation Model ì—°êµ¬ì˜ ë¯¼ì£¼í™”ë¥¼ ì´ëŒì—ˆìŠµë‹ˆë‹¤.</p>
<div class="highlight"><pre><span></span><code>â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    LLaMAì˜ ì—­ì‚¬ì  ì˜ì˜                            â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                 â”‚
â”‚  Before LLaMA (2022):                                           â”‚
â”‚  â€¢ ìµœê³  ì„±ëŠ¥ ëª¨ë¸ì€ APIë§Œ ì œê³µ (GPT-3.5, PaLM)                    â”‚
â”‚  â€¢ í•™ìˆ  ì—°êµ¬ìš© ëª¨ë¸ì€ ì„±ëŠ¥ ë¶€ì¡±                                    â”‚
â”‚  â€¢ ì˜¤í”ˆì†ŒìŠ¤ ì»¤ë®¤ë‹ˆí‹°ì˜ LLM ì ‘ê·¼ ì œí•œì                              â”‚
â”‚                                                                 â”‚
â”‚  After LLaMA (2023):                                            â”‚
â”‚  â€¢ ì—°êµ¬ìë“¤ì´ ìµœì²¨ë‹¨ ëª¨ë¸ ì§ì ‘ ì‹¤í—˜ ê°€ëŠ¥                           â”‚
â”‚  â€¢ Alpaca, Vicuna ë“± íŒŒìƒ ëª¨ë¸ í­ë°œì  ì¦ê°€                        â”‚
â”‚  â€¢ LLM ì—°êµ¬ ì†ë„ ê¸‰ê²©íˆ ê°€ì†í™”                                    â”‚
â”‚                                                                 â”‚
â”‚  í•µì‹¬ ê¸°ì—¬:                                                       â”‚
â”‚  â€¢ Chinchilla ê·œì¹™ ì ìš© (D=20N ì´ìƒ)                             â”‚
â”‚  â€¢ íš¨ìœ¨ì  ì•„í‚¤í…ì²˜ ì„ íƒ ê²€ì¦                                      â”‚
â”‚  â€¢ í•™ìŠµ ë°ì´í„° êµ¬ì„± ê³µê°œ                                          â”‚
â”‚                                                                 â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
</code></pre></div>

<h3 id="12">1.2 ë²„ì „ ë¹„êµ<a class="header-link" href="#12" title="Permanent link">&para;</a></h3>
<table>
<thead>
<tr>
<th>íŠ¹ì„±</th>
<th>LLaMA 1</th>
<th>LLaMA 2</th>
<th>LLaMA 3</th>
<th>LLaMA 3.1</th>
<th>LLaMA 3.2</th>
</tr>
</thead>
<tbody>
<tr>
<td>ì¶œì‹œ</td>
<td>2023.02</td>
<td>2023.07</td>
<td>2024.04</td>
<td>2024.07</td>
<td>2024.09</td>
</tr>
<tr>
<td>í¬ê¸°</td>
<td>7/13/33/65B</td>
<td>7/13/70B</td>
<td>8/70B</td>
<td>8/70/405B</td>
<td>1/3/11/90B</td>
</tr>
<tr>
<td>í† í°</td>
<td>1.4T</td>
<td>2T</td>
<td>15T+</td>
<td>15T+</td>
<td>15T+</td>
</tr>
<tr>
<td>Context</td>
<td>2K</td>
<td>4K</td>
<td>8K</td>
<td>128K</td>
<td>128K</td>
</tr>
<tr>
<td>License</td>
<td>ì—°êµ¬ìš©</td>
<td>ìƒì—…ì  (ì¡°ê±´ë¶€)</td>
<td>ìƒì—…ì  (ì™„í™”)</td>
<td>ìƒì—…ì  (ì™„í™”)</td>
<td>ìƒì—…ì  (ì™„í™”)</td>
</tr>
<tr>
<td>GQA</td>
<td>âŒ</td>
<td>âœ… (70B)</td>
<td>âœ… (ì „ì²´)</td>
<td>âœ… (ì „ì²´)</td>
<td>âœ… (ì „ì²´)</td>
</tr>
<tr>
<td>íŠ¹ì§•</td>
<td>ê¸°ë³¸ ì•„í‚¤í…ì²˜</td>
<td>RLHF, Safety</td>
<td>ê°œì„ ëœ ì¶”ë¡ </td>
<td>128K ë„¤ì´í‹°ë¸Œ, Tool Use</td>
<td>ë¹„ì „ ëª¨ë¸ ì¶”ê°€</td>
</tr>
</tbody>
</table>
<blockquote>
<p><strong>LLaMA 3.1/3.2 ì£¼ìš” ì—…ë°ì´íŠ¸</strong> (2024):
- <strong>LLaMA 3.1</strong>: 128K ë„¤ì´í‹°ë¸Œ ì»¨í…ìŠ¤íŠ¸, 405B í”Œë˜ê·¸ì‹­ ëª¨ë¸, Tool Use ê¸°ëŠ¥
- <strong>LLaMA 3.2</strong>: ê²½ëŸ‰ ëª¨ë¸(1B/3B)ê³¼ ë¹„ì „ ëª¨ë¸(11B/90B) ì¶”ê°€</p>
</blockquote>
<hr />
<h2 id="2-llama">2. LLaMA ì•„í‚¤í…ì²˜<a class="header-link" href="#2-llama" title="Permanent link">&para;</a></h2>
<h3 id="21">2.1 í•µì‹¬ êµ¬ì„± ìš”ì†Œ<a class="header-link" href="#21" title="Permanent link">&para;</a></h3>
<div class="highlight"><pre><span></span><code>â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    LLaMA Architecture                            â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                 â”‚
â”‚  Input Tokens                                                   â”‚
â”‚       â”‚                                                         â”‚
â”‚       â–¼                                                         â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                    â”‚
â”‚  â”‚         Token Embedding                 â”‚                    â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                    â”‚
â”‚       â”‚                                                         â”‚
â”‚       â–¼                                                         â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                    â”‚
â”‚  â”‚     RoPE (Rotary Position Embedding)    â”‚  â† ìœ„ì¹˜ ì •ë³´        â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                    â”‚
â”‚       â”‚                                                         â”‚
â”‚  â”Œâ”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                    â”‚
â”‚  â”‚         Transformer Block Ã— N           â”‚                    â”‚
â”‚  â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚                    â”‚
â”‚  â”‚  â”‚  RMSNorm (Pre-normalization)      â”‚  â”‚  â† LayerNorm ëŒ€ì²´   â”‚
â”‚  â”‚  â”‚            â†“                      â”‚  â”‚                    â”‚
â”‚  â”‚  â”‚  Grouped Query Attention (GQA)    â”‚  â”‚  â† KV Cache íš¨ìœ¨   â”‚
â”‚  â”‚  â”‚            â†“                      â”‚  â”‚                    â”‚
â”‚  â”‚  â”‚  Residual Connection              â”‚  â”‚                    â”‚
â”‚  â”‚  â”‚            â†“                      â”‚  â”‚                    â”‚
â”‚  â”‚  â”‚  RMSNorm                          â”‚  â”‚                    â”‚
â”‚  â”‚  â”‚            â†“                      â”‚  â”‚                    â”‚
â”‚  â”‚  â”‚  SwiGLU FFN                       â”‚  â”‚  â† GELU ëŒ€ì²´        â”‚
â”‚  â”‚  â”‚            â†“                      â”‚  â”‚                    â”‚
â”‚  â”‚  â”‚  Residual Connection              â”‚  â”‚                    â”‚
â”‚  â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚                    â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                    â”‚
â”‚       â”‚                                                         â”‚
â”‚       â–¼                                                         â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                    â”‚
â”‚  â”‚         RMSNorm â†’ Linear â†’ Vocab        â”‚                    â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                    â”‚
â”‚       â”‚                                                         â”‚
â”‚       â–¼                                                         â”‚
â”‚  Output Logits                                                  â”‚
â”‚                                                                 â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
</code></pre></div>

<h3 id="22">2.2 í•˜ì´í¼íŒŒë¼ë¯¸í„°<a class="header-link" href="#22" title="Permanent link">&para;</a></h3>
<div class="highlight"><pre><span></span><code><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">LLaMA ëª¨ë¸ ì‚¬ì–‘ ë¹„êµ</span>
<span class="sd">&quot;&quot;&quot;</span>

<span class="n">LLAMA_CONFIGS</span> <span class="o">=</span> <span class="p">{</span>
    <span class="s2">&quot;llama-7b&quot;</span><span class="p">:</span> <span class="p">{</span>
        <span class="s2">&quot;dim&quot;</span><span class="p">:</span> <span class="mi">4096</span><span class="p">,</span>
        <span class="s2">&quot;n_layers&quot;</span><span class="p">:</span> <span class="mi">32</span><span class="p">,</span>
        <span class="s2">&quot;n_heads&quot;</span><span class="p">:</span> <span class="mi">32</span><span class="p">,</span>
        <span class="s2">&quot;n_kv_heads&quot;</span><span class="p">:</span> <span class="mi">32</span><span class="p">,</span>  <span class="c1"># MHA (GQA ë¯¸ì‚¬ìš©)</span>
        <span class="s2">&quot;vocab_size&quot;</span><span class="p">:</span> <span class="mi">32000</span><span class="p">,</span>
        <span class="s2">&quot;ffn_dim&quot;</span><span class="p">:</span> <span class="mi">11008</span><span class="p">,</span>  <span class="c1"># ì•½ 2.67 Ã— dim</span>
        <span class="s2">&quot;context_length&quot;</span><span class="p">:</span> <span class="mi">2048</span><span class="p">,</span>
    <span class="p">},</span>
    <span class="s2">&quot;llama-13b&quot;</span><span class="p">:</span> <span class="p">{</span>
        <span class="s2">&quot;dim&quot;</span><span class="p">:</span> <span class="mi">5120</span><span class="p">,</span>
        <span class="s2">&quot;n_layers&quot;</span><span class="p">:</span> <span class="mi">40</span><span class="p">,</span>
        <span class="s2">&quot;n_heads&quot;</span><span class="p">:</span> <span class="mi">40</span><span class="p">,</span>
        <span class="s2">&quot;n_kv_heads&quot;</span><span class="p">:</span> <span class="mi">40</span><span class="p">,</span>
        <span class="s2">&quot;vocab_size&quot;</span><span class="p">:</span> <span class="mi">32000</span><span class="p">,</span>
        <span class="s2">&quot;ffn_dim&quot;</span><span class="p">:</span> <span class="mi">13824</span><span class="p">,</span>
        <span class="s2">&quot;context_length&quot;</span><span class="p">:</span> <span class="mi">2048</span><span class="p">,</span>
    <span class="p">},</span>
    <span class="s2">&quot;llama-70b&quot;</span><span class="p">:</span> <span class="p">{</span>
        <span class="s2">&quot;dim&quot;</span><span class="p">:</span> <span class="mi">8192</span><span class="p">,</span>
        <span class="s2">&quot;n_layers&quot;</span><span class="p">:</span> <span class="mi">80</span><span class="p">,</span>
        <span class="s2">&quot;n_heads&quot;</span><span class="p">:</span> <span class="mi">64</span><span class="p">,</span>
        <span class="s2">&quot;n_kv_heads&quot;</span><span class="p">:</span> <span class="mi">8</span><span class="p">,</span>  <span class="c1"># GQA! 8ê°œ KV heads</span>
        <span class="s2">&quot;vocab_size&quot;</span><span class="p">:</span> <span class="mi">32000</span><span class="p">,</span>
        <span class="s2">&quot;ffn_dim&quot;</span><span class="p">:</span> <span class="mi">28672</span><span class="p">,</span>
        <span class="s2">&quot;context_length&quot;</span><span class="p">:</span> <span class="mi">4096</span><span class="p">,</span>
    <span class="p">},</span>
    <span class="s2">&quot;llama3-8b&quot;</span><span class="p">:</span> <span class="p">{</span>
        <span class="s2">&quot;dim&quot;</span><span class="p">:</span> <span class="mi">4096</span><span class="p">,</span>
        <span class="s2">&quot;n_layers&quot;</span><span class="p">:</span> <span class="mi">32</span><span class="p">,</span>
        <span class="s2">&quot;n_heads&quot;</span><span class="p">:</span> <span class="mi">32</span><span class="p">,</span>
        <span class="s2">&quot;n_kv_heads&quot;</span><span class="p">:</span> <span class="mi">8</span><span class="p">,</span>  <span class="c1"># GQA</span>
        <span class="s2">&quot;vocab_size&quot;</span><span class="p">:</span> <span class="mi">128256</span><span class="p">,</span>  <span class="c1"># í™•ì¥ëœ vocab</span>
        <span class="s2">&quot;ffn_dim&quot;</span><span class="p">:</span> <span class="mi">14336</span><span class="p">,</span>
        <span class="s2">&quot;context_length&quot;</span><span class="p">:</span> <span class="mi">8192</span><span class="p">,</span>
    <span class="p">},</span>
    <span class="s2">&quot;llama3-70b&quot;</span><span class="p">:</span> <span class="p">{</span>
        <span class="s2">&quot;dim&quot;</span><span class="p">:</span> <span class="mi">8192</span><span class="p">,</span>
        <span class="s2">&quot;n_layers&quot;</span><span class="p">:</span> <span class="mi">80</span><span class="p">,</span>
        <span class="s2">&quot;n_heads&quot;</span><span class="p">:</span> <span class="mi">64</span><span class="p">,</span>
        <span class="s2">&quot;n_kv_heads&quot;</span><span class="p">:</span> <span class="mi">8</span><span class="p">,</span>  <span class="c1"># GQA</span>
        <span class="s2">&quot;vocab_size&quot;</span><span class="p">:</span> <span class="mi">128256</span><span class="p">,</span>
        <span class="s2">&quot;ffn_dim&quot;</span><span class="p">:</span> <span class="mi">28672</span><span class="p">,</span>
        <span class="s2">&quot;context_length&quot;</span><span class="p">:</span> <span class="mi">8192</span><span class="p">,</span>
    <span class="p">},</span>
    <span class="c1"># LLaMA 3.1 (2024.07)</span>
    <span class="s2">&quot;llama3.1-8b&quot;</span><span class="p">:</span> <span class="p">{</span>
        <span class="s2">&quot;dim&quot;</span><span class="p">:</span> <span class="mi">4096</span><span class="p">,</span>
        <span class="s2">&quot;n_layers&quot;</span><span class="p">:</span> <span class="mi">32</span><span class="p">,</span>
        <span class="s2">&quot;n_heads&quot;</span><span class="p">:</span> <span class="mi">32</span><span class="p">,</span>
        <span class="s2">&quot;n_kv_heads&quot;</span><span class="p">:</span> <span class="mi">8</span><span class="p">,</span>
        <span class="s2">&quot;vocab_size&quot;</span><span class="p">:</span> <span class="mi">128256</span><span class="p">,</span>
        <span class="s2">&quot;ffn_dim&quot;</span><span class="p">:</span> <span class="mi">14336</span><span class="p">,</span>
        <span class="s2">&quot;context_length&quot;</span><span class="p">:</span> <span class="mi">131072</span><span class="p">,</span>  <span class="c1"># 128K ë„¤ì´í‹°ë¸Œ</span>
    <span class="p">},</span>
    <span class="s2">&quot;llama3.1-405b&quot;</span><span class="p">:</span> <span class="p">{</span>
        <span class="s2">&quot;dim&quot;</span><span class="p">:</span> <span class="mi">16384</span><span class="p">,</span>
        <span class="s2">&quot;n_layers&quot;</span><span class="p">:</span> <span class="mi">126</span><span class="p">,</span>
        <span class="s2">&quot;n_heads&quot;</span><span class="p">:</span> <span class="mi">128</span><span class="p">,</span>
        <span class="s2">&quot;n_kv_heads&quot;</span><span class="p">:</span> <span class="mi">8</span><span class="p">,</span>
        <span class="s2">&quot;vocab_size&quot;</span><span class="p">:</span> <span class="mi">128256</span><span class="p">,</span>
        <span class="s2">&quot;ffn_dim&quot;</span><span class="p">:</span> <span class="mi">53248</span><span class="p">,</span>
        <span class="s2">&quot;context_length&quot;</span><span class="p">:</span> <span class="mi">131072</span><span class="p">,</span>  <span class="c1"># 128K ë„¤ì´í‹°ë¸Œ</span>
    <span class="p">},</span>
    <span class="c1"># LLaMA 3.2 (2024.09) - ê²½ëŸ‰ í…ìŠ¤íŠ¸ ëª¨ë¸</span>
    <span class="s2">&quot;llama3.2-1b&quot;</span><span class="p">:</span> <span class="p">{</span>
        <span class="s2">&quot;dim&quot;</span><span class="p">:</span> <span class="mi">2048</span><span class="p">,</span>
        <span class="s2">&quot;n_layers&quot;</span><span class="p">:</span> <span class="mi">16</span><span class="p">,</span>
        <span class="s2">&quot;n_heads&quot;</span><span class="p">:</span> <span class="mi">32</span><span class="p">,</span>
        <span class="s2">&quot;n_kv_heads&quot;</span><span class="p">:</span> <span class="mi">8</span><span class="p">,</span>
        <span class="s2">&quot;vocab_size&quot;</span><span class="p">:</span> <span class="mi">128256</span><span class="p">,</span>
        <span class="s2">&quot;ffn_dim&quot;</span><span class="p">:</span> <span class="mi">8192</span><span class="p">,</span>
        <span class="s2">&quot;context_length&quot;</span><span class="p">:</span> <span class="mi">131072</span><span class="p">,</span>
    <span class="p">},</span>
    <span class="s2">&quot;llama3.2-3b&quot;</span><span class="p">:</span> <span class="p">{</span>
        <span class="s2">&quot;dim&quot;</span><span class="p">:</span> <span class="mi">3072</span><span class="p">,</span>
        <span class="s2">&quot;n_layers&quot;</span><span class="p">:</span> <span class="mi">28</span><span class="p">,</span>
        <span class="s2">&quot;n_heads&quot;</span><span class="p">:</span> <span class="mi">24</span><span class="p">,</span>
        <span class="s2">&quot;n_kv_heads&quot;</span><span class="p">:</span> <span class="mi">8</span><span class="p">,</span>
        <span class="s2">&quot;vocab_size&quot;</span><span class="p">:</span> <span class="mi">128256</span><span class="p">,</span>
        <span class="s2">&quot;ffn_dim&quot;</span><span class="p">:</span> <span class="mi">8192</span><span class="p">,</span>
        <span class="s2">&quot;context_length&quot;</span><span class="p">:</span> <span class="mi">131072</span><span class="p">,</span>
    <span class="p">},</span>
<span class="p">}</span>
</code></pre></div>

<hr />
<h2 id="3-rope-rotary-position-embedding">3. RoPE (Rotary Position Embedding)<a class="header-link" href="#3-rope-rotary-position-embedding" title="Permanent link">&para;</a></h2>
<h3 id="31">3.1 ê°œë…<a class="header-link" href="#31" title="Permanent link">&para;</a></h3>
<p><strong>RoPE</strong>ëŠ” ìœ„ì¹˜ ì •ë³´ë¥¼ íšŒì „ í–‰ë ¬ë¡œ ì¸ì½”ë”©í•˜ëŠ” ë°©ì‹ì…ë‹ˆë‹¤.</p>
<div class="highlight"><pre><span></span><code>â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    Position Encoding ë¹„êµ                        â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                 â”‚
â”‚  1. Sinusoidal (Transformer ì›ë³¸)                               â”‚
â”‚     PE(pos, 2i) = sin(pos / 10000^(2i/d))                       â”‚
â”‚     PE(pos, 2i+1) = cos(pos / 10000^(2i/d))                     â”‚
â”‚     â†’ ì…ë ¥ì— ë”í•¨ (additive)                                     â”‚
â”‚     â†’ ìƒëŒ€ ìœ„ì¹˜ ì •ë³´ ì•½í•¨                                         â”‚
â”‚                                                                 â”‚
â”‚  2. Learned (BERT, GPT)                                         â”‚
â”‚     PE = Embedding(position)                                    â”‚
â”‚     â†’ í•™ìŠµëœ ë²¡í„°                                                 â”‚
â”‚     â†’ í•™ìŠµ ì¤‘ ë³¸ ê¸¸ì´ ì´ìƒ ì¼ë°˜í™” ì–´ë ¤ì›€                           â”‚
â”‚                                                                 â”‚
â”‚  3. RoPE (LLaMA)                                                â”‚
â”‚     R(Î¸) = íšŒì „ í–‰ë ¬, Î¸ = f(position)                           â”‚
â”‚     q&#39; = R(Î¸_m) Ã— q, k&#39; = R(Î¸_n) Ã— k                           â”‚
â”‚     q&#39; Â· k&#39; = q Â· k Ã— cos(Î¸_m - Î¸_n)                           â”‚
â”‚     â†’ ìƒëŒ€ ìœ„ì¹˜ ìì—°ìŠ¤ëŸ½ê²Œ ì¸ì½”ë”©                                  â”‚
â”‚     â†’ ê¸¸ì´ ì™¸ì‚½ ê°€ëŠ¥ (with modifications)                        â”‚
â”‚                                                                 â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
</code></pre></div>

<h3 id="32">3.2 ìˆ˜í•™ì  ì´í•´<a class="header-link" href="#32" title="Permanent link">&para;</a></h3>
<div class="highlight"><pre><span></span><code><span class="kn">import</span><span class="w"> </span><span class="nn">torch</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">math</span>

<span class="k">def</span><span class="w"> </span><span class="nf">precompute_freqs_cis</span><span class="p">(</span><span class="n">dim</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span> <span class="n">seq_len</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span> <span class="n">theta</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="mf">10000.0</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    RoPEë¥¼ ìœ„í•œ ë³µì†Œìˆ˜ ì£¼íŒŒìˆ˜ ì‚¬ì „ ê³„ì‚°</span>

<span class="sd">    Args:</span>
<span class="sd">        dim: ì„ë² ë”© ì°¨ì› (head_dim)</span>
<span class="sd">        seq_len: ìµœëŒ€ ì‹œí€€ìŠ¤ ê¸¸ì´</span>
<span class="sd">        theta: ê¸°ë³¸ ì£¼íŒŒìˆ˜ (10000)</span>

<span class="sd">    Returns:</span>
<span class="sd">        freqs_cis: (seq_len, dim//2) ë³µì†Œìˆ˜ í…ì„œ</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="c1"># ì£¼íŒŒìˆ˜ ê³„ì‚°: Î¸_i = 1 / (theta^(2i/d))</span>
    <span class="n">freqs</span> <span class="o">=</span> <span class="mf">1.0</span> <span class="o">/</span> <span class="p">(</span><span class="n">theta</span> <span class="o">**</span> <span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">dim</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span><span class="o">.</span><span class="n">float</span><span class="p">()</span> <span class="o">/</span> <span class="n">dim</span><span class="p">))</span>
    <span class="c1"># ìœ„ì¹˜ë³„ ê°ë„: m * Î¸_i</span>
    <span class="n">t</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="n">seq_len</span><span class="p">)</span>
    <span class="n">freqs</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">outer</span><span class="p">(</span><span class="n">t</span><span class="p">,</span> <span class="n">freqs</span><span class="p">)</span>  <span class="c1"># (seq_len, dim//2)</span>
    <span class="c1"># ë³µì†Œìˆ˜ í˜•íƒœ: e^(i*Î¸) = cos(Î¸) + i*sin(Î¸)</span>
    <span class="n">freqs_cis</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">polar</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">ones_like</span><span class="p">(</span><span class="n">freqs</span><span class="p">),</span> <span class="n">freqs</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">freqs_cis</span>

<span class="k">def</span><span class="w"> </span><span class="nf">apply_rotary_emb</span><span class="p">(</span><span class="n">xq</span><span class="p">,</span> <span class="n">xk</span><span class="p">,</span> <span class="n">freqs_cis</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Queryì™€ Keyì— RoPE ì ìš©</span>

<span class="sd">    Args:</span>
<span class="sd">        xq: Query (batch, seq_len, n_heads, head_dim)</span>
<span class="sd">        xk: Key (batch, seq_len, n_kv_heads, head_dim)</span>
<span class="sd">        freqs_cis: ì‚¬ì „ ê³„ì‚°ëœ ë³µì†Œìˆ˜ ì£¼íŒŒìˆ˜</span>

<span class="sd">    Returns:</span>
<span class="sd">        íšŒì „ëœ Queryì™€ Key</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="c1"># ì‹¤ìˆ˜ë¥¼ ë³µì†Œìˆ˜ë¡œ ë³€í™˜ (ì¸ì ‘í•œ 2ê°œì”© ë¬¶ìŒ)</span>
    <span class="n">xq_</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">view_as_complex</span><span class="p">(</span><span class="n">xq</span><span class="o">.</span><span class="n">float</span><span class="p">()</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="o">*</span><span class="n">xq</span><span class="o">.</span><span class="n">shape</span><span class="p">[:</span><span class="o">-</span><span class="mi">1</span><span class="p">],</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">))</span>
    <span class="n">xk_</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">view_as_complex</span><span class="p">(</span><span class="n">xk</span><span class="o">.</span><span class="n">float</span><span class="p">()</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="o">*</span><span class="n">xk</span><span class="o">.</span><span class="n">shape</span><span class="p">[:</span><span class="o">-</span><span class="mi">1</span><span class="p">],</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">))</span>

    <span class="c1"># íšŒì „ ì ìš© (ë³µì†Œìˆ˜ ê³±)</span>
    <span class="n">freqs_cis</span> <span class="o">=</span> <span class="n">freqs_cis</span><span class="o">.</span><span class="n">unsqueeze</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span><span class="o">.</span><span class="n">unsqueeze</span><span class="p">(</span><span class="mi">2</span><span class="p">)</span>  <span class="c1"># (1, seq, 1, dim//2)</span>
    <span class="n">xq_out</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">view_as_real</span><span class="p">(</span><span class="n">xq_</span> <span class="o">*</span> <span class="n">freqs_cis</span><span class="p">)</span><span class="o">.</span><span class="n">flatten</span><span class="p">(</span><span class="mi">3</span><span class="p">)</span>
    <span class="n">xk_out</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">view_as_real</span><span class="p">(</span><span class="n">xk_</span> <span class="o">*</span> <span class="n">freqs_cis</span><span class="p">)</span><span class="o">.</span><span class="n">flatten</span><span class="p">(</span><span class="mi">3</span><span class="p">)</span>

    <span class="k">return</span> <span class="n">xq_out</span><span class="o">.</span><span class="n">type_as</span><span class="p">(</span><span class="n">xq</span><span class="p">),</span> <span class="n">xk_out</span><span class="o">.</span><span class="n">type_as</span><span class="p">(</span><span class="n">xk</span><span class="p">)</span>

<span class="c1"># ì˜ˆì‹œ</span>
<span class="n">batch</span><span class="p">,</span> <span class="n">seq_len</span><span class="p">,</span> <span class="n">n_heads</span><span class="p">,</span> <span class="n">head_dim</span> <span class="o">=</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">128</span><span class="p">,</span> <span class="mi">32</span><span class="p">,</span> <span class="mi">128</span>
<span class="n">xq</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="n">batch</span><span class="p">,</span> <span class="n">seq_len</span><span class="p">,</span> <span class="n">n_heads</span><span class="p">,</span> <span class="n">head_dim</span><span class="p">)</span>
<span class="n">xk</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="n">batch</span><span class="p">,</span> <span class="n">seq_len</span><span class="p">,</span> <span class="n">n_heads</span><span class="p">,</span> <span class="n">head_dim</span><span class="p">)</span>
<span class="n">freqs_cis</span> <span class="o">=</span> <span class="n">precompute_freqs_cis</span><span class="p">(</span><span class="n">head_dim</span><span class="p">,</span> <span class="n">seq_len</span><span class="p">)</span>

<span class="n">xq_rope</span><span class="p">,</span> <span class="n">xk_rope</span> <span class="o">=</span> <span class="n">apply_rotary_emb</span><span class="p">(</span><span class="n">xq</span><span class="p">,</span> <span class="n">xk</span><span class="p">,</span> <span class="n">freqs_cis</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Output shape: </span><span class="si">{</span><span class="n">xq_rope</span><span class="o">.</span><span class="n">shape</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>  <span class="c1"># (2, 128, 32, 128)</span>
</code></pre></div>

<h3 id="33-rope">3.3 RoPEì˜ ì¥ì <a class="header-link" href="#33-rope" title="Permanent link">&para;</a></h3>
<div class="highlight"><pre><span></span><code><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">RoPEì˜ ì¥ì :</span>

<span class="sd">1. ìƒëŒ€ ìœ„ì¹˜ ìì—°ìŠ¤ëŸ½ê²Œ ì¸ì½”ë”©</span>
<span class="sd">   - q_m Â· k_n âˆ cos(Î¸_m - Î¸_n)</span>
<span class="sd">   - ì ˆëŒ€ ìœ„ì¹˜ê°€ ì•„ë‹Œ ìƒëŒ€ ê±°ë¦¬ ì˜ì¡´</span>

<span class="sd">2. ì™¸ì‚½ ê°€ëŠ¥ì„±</span>
<span class="sd">   - í•™ìŠµ ì‹œ ë³¸ ê¸¸ì´ ì´ìƒìœ¼ë¡œ í™•ì¥ ê°€ëŠ¥</span>
<span class="sd">   - (ë‹¨, ì„±ëŠ¥ ì €í•˜ ìˆìŒ â†’ NTK, YaRNìœ¼ë¡œ ê°œì„ )</span>

<span class="sd">3. íš¨ìœ¨ì„±</span>
<span class="sd">   - ì¶”ê°€ íŒŒë¼ë¯¸í„° ì—†ìŒ</span>
<span class="sd">   - Element-wise ì—°ì‚°ìœ¼ë¡œ ë¹ ë¦„</span>

<span class="sd">4. ì„ í˜• Self-attentionê³¼ í˜¸í™˜</span>
<span class="sd">   - ì¼ë¶€ íš¨ìœ¨ì  attention ë°©ì‹ê³¼ ê²°í•© ê°€ëŠ¥</span>
<span class="sd">&quot;&quot;&quot;</span>
</code></pre></div>

<hr />
<h2 id="4-rmsnorm">4. RMSNorm<a class="header-link" href="#4-rmsnorm" title="Permanent link">&para;</a></h2>
<h3 id="41">4.1 ê°œë…<a class="header-link" href="#41" title="Permanent link">&para;</a></h3>
<p><strong>RMSNorm</strong>ì€ LayerNormì˜ ë‹¨ìˆœí™” ë²„ì „ìœ¼ë¡œ, í‰ê·  ê³„ì‚°ì„ ì œê±°í•©ë‹ˆë‹¤.</p>
<div class="highlight"><pre><span></span><code>â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    LayerNorm vs RMSNorm                          â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                 â”‚
â”‚  LayerNorm:                                                     â”‚
â”‚  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€                             â”‚
â”‚  Î¼ = mean(x)                                                    â”‚
â”‚  Ïƒ = std(x)                                                     â”‚
â”‚  y = Î³ Ã— (x - Î¼) / Ïƒ + Î²                                        â”‚
â”‚                                                                 â”‚
â”‚  â€¢ í‰ê·  ë¹¼ê¸° + ë¶„ì‚°ìœ¼ë¡œ ë‚˜ëˆ„ê¸°                                     â”‚
â”‚  â€¢ í•™ìŠµ ê°€ëŠ¥í•œ scale(Î³)ì™€ shift(Î²)                               â”‚
â”‚                                                                 â”‚
â”‚  RMSNorm:                                                       â”‚
â”‚  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€                             â”‚
â”‚  RMS(x) = sqrt(mean(x^2))                                       â”‚
â”‚  y = Î³ Ã— x / RMS(x)                                             â”‚
â”‚                                                                 â”‚
â”‚  â€¢ í‰ê·  ë¹¼ê¸° ì—†ìŒ â†’ Re-centering ì œê±°                             â”‚
â”‚  â€¢ RMSë¡œë§Œ ìŠ¤ì¼€ì¼ë§                                               â”‚
â”‚  â€¢ shift(Î²) ì—†ìŒ                                                 â”‚
â”‚  â€¢ ì—°ì‚°ëŸ‰ ê°ì†Œ, ì„±ëŠ¥ ìœ ì‚¬                                          â”‚
â”‚                                                                 â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
</code></pre></div>

<h3 id="42">4.2 êµ¬í˜„<a class="header-link" href="#42" title="Permanent link">&para;</a></h3>
<div class="highlight"><pre><span></span><code><span class="kn">import</span><span class="w"> </span><span class="nn">torch</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">torch.nn</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">nn</span>

<span class="k">class</span><span class="w"> </span><span class="nc">RMSNorm</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Root Mean Square Layer Normalization</span>

<span class="sd">    ë…¼ë¬¸: https://arxiv.org/abs/1910.07467</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">def</span><span class="w"> </span><span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">dim</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span> <span class="n">eps</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="mf">1e-6</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">eps</span> <span class="o">=</span> <span class="n">eps</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">weight</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Parameter</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">ones</span><span class="p">(</span><span class="n">dim</span><span class="p">))</span>  <span class="c1"># scale parameter Î³</span>

    <span class="k">def</span><span class="w"> </span><span class="nf">_norm</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
        <span class="c1"># RMS = sqrt(mean(x^2))</span>
        <span class="k">return</span> <span class="n">x</span> <span class="o">*</span> <span class="n">torch</span><span class="o">.</span><span class="n">rsqrt</span><span class="p">(</span><span class="n">x</span><span class="o">.</span><span class="n">pow</span><span class="p">(</span><span class="mi">2</span><span class="p">)</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="n">keepdim</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span> <span class="o">+</span> <span class="bp">self</span><span class="o">.</span><span class="n">eps</span><span class="p">)</span>

    <span class="k">def</span><span class="w"> </span><span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
        <span class="c1"># output = Î³ Ã— (x / RMS(x))</span>
        <span class="n">output</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_norm</span><span class="p">(</span><span class="n">x</span><span class="o">.</span><span class="n">float</span><span class="p">())</span><span class="o">.</span><span class="n">type_as</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">output</span> <span class="o">*</span> <span class="bp">self</span><span class="o">.</span><span class="n">weight</span>

<span class="c1"># LayerNormê³¼ ë¹„êµ</span>
<span class="n">x</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">10</span><span class="p">,</span> <span class="mi">512</span><span class="p">)</span>

<span class="n">layer_norm</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">LayerNorm</span><span class="p">(</span><span class="mi">512</span><span class="p">)</span>
<span class="n">rms_norm</span> <span class="o">=</span> <span class="n">RMSNorm</span><span class="p">(</span><span class="mi">512</span><span class="p">)</span>

<span class="c1"># ì—°ì‚° ì‹œê°„ ë¹„êµ (RMSNormì´ ì•½ê°„ ë¹ ë¦„)</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">time</span>

<span class="n">start</span> <span class="o">=</span> <span class="n">time</span><span class="o">.</span><span class="n">time</span><span class="p">()</span>
<span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">1000</span><span class="p">):</span>
    <span class="n">_</span> <span class="o">=</span> <span class="n">layer_norm</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;LayerNorm: </span><span class="si">{</span><span class="n">time</span><span class="o">.</span><span class="n">time</span><span class="p">()</span><span class="w"> </span><span class="o">-</span><span class="w"> </span><span class="n">start</span><span class="si">:</span><span class="s2">.4f</span><span class="si">}</span><span class="s2">s&quot;</span><span class="p">)</span>

<span class="n">start</span> <span class="o">=</span> <span class="n">time</span><span class="o">.</span><span class="n">time</span><span class="p">()</span>
<span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">1000</span><span class="p">):</span>
    <span class="n">_</span> <span class="o">=</span> <span class="n">rms_norm</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;RMSNorm: </span><span class="si">{</span><span class="n">time</span><span class="o">.</span><span class="n">time</span><span class="p">()</span><span class="w"> </span><span class="o">-</span><span class="w"> </span><span class="n">start</span><span class="si">:</span><span class="s2">.4f</span><span class="si">}</span><span class="s2">s&quot;</span><span class="p">)</span>
</code></pre></div>

<hr />
<h2 id="5-swiglu">5. SwiGLU<a class="header-link" href="#5-swiglu" title="Permanent link">&para;</a></h2>
<h3 id="51">5.1 ê°œë…<a class="header-link" href="#51" title="Permanent link">&para;</a></h3>
<p><strong>SwiGLU</strong>ëŠ” GLU(Gated Linear Unit)ì˜ ë³€í˜•ìœ¼ë¡œ, Swish í™œì„±í™” í•¨ìˆ˜ë¥¼ ì‚¬ìš©í•©ë‹ˆë‹¤.</p>
<div class="highlight"><pre><span></span><code>â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    FFN í™œì„±í™” í•¨ìˆ˜ ë¹„êµ                           â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                 â”‚
â”‚  1. ReLU FFN (Transformer ì›ë³¸):                                â”‚
â”‚     FFN(x) = max(0, xWâ‚ + bâ‚)Wâ‚‚ + bâ‚‚                            â”‚
â”‚     â€¢ ë‹¨ìˆœí•˜ì§€ë§Œ ìŒìˆ˜ ì˜ì—­ ì •ë³´ ì†ì‹¤                               â”‚
â”‚                                                                 â”‚
â”‚  2. GELU FFN (BERT, GPT):                                       â”‚
â”‚     FFN(x) = GELU(xWâ‚)Wâ‚‚                                        â”‚
â”‚     GELU(x) = x Ã— Î¦(x)  (Î¦ = CDF of normal)                     â”‚
â”‚     â€¢ ë¶€ë“œëŸ¬ìš´ í™œì„±í™”, ì„±ëŠ¥ í–¥ìƒ                                   â”‚
â”‚                                                                 â”‚
â”‚  3. SwiGLU FFN (LLaMA):                                         â”‚
â”‚     FFN(x) = (Swish(xWâ‚) âŠ™ xV)Wâ‚‚                                â”‚
â”‚     Swish(x) = x Ã— Ïƒ(x)  (Ïƒ = sigmoid)                          â”‚
â”‚     âŠ™ = element-wise multiplication                             â”‚
â”‚                                                                 â”‚
â”‚     â€¢ Gating mechanismìœ¼ë¡œ ì •ë³´ íë¦„ ì œì–´                         â”‚
â”‚     â€¢ ë” ë§ì€ íŒŒë¼ë¯¸í„°, ë” ì¢‹ì€ ì„±ëŠ¥                               â”‚
â”‚     â€¢ 2/3 Ã— 4d hidden dim (íŒŒë¼ë¯¸í„° ìˆ˜ ìœ ì§€)                      â”‚
â”‚                                                                 â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
</code></pre></div>

<h3 id="52">5.2 êµ¬í˜„<a class="header-link" href="#52" title="Permanent link">&para;</a></h3>
<div class="highlight"><pre><span></span><code><span class="kn">import</span><span class="w"> </span><span class="nn">torch</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">torch.nn</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">nn</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">torch.nn.functional</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">F</span>

<span class="k">class</span><span class="w"> </span><span class="nc">SwiGLU</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    SwiGLU: Swish-Gated Linear Unit</span>

<span class="sd">    FFN(x) = (Swish(xWâ‚) âŠ™ xV) Wâ‚‚</span>

<span class="sd">    ë…¼ë¬¸: https://arxiv.org/abs/2002.05202</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">def</span><span class="w"> </span><span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">dim</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span> <span class="n">hidden_dim</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span> <span class="n">multiple_of</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">256</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>

        <span class="c1"># hidden_dim ê³„ì‚°: 2/3 Ã— 4d, 256ì˜ ë°°ìˆ˜ë¡œ ë°˜ì˜¬ë¦¼</span>
        <span class="k">if</span> <span class="n">hidden_dim</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">hidden_dim</span> <span class="o">=</span> <span class="nb">int</span><span class="p">(</span><span class="mi">2</span> <span class="o">*</span> <span class="p">(</span><span class="mi">4</span> <span class="o">*</span> <span class="n">dim</span><span class="p">)</span> <span class="o">/</span> <span class="mi">3</span><span class="p">)</span>
            <span class="n">hidden_dim</span> <span class="o">=</span> <span class="n">multiple_of</span> <span class="o">*</span> <span class="p">((</span><span class="n">hidden_dim</span> <span class="o">+</span> <span class="n">multiple_of</span> <span class="o">-</span> <span class="mi">1</span><span class="p">)</span> <span class="o">//</span> <span class="n">multiple_of</span><span class="p">)</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">w1</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">dim</span><span class="p">,</span> <span class="n">hidden_dim</span><span class="p">,</span> <span class="n">bias</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>  <span class="c1"># gate</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">w2</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">hidden_dim</span><span class="p">,</span> <span class="n">dim</span><span class="p">,</span> <span class="n">bias</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>  <span class="c1"># down projection</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">w3</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">dim</span><span class="p">,</span> <span class="n">hidden_dim</span><span class="p">,</span> <span class="n">bias</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>  <span class="c1"># up projection</span>

    <span class="k">def</span><span class="w"> </span><span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
        <span class="c1"># SwiGLU: Swish(xWâ‚) âŠ™ (xWâ‚ƒ) â†’ Wâ‚‚</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">w2</span><span class="p">(</span><span class="n">F</span><span class="o">.</span><span class="n">silu</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">w1</span><span class="p">(</span><span class="n">x</span><span class="p">))</span> <span class="o">*</span> <span class="bp">self</span><span class="o">.</span><span class="n">w3</span><span class="p">(</span><span class="n">x</span><span class="p">))</span>

<span class="c1"># ê¸°ì¡´ FFNê³¼ ë¹„êµ</span>
<span class="k">class</span><span class="w"> </span><span class="nc">StandardFFN</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="k">def</span><span class="w"> </span><span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">dim</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span> <span class="n">hidden_dim</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="kc">None</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
        <span class="k">if</span> <span class="n">hidden_dim</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">hidden_dim</span> <span class="o">=</span> <span class="mi">4</span> <span class="o">*</span> <span class="n">dim</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">w1</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">dim</span><span class="p">,</span> <span class="n">hidden_dim</span><span class="p">,</span> <span class="n">bias</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">w2</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">hidden_dim</span><span class="p">,</span> <span class="n">dim</span><span class="p">,</span> <span class="n">bias</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>

    <span class="k">def</span><span class="w"> </span><span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">w2</span><span class="p">(</span><span class="n">F</span><span class="o">.</span><span class="n">gelu</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">w1</span><span class="p">(</span><span class="n">x</span><span class="p">)))</span>

<span class="c1"># íŒŒë¼ë¯¸í„° ìˆ˜ ë¹„êµ</span>
<span class="n">dim</span> <span class="o">=</span> <span class="mi">4096</span>
<span class="n">swiglu</span> <span class="o">=</span> <span class="n">SwiGLU</span><span class="p">(</span><span class="n">dim</span><span class="p">)</span>  <span class="c1"># 3ê°œì˜ linear: dimâ†’hidden, dimâ†’hidden, hiddenâ†’dim</span>
<span class="n">standard</span> <span class="o">=</span> <span class="n">StandardFFN</span><span class="p">(</span><span class="n">dim</span><span class="p">)</span>  <span class="c1"># 2ê°œì˜ linear: dimâ†’4*dim, 4*dimâ†’dim</span>

<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;SwiGLU params: </span><span class="si">{</span><span class="nb">sum</span><span class="p">(</span><span class="n">p</span><span class="o">.</span><span class="n">numel</span><span class="p">()</span><span class="w"> </span><span class="k">for</span><span class="w"> </span><span class="n">p</span><span class="w"> </span><span class="ow">in</span><span class="w"> </span><span class="n">swiglu</span><span class="o">.</span><span class="n">parameters</span><span class="p">())</span><span class="si">:</span><span class="s2">,</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Standard FFN params: </span><span class="si">{</span><span class="nb">sum</span><span class="p">(</span><span class="n">p</span><span class="o">.</span><span class="n">numel</span><span class="p">()</span><span class="w"> </span><span class="k">for</span><span class="w"> </span><span class="n">p</span><span class="w"> </span><span class="ow">in</span><span class="w"> </span><span class="n">standard</span><span class="o">.</span><span class="n">parameters</span><span class="p">())</span><span class="si">:</span><span class="s2">,</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
<span class="c1"># SwiGLUê°€ ì•½ê°„ ë” ë§ì§€ë§Œ hidden_dim ì¡°ì •ìœ¼ë¡œ ë¹„ìŠ·í•˜ê²Œ ë§ì¶¤</span>
</code></pre></div>

<hr />
<h2 id="6-grouped-query-attention-gqa">6. Grouped Query Attention (GQA)<a class="header-link" href="#6-grouped-query-attention-gqa" title="Permanent link">&para;</a></h2>
<h3 id="61">6.1 ê°œë…<a class="header-link" href="#61" title="Permanent link">&para;</a></h3>
<p><strong>GQA</strong>ëŠ” Multi-Head Attentionê³¼ Multi-Query Attentionì˜ ì¤‘ê°„ í˜•íƒœì…ë‹ˆë‹¤.</p>
<div class="highlight"><pre><span></span><code>â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    Attention ë°©ì‹ ë¹„êµ                           â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                 â”‚
â”‚  1. Multi-Head Attention (MHA):                                 â”‚
â”‚     Q heads: 32  â”‚  K heads: 32  â”‚  V heads: 32                 â”‚
â”‚     â€¢ ê° Q headê°€ ë…ë¦½ì ì¸ KV head ì‚¬ìš©                           â”‚
â”‚     â€¢ ë©”ëª¨ë¦¬: ë§ìŒ (32 Ã— KV cache)                                â”‚
â”‚                                                                 â”‚
â”‚  2. Multi-Query Attention (MQA):                                â”‚
â”‚     Q heads: 32  â”‚  K heads: 1   â”‚  V heads: 1                  â”‚
â”‚     â€¢ ëª¨ë“  Q headê°€ ê°™ì€ KV ê³µìœ                                   â”‚
â”‚     â€¢ ë©”ëª¨ë¦¬: ìµœì†Œ (1 Ã— KV cache)                                 â”‚
â”‚     â€¢ í’ˆì§ˆ: MHAë³´ë‹¤ ì•½ê°„ ë‚®ìŒ                                     â”‚
â”‚                                                                 â”‚
â”‚  3. Grouped Query Attention (GQA):                              â”‚
â”‚     Q heads: 32  â”‚  K heads: 8   â”‚  V heads: 8                  â”‚
â”‚     â€¢ Q headsë¥¼ ê·¸ë£¹ìœ¼ë¡œ ë‚˜ëˆ  KV ê³µìœ                               â”‚
â”‚     â€¢ ì˜ˆ: 4ê°œì˜ Q headê°€ 1ê°œì˜ KV head ê³µìœ                        â”‚
â”‚     â€¢ ë©”ëª¨ë¦¬: ì¤‘ê°„ (8 Ã— KV cache)                                 â”‚
â”‚     â€¢ í’ˆì§ˆ: MHAì™€ ê±°ì˜ ë™ì¼                                       â”‚
â”‚                                                                 â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”               â”‚
â”‚  â”‚ MHA          â”‚ MQA           â”‚ GQA           â”‚               â”‚
â”‚  â”‚ Q Q Q Q Q Q  â”‚ Q Q Q Q Q Q   â”‚ Q Qâ”‚Q Qâ”‚Q Q   â”‚               â”‚
â”‚  â”‚ â†“ â†“ â†“ â†“ â†“ â†“  â”‚ â†“ â†“ â†“ â†“ â†“ â†“   â”‚ â†“ â†“â”‚â†“ â†“â”‚â†“ â†“   â”‚               â”‚
â”‚  â”‚ K K K K K K  â”‚     K         â”‚  K â”‚ K â”‚ K    â”‚               â”‚
â”‚  â”‚ V V V V V V  â”‚     V         â”‚  V â”‚ V â”‚ V    â”‚               â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜               â”‚
â”‚                                                                 â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
</code></pre></div>

<h3 id="62">6.2 êµ¬í˜„<a class="header-link" href="#62" title="Permanent link">&para;</a></h3>
<div class="highlight"><pre><span></span><code><span class="kn">import</span><span class="w"> </span><span class="nn">torch</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">torch.nn</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">nn</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">torch.nn.functional</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">F</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">math</span>

<span class="k">class</span><span class="w"> </span><span class="nc">GroupedQueryAttention</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Grouped Query Attention (GQA)</span>

<span class="sd">    ë…¼ë¬¸: https://arxiv.org/abs/2305.13245</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">def</span><span class="w"> </span><span class="fm">__init__</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">dim</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
        <span class="n">n_heads</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">32</span><span class="p">,</span>
        <span class="n">n_kv_heads</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">8</span><span class="p">,</span>  <span class="c1"># KV heads ìˆ˜ (&lt; n_heads)</span>
        <span class="n">head_dim</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="p">):</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">n_heads</span> <span class="o">=</span> <span class="n">n_heads</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">n_kv_heads</span> <span class="o">=</span> <span class="n">n_kv_heads</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">head_dim</span> <span class="o">=</span> <span class="n">head_dim</span> <span class="ow">or</span> <span class="n">dim</span> <span class="o">//</span> <span class="n">n_heads</span>

        <span class="c1"># Q heads &gt; KV heads ê²€ì¦</span>
        <span class="k">assert</span> <span class="n">n_heads</span> <span class="o">%</span> <span class="n">n_kv_heads</span> <span class="o">==</span> <span class="mi">0</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">n_rep</span> <span class="o">=</span> <span class="n">n_heads</span> <span class="o">//</span> <span class="n">n_kv_heads</span>  <span class="c1"># ê° KV headê°€ ë‹´ë‹¹í•˜ëŠ” Q head ìˆ˜</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">wq</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">dim</span><span class="p">,</span> <span class="n">n_heads</span> <span class="o">*</span> <span class="bp">self</span><span class="o">.</span><span class="n">head_dim</span><span class="p">,</span> <span class="n">bias</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">wk</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">dim</span><span class="p">,</span> <span class="n">n_kv_heads</span> <span class="o">*</span> <span class="bp">self</span><span class="o">.</span><span class="n">head_dim</span><span class="p">,</span> <span class="n">bias</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">wv</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">dim</span><span class="p">,</span> <span class="n">n_kv_heads</span> <span class="o">*</span> <span class="bp">self</span><span class="o">.</span><span class="n">head_dim</span><span class="p">,</span> <span class="n">bias</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">wo</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">n_heads</span> <span class="o">*</span> <span class="bp">self</span><span class="o">.</span><span class="n">head_dim</span><span class="p">,</span> <span class="n">dim</span><span class="p">,</span> <span class="n">bias</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>

    <span class="k">def</span><span class="w"> </span><span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">freqs_cis</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">mask</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">kv_cache</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
        <span class="n">batch</span><span class="p">,</span> <span class="n">seq_len</span><span class="p">,</span> <span class="n">_</span> <span class="o">=</span> <span class="n">x</span><span class="o">.</span><span class="n">shape</span>

        <span class="c1"># Q, K, V ê³„ì‚°</span>
        <span class="n">xq</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">wq</span><span class="p">(</span><span class="n">x</span><span class="p">)</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="n">batch</span><span class="p">,</span> <span class="n">seq_len</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">n_heads</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">head_dim</span><span class="p">)</span>
        <span class="n">xk</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">wk</span><span class="p">(</span><span class="n">x</span><span class="p">)</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="n">batch</span><span class="p">,</span> <span class="n">seq_len</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">n_kv_heads</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">head_dim</span><span class="p">)</span>
        <span class="n">xv</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">wv</span><span class="p">(</span><span class="n">x</span><span class="p">)</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="n">batch</span><span class="p">,</span> <span class="n">seq_len</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">n_kv_heads</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">head_dim</span><span class="p">)</span>

        <span class="c1"># RoPE ì ìš© (ìˆëŠ” ê²½ìš°)</span>
        <span class="k">if</span> <span class="n">freqs_cis</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">xq</span><span class="p">,</span> <span class="n">xk</span> <span class="o">=</span> <span class="n">apply_rotary_emb</span><span class="p">(</span><span class="n">xq</span><span class="p">,</span> <span class="n">xk</span><span class="p">,</span> <span class="n">freqs_cis</span><span class="p">)</span>

        <span class="c1"># KV Cache ì²˜ë¦¬ (ì¶”ë¡  ì‹œ)</span>
        <span class="k">if</span> <span class="n">kv_cache</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">cache_k</span><span class="p">,</span> <span class="n">cache_v</span> <span class="o">=</span> <span class="n">kv_cache</span>
            <span class="n">xk</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">cat</span><span class="p">([</span><span class="n">cache_k</span><span class="p">,</span> <span class="n">xk</span><span class="p">],</span> <span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
            <span class="n">xv</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">cat</span><span class="p">([</span><span class="n">cache_v</span><span class="p">,</span> <span class="n">xv</span><span class="p">],</span> <span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>

        <span class="c1"># KV heads í™•ì¥: n_kv_heads â†’ n_heads</span>
        <span class="c1"># (batch, seq, n_kv_heads, head_dim) â†’ (batch, seq, n_heads, head_dim)</span>
        <span class="n">xk</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_repeat_kv</span><span class="p">(</span><span class="n">xk</span><span class="p">)</span>
        <span class="n">xv</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_repeat_kv</span><span class="p">(</span><span class="n">xv</span><span class="p">)</span>

        <span class="c1"># Attention ê³„ì‚°</span>
        <span class="n">xq</span> <span class="o">=</span> <span class="n">xq</span><span class="o">.</span><span class="n">transpose</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>  <span class="c1"># (batch, n_heads, seq, head_dim)</span>
        <span class="n">xk</span> <span class="o">=</span> <span class="n">xk</span><span class="o">.</span><span class="n">transpose</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>
        <span class="n">xv</span> <span class="o">=</span> <span class="n">xv</span><span class="o">.</span><span class="n">transpose</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>

        <span class="n">scores</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">matmul</span><span class="p">(</span><span class="n">xq</span><span class="p">,</span> <span class="n">xk</span><span class="o">.</span><span class="n">transpose</span><span class="p">(</span><span class="o">-</span><span class="mi">2</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">))</span> <span class="o">/</span> <span class="n">math</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">head_dim</span><span class="p">)</span>

        <span class="k">if</span> <span class="n">mask</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">scores</span> <span class="o">=</span> <span class="n">scores</span> <span class="o">+</span> <span class="n">mask</span>

        <span class="n">attn</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">softmax</span><span class="p">(</span><span class="n">scores</span><span class="p">,</span> <span class="n">dim</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span>
        <span class="n">output</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">matmul</span><span class="p">(</span><span class="n">attn</span><span class="p">,</span> <span class="n">xv</span><span class="p">)</span>

        <span class="c1"># ê²°ê³¼ í•©ì¹˜ê¸°</span>
        <span class="n">output</span> <span class="o">=</span> <span class="n">output</span><span class="o">.</span><span class="n">transpose</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span><span class="o">.</span><span class="n">contiguous</span><span class="p">()</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="n">batch</span><span class="p">,</span> <span class="n">seq_len</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">)</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">wo</span><span class="p">(</span><span class="n">output</span><span class="p">),</span> <span class="p">(</span><span class="n">xk</span><span class="p">,</span> <span class="n">xv</span><span class="p">)</span>

    <span class="k">def</span><span class="w"> </span><span class="nf">_repeat_kv</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;KV headsë¥¼ Q heads ìˆ˜ë§Œí¼ ë°˜ë³µ&quot;&quot;&quot;</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">n_rep</span> <span class="o">==</span> <span class="mi">1</span><span class="p">:</span>
            <span class="k">return</span> <span class="n">x</span>
        <span class="n">batch</span><span class="p">,</span> <span class="n">seq_len</span><span class="p">,</span> <span class="n">n_kv_heads</span><span class="p">,</span> <span class="n">head_dim</span> <span class="o">=</span> <span class="n">x</span><span class="o">.</span><span class="n">shape</span>
        <span class="n">x</span> <span class="o">=</span> <span class="n">x</span><span class="p">[:,</span> <span class="p">:,</span> <span class="p">:,</span> <span class="kc">None</span><span class="p">,</span> <span class="p">:]</span><span class="o">.</span><span class="n">expand</span><span class="p">(</span><span class="n">batch</span><span class="p">,</span> <span class="n">seq_len</span><span class="p">,</span> <span class="n">n_kv_heads</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">n_rep</span><span class="p">,</span> <span class="n">head_dim</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">x</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="n">batch</span><span class="p">,</span> <span class="n">seq_len</span><span class="p">,</span> <span class="n">n_kv_heads</span> <span class="o">*</span> <span class="bp">self</span><span class="o">.</span><span class="n">n_rep</span><span class="p">,</span> <span class="n">head_dim</span><span class="p">)</span>

<span class="c1"># ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ë¹„êµ</span>
<span class="k">def</span><span class="w"> </span><span class="nf">compare_kv_cache_memory</span><span class="p">(</span><span class="n">seq_len</span><span class="p">,</span> <span class="n">batch_size</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">dtype_bytes</span><span class="o">=</span><span class="mi">2</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;KV cache ë©”ëª¨ë¦¬ ë¹„êµ (FP16 ê¸°ì¤€)&quot;&quot;&quot;</span>
    <span class="n">configs</span> <span class="o">=</span> <span class="p">{</span>
        <span class="s2">&quot;LLaMA-70B (MHA)&quot;</span><span class="p">:</span> <span class="p">{</span><span class="s2">&quot;n_layers&quot;</span><span class="p">:</span> <span class="mi">80</span><span class="p">,</span> <span class="s2">&quot;n_kv_heads&quot;</span><span class="p">:</span> <span class="mi">64</span><span class="p">,</span> <span class="s2">&quot;head_dim&quot;</span><span class="p">:</span> <span class="mi">128</span><span class="p">},</span>
        <span class="s2">&quot;LLaMA-70B (GQA)&quot;</span><span class="p">:</span> <span class="p">{</span><span class="s2">&quot;n_layers&quot;</span><span class="p">:</span> <span class="mi">80</span><span class="p">,</span> <span class="s2">&quot;n_kv_heads&quot;</span><span class="p">:</span> <span class="mi">8</span><span class="p">,</span> <span class="s2">&quot;head_dim&quot;</span><span class="p">:</span> <span class="mi">128</span><span class="p">},</span>
    <span class="p">}</span>

    <span class="k">for</span> <span class="n">name</span><span class="p">,</span> <span class="n">cfg</span> <span class="ow">in</span> <span class="n">configs</span><span class="o">.</span><span class="n">items</span><span class="p">():</span>
        <span class="n">kv_mem</span> <span class="o">=</span> <span class="p">(</span><span class="mi">2</span> <span class="o">*</span>  <span class="c1"># K and V</span>
                  <span class="n">batch_size</span> <span class="o">*</span>
                  <span class="n">cfg</span><span class="p">[</span><span class="s2">&quot;n_layers&quot;</span><span class="p">]</span> <span class="o">*</span>
                  <span class="n">seq_len</span> <span class="o">*</span>
                  <span class="n">cfg</span><span class="p">[</span><span class="s2">&quot;n_kv_heads&quot;</span><span class="p">]</span> <span class="o">*</span>
                  <span class="n">cfg</span><span class="p">[</span><span class="s2">&quot;head_dim&quot;</span><span class="p">]</span> <span class="o">*</span>
                  <span class="n">dtype_bytes</span><span class="p">)</span>
        <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="n">name</span><span class="si">}</span><span class="s2">: </span><span class="si">{</span><span class="n">kv_mem</span><span class="w"> </span><span class="o">/</span><span class="w"> </span><span class="mf">1e9</span><span class="si">:</span><span class="s2">.2f</span><span class="si">}</span><span class="s2"> GB for </span><span class="si">{</span><span class="n">seq_len</span><span class="si">}</span><span class="s2"> tokens&quot;</span><span class="p">)</span>

<span class="n">compare_kv_cache_memory</span><span class="p">(</span><span class="mi">4096</span><span class="p">)</span>
<span class="c1"># LLaMA-70B (MHA): 5.24 GB for 4096 tokens</span>
<span class="c1"># LLaMA-70B (GQA): 0.66 GB for 4096 tokens  â† 8ë°° ì ˆì•½!</span>
</code></pre></div>

<hr />
<h2 id="7-llama">7. LLaMA ì‹¤ìŠµ<a class="header-link" href="#7-llama" title="Permanent link">&para;</a></h2>
<h3 id="71-huggingface">7.1 HuggingFaceë¡œ ì‚¬ìš©í•˜ê¸°<a class="header-link" href="#71-huggingface" title="Permanent link">&para;</a></h3>
<div class="highlight"><pre><span></span><code><span class="kn">from</span><span class="w"> </span><span class="nn">transformers</span><span class="w"> </span><span class="kn">import</span> <span class="n">AutoModelForCausalLM</span><span class="p">,</span> <span class="n">AutoTokenizer</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">torch</span>

<span class="c1"># LLaMA 2 7B ë¡œë“œ</span>
<span class="n">model_name</span> <span class="o">=</span> <span class="s2">&quot;meta-llama/Llama-2-7b-hf&quot;</span>

<span class="n">tokenizer</span> <span class="o">=</span> <span class="n">AutoTokenizer</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="n">model_name</span><span class="p">)</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">AutoModelForCausalLM</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span>
    <span class="n">model_name</span><span class="p">,</span>
    <span class="n">torch_dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">float16</span><span class="p">,</span>
    <span class="n">device_map</span><span class="o">=</span><span class="s2">&quot;auto&quot;</span>
<span class="p">)</span>

<span class="c1"># í…ìŠ¤íŠ¸ ìƒì„±</span>
<span class="k">def</span><span class="w"> </span><span class="nf">generate_text</span><span class="p">(</span><span class="n">prompt</span><span class="p">,</span> <span class="n">max_new_tokens</span><span class="o">=</span><span class="mi">100</span><span class="p">,</span> <span class="n">temperature</span><span class="o">=</span><span class="mf">0.7</span><span class="p">):</span>
    <span class="n">inputs</span> <span class="o">=</span> <span class="n">tokenizer</span><span class="p">(</span><span class="n">prompt</span><span class="p">,</span> <span class="n">return_tensors</span><span class="o">=</span><span class="s2">&quot;pt&quot;</span><span class="p">)</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">model</span><span class="o">.</span><span class="n">device</span><span class="p">)</span>

    <span class="n">outputs</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">generate</span><span class="p">(</span>
        <span class="o">**</span><span class="n">inputs</span><span class="p">,</span>
        <span class="n">max_new_tokens</span><span class="o">=</span><span class="n">max_new_tokens</span><span class="p">,</span>
        <span class="n">temperature</span><span class="o">=</span><span class="n">temperature</span><span class="p">,</span>
        <span class="n">do_sample</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
        <span class="n">top_p</span><span class="o">=</span><span class="mf">0.9</span><span class="p">,</span>
        <span class="n">pad_token_id</span><span class="o">=</span><span class="n">tokenizer</span><span class="o">.</span><span class="n">eos_token_id</span>
    <span class="p">)</span>

    <span class="k">return</span> <span class="n">tokenizer</span><span class="o">.</span><span class="n">decode</span><span class="p">(</span><span class="n">outputs</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">skip_special_tokens</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>

<span class="c1"># ì‚¬ìš© ì˜ˆì‹œ</span>
<span class="n">prompt</span> <span class="o">=</span> <span class="s2">&quot;Explain the concept of machine learning in simple terms:&quot;</span>
<span class="n">response</span> <span class="o">=</span> <span class="n">generate_text</span><span class="p">(</span><span class="n">prompt</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">response</span><span class="p">)</span>
</code></pre></div>

<h3 id="72">7.2 ì–‘ìí™”ë¡œ íš¨ìœ¨ì  ì‚¬ìš©<a class="header-link" href="#72" title="Permanent link">&para;</a></h3>
<div class="highlight"><pre><span></span><code><span class="kn">from</span><span class="w"> </span><span class="nn">transformers</span><span class="w"> </span><span class="kn">import</span> <span class="n">AutoModelForCausalLM</span><span class="p">,</span> <span class="n">AutoTokenizer</span><span class="p">,</span> <span class="n">BitsAndBytesConfig</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">torch</span>

<span class="c1"># 4-bit ì–‘ìí™” ì„¤ì •</span>
<span class="n">bnb_config</span> <span class="o">=</span> <span class="n">BitsAndBytesConfig</span><span class="p">(</span>
    <span class="n">load_in_4bit</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
    <span class="n">bnb_4bit_quant_type</span><span class="o">=</span><span class="s2">&quot;nf4&quot;</span><span class="p">,</span>          <span class="c1"># NormalFloat4</span>
    <span class="n">bnb_4bit_compute_dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">float16</span><span class="p">,</span>
    <span class="n">bnb_4bit_use_double_quant</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>     <span class="c1"># ì´ì¤‘ ì–‘ìí™”</span>
<span class="p">)</span>

<span class="c1"># ì–‘ìí™”ëœ ëª¨ë¸ ë¡œë“œ</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">AutoModelForCausalLM</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span>
    <span class="s2">&quot;meta-llama/Llama-2-7b-hf&quot;</span><span class="p">,</span>
    <span class="n">quantization_config</span><span class="o">=</span><span class="n">bnb_config</span><span class="p">,</span>
    <span class="n">device_map</span><span class="o">=</span><span class="s2">&quot;auto&quot;</span>
<span class="p">)</span>

<span class="c1"># ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ í™•ì¸</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Model memory: </span><span class="si">{</span><span class="n">model</span><span class="o">.</span><span class="n">get_memory_footprint</span><span class="p">()</span><span class="w"> </span><span class="o">/</span><span class="w"> </span><span class="mf">1e9</span><span class="si">:</span><span class="s2">.2f</span><span class="si">}</span><span class="s2"> GB&quot;</span><span class="p">)</span>
<span class="c1"># ì•½ 4GB (FP16 ëŒ€ë¹„ 75% ì ˆì•½)</span>
</code></pre></div>

<h3 id="73-llama-3">7.3 LLaMA 3 ì‚¬ìš©<a class="header-link" href="#73-llama-3" title="Permanent link">&para;</a></h3>
<div class="highlight"><pre><span></span><code><span class="c1"># LLaMA 3 8B (128K í† í¬ë‚˜ì´ì €)</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">transformers</span><span class="w"> </span><span class="kn">import</span> <span class="n">AutoModelForCausalLM</span><span class="p">,</span> <span class="n">AutoTokenizer</span>

<span class="n">model_name</span> <span class="o">=</span> <span class="s2">&quot;meta-llama/Meta-Llama-3-8B&quot;</span>

<span class="n">tokenizer</span> <span class="o">=</span> <span class="n">AutoTokenizer</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="n">model_name</span><span class="p">)</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">AutoModelForCausalLM</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span>
    <span class="n">model_name</span><span class="p">,</span>
    <span class="n">torch_dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">bfloat16</span><span class="p">,</span>  <span class="c1"># LLaMA 3ëŠ” bfloat16 ê¶Œì¥</span>
    <span class="n">device_map</span><span class="o">=</span><span class="s2">&quot;auto&quot;</span>
<span class="p">)</span>

<span class="c1"># LLaMA 3 íŠ¹ì§•:</span>
<span class="c1"># - 128K í† í¬ë‚˜ì´ì € (ë” íš¨ìœ¨ì )</span>
<span class="c1"># - 8K ê¸°ë³¸ ì»¨í…ìŠ¤íŠ¸ (128Kê¹Œì§€ í™•ì¥ ê°€ëŠ¥)</span>
<span class="c1"># - ê°œì„ ëœ ì¶”ë¡  ëŠ¥ë ¥</span>

<span class="n">prompt</span> <span class="o">=</span> <span class="s2">&quot;&quot;&quot;&lt;|begin_of_text|&gt;&lt;|start_header_id|&gt;user&lt;|end_header_id|&gt;</span>

<span class="s2">What is the capital of France?&lt;|eot_id|&gt;&lt;|start_header_id|&gt;assistant&lt;|end_header_id|&gt;</span>

<span class="s2">&quot;&quot;&quot;</span>

<span class="n">inputs</span> <span class="o">=</span> <span class="n">tokenizer</span><span class="p">(</span><span class="n">prompt</span><span class="p">,</span> <span class="n">return_tensors</span><span class="o">=</span><span class="s2">&quot;pt&quot;</span><span class="p">)</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">model</span><span class="o">.</span><span class="n">device</span><span class="p">)</span>
<span class="n">outputs</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">generate</span><span class="p">(</span><span class="o">**</span><span class="n">inputs</span><span class="p">,</span> <span class="n">max_new_tokens</span><span class="o">=</span><span class="mi">50</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">tokenizer</span><span class="o">.</span><span class="n">decode</span><span class="p">(</span><span class="n">outputs</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">skip_special_tokens</span><span class="o">=</span><span class="kc">True</span><span class="p">))</span>
</code></pre></div>

<hr />
<h2 id="8-llama-3132">8. LLaMA 3.1/3.2 ìƒì„¸<a class="header-link" href="#8-llama-3132" title="Permanent link">&para;</a></h2>
<h3 id="81-llama-31-2024-7">8.1 LLaMA 3.1 (2024ë…„ 7ì›”)<a class="header-link" href="#81-llama-31-2024-7" title="Permanent link">&para;</a></h3>
<div class="highlight"><pre><span></span><code>â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    LLaMA 3.1 ì£¼ìš” íŠ¹ì§•                           â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                 â”‚
â”‚  1. 128K ë„¤ì´í‹°ë¸Œ ì»¨í…ìŠ¤íŠ¸                                        â”‚
â”‚     â€¢ í•™ìŠµ ì‹œë¶€í„° 128K í† í° ì§€ì›                                  â”‚
â”‚     â€¢ RoPE scaling ì—†ì´ ê¸´ ë¬¸ë§¥ ì²˜ë¦¬                             â”‚
â”‚                                                                 â”‚
â”‚  2. 405B í”Œë˜ê·¸ì‹­ ëª¨ë¸                                           â”‚
â”‚     â€¢ GPT-4 ìˆ˜ì¤€ ì„±ëŠ¥                                            â”‚
â”‚     â€¢ 126ê°œ ë ˆì´ì–´, 16K ì„ë² ë”© ì°¨ì›                               â”‚
â”‚                                                                 â”‚
â”‚  3. Tool Use ê¸°ëŠ¥                                                â”‚
â”‚     â€¢ í•¨ìˆ˜ í˜¸ì¶œ (Function Calling)                               â”‚
â”‚     â€¢ ì½”ë“œ ì¸í„°í”„ë¦¬í„°                                            â”‚
â”‚     â€¢ ê²€ìƒ‰ ë„êµ¬ í†µí•©                                             â”‚
â”‚                                                                 â”‚
â”‚  4. ë‹¤êµ­ì–´ ì§€ì› ê°•í™”                                             â”‚
â”‚     â€¢ ì˜ì–´, ë…ì¼ì–´, í”„ë‘ìŠ¤ì–´, ì´íƒˆë¦¬ì•„ì–´                          â”‚
â”‚     â€¢ í¬ë¥´íˆ¬ê°ˆì–´, íŒë””ì–´, ìŠ¤í˜ì¸ì–´, íƒœêµ­ì–´                        â”‚
â”‚                                                                 â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
</code></pre></div>

<div class="highlight"><pre><span></span><code><span class="c1"># LLaMA 3.1 Tool Use ì˜ˆì œ</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">transformers</span><span class="w"> </span><span class="kn">import</span> <span class="n">AutoModelForCausalLM</span><span class="p">,</span> <span class="n">AutoTokenizer</span>

<span class="n">model_name</span> <span class="o">=</span> <span class="s2">&quot;meta-llama/Llama-3.1-8B-Instruct&quot;</span>

<span class="n">tokenizer</span> <span class="o">=</span> <span class="n">AutoTokenizer</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="n">model_name</span><span class="p">)</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">AutoModelForCausalLM</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span>
    <span class="n">model_name</span><span class="p">,</span>
    <span class="n">torch_dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">bfloat16</span><span class="p">,</span>
    <span class="n">device_map</span><span class="o">=</span><span class="s2">&quot;auto&quot;</span>
<span class="p">)</span>

<span class="c1"># Tool Use í˜•ì‹ (LLaMA 3.1 íŠ¹í™”)</span>
<span class="n">tools</span> <span class="o">=</span> <span class="p">[</span>
    <span class="p">{</span>
        <span class="s2">&quot;type&quot;</span><span class="p">:</span> <span class="s2">&quot;function&quot;</span><span class="p">,</span>
        <span class="s2">&quot;function&quot;</span><span class="p">:</span> <span class="p">{</span>
            <span class="s2">&quot;name&quot;</span><span class="p">:</span> <span class="s2">&quot;get_weather&quot;</span><span class="p">,</span>
            <span class="s2">&quot;description&quot;</span><span class="p">:</span> <span class="s2">&quot;Get current weather for a location&quot;</span><span class="p">,</span>
            <span class="s2">&quot;parameters&quot;</span><span class="p">:</span> <span class="p">{</span>
                <span class="s2">&quot;type&quot;</span><span class="p">:</span> <span class="s2">&quot;object&quot;</span><span class="p">,</span>
                <span class="s2">&quot;properties&quot;</span><span class="p">:</span> <span class="p">{</span>
                    <span class="s2">&quot;location&quot;</span><span class="p">:</span> <span class="p">{</span><span class="s2">&quot;type&quot;</span><span class="p">:</span> <span class="s2">&quot;string&quot;</span><span class="p">,</span> <span class="s2">&quot;description&quot;</span><span class="p">:</span> <span class="s2">&quot;City name&quot;</span><span class="p">}</span>
                <span class="p">},</span>
                <span class="s2">&quot;required&quot;</span><span class="p">:</span> <span class="p">[</span><span class="s2">&quot;location&quot;</span><span class="p">]</span>
            <span class="p">}</span>
        <span class="p">}</span>
    <span class="p">}</span>
<span class="p">]</span>

<span class="n">messages</span> <span class="o">=</span> <span class="p">[</span>
    <span class="p">{</span><span class="s2">&quot;role&quot;</span><span class="p">:</span> <span class="s2">&quot;system&quot;</span><span class="p">,</span> <span class="s2">&quot;content&quot;</span><span class="p">:</span> <span class="s2">&quot;You are a helpful assistant with access to tools.&quot;</span><span class="p">},</span>
    <span class="p">{</span><span class="s2">&quot;role&quot;</span><span class="p">:</span> <span class="s2">&quot;user&quot;</span><span class="p">,</span> <span class="s2">&quot;content&quot;</span><span class="p">:</span> <span class="s2">&quot;What&#39;s the weather in Seoul?&quot;</span><span class="p">}</span>
<span class="p">]</span>

<span class="c1"># Tool í˜¸ì¶œ ìƒì„±</span>
<span class="n">inputs</span> <span class="o">=</span> <span class="n">tokenizer</span><span class="o">.</span><span class="n">apply_chat_template</span><span class="p">(</span>
    <span class="n">messages</span><span class="p">,</span>
    <span class="n">tools</span><span class="o">=</span><span class="n">tools</span><span class="p">,</span>
    <span class="n">return_tensors</span><span class="o">=</span><span class="s2">&quot;pt&quot;</span>
<span class="p">)</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">model</span><span class="o">.</span><span class="n">device</span><span class="p">)</span>

<span class="n">outputs</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">generate</span><span class="p">(</span><span class="n">inputs</span><span class="p">,</span> <span class="n">max_new_tokens</span><span class="o">=</span><span class="mi">256</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">tokenizer</span><span class="o">.</span><span class="n">decode</span><span class="p">(</span><span class="n">outputs</span><span class="p">[</span><span class="mi">0</span><span class="p">]))</span>
</code></pre></div>

<h3 id="82-llama-32-2024-9">8.2 LLaMA 3.2 (2024ë…„ 9ì›”)<a class="header-link" href="#82-llama-32-2024-9" title="Permanent link">&para;</a></h3>
<div class="highlight"><pre><span></span><code>â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    LLaMA 3.2 ëª¨ë¸ ë¼ì¸ì—…                          â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                 â”‚
â”‚  ê²½ëŸ‰ í…ìŠ¤íŠ¸ ëª¨ë¸ (on-device ìµœì í™”):                             â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                â”‚
â”‚  â”‚  LLaMA 3.2 1B: ëª¨ë°”ì¼/ì—ì§€ ë””ë°”ì´ìŠ¤ìš©         â”‚                â”‚
â”‚  â”‚  LLaMA 3.2 3B: ê²½ëŸ‰ ì• í”Œë¦¬ì¼€ì´ì…˜ìš©            â”‚                â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                â”‚
â”‚                                                                 â”‚
â”‚  ë¹„ì „ ëª¨ë¸ (ë©€í‹°ëª¨ë‹¬):                                            â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                â”‚
â”‚  â”‚  LLaMA 3.2 11B-Vision: ì´ë¯¸ì§€ ì´í•´           â”‚                â”‚
â”‚  â”‚  LLaMA 3.2 90B-Vision: ê³ ì„±ëŠ¥ ë¹„ì „ íƒœìŠ¤í¬    â”‚                â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                â”‚
â”‚                                                                 â”‚
â”‚  íŠ¹ì§•:                                                           â”‚
â”‚  â€¢ 1B/3B: 128K ì»¨í…ìŠ¤íŠ¸, on-device ì¶”ë¡  ê°€ëŠ¥                     â”‚
â”‚  â€¢ 11B/90B: ë¹„ì „ ì¸ì½”ë” í†µí•©, ì´ë¯¸ì§€+í…ìŠ¤íŠ¸ ì²˜ë¦¬                   â”‚
â”‚  â€¢ Qualcomm, MediaTek í•˜ë“œì›¨ì–´ ìµœì í™”                            â”‚
â”‚                                                                 â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
</code></pre></div>

<div class="highlight"><pre><span></span><code><span class="c1"># LLaMA 3.2 Vision ì˜ˆì œ</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">transformers</span><span class="w"> </span><span class="kn">import</span> <span class="n">MllamaForConditionalGeneration</span><span class="p">,</span> <span class="n">AutoProcessor</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">PIL</span><span class="w"> </span><span class="kn">import</span> <span class="n">Image</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">requests</span>

<span class="n">model_id</span> <span class="o">=</span> <span class="s2">&quot;meta-llama/Llama-3.2-11B-Vision-Instruct&quot;</span>

<span class="n">model</span> <span class="o">=</span> <span class="n">MllamaForConditionalGeneration</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span>
    <span class="n">model_id</span><span class="p">,</span>
    <span class="n">torch_dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">bfloat16</span><span class="p">,</span>
    <span class="n">device_map</span><span class="o">=</span><span class="s2">&quot;auto&quot;</span><span class="p">,</span>
<span class="p">)</span>
<span class="n">processor</span> <span class="o">=</span> <span class="n">AutoProcessor</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="n">model_id</span><span class="p">)</span>

<span class="c1"># ì´ë¯¸ì§€ ë¡œë“œ</span>
<span class="n">url</span> <span class="o">=</span> <span class="s2">&quot;https://example.com/image.jpg&quot;</span>
<span class="n">image</span> <span class="o">=</span> <span class="n">Image</span><span class="o">.</span><span class="n">open</span><span class="p">(</span><span class="n">requests</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="n">url</span><span class="p">,</span> <span class="n">stream</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span><span class="o">.</span><span class="n">raw</span><span class="p">)</span>

<span class="c1"># ë¹„ì „ ëŒ€í™”</span>
<span class="n">messages</span> <span class="o">=</span> <span class="p">[</span>
    <span class="p">{</span>
        <span class="s2">&quot;role&quot;</span><span class="p">:</span> <span class="s2">&quot;user&quot;</span><span class="p">,</span>
        <span class="s2">&quot;content&quot;</span><span class="p">:</span> <span class="p">[</span>
            <span class="p">{</span><span class="s2">&quot;type&quot;</span><span class="p">:</span> <span class="s2">&quot;image&quot;</span><span class="p">},</span>
            <span class="p">{</span><span class="s2">&quot;type&quot;</span><span class="p">:</span> <span class="s2">&quot;text&quot;</span><span class="p">,</span> <span class="s2">&quot;text&quot;</span><span class="p">:</span> <span class="s2">&quot;ì´ ì´ë¯¸ì§€ì—ì„œ ë¬´ì—‡ì´ ë³´ì´ë‚˜ìš”?&quot;</span><span class="p">}</span>
        <span class="p">]</span>
    <span class="p">}</span>
<span class="p">]</span>

<span class="n">input_text</span> <span class="o">=</span> <span class="n">processor</span><span class="o">.</span><span class="n">apply_chat_template</span><span class="p">(</span><span class="n">messages</span><span class="p">,</span> <span class="n">add_generation_prompt</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="n">inputs</span> <span class="o">=</span> <span class="n">processor</span><span class="p">(</span><span class="n">image</span><span class="p">,</span> <span class="n">input_text</span><span class="p">,</span> <span class="n">return_tensors</span><span class="o">=</span><span class="s2">&quot;pt&quot;</span><span class="p">)</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">model</span><span class="o">.</span><span class="n">device</span><span class="p">)</span>

<span class="n">output</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">generate</span><span class="p">(</span><span class="o">**</span><span class="n">inputs</span><span class="p">,</span> <span class="n">max_new_tokens</span><span class="o">=</span><span class="mi">256</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">processor</span><span class="o">.</span><span class="n">decode</span><span class="p">(</span><span class="n">output</span><span class="p">[</span><span class="mi">0</span><span class="p">]))</span>
</code></pre></div>

<hr />
<h2 id="_2">ì •ë¦¬<a class="header-link" href="#_2" title="Permanent link">&para;</a></h2>
<h3 id="llama">LLaMA í•µì‹¬ ê¸°ìˆ <a class="header-link" href="#llama" title="Permanent link">&para;</a></h3>
<table>
<thead>
<tr>
<th>ê¸°ìˆ </th>
<th>íš¨ê³¼</th>
</tr>
</thead>
<tbody>
<tr>
<td><strong>RoPE</strong></td>
<td>ìƒëŒ€ ìœ„ì¹˜ ì¸ì½”ë”©, ê¸¸ì´ í™•ì¥ ê°€ëŠ¥</td>
</tr>
<tr>
<td><strong>RMSNorm</strong></td>
<td>LayerNormë³´ë‹¤ ë¹ ë¥´ê³  íš¨ê³¼ì </td>
</tr>
<tr>
<td><strong>SwiGLU</strong></td>
<td>GELUë³´ë‹¤ ì¢‹ì€ ì„±ëŠ¥</td>
</tr>
<tr>
<td><strong>GQA</strong></td>
<td>KV cache ë©”ëª¨ë¦¬ 8ë°° ì ˆì•½</td>
</tr>
</tbody>
</table>
<h3 id="_3">ì‹¤ë¬´ ê¶Œì¥ ì‚¬í•­<a class="header-link" href="#_3" title="Permanent link">&para;</a></h3>
<ol>
<li><strong>7B/8B</strong>: ë‹¨ì¼ GPU (16GB+), ë¹ ë¥¸ ì‹¤í—˜ìš©</li>
<li><strong>13B</strong>: 24GB GPU, ê· í˜• ì¡íŒ ì„ íƒ</li>
<li><strong>70B</strong>: ì—¬ëŸ¬ GPU, ìµœê³  ì„±ëŠ¥ í•„ìš” ì‹œ</li>
<li><strong>ì–‘ìí™”</strong>: 4-bitìœ¼ë¡œ ë©”ëª¨ë¦¬ 75% ì ˆì•½</li>
</ol>
<h3 id="_4">ë‹¤ìŒ ë‹¨ê³„<a class="header-link" href="#_4" title="Permanent link">&para;</a></h3>
<ul>
<li><a href="09_Mistral_MoE.md">09_Mistral_MoE.md</a>: Mixture of Experts ì•„í‚¤í…ì²˜</li>
<li><a href="19_PEFT_Unified.md">19_PEFT_Unified.md</a>: LLaMA Fine-tuning (LoRA)</li>
</ul>
<hr />
<h2 id="_5">ì°¸ê³  ìë£Œ<a class="header-link" href="#_5" title="Permanent link">&para;</a></h2>
<h3 id="_6">í•µì‹¬ ë…¼ë¬¸<a class="header-link" href="#_6" title="Permanent link">&para;</a></h3>
<ul>
<li>Touvron et al. (2023). "LLaMA: Open and Efficient Foundation Language Models"</li>
<li>Touvron et al. (2023). "LLaMA 2: Open Foundation and Fine-Tuned Chat Models"</li>
<li>Su et al. (2021). "RoFormer: Enhanced Transformer with Rotary Position Embedding"</li>
<li>Ainslie et al. (2023). "GQA: Training Generalized Multi-Query Transformer Models"</li>
</ul>
<h3 id="_7">ì½”ë“œ &amp; ìë£Œ<a class="header-link" href="#_7" title="Permanent link">&para;</a></h3>
<ul>
<li><a href="https://github.com/facebookresearch/llama">LLaMA GitHub</a></li>
<li><a href="https://huggingface.co/meta-llama">HuggingFace LLaMA</a></li>
<li><a href="https://github.com/meta-llama/llama-recipes">LLaMA 3 Recipes</a></li>
</ul>
    </div>

    
<div class="lesson-toolbar lesson-toolbar--bottom">
    <div class="toolbar-nav toolbar-nav--prev">
        
        <a href="/study/ko/Foundation_Models/07_Tokenization_Deep_Dive.html" class="nav-prev">
            <span class="nav-label">Previous</span>
            <span class="nav-title">07. Tokenization ì‹¬í™”</span>
        </a>
        
    </div>
    <div class="toolbar-actions">
        <button class="btn btn-copy-link" data-action="copy-link" title="Copy link">
            <span class="icon">ğŸ”—</span>
            <span class="text">Copy link</span>
        </button>
        <a href="/study/ko/Foundation_Models/" class="btn btn-topic-link" title="Back to topic list">
            <span class="icon">ğŸ“‹</span>
            <span class="text">Topic list</span>
        </a>
    </div>
    <div class="toolbar-nav toolbar-nav--next">
        
        <a href="/study/ko/Foundation_Models/09_Mistral_MoE.html" class="nav-next">
            <span class="nav-label">Next</span>
            <span class="nav-title">Mistral & Mixture of Experts</span>
        </a>
        
    </div>
</div>


    <div class="toolbar-keyboard-hint">
        <kbd>&larr;</kbd> <kbd>&rarr;</kbd> to navigate between lessons
    </div>
</article>

<!-- Scroll to top floating button -->
<button class="scroll-to-top" id="scroll-to-top" title="Scroll to top" aria-label="Scroll to top">â†‘</button>

            </div>
        </main>
    </div>

    <script src="/study/static/js/app.js"></script>
    <script>
        function switchLanguage(newLang) {
            const path = window.location.pathname;
            const base = '/study';
            const currentLang = 'ko';
            const newPath = path.replace(base + '/' + currentLang + '/', base + '/' + newLang + '/');
            window.location.href = newPath + window.location.search;
        }
    </script>
    
<script>
    document.addEventListener('DOMContentLoaded', function() {
        // Copy link - independent feedback per button
        document.querySelectorAll('[data-action="copy-link"]').forEach(function(btn) {
            btn.addEventListener('click', function() {
                navigator.clipboard.writeText(window.location.href);
                var textEl = this.querySelector('.text');
                var originalText = textEl.textContent;
                textEl.textContent = 'Copied!';
                setTimeout(function() { textEl.textContent = originalText; }, 2000);
            });
        });

        // Add copy buttons to code blocks
        document.querySelectorAll('.lesson-content pre').forEach(function(pre) {
            var copyBtn = document.createElement('button');
            copyBtn.className = 'code-copy-btn';
            copyBtn.textContent = 'Copy';
            copyBtn.addEventListener('click', function() {
                var code = pre.querySelector('code');
                var text = code ? code.textContent : pre.textContent;
                navigator.clipboard.writeText(text);
                copyBtn.textContent = 'Copied!';
                setTimeout(function() { copyBtn.textContent = 'Copy'; }, 2000);
            });
            pre.style.position = 'relative';
            pre.appendChild(copyBtn);
        });

        // Scroll to top button
        var scrollBtn = document.getElementById('scroll-to-top');
        window.addEventListener('scroll', function() {
            scrollBtn.classList.toggle('visible', window.scrollY > 300);
        });
        scrollBtn.addEventListener('click', function() {
            window.scrollTo({ top: 0, behavior: 'smooth' });
        });

        // Keyboard shortcuts: left/right arrows for lesson navigation
        document.addEventListener('keydown', function(e) {
            var tag = document.activeElement.tagName;
            if (tag === 'INPUT' || tag === 'TEXTAREA' || tag === 'SELECT') return;

            if (e.key === 'ArrowLeft') {
                var prevLink = document.querySelector('.nav-prev');
                if (prevLink) prevLink.click();
            } else if (e.key === 'ArrowRight') {
                var nextLink = document.querySelector('.nav-next');
                if (nextLink) nextLink.click();
            }
        });
    });
</script>

</body>
</html>
{% endraw %}