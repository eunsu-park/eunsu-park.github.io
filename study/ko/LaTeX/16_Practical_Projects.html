{% raw %}
<!DOCTYPE html>
<html lang="ko" data-theme="light">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>ì‹¤ì „ í”„ë¡œì íŠ¸(Practical Projects) - Study Materials</title>
    <link rel="stylesheet" href="/study/static/css/style.css">
    <link rel="stylesheet" href="/study/static/css/highlight.css">
    <!-- KaTeX for LaTeX math rendering -->
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.11/dist/katex.min.css">
    <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.11/dist/katex.min.js"></script>
    <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.11/dist/contrib/auto-render.min.js"
        onload="renderMathInElement(document.body, {
            delimiters: [
                {left: '$$', right: '$$', display: true},
                {left: '$', right: '$', display: false}
            ],
            throwOnError: false
        });"></script>
    
</head>
<body>
    <div class="app-container">
        <!-- Sidebar -->
        <aside class="sidebar" id="sidebar">
            <div class="sidebar-header">
                <a href="/study/ko/" class="logo">Study Materials</a>
                <button class="sidebar-toggle" id="sidebar-toggle" aria-label="Toggle sidebar">
                    <span></span>
                </button>
            </div>

            <nav class="sidebar-nav">
                <a href="/study/ko/" class="nav-item ">
                    <span class="nav-icon">ğŸ </span>
                    <span class="nav-text">Home</span>
                </a>
                <a href="/study/examples/" class="nav-item ">
                    <span class="nav-icon">ğŸ’»</span>
                    <span class="nav-text">Examples</span>
                </a>
            </nav>

            <div class="sidebar-search">
                <form action="/study/ko/search.html" method="get" id="search-form">
                    <input type="search" name="q" placeholder="Search..." id="search-sidebar-input">
                </form>
            </div>

            <!-- Language Selector -->
            <div class="sidebar-lang">
                <select id="lang-select" class="lang-selector" onchange="switchLanguage(this.value)">
                    
                    <option value="en" >
                        English
                    </option>
                    
                    <option value="ko" selected>
                        í•œêµ­ì–´
                    </option>
                    
                </select>
            </div>

            <div class="sidebar-footer">
                <button class="theme-toggle" id="theme-toggle" aria-label="Toggle theme">
                    <span class="theme-icon light">â˜€ï¸</span>
                    <span class="theme-icon dark">ğŸŒ™</span>
                </button>
            </div>
        </aside>

        <!-- Main Content -->
        <main class="main-content">
            <header class="main-header">
                <button class="menu-toggle" id="menu-toggle" aria-label="Open menu">
                    <span></span>
                </button>
                
<nav class="breadcrumb">
    <a href="/study/ko/">Topics</a>
    <span class="separator">/</span>
    <a href="/study/ko/LaTeX/">LaTeX</a>
    <span class="separator">/</span>
    <span class="current">ì‹¤ì „ í”„ë¡œì íŠ¸(Practical Projects)</span>
</nav>

            </header>

            <div class="content">
                
<article class="lesson-article">
    <header class="lesson-header">
        <h1>ì‹¤ì „ í”„ë¡œì íŠ¸(Practical Projects)</h1>
    </header>

    
<div class="lesson-toolbar lesson-toolbar--top">
    <div class="toolbar-nav toolbar-nav--prev">
        
        <a href="/study/ko/LaTeX/15_Automation_and_Build.html" class="nav-prev">
            <span class="nav-label">Previous</span>
            <span class="nav-title">ë¹Œë“œ ì‹œìŠ¤í…œ ë° ìë™í™”(Build Systems & Automation)</span>
        </a>
        
    </div>
    <div class="toolbar-actions">
        <button class="btn btn-copy-link" data-action="copy-link" title="Copy link">
            <span class="icon">ğŸ”—</span>
            <span class="text">Copy link</span>
        </button>
        <a href="/study/ko/LaTeX/" class="btn btn-topic-link" title="Back to topic list">
            <span class="icon">ğŸ“‹</span>
            <span class="text">Topic list</span>
        </a>
    </div>
    <div class="toolbar-nav toolbar-nav--next">
        
    </div>
</div>


    
    <nav class="toc" id="toc">
        <h2>Table of Contents</h2>
        <div class="toc">
<ul>
<li><a href="#_1">ì†Œê°œ</a></li>
<li><a href="#1">í”„ë¡œì íŠ¸ 1: í•™ìˆ  ë…¼ë¬¸</a><ul>
<li><a href="#_2">ê°œìš”</a></li>
<li><a href="#_3">ì™„ì „í•œ ì†ŒìŠ¤ ì½”ë“œ</a></li>
<li><a href="#_4">ì»´íŒŒì¼</a></li>
<li><a href="#_5">ì¼ë°˜ì ì¸ í•¨ì •</a></li>
<li><a href="#_6">ì‚¬ìš©ì ì •ì˜ íŒ</a></li>
</ul>
</li>
<li><a href="#2-beamer">í”„ë¡œì íŠ¸ 2: Beamer í”„ë ˆì  í…Œì´ì…˜</a><ul>
<li><a href="#_7">ê°œìš”</a></li>
<li><a href="#_8">ì™„ì „í•œ ì†ŒìŠ¤ ì½”ë“œ</a></li>
<li><a href="#_9">ì»´íŒŒì¼</a></li>
<li><a href="#_10">í•¸ë“œì•„ì›ƒ ìƒì„±</a></li>
<li><a href="#_11">ë°œí‘œì ë…¸íŠ¸</a></li>
<li><a href="#_12">ì¼ë°˜ì ì¸ í•¨ì •</a></li>
</ul>
</li>
<li><a href="#3-tikz">í”„ë¡œì íŠ¸ 3: TikZ ê³¼í•™ í¬ìŠ¤í„°</a><ul>
<li><a href="#_13">ê°œìš”</a></li>
<li><a href="#_14">ì™„ì „í•œ ì†ŒìŠ¤ ì½”ë“œ</a></li>
<li><a href="#_15">ì»´íŒŒì¼</a></li>
<li><a href="#_16">ì¸ì‡„</a></li>
<li><a href="#_17">ì¼ë°˜ì ì¸ í•¨ì •</a></li>
</ul>
</li>
<li><a href="#_18">ë ˆìŠ¨ í†µí•©: í†µí•© ë§µ</a></li>
<li><a href="#_19">ë‹¤ìŒ ë‹¨ê³„</a><ul>
<li><a href="#_20">ê³ ê¸‰ ì£¼ì œ íƒìƒ‰</a></li>
<li><a href="#_21">ì»¤ë®¤ë‹ˆí‹° ì°¸ì—¬</a></li>
<li><a href="#_22">ì—°ìŠµ í”„ë¡œì íŠ¸</a></li>
</ul>
</li>
<li><a href="#_23">ìš”ì•½</a></li>
</ul>
</div>

    </nav>
    

    <div class="lesson-content markdown-body">
        <h1 id="practical-projects">ì‹¤ì „ í”„ë¡œì íŠ¸(Practical Projects)<a class="header-link" href="#practical-projects" title="Permanent link">&para;</a></h1>
<blockquote>
<p><strong>í† í”½</strong>: LaTeX
<strong>ë ˆìŠ¨</strong>: 16 of 16
<strong>ì„ ìˆ˜ì§€ì‹</strong>: ëª¨ë“  ì´ì „ ë ˆìŠ¨ (01-15)
<strong>ëª©í‘œ</strong>: í•™ìŠµí•œ ëª¨ë“  ê°œë…ì„ ì„¸ ê°€ì§€ ì™„ì „í•œ ì‹¤ì œ í”„ë¡œì íŠ¸ì— ì ìš©: í•™ìˆ  ë…¼ë¬¸, Beamer í”„ë ˆì  í…Œì´ì…˜, ê³¼í•™ í¬ìŠ¤í„°</p>
</blockquote>
<h2 id="_1">ì†Œê°œ<a class="header-link" href="#_1" title="Permanent link">&para;</a></h2>
<p>ì´ ë§ˆì§€ë§‰ ë ˆìŠ¨ì€ ì´ì „ 15ê°œ ë ˆìŠ¨ì˜ ëª¨ë“  ë‚´ìš©ì„ <strong>ì„¸ ê°€ì§€ ì™„ì „í•˜ê³  ì»´íŒŒì¼ ê°€ëŠ¥í•œ í”„ë¡œì íŠ¸</strong>ë¡œ í†µí•©í•©ë‹ˆë‹¤:</p>
<ol>
<li><strong>í•™ìˆ  ë…¼ë¬¸(Academic Paper)</strong>: ì´ˆë¡, ì„¹ì…˜, ê·¸ë¦¼, í‘œ, ìˆ˜ì‹, ì°¸ê³ ë¬¸í—Œì´ ìˆëŠ” ì™„ì „í•œ ì—°êµ¬ ë…¼ë¬¸</li>
<li><strong>Beamer í”„ë ˆì  í…Œì´ì…˜(Beamer Presentation)</strong>: ì‚¬ìš©ì ì •ì˜ í…Œë§ˆ, ì˜¤ë²„ë ˆì´, TikZ ë‹¤ì´ì–´ê·¸ë¨ì´ ìˆëŠ” 15ìŠ¬ë¼ì´ë“œ í•™íšŒ ë°œí‘œ</li>
<li><strong>TikZ ê³¼í•™ í¬ìŠ¤í„°(TikZ Scientific Poster)</strong>: ë‹¤ì¤‘ ì—´ ë ˆì´ì•„ì›ƒ, í”Œë¡¯, QR ì½”ë“œê°€ ìˆëŠ” A0 í¬ìŠ¤í„°</li>
</ol>
<p>ê° í”„ë¡œì íŠ¸ëŠ” ë‹¤ìŒì„ í¬í•¨í•©ë‹ˆë‹¤:
- ì™„ì „í•œ ì†ŒìŠ¤ ì½”ë“œ
- ì»´íŒŒì¼ ì§€ì¹¨
- ì¼ë°˜ì ì¸ í•¨ì •ê³¼ í•´ê²°ì±…
- ì‚¬ìš©ì ì •ì˜ íŒ
- ì‹¤ì œ ëª¨ë²” ì‚¬ë¡€</p>
<hr />
<h2 id="1">í”„ë¡œì íŠ¸ 1: í•™ìˆ  ë…¼ë¬¸<a class="header-link" href="#1" title="Permanent link">&para;</a></h2>
<h3 id="_2">ê°œìš”<a class="header-link" href="#_2" title="Permanent link">&para;</a></h3>
<p>ë‹¤ìŒì— ì í•©í•œ ì™„ì „í•œ ì—°êµ¬ ë…¼ë¬¸ í…œí”Œë¦¿:
- í•™íšŒ ì œì¶œ
- í•™ìˆ ì§€ ê¸°ì‚¬
- ê¸°ìˆ  ë³´ê³ ì„œ
- ê³¼ì • í•™ê¸° ë…¼ë¬¸</p>
<p><strong>íŠ¹ì§•</strong>:
- ì—¬ëŸ¬ ì €ìì™€ ì†Œì†ì´ ìˆëŠ” ì œëª© í˜ì´ì§€
- ì´ˆë¡ê³¼ í‚¤ì›Œë“œ
- 2ë‹¨ í˜•ì‹
- í•˜ìœ„ ì„¹ì…˜ì´ ìˆëŠ” ì„¹ì…˜
- í•˜ìœ„ ê·¸ë¦¼ì´ ìˆëŠ” ê·¸ë¦¼
- ìº¡ì…˜ì´ ìˆëŠ” í‘œ
- ìˆ˜í•™ ìˆ˜ì‹ (ë²ˆí˜¸ ìˆìŒ/ì—†ìŒ)
- ì•Œê³ ë¦¬ì¦˜ ì˜ì‚¬ì½”ë“œ
- ìƒí˜¸ ì°¸ì¡°
- BibLaTeXë¥¼ ì‚¬ìš©í•œ ì°¸ê³ ë¬¸í—Œ
- í•˜ì´í¼ë§í¬</p>
<h3 id="_3">ì™„ì „í•œ ì†ŒìŠ¤ ì½”ë“œ<a class="header-link" href="#_3" title="Permanent link">&para;</a></h3>
<p><strong>íŒŒì¼: <code>paper.tex</code></strong></p>
<div class="highlight"><pre><span></span><code><span class="k">\documentclass</span><span class="na">[conference]</span><span class="nb">{</span>IEEEtran<span class="nb">}</span>

<span class="c">% Packages</span>
<span class="k">\usepackage</span><span class="na">[utf8]</span><span class="nb">{</span>inputenc<span class="nb">}</span>
<span class="k">\usepackage</span><span class="na">[T1]</span><span class="nb">{</span>fontenc<span class="nb">}</span>
<span class="k">\usepackage</span><span class="nb">{</span>amsmath,amssymb,amsthm<span class="nb">}</span>
<span class="k">\usepackage</span><span class="nb">{</span>graphicx<span class="nb">}</span>
<span class="k">\usepackage</span><span class="nb">{</span>subcaption<span class="nb">}</span>
<span class="k">\usepackage</span><span class="nb">{</span>booktabs<span class="nb">}</span>
<span class="k">\usepackage</span><span class="nb">{</span>algorithm<span class="nb">}</span>
<span class="k">\usepackage</span><span class="nb">{</span>algpseudocode<span class="nb">}</span>
<span class="k">\usepackage</span><span class="na">[backend=biber,style=ieee,sorting=none]</span><span class="nb">{</span>biblatex<span class="nb">}</span>
<span class="k">\usepackage</span><span class="na">[hidelinks]</span><span class="nb">{</span>hyperref<span class="nb">}</span>
<span class="k">\usepackage</span><span class="nb">{</span>cleveref<span class="nb">}</span>

<span class="c">% Bibliography</span>
<span class="k">\addbibresource</span><span class="nb">{</span>references.bib<span class="nb">}</span>

<span class="c">% Custom commands (from Lesson 13)</span>
<span class="k">\newcommand</span><span class="nb">{</span><span class="k">\R</span><span class="nb">}{</span><span class="k">\mathbb</span><span class="nb">{</span>R<span class="nb">}}</span>
<span class="k">\newcommand</span><span class="nb">{</span><span class="k">\norm</span><span class="nb">}</span>[1]<span class="nb">{</span><span class="k">\left\|</span> #1 <span class="k">\right\|</span><span class="nb">}</span>
<span class="k">\DeclareMathOperator*</span><span class="nb">{</span><span class="k">\argmin</span><span class="nb">}{</span>arg<span class="k">\,</span>min<span class="nb">}</span>

<span class="c">% Title and authors</span>
<span class="k">\title</span><span class="nb">{</span>Deep Learning for Time Series Forecasting:<span class="k">\\</span>
A Comparative Study of LSTM and Transformer Models<span class="nb">}</span>

<span class="k">\author</span><span class="nb">{</span>
  <span class="k">\IEEEauthorblockN</span><span class="nb">{</span>Alice Johnson<span class="nb">}</span>
  <span class="k">\IEEEauthorblockA</span><span class="nb">{</span>Department of Computer Science<span class="k">\\</span>
    University of Example<span class="k">\\</span>
    alice.johnson@example.edu<span class="nb">}</span>
  <span class="k">\and</span>
  <span class="k">\IEEEauthorblockN</span><span class="nb">{</span>Bob Smith<span class="nb">}</span>
  <span class="k">\IEEEauthorblockA</span><span class="nb">{</span>Research Lab XYZ<span class="k">\\</span>
    Institute of Technology<span class="k">\\</span>
    bob.smith@xyz.org<span class="nb">}</span>
<span class="nb">}</span>

<span class="k">\begin</span><span class="nb">{</span>document<span class="nb">}</span>

<span class="k">\maketitle</span>

<span class="c">% Abstract</span>
<span class="k">\begin</span><span class="nb">{</span>abstract<span class="nb">}</span>
Time series forecasting is a critical task in many domains including finance, weather prediction, and energy management. Recent advances in deep learning have led to powerful models such as Long Short-Term Memory (LSTM) networks and Transformers. This paper presents a comprehensive comparison of LSTM and Transformer architectures for time series forecasting on three benchmark datasets. We evaluate model performance using Mean Absolute Error (MAE) and Root Mean Squared Error (RMSE). Our experiments show that while Transformers achieve superior accuracy on datasets with long-range dependencies, LSTMs remain competitive on shorter sequences and require significantly less computational resources. We provide implementation details and hyperparameter configurations to facilitate reproducibility.
<span class="k">\end</span><span class="nb">{</span>abstract<span class="nb">}</span>

<span class="k">\begin</span><span class="nb">{</span>IEEEkeywords<span class="nb">}</span>
Time series forecasting, LSTM, Transformer, deep learning, sequence modeling
<span class="k">\end</span><span class="nb">{</span>IEEEkeywords<span class="nb">}</span>

<span class="c">% Introduction</span>
<span class="k">\section</span><span class="nb">{</span>Introduction<span class="nb">}</span>
<span class="k">\label</span><span class="nb">{</span>sec:intro<span class="nb">}</span>

Time series forecasting aims to predict future values based on historical observations. Traditional methods such as ARIMA <span class="k">\cite</span><span class="nb">{</span>box2015time<span class="nb">}</span> and exponential smoothing have been widely used, but recent deep learning approaches have demonstrated superior performance on complex, high-dimensional data.

Long Short-Term Memory (LSTM) networks <span class="k">\cite</span><span class="nb">{</span>hochreiter1997long<span class="nb">}</span>, introduced by Hochreiter and Schmidhuber, were specifically designed to capture long-term dependencies in sequential data. More recently, the Transformer architecture <span class="k">\cite</span><span class="nb">{</span>vaswani2017attention<span class="nb">}</span>, originally developed for natural language processing, has been adapted for time series tasks <span class="k">\cite</span><span class="nb">{</span>zhou2021informer<span class="nb">}</span>.

This paper makes the following contributions:
<span class="k">\begin</span><span class="nb">{</span>itemize<span class="nb">}</span>
  <span class="k">\item</span> A systematic comparison of LSTM and Transformer models on three benchmark datasets
  <span class="k">\item</span> Analysis of computational efficiency and scalability
  <span class="k">\item</span> Open-source implementation and trained model weights
  <span class="k">\item</span> Guidelines for practitioners on model selection
<span class="k">\end</span><span class="nb">{</span>itemize<span class="nb">}</span>

The rest of this paper is organized as follows. <span class="k">\Cref</span><span class="nb">{</span>sec:related<span class="nb">}</span> reviews related work. <span class="k">\Cref</span><span class="nb">{</span>sec:methods<span class="nb">}</span> describes the models and datasets. <span class="k">\Cref</span><span class="nb">{</span>sec:results<span class="nb">}</span> presents experimental results. <span class="k">\Cref</span><span class="nb">{</span>sec:conclusion<span class="nb">}</span> concludes the paper.

<span class="c">% Related Work</span>
<span class="k">\section</span><span class="nb">{</span>Related Work<span class="nb">}</span>
<span class="k">\label</span><span class="nb">{</span>sec:related<span class="nb">}</span>

<span class="k">\subsection</span><span class="nb">{</span>Classical Methods<span class="nb">}</span>

Classical time series forecasting methods include ARIMA <span class="k">\cite</span><span class="nb">{</span>box2015time<span class="nb">}</span>, which models temporal dependencies through autoregressive and moving average components. Holt-Winters exponential smoothing extends these ideas to handle trends and seasonality.

<span class="k">\subsection</span><span class="nb">{</span>Deep Learning Approaches<span class="nb">}</span>

Recurrent Neural Networks (RNNs) and their variants have become popular for sequence modeling. LSTM networks <span class="k">\cite</span><span class="nb">{</span>hochreiter1997long<span class="nb">}</span> address the vanishing gradient problem through gating mechanisms. Gated Recurrent Units (GRUs) <span class="k">\cite</span><span class="nb">{</span>cho2014learning<span class="nb">}</span> simplify LSTM architecture while maintaining performance.

<span class="k">\subsection</span><span class="nb">{</span>Attention Mechanisms<span class="nb">}</span>

The Transformer architecture <span class="k">\cite</span><span class="nb">{</span>vaswani2017attention<span class="nb">}</span> relies entirely on self-attention mechanisms, eliminating recurrence. Informer <span class="k">\cite</span><span class="nb">{</span>zhou2021informer<span class="nb">}</span> adapts Transformers for long-sequence time series forecasting with efficient attention mechanisms.

<span class="c">% Methodology</span>
<span class="k">\section</span><span class="nb">{</span>Methodology<span class="nb">}</span>
<span class="k">\label</span><span class="nb">{</span>sec:methods<span class="nb">}</span>

<span class="k">\subsection</span><span class="nb">{</span>Problem Formulation<span class="nb">}</span>

Let <span class="s">$</span><span class="nv">\mathbf</span><span class="nb">{x} </span><span class="o">=</span><span class="nb"> </span><span class="o">(</span><span class="nb">x_</span><span class="m">1</span><span class="nb">, x_</span><span class="m">2</span><span class="nb">, </span><span class="nv">\ldots</span><span class="nb">, x_T</span><span class="o">)</span><span class="nb"> </span><span class="nv">\in</span><span class="nb"> </span><span class="nv">\R</span><span class="nb">^T</span><span class="s">$</span> denote a univariate time series. The forecasting task is to predict future values <span class="s">$</span><span class="nv">\mathbf</span><span class="nb">{y} </span><span class="o">=</span><span class="nb"> </span><span class="o">(</span><span class="nb">y_</span><span class="m">1</span><span class="nb">, y_</span><span class="m">2</span><span class="nb">, </span><span class="nv">\ldots</span><span class="nb">, y_H</span><span class="o">)</span><span class="s">$</span> given historical observations:
<span class="k">\begin</span><span class="nb">{</span>equation<span class="nb">}</span>
  <span class="k">\mathbf</span><span class="nb">{</span>y<span class="nb">}</span> = f(<span class="k">\mathbf</span><span class="nb">{</span>x<span class="nb">}</span>; <span class="k">\theta</span>)
  <span class="k">\label</span><span class="nb">{</span>eq:forecast<span class="nb">}</span>
<span class="k">\end</span><span class="nb">{</span>equation<span class="nb">}</span>
where <span class="s">$</span><span class="nb">f</span><span class="s">$</span> is the model parameterized by <span class="s">$</span><span class="nv">\theta</span><span class="s">$</span>, and <span class="s">$</span><span class="nb">H</span><span class="s">$</span> is the forecast horizon.

<span class="k">\subsection</span><span class="nb">{</span>LSTM Model<span class="nb">}</span>

The LSTM cell computes hidden state <span class="s">$</span><span class="nv">\mathbf</span><span class="nb">{h}_t</span><span class="s">$</span> and cell state <span class="s">$</span><span class="nv">\mathbf</span><span class="nb">{c}_t</span><span class="s">$</span> as:
<span class="k">\begin</span><span class="nb">{</span>align<span class="nb">}</span>
  <span class="k">\mathbf</span><span class="nb">{</span>f<span class="nb">}_</span>t <span class="nb">&amp;</span>= <span class="k">\sigma</span>(<span class="k">\mathbf</span><span class="nb">{</span>W<span class="nb">}_</span>f [<span class="k">\mathbf</span><span class="nb">{</span>h<span class="nb">}_{</span>t-1<span class="nb">}</span>, x<span class="nb">_</span>t] + <span class="k">\mathbf</span><span class="nb">{</span>b<span class="nb">}_</span>f) <span class="k">\label</span><span class="nb">{</span>eq:forget-gate<span class="nb">}</span> <span class="k">\\</span>
  <span class="k">\mathbf</span><span class="nb">{</span>i<span class="nb">}_</span>t <span class="nb">&amp;</span>= <span class="k">\sigma</span>(<span class="k">\mathbf</span><span class="nb">{</span>W<span class="nb">}_</span>i [<span class="k">\mathbf</span><span class="nb">{</span>h<span class="nb">}_{</span>t-1<span class="nb">}</span>, x<span class="nb">_</span>t] + <span class="k">\mathbf</span><span class="nb">{</span>b<span class="nb">}_</span>i) <span class="k">\label</span><span class="nb">{</span>eq:input-gate<span class="nb">}</span> <span class="k">\\</span>
  <span class="k">\tilde</span><span class="nb">{</span><span class="k">\mathbf</span><span class="nb">{</span>c<span class="nb">}}_</span>t <span class="nb">&amp;</span>= <span class="k">\tanh</span>(<span class="k">\mathbf</span><span class="nb">{</span>W<span class="nb">}_</span>c [<span class="k">\mathbf</span><span class="nb">{</span>h<span class="nb">}_{</span>t-1<span class="nb">}</span>, x<span class="nb">_</span>t] + <span class="k">\mathbf</span><span class="nb">{</span>b<span class="nb">}_</span>c) <span class="k">\\</span>
  <span class="k">\mathbf</span><span class="nb">{</span>c<span class="nb">}_</span>t <span class="nb">&amp;</span>= <span class="k">\mathbf</span><span class="nb">{</span>f<span class="nb">}_</span>t <span class="k">\odot</span> <span class="k">\mathbf</span><span class="nb">{</span>c<span class="nb">}_{</span>t-1<span class="nb">}</span> + <span class="k">\mathbf</span><span class="nb">{</span>i<span class="nb">}_</span>t <span class="k">\odot</span> <span class="k">\tilde</span><span class="nb">{</span><span class="k">\mathbf</span><span class="nb">{</span>c<span class="nb">}}_</span>t <span class="k">\\</span>
  <span class="k">\mathbf</span><span class="nb">{</span>o<span class="nb">}_</span>t <span class="nb">&amp;</span>= <span class="k">\sigma</span>(<span class="k">\mathbf</span><span class="nb">{</span>W<span class="nb">}_</span>o [<span class="k">\mathbf</span><span class="nb">{</span>h<span class="nb">}_{</span>t-1<span class="nb">}</span>, x<span class="nb">_</span>t] + <span class="k">\mathbf</span><span class="nb">{</span>b<span class="nb">}_</span>o) <span class="k">\\</span>
  <span class="k">\mathbf</span><span class="nb">{</span>h<span class="nb">}_</span>t <span class="nb">&amp;</span>= <span class="k">\mathbf</span><span class="nb">{</span>o<span class="nb">}_</span>t <span class="k">\odot</span> <span class="k">\tanh</span>(<span class="k">\mathbf</span><span class="nb">{</span>c<span class="nb">}_</span>t)
<span class="k">\end</span><span class="nb">{</span>align<span class="nb">}</span>
where <span class="s">$</span><span class="nv">\sigma</span><span class="s">$</span> is the sigmoid function and <span class="s">$</span><span class="nv">\odot</span><span class="s">$</span> denotes element-wise multiplication.

<span class="k">\subsection</span><span class="nb">{</span>Transformer Model<span class="nb">}</span>

The Transformer uses multi-head self-attention:
<span class="k">\begin</span><span class="nb">{</span>equation<span class="nb">}</span>
  <span class="k">\text</span><span class="nb">{</span>Attention<span class="nb">}</span>(<span class="k">\mathbf</span><span class="nb">{</span>Q<span class="nb">}</span>, <span class="k">\mathbf</span><span class="nb">{</span>K<span class="nb">}</span>, <span class="k">\mathbf</span><span class="nb">{</span>V<span class="nb">}</span>) = <span class="k">\text</span><span class="nb">{</span>softmax<span class="nb">}</span><span class="k">\left</span>(<span class="k">\frac</span><span class="nb">{</span><span class="k">\mathbf</span><span class="nb">{</span>Q<span class="nb">}</span><span class="k">\mathbf</span><span class="nb">{</span>K<span class="nb">}^</span>T<span class="nb">}{</span><span class="k">\sqrt</span><span class="nb">{</span>d<span class="nb">_</span>k<span class="nb">}}</span><span class="k">\right</span>) <span class="k">\mathbf</span><span class="nb">{</span>V<span class="nb">}</span>
<span class="k">\end</span><span class="nb">{</span>equation<span class="nb">}</span>
where <span class="s">$</span><span class="nv">\mathbf</span><span class="nb">{Q}</span><span class="s">$</span>, <span class="s">$</span><span class="nv">\mathbf</span><span class="nb">{K}</span><span class="s">$</span>, <span class="s">$</span><span class="nv">\mathbf</span><span class="nb">{V}</span><span class="s">$</span> are query, key, and value matrices derived from input embeddings.

<span class="k">\subsection</span><span class="nb">{</span>Datasets<span class="nb">}</span>

We evaluate models on three benchmark datasets (<span class="k">\cref</span><span class="nb">{</span>tab:datasets<span class="nb">}</span>):

<span class="k">\begin</span><span class="nb">{</span>table<span class="nb">}</span>[htbp]
<span class="k">\caption</span><span class="nb">{</span>Dataset Statistics<span class="nb">}</span>
<span class="k">\label</span><span class="nb">{</span>tab:datasets<span class="nb">}</span>
<span class="k">\centering</span>
<span class="k">\begin</span><span class="nb">{</span>tabular<span class="nb">}{</span>@<span class="nb">{}</span>lccc@<span class="nb">{}}</span>
<span class="k">\toprule</span>
Dataset <span class="nb">&amp;</span> Samples <span class="nb">&amp;</span> Features <span class="nb">&amp;</span> Frequency <span class="k">\\</span>
<span class="k">\midrule</span>
ETTh1 <span class="nb">&amp;</span> 17,420 <span class="nb">&amp;</span> 7 <span class="nb">&amp;</span> Hourly <span class="k">\\</span>
Weather <span class="nb">&amp;</span> 52,696 <span class="nb">&amp;</span> 21 <span class="nb">&amp;</span> 10 min <span class="k">\\</span>
Electricity <span class="nb">&amp;</span> 26,304 <span class="nb">&amp;</span> 321 <span class="nb">&amp;</span> Hourly <span class="k">\\</span>
<span class="k">\bottomrule</span>
<span class="k">\end</span><span class="nb">{</span>tabular<span class="nb">}</span>
<span class="k">\end</span><span class="nb">{</span>table<span class="nb">}</span>

<span class="k">\subsection</span><span class="nb">{</span>Training Procedure<span class="nb">}</span>

We minimize the Mean Squared Error (MSE) loss:
<span class="k">\begin</span><span class="nb">{</span>equation<span class="nb">}</span>
  L(<span class="k">\theta</span>) = <span class="k">\frac</span><span class="nb">{</span>1<span class="nb">}{</span>N<span class="nb">}</span> <span class="k">\sum_</span><span class="nb">{</span>i=1<span class="nb">}^</span>N <span class="k">\norm</span><span class="nb">{</span><span class="k">\mathbf</span><span class="nb">{</span>y<span class="nb">}^{</span>(i)<span class="nb">}</span> - f(<span class="k">\mathbf</span><span class="nb">{</span>x<span class="nb">}^{</span>(i)<span class="nb">}</span>; <span class="k">\theta</span>)<span class="nb">}^</span>2
<span class="k">\end</span><span class="nb">{</span>equation<span class="nb">}</span>

Models are trained using Adam optimizer <span class="k">\cite</span><span class="nb">{</span>kingma2014adam<span class="nb">}</span> with learning rate <span class="s">$</span><span class="nv">\eta</span><span class="nb"> </span><span class="o">=</span><span class="nb"> </span><span class="m">10</span><span class="nb">^{</span><span class="o">-</span><span class="m">4</span><span class="nb">}</span><span class="s">$</span>. <span class="k">\Cref</span><span class="nb">{</span>alg:training<span class="nb">}</span> summarizes the training procedure.

<span class="k">\begin</span><span class="nb">{</span>algorithm<span class="nb">}</span>[htbp]
<span class="k">\caption</span><span class="nb">{</span>Model Training<span class="nb">}</span>
<span class="k">\label</span><span class="nb">{</span>alg:training<span class="nb">}</span>
<span class="k">\begin</span><span class="nb">{</span>algorithmic<span class="nb">}</span>[1]
<span class="k">\Require</span> Dataset <span class="s">$</span><span class="nv">\mathcal</span><span class="nb">{D} </span><span class="o">=</span><span class="nb"> </span><span class="nv">\{</span><span class="o">(</span><span class="nv">\mathbf</span><span class="nb">{x}^{</span><span class="o">(</span><span class="nb">i</span><span class="o">)</span><span class="nb">}, </span><span class="nv">\mathbf</span><span class="nb">{y}^{</span><span class="o">(</span><span class="nb">i</span><span class="o">)</span><span class="nb">}</span><span class="o">)</span><span class="nv">\}</span><span class="nb">_{i</span><span class="o">=</span><span class="m">1</span><span class="nb">}^N</span><span class="s">$</span>, epochs <span class="s">$</span><span class="nb">E</span><span class="s">$</span>
<span class="k">\State</span> Initialize parameters <span class="s">$</span><span class="nv">\theta</span><span class="s">$</span>
<span class="k">\For</span><span class="nb">{</span><span class="s">$</span><span class="nb">e </span><span class="o">=</span><span class="nb"> </span><span class="m">1</span><span class="s">$</span> to <span class="s">$</span><span class="nb">E</span><span class="s">$</span><span class="nb">}</span>
  <span class="k">\For</span><span class="nb">{</span>each batch <span class="s">$</span><span class="nv">\mathcal</span><span class="nb">{B} </span><span class="nv">\subset</span><span class="nb"> </span><span class="nv">\mathcal</span><span class="nb">{D}</span><span class="s">$</span><span class="nb">}</span>
    <span class="k">\State</span> Compute predictions <span class="s">$</span><span class="nv">\hat</span><span class="nb">{</span><span class="nv">\mathbf</span><span class="nb">{y}} </span><span class="o">=</span><span class="nb"> f</span><span class="o">(</span><span class="nv">\mathbf</span><span class="nb">{x}; </span><span class="nv">\theta</span><span class="o">)</span><span class="s">$</span> for <span class="s">$</span><span class="nv">\mathbf</span><span class="nb">{x} </span><span class="nv">\in</span><span class="nb"> </span><span class="nv">\mathcal</span><span class="nb">{B}</span><span class="s">$</span>
    <span class="k">\State</span> Compute loss <span class="s">$</span><span class="nb">L </span><span class="o">=</span><span class="nb"> </span><span class="nv">\frac</span><span class="nb">{</span><span class="m">1</span><span class="nb">}{|</span><span class="nv">\mathcal</span><span class="nb">{B}|} </span><span class="nv">\sum</span><span class="nb">_{</span><span class="o">(</span><span class="nv">\mathbf</span><span class="nb">{x}, </span><span class="nv">\mathbf</span><span class="nb">{y}</span><span class="o">)</span><span class="nb"> </span><span class="nv">\in</span><span class="nb"> </span><span class="nv">\mathcal</span><span class="nb">{B}} </span><span class="nv">\norm</span><span class="nb">{</span><span class="nv">\mathbf</span><span class="nb">{y} </span><span class="o">-</span><span class="nb"> </span><span class="nv">\hat</span><span class="nb">{</span><span class="nv">\mathbf</span><span class="nb">{y}}}^</span><span class="m">2</span><span class="s">$</span>
    <span class="k">\State</span> Update <span class="s">$</span><span class="nv">\theta</span><span class="nb"> </span><span class="nv">\leftarrow</span><span class="nb"> </span><span class="nv">\theta</span><span class="nb"> </span><span class="o">-</span><span class="nb"> </span><span class="nv">\eta</span><span class="nb"> </span><span class="nv">\nabla</span><span class="nb">_</span><span class="nv">\theta</span><span class="nb"> L</span><span class="s">$</span>
  <span class="k">\EndFor</span>
<span class="k">\EndFor</span>
<span class="k">\State</span> <span class="k">\Return</span> <span class="s">$</span><span class="nv">\theta</span><span class="s">$</span>
<span class="k">\end</span><span class="nb">{</span>algorithmic<span class="nb">}</span>
<span class="k">\end</span><span class="nb">{</span>algorithm<span class="nb">}</span>

<span class="c">% Results</span>
<span class="k">\section</span><span class="nb">{</span>Experimental Results<span class="nb">}</span>
<span class="k">\label</span><span class="nb">{</span>sec:results<span class="nb">}</span>

<span class="k">\subsection</span><span class="nb">{</span>Quantitative Comparison<span class="nb">}</span>

<span class="k">\Cref</span><span class="nb">{</span>fig:results<span class="nb">}</span> shows MAE and RMSE for both models across datasets. Transformers achieve lower error on ETTh1 and Weather datasets, while LSTMs perform comparably on Electricity with 50<span class="k">\%</span> fewer parameters.

<span class="k">\begin</span><span class="nb">{</span>figure<span class="nb">}</span>[htbp]
  <span class="k">\centering</span>
  <span class="k">\begin</span><span class="nb">{</span>subfigure<span class="nb">}</span>[b]<span class="nb">{</span>0.45<span class="k">\columnwidth</span><span class="nb">}</span>
    <span class="k">\centering</span>
    <span class="k">\includegraphics</span><span class="na">[width=\textwidth]</span><span class="nb">{</span>mae<span class="nb">_</span>comparison.pdf<span class="nb">}</span>
    <span class="k">\caption</span><span class="nb">{</span>Mean Absolute Error<span class="nb">}</span>
    <span class="k">\label</span><span class="nb">{</span>fig:mae<span class="nb">}</span>
  <span class="k">\end</span><span class="nb">{</span>subfigure<span class="nb">}</span>
  <span class="k">\hfill</span>
  <span class="k">\begin</span><span class="nb">{</span>subfigure<span class="nb">}</span>[b]<span class="nb">{</span>0.45<span class="k">\columnwidth</span><span class="nb">}</span>
    <span class="k">\centering</span>
    <span class="k">\includegraphics</span><span class="na">[width=\textwidth]</span><span class="nb">{</span>rmse<span class="nb">_</span>comparison.pdf<span class="nb">}</span>
    <span class="k">\caption</span><span class="nb">{</span>Root Mean Squared Error<span class="nb">}</span>
    <span class="k">\label</span><span class="nb">{</span>fig:rmse<span class="nb">}</span>
  <span class="k">\end</span><span class="nb">{</span>subfigure<span class="nb">}</span>
  <span class="k">\caption</span><span class="nb">{</span>Performance comparison on three datasets. Lower is better.<span class="nb">}</span>
  <span class="k">\label</span><span class="nb">{</span>fig:results<span class="nb">}</span>
<span class="k">\end</span><span class="nb">{</span>figure<span class="nb">}</span>

<span class="k">\subsection</span><span class="nb">{</span>Computational Efficiency<span class="nb">}</span>

Training time and memory usage are shown in <span class="k">\cref</span><span class="nb">{</span>tab:efficiency<span class="nb">}</span>. LSTMs train 2-3Ã— faster than Transformers and use less GPU memory.

<span class="k">\begin</span><span class="nb">{</span>table<span class="nb">}</span>[htbp]
<span class="k">\caption</span><span class="nb">{</span>Computational Efficiency (batch size 32)<span class="nb">}</span>
<span class="k">\label</span><span class="nb">{</span>tab:efficiency<span class="nb">}</span>
<span class="k">\centering</span>
<span class="k">\begin</span><span class="nb">{</span>tabular<span class="nb">}{</span>@<span class="nb">{}</span>lcc@<span class="nb">{}}</span>
<span class="k">\toprule</span>
Model <span class="nb">&amp;</span> Training Time (s/epoch) <span class="nb">&amp;</span> GPU Memory (GB) <span class="k">\\</span>
<span class="k">\midrule</span>
LSTM <span class="nb">&amp;</span> 45.2 <span class="nb">&amp;</span> 3.8 <span class="k">\\</span>
Transformer <span class="nb">&amp;</span> 128.7 <span class="nb">&amp;</span> 7.2 <span class="k">\\</span>
<span class="k">\bottomrule</span>
<span class="k">\end</span><span class="nb">{</span>tabular<span class="nb">}</span>
<span class="k">\end</span><span class="nb">{</span>table<span class="nb">}</span>

<span class="c">% Conclusion</span>
<span class="k">\section</span><span class="nb">{</span>Conclusion<span class="nb">}</span>
<span class="k">\label</span><span class="nb">{</span>sec:conclusion<span class="nb">}</span>

This paper presented a comprehensive comparison of LSTM and Transformer models for time series forecasting. Our experiments demonstrate that Transformers excel at capturing long-range dependencies but require more computational resources. LSTMs remain a strong choice for resource-constrained scenarios and shorter sequences.

Future work will explore hybrid architectures combining LSTM and attention mechanisms, and evaluation on multivariate forecasting tasks with more complex dependencies.

<span class="c">% Bibliography</span>
<span class="k">\printbibliography</span>

<span class="k">\end</span><span class="nb">{</span>document<span class="nb">}</span>
</code></pre></div>

<p><strong>íŒŒì¼: <code>references.bib</code></strong></p>
<div class="highlight"><pre><span></span><code><span class="nc">@book</span><span class="p">{</span><span class="nl">box2015time</span><span class="p">,</span>
<span class="w">  </span><span class="na">title</span><span class="p">=</span><span class="s">{Time series analysis: forecasting and control}</span><span class="p">,</span>
<span class="w">  </span><span class="na">author</span><span class="p">=</span><span class="s">{Box, George EP and Jenkins, Gwilym M and Reinsel, Gregory C and Ljung, Greta M}</span><span class="p">,</span>
<span class="w">  </span><span class="na">year</span><span class="p">=</span><span class="s">{2015}</span><span class="p">,</span>
<span class="w">  </span><span class="na">publisher</span><span class="p">=</span><span class="s">{John Wiley \&amp; Sons}</span>
<span class="p">}</span>

<span class="nc">@article</span><span class="p">{</span><span class="nl">hochreiter1997long</span><span class="p">,</span>
<span class="w">  </span><span class="na">title</span><span class="p">=</span><span class="s">{Long short-term memory}</span><span class="p">,</span>
<span class="w">  </span><span class="na">author</span><span class="p">=</span><span class="s">{Hochreiter, Sepp and Schmidhuber, J{\&quot;u}rgen}</span><span class="p">,</span>
<span class="w">  </span><span class="na">journal</span><span class="p">=</span><span class="s">{Neural computation}</span><span class="p">,</span>
<span class="w">  </span><span class="na">volume</span><span class="p">=</span><span class="s">{9}</span><span class="p">,</span>
<span class="w">  </span><span class="na">number</span><span class="p">=</span><span class="s">{8}</span><span class="p">,</span>
<span class="w">  </span><span class="na">pages</span><span class="p">=</span><span class="s">{1735--1780}</span><span class="p">,</span>
<span class="w">  </span><span class="na">year</span><span class="p">=</span><span class="s">{1997}</span><span class="p">,</span>
<span class="w">  </span><span class="na">publisher</span><span class="p">=</span><span class="s">{MIT Press}</span>
<span class="p">}</span>

<span class="nc">@article</span><span class="p">{</span><span class="nl">vaswani2017attention</span><span class="p">,</span>
<span class="w">  </span><span class="na">title</span><span class="p">=</span><span class="s">{Attention is all you need}</span><span class="p">,</span>
<span class="w">  </span><span class="na">author</span><span class="p">=</span><span class="s">{Vaswani, Ashish and Shazeer, Noam and Parmar, Niki and Uszkoreit, Jakob and Jones, Llion and Gomez, Aidan N and Kaiser, {\L}ukasz and Polosukhin, Illia}</span><span class="p">,</span>
<span class="w">  </span><span class="na">journal</span><span class="p">=</span><span class="s">{Advances in neural information processing systems}</span><span class="p">,</span>
<span class="w">  </span><span class="na">volume</span><span class="p">=</span><span class="s">{30}</span><span class="p">,</span>
<span class="w">  </span><span class="na">year</span><span class="p">=</span><span class="s">{2017}</span>
<span class="p">}</span>

<span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">zhou2021informer</span><span class="p">,</span>
<span class="w">  </span><span class="na">title</span><span class="p">=</span><span class="s">{Informer: Beyond efficient transformer for long sequence time-series forecasting}</span><span class="p">,</span>
<span class="w">  </span><span class="na">author</span><span class="p">=</span><span class="s">{Zhou, Haoyi and Zhang, Shanghang and Peng, Jieqi and Zhang, Shuai and Li, Jianxin and Xiong, Hui and Zhang, Wancai}</span><span class="p">,</span>
<span class="w">  </span><span class="na">booktitle</span><span class="p">=</span><span class="s">{Proceedings of AAAI}</span><span class="p">,</span>
<span class="w">  </span><span class="na">year</span><span class="p">=</span><span class="s">{2021}</span>
<span class="p">}</span>

<span class="nc">@article</span><span class="p">{</span><span class="nl">kingma2014adam</span><span class="p">,</span>
<span class="w">  </span><span class="na">title</span><span class="p">=</span><span class="s">{Adam: A method for stochastic optimization}</span><span class="p">,</span>
<span class="w">  </span><span class="na">author</span><span class="p">=</span><span class="s">{Kingma, Diederik P and Ba, Jimmy}</span><span class="p">,</span>
<span class="w">  </span><span class="na">journal</span><span class="p">=</span><span class="s">{arXiv preprint arXiv:1412.6980}</span><span class="p">,</span>
<span class="w">  </span><span class="na">year</span><span class="p">=</span><span class="s">{2014}</span>
<span class="p">}</span>

<span class="nc">@article</span><span class="p">{</span><span class="nl">cho2014learning</span><span class="p">,</span>
<span class="w">  </span><span class="na">title</span><span class="p">=</span><span class="s">{Learning phrase representations using RNN encoder-decoder for statistical machine translation}</span><span class="p">,</span>
<span class="w">  </span><span class="na">author</span><span class="p">=</span><span class="s">{Cho, Kyunghyun and Van Merri{\&quot;e}nboer, Bart and Gulcehre, Caglar and Bahdanau, Dzmitry and Bougares, Fethi and Schwenk, Holger and Bengio, Yoshua}</span><span class="p">,</span>
<span class="w">  </span><span class="na">journal</span><span class="p">=</span><span class="s">{arXiv preprint arXiv:1406.1078}</span><span class="p">,</span>
<span class="w">  </span><span class="na">year</span><span class="p">=</span><span class="s">{2014}</span>
<span class="p">}</span>
</code></pre></div>

<h3 id="_4">ì»´íŒŒì¼<a class="header-link" href="#_4" title="Permanent link">&para;</a></h3>
<div class="highlight"><pre><span></span><code>pdflatex<span class="w"> </span>paper.tex
biber<span class="w"> </span>paper
pdflatex<span class="w"> </span>paper.tex
pdflatex<span class="w"> </span>paper.tex
</code></pre></div>

<p>ë˜ëŠ” <code>latexmk</code> ì‚¬ìš©:</p>
<div class="highlight"><pre><span></span><code>latexmk<span class="w"> </span>-pdf<span class="w"> </span>paper.tex
</code></pre></div>

<h3 id="_5">ì¼ë°˜ì ì¸ í•¨ì •<a class="header-link" href="#_5" title="Permanent link">&para;</a></h3>
<p><strong>ë¬¸ì œ</strong>: "Undefined references" ë˜ëŠ” PDFì— <code>??</code>
- <strong>í•´ê²°ì±…</strong>: <code>biber paper</code> ì‹¤í–‰ (not <code>bibtex</code>), ê·¸ ë‹¤ìŒ ë‘ ë²ˆ ë” ì»´íŒŒì¼</p>
<p><strong>ë¬¸ì œ</strong>: ê·¸ë¦¼ì´ ë‚˜íƒ€ë‚˜ì§€ ì•ŠìŒ
- <strong>í•´ê²°ì±…</strong>: í”Œë ˆì´ìŠ¤í™€ë” PDF ìƒì„± ë˜ëŠ” <code>\includegraphics</code> ì¤„ ì£¼ì„ ì²˜ë¦¬</p>
<p><strong>ë¬¸ì œ</strong>: 2ë‹¨ ìˆ˜ì‹ ì˜¤ë²„í”Œë¡œìš°
- <strong>í•´ê²°ì±…</strong>: ì‘ì€ ê¸€ê¼´ë¡œ <code>equation*</code> ì‚¬ìš©, ë˜ëŠ” <code>figure*</code>ë¡œ ë‹¨ì¼ ì—´ë¡œ ì „í™˜</p>
<h3 id="_6">ì‚¬ìš©ì ì •ì˜ íŒ<a class="header-link" href="#_6" title="Permanent link">&para;</a></h3>
<ul>
<li><strong>ë‹¨ì¼ ì—´</strong>: <code>conference</code> ì˜µì…˜ ì œê±°: <code>\documentclass{IEEEtran}</code></li>
<li><strong>ë‹¤ë¥¸ ì°¸ê³ ë¬¸í—Œ ìŠ¤íƒ€ì¼</strong>: <code>style=ieee</code>ë¥¼ <code>style=apa</code>, <code>style=nature</code> ë“±ìœ¼ë¡œ ë³€ê²½</li>
<li><strong>ì¤„ ë²ˆí˜¸ ì¶”ê°€</strong>: <code>\usepackage{lineno}</code>ì™€ <code>\begin{document}</code> ì „ì— <code>\linenumbers</code></li>
<li><strong>ë¸”ë¼ì¸ë“œ ë¦¬ë·°</strong>: <code>\author{}</code> ì£¼ì„ ì²˜ë¦¬, <code>\author{Anonymous}</code> ì‚¬ìš©</li>
</ul>
<hr />
<h2 id="2-beamer">í”„ë¡œì íŠ¸ 2: Beamer í”„ë ˆì  í…Œì´ì…˜<a class="header-link" href="#2-beamer" title="Permanent link">&para;</a></h2>
<h3 id="_7">ê°œìš”<a class="header-link" href="#_7" title="Permanent link">&para;</a></h3>
<p>ë‹¤ìŒì„ í¬í•¨í•˜ëŠ” 15ìŠ¬ë¼ì´ë“œ í•™íšŒ ë°œí‘œ:
- ì‚¬ìš©ì ì •ì˜ ìƒ‰ìƒ í…Œë§ˆ
- ì§„í–‰ í‘œì‹œê¸°ê°€ ìˆëŠ” ì„¹ì…˜ ìŠ¬ë¼ì´ë“œ
- ì˜¤ë²„ë ˆì´ê°€ ìˆëŠ” ì½˜í…ì¸  ìŠ¬ë¼ì´ë“œ (ì ì§„ì  ê³µê°œ)
- TikZ ë‹¤ì´ì–´ê·¸ë¨
- ì½”ë“œ ëª©ë¡
- ë°œí‘œì ë…¸íŠ¸
- í•¸ë“œì•„ì›ƒ ìƒì„±</p>
<h3 id="_8">ì™„ì „í•œ ì†ŒìŠ¤ ì½”ë“œ<a class="header-link" href="#_8" title="Permanent link">&para;</a></h3>
<p><strong>íŒŒì¼: <code>presentation.tex</code></strong></p>
<div class="highlight"><pre><span></span><code><span class="k">\documentclass</span><span class="na">[aspectratio=169]</span><span class="nb">{</span>beamer<span class="nb">}</span>

<span class="c">% Theme</span>
<span class="k">\usetheme</span><span class="nb">{</span>Madrid<span class="nb">}</span>
<span class="k">\usecolortheme</span><span class="nb">{</span>default<span class="nb">}</span>

<span class="c">% Custom colors</span>
<span class="k">\definecolor</span><span class="nb">{</span>primaryblue<span class="nb">}{</span>RGB<span class="nb">}{</span>0,82,155<span class="nb">}</span>
<span class="k">\definecolor</span><span class="nb">{</span>secondaryorange<span class="nb">}{</span>RGB<span class="nb">}{</span>255,127,0<span class="nb">}</span>
<span class="k">\setbeamercolor</span><span class="nb">{</span>structure<span class="nb">}{</span>fg=primaryblue<span class="nb">}</span>
<span class="k">\setbeamercolor</span><span class="nb">{</span>alerted text<span class="nb">}{</span>fg=secondaryorange<span class="nb">}</span>

<span class="c">% Packages</span>
<span class="k">\usepackage</span><span class="na">[utf8]</span><span class="nb">{</span>inputenc<span class="nb">}</span>
<span class="k">\usepackage</span><span class="nb">{</span>amsmath,amssymb<span class="nb">}</span>
<span class="k">\usepackage</span><span class="nb">{</span>graphicx<span class="nb">}</span>
<span class="k">\usepackage</span><span class="nb">{</span>tikz<span class="nb">}</span>
<span class="k">\usetikzlibrary</span><span class="nb">{</span>shapes,arrows,positioning<span class="nb">}</span>
<span class="k">\usepackage</span><span class="nb">{</span>listings<span class="nb">}</span>
<span class="k">\usepackage</span><span class="nb">{</span>booktabs<span class="nb">}</span>

<span class="c">% Listings style</span>
<span class="k">\lstset</span><span class="nb">{</span>
  basicstyle=<span class="k">\ttfamily\small</span>,
  keywordstyle=<span class="k">\color</span><span class="nb">{</span>primaryblue<span class="nb">}</span><span class="k">\bfseries</span>,
  commentstyle=<span class="k">\color</span><span class="nb">{</span>gray<span class="nb">}</span><span class="k">\itshape</span>,
  stringstyle=<span class="k">\color</span><span class="nb">{</span>secondaryorange<span class="nb">}</span>,
  showstringspaces=false,
  frame=single
<span class="nb">}</span>

<span class="c">% Title</span>
<span class="k">\title</span><span class="nb">{</span>Deep Learning for Time Series Forecasting<span class="nb">}</span>
<span class="k">\subtitle</span><span class="nb">{</span>LSTM vs. Transformer: A Comparative Study<span class="nb">}</span>
<span class="k">\author</span><span class="nb">{</span>Alice Johnson<span class="nb">}</span>
<span class="k">\institute</span><span class="nb">{</span>University of Example<span class="nb">}</span>
<span class="k">\date</span><span class="nb">{</span>March 15, 2026<span class="nb">}</span>

<span class="c">% Remove navigation symbols</span>
<span class="k">\setbeamertemplate</span><span class="nb">{</span>navigation symbols<span class="nb">}{}</span>

<span class="c">% Footer with progress</span>
<span class="k">\setbeamertemplate</span><span class="nb">{</span>footline<span class="nb">}</span>[frame number]

<span class="k">\begin</span><span class="nb">{</span>document<span class="nb">}</span>

<span class="c">% Title slide</span>
<span class="k">\begin</span><span class="nb">{</span>frame<span class="nb">}</span>
  <span class="k">\titlepage</span>
  <span class="k">\note</span><span class="nb">{</span>Welcome everyone. Today I&#39;ll present our work on time series forecasting.<span class="nb">}</span>
<span class="k">\end</span><span class="nb">{</span>frame<span class="nb">}</span>

<span class="c">% Outline</span>
<span class="k">\begin</span><span class="nb">{</span>frame<span class="nb">}{</span>Outline<span class="nb">}</span>
  <span class="k">\tableofcontents</span>
  <span class="k">\note</span><span class="nb">{</span>Here&#39;s what we&#39;ll cover in the next 15 minutes.<span class="nb">}</span>
<span class="k">\end</span><span class="nb">{</span>frame<span class="nb">}</span>

<span class="c">% Section 1</span>
<span class="k">\section</span><span class="nb">{</span>Introduction<span class="nb">}</span>

<span class="k">\begin</span><span class="nb">{</span>frame<span class="nb">}{</span>Motivation<span class="nb">}</span>
  <span class="k">\begin</span><span class="nb">{</span>block<span class="nb">}{</span>Time Series Forecasting<span class="nb">}</span>
    Predicting future values based on historical observations
  <span class="k">\end</span><span class="nb">{</span>block<span class="nb">}</span>

  <span class="k">\vspace</span><span class="nb">{</span>0.5cm<span class="nb">}</span>

  <span class="k">\textbf</span><span class="nb">{</span>Applications<span class="nb">}</span>:
  <span class="k">\begin</span><span class="nb">{</span>itemize<span class="nb">}</span>
    <span class="k">\item</span>&lt;2-&gt; Finance: Stock price prediction
    <span class="k">\item</span>&lt;3-&gt; Energy: Electricity demand forecasting
    <span class="k">\item</span>&lt;4-&gt; Weather: Temperature and precipitation
    <span class="k">\item</span>&lt;5-&gt; Healthcare: Patient monitoring and early warning
  <span class="k">\end</span><span class="nb">{</span>itemize<span class="nb">}</span>

  <span class="k">\note</span>&lt;1-&gt;<span class="nb">{</span>Introduce the problem<span class="nb">}</span>
  <span class="k">\note</span>&lt;2-&gt;<span class="nb">{</span>Financial applications are critical<span class="nb">}</span>
  <span class="k">\note</span>&lt;3-&gt;<span class="nb">{</span>Energy sector needs accurate forecasts for grid management<span class="nb">}</span>
<span class="k">\end</span><span class="nb">{</span>frame<span class="nb">}</span>

<span class="k">\begin</span><span class="nb">{</span>frame<span class="nb">}{</span>Research Questions<span class="nb">}</span>
  <span class="k">\begin</span><span class="nb">{</span>enumerate<span class="nb">}</span>
    <span class="k">\item</span> How do LSTM and Transformer models compare in accuracy?
    <span class="k">\item</span> What are the computational trade-offs?
    <span class="k">\item</span> Which model should practitioners choose?
  <span class="k">\end</span><span class="nb">{</span>enumerate<span class="nb">}</span>

  <span class="k">\vspace</span><span class="nb">{</span>1cm<span class="nb">}</span>

  <span class="k">\pause</span>

  <span class="k">\begin</span><span class="nb">{</span>alertblock<span class="nb">}{</span>Hypothesis<span class="nb">}</span>
    Transformers achieve better accuracy on long sequences, but LSTMs are more efficient.
  <span class="k">\end</span><span class="nb">{</span>alertblock<span class="nb">}</span>
<span class="k">\end</span><span class="nb">{</span>frame<span class="nb">}</span>

<span class="c">% Section 2</span>
<span class="k">\section</span><span class="nb">{</span>Methodology<span class="nb">}</span>

<span class="k">\begin</span><span class="nb">{</span>frame<span class="nb">}{</span>Model Architectures<span class="nb">}</span>
  <span class="k">\begin</span><span class="nb">{</span>columns<span class="nb">}</span>[T]
    <span class="k">\begin</span><span class="nb">{</span>column<span class="nb">}{</span>0.48<span class="k">\textwidth</span><span class="nb">}</span>
      <span class="k">\centering</span>
      <span class="k">\textbf</span><span class="nb">{</span>LSTM<span class="nb">}</span>
      <span class="k">\vspace</span><span class="nb">{</span>0.3cm<span class="nb">}</span>

      <span class="k">\begin</span><span class="nb">{</span>tikzpicture<span class="nb">}</span>[scale=0.7,
        node/.style=<span class="nb">{</span>rectangle,draw,minimum width=1.5cm,minimum height=0.8cm<span class="nb">}</span>]
        <span class="k">\node</span><span class="na">[node]</span> (x) at (0,0) <span class="nb">{</span>Input<span class="nb">}</span>;
        <span class="k">\node</span><span class="na">[node]</span> (lstm1) at (0,1.5) <span class="nb">{</span>LSTM<span class="nb">}</span>;
        <span class="k">\node</span><span class="na">[node]</span> (lstm2) at (0,3) <span class="nb">{</span>LSTM<span class="nb">}</span>;
        <span class="k">\node</span><span class="na">[node]</span> (fc) at (0,4.5) <span class="nb">{</span>Dense<span class="nb">}</span>;
        <span class="k">\node</span><span class="na">[node]</span> (y) at (0,6) <span class="nb">{</span>Output<span class="nb">}</span>;

        <span class="k">\draw</span><span class="na">[-&gt;]</span> (x) -- (lstm1);
        <span class="k">\draw</span><span class="na">[-&gt;]</span> (lstm1) -- (lstm2);
        <span class="k">\draw</span><span class="na">[-&gt;]</span> (lstm2) -- (fc);
        <span class="k">\draw</span><span class="na">[-&gt;]</span> (fc) -- (y);
      <span class="k">\end</span><span class="nb">{</span>tikzpicture<span class="nb">}</span>
    <span class="k">\end</span><span class="nb">{</span>column<span class="nb">}</span>

    <span class="k">\begin</span><span class="nb">{</span>column<span class="nb">}{</span>0.48<span class="k">\textwidth</span><span class="nb">}</span>
      <span class="k">\centering</span>
      <span class="k">\textbf</span><span class="nb">{</span>Transformer<span class="nb">}</span>
      <span class="k">\vspace</span><span class="nb">{</span>0.3cm<span class="nb">}</span>

      <span class="k">\begin</span><span class="nb">{</span>tikzpicture<span class="nb">}</span>[scale=0.7,
        node/.style=<span class="nb">{</span>rectangle,draw,minimum width=1.5cm,minimum height=0.8cm<span class="nb">}</span>]
        <span class="k">\node</span><span class="na">[node]</span> (x) at (0,0) <span class="nb">{</span>Input<span class="nb">}</span>;
        <span class="k">\node</span><span class="na">[node]</span> (emb) at (0,1.5) <span class="nb">{</span>Embedding<span class="nb">}</span>;
        <span class="k">\node</span><span class="na">[node]</span> (att1) at (0,3) <span class="nb">{</span>Attention<span class="nb">}</span>;
        <span class="k">\node</span><span class="na">[node]</span> (att2) at (0,4.5) <span class="nb">{</span>Attention<span class="nb">}</span>;
        <span class="k">\node</span><span class="na">[node]</span> (y) at (0,6) <span class="nb">{</span>Output<span class="nb">}</span>;

        <span class="k">\draw</span><span class="na">[-&gt;]</span> (x) -- (emb);
        <span class="k">\draw</span><span class="na">[-&gt;]</span> (emb) -- (att1);
        <span class="k">\draw</span><span class="na">[-&gt;]</span> (att1) -- (att2);
        <span class="k">\draw</span><span class="na">[-&gt;]</span> (att2) -- (y);
      <span class="k">\end</span><span class="nb">{</span>tikzpicture<span class="nb">}</span>
    <span class="k">\end</span><span class="nb">{</span>column<span class="nb">}</span>
  <span class="k">\end</span><span class="nb">{</span>columns<span class="nb">}</span>

  <span class="k">\note</span><span class="nb">{</span>LSTM processes sequences recurrently; Transformer uses parallel attention.<span class="nb">}</span>
<span class="k">\end</span><span class="nb">{</span>frame<span class="nb">}</span>

<span class="k">\begin</span><span class="nb">{</span>frame<span class="nb">}</span>[fragile]<span class="nb">{</span>LSTM Equations<span class="nb">}</span>
  The LSTM cell updates hidden state via gating:

  <span class="k">\begin</span><span class="nb">{</span>align*<span class="nb">}</span>
    <span class="k">\mathbf</span><span class="nb">{</span>f<span class="nb">}_</span>t <span class="nb">&amp;</span>= <span class="k">\sigma</span>(<span class="k">\mathbf</span><span class="nb">{</span>W<span class="nb">}_</span>f [<span class="k">\mathbf</span><span class="nb">{</span>h<span class="nb">}_{</span>t-1<span class="nb">}</span>, x<span class="nb">_</span>t] + <span class="k">\mathbf</span><span class="nb">{</span>b<span class="nb">}_</span>f) <span class="k">\\</span>
    <span class="k">\mathbf</span><span class="nb">{</span>i<span class="nb">}_</span>t <span class="nb">&amp;</span>= <span class="k">\sigma</span>(<span class="k">\mathbf</span><span class="nb">{</span>W<span class="nb">}_</span>i [<span class="k">\mathbf</span><span class="nb">{</span>h<span class="nb">}_{</span>t-1<span class="nb">}</span>, x<span class="nb">_</span>t] + <span class="k">\mathbf</span><span class="nb">{</span>b<span class="nb">}_</span>i) <span class="k">\\</span>
    <span class="k">\mathbf</span><span class="nb">{</span>c<span class="nb">}_</span>t <span class="nb">&amp;</span>= <span class="k">\mathbf</span><span class="nb">{</span>f<span class="nb">}_</span>t <span class="k">\odot</span> <span class="k">\mathbf</span><span class="nb">{</span>c<span class="nb">}_{</span>t-1<span class="nb">}</span> + <span class="k">\mathbf</span><span class="nb">{</span>i<span class="nb">}_</span>t <span class="k">\odot</span> <span class="k">\tilde</span><span class="nb">{</span><span class="k">\mathbf</span><span class="nb">{</span>c<span class="nb">}}_</span>t
  <span class="k">\end</span><span class="nb">{</span>align*<span class="nb">}</span>

  <span class="k">\vspace</span><span class="nb">{</span>0.5cm<span class="nb">}</span>

  <span class="k">\pause</span>

  <span class="k">\begin</span><span class="nb">{</span>lstlisting<span class="nb">}</span>[language=Python]
import torch.nn as nn

lstm = nn.LSTM(input<span class="nb">_</span>size=10, hidden<span class="nb">_</span>size=128,
               num<span class="nb">_</span>layers=2, batch<span class="nb">_</span>first=True)
  <span class="k">\end</span><span class="nb">{</span>lstlisting<span class="nb">}</span>
<span class="k">\end</span><span class="nb">{</span>frame<span class="nb">}</span>

<span class="k">\begin</span><span class="nb">{</span>frame<span class="nb">}{</span>Datasets<span class="nb">}</span>
  <span class="k">\begin</span><span class="nb">{</span>table<span class="nb">}</span>
    <span class="k">\centering</span>
    <span class="k">\begin</span><span class="nb">{</span>tabular<span class="nb">}{</span>@<span class="nb">{}</span>lccc@<span class="nb">{}}</span>
      <span class="k">\toprule</span>
      Dataset <span class="nb">&amp;</span> Samples <span class="nb">&amp;</span> Features <span class="nb">&amp;</span> Frequency <span class="k">\\</span>
      <span class="k">\midrule</span>
      ETTh1 <span class="nb">&amp;</span> 17,420 <span class="nb">&amp;</span> 7 <span class="nb">&amp;</span> Hourly <span class="k">\\</span>
      Weather <span class="nb">&amp;</span> 52,696 <span class="nb">&amp;</span> 21 <span class="nb">&amp;</span> 10 min <span class="k">\\</span>
      Electricity <span class="nb">&amp;</span> 26,304 <span class="nb">&amp;</span> 321 <span class="nb">&amp;</span> Hourly <span class="k">\\</span>
      <span class="k">\bottomrule</span>
    <span class="k">\end</span><span class="nb">{</span>tabular<span class="nb">}</span>
  <span class="k">\end</span><span class="nb">{</span>table<span class="nb">}</span>

  <span class="k">\vspace</span><span class="nb">{</span>0.5cm<span class="nb">}</span>

  <span class="k">\begin</span><span class="nb">{</span>itemize<span class="nb">}</span>
    <span class="k">\item</span> Split: 70<span class="k">\%</span> train, 15<span class="k">\%</span> validation, 15<span class="k">\%</span> test
    <span class="k">\item</span> Metrics: MAE, RMSE
  <span class="k">\end</span><span class="nb">{</span>itemize<span class="nb">}</span>
<span class="k">\end</span><span class="nb">{</span>frame<span class="nb">}</span>

<span class="c">% Section 3</span>
<span class="k">\section</span><span class="nb">{</span>Results<span class="nb">}</span>

<span class="k">\begin</span><span class="nb">{</span>frame<span class="nb">}{</span>Accuracy Comparison<span class="nb">}</span>
  <span class="k">\centering</span>
  <span class="k">\includegraphics</span><span class="na">[width=0.7\textwidth]</span><span class="nb">{</span>mae<span class="nb">_</span>comparison.pdf<span class="nb">}</span>

  <span class="k">\vspace</span><span class="nb">{</span>0.3cm<span class="nb">}</span>

  <span class="k">\begin</span><span class="nb">{</span>itemize<span class="nb">}</span>
    <span class="k">\item</span> Transformer: 12<span class="k">\%</span> lower MAE on ETTh1
    <span class="k">\item</span> LSTM: Competitive on Electricity dataset
  <span class="k">\end</span><span class="nb">{</span>itemize<span class="nb">}</span>

  <span class="k">\note</span><span class="nb">{</span>Highlight the accuracy advantage of Transformers on long sequences.<span class="nb">}</span>
<span class="k">\end</span><span class="nb">{</span>frame<span class="nb">}</span>

<span class="k">\begin</span><span class="nb">{</span>frame<span class="nb">}{</span>Computational Efficiency<span class="nb">}</span>
  <span class="k">\begin</span><span class="nb">{</span>columns<span class="nb">}</span>[T]
    <span class="k">\begin</span><span class="nb">{</span>column<span class="nb">}{</span>0.5<span class="k">\textwidth</span><span class="nb">}</span>
      <span class="k">\textbf</span><span class="nb">{</span>Training Time<span class="nb">}</span>
      <span class="k">\begin</span><span class="nb">{</span>itemize<span class="nb">}</span>
        <span class="k">\item</span> LSTM: 45 s/epoch
        <span class="k">\item</span> Transformer: 129 s/epoch
        <span class="k">\item</span> <span class="k">\alert</span><span class="nb">{</span>2.9Ã— faster<span class="nb">}</span>
      <span class="k">\end</span><span class="nb">{</span>itemize<span class="nb">}</span>
    <span class="k">\end</span><span class="nb">{</span>column<span class="nb">}</span>

    <span class="k">\begin</span><span class="nb">{</span>column<span class="nb">}{</span>0.5<span class="k">\textwidth</span><span class="nb">}</span>
      <span class="k">\textbf</span><span class="nb">{</span>GPU Memory<span class="nb">}</span>
      <span class="k">\begin</span><span class="nb">{</span>itemize<span class="nb">}</span>
        <span class="k">\item</span> LSTM: 3.8 GB
        <span class="k">\item</span> Transformer: 7.2 GB
        <span class="k">\item</span> <span class="k">\alert</span><span class="nb">{</span>1.9Ã— less memory<span class="nb">}</span>
      <span class="k">\end</span><span class="nb">{</span>itemize<span class="nb">}</span>
    <span class="k">\end</span><span class="nb">{</span>column<span class="nb">}</span>
  <span class="k">\end</span><span class="nb">{</span>columns<span class="nb">}</span>

  <span class="k">\vspace</span><span class="nb">{</span>1cm<span class="nb">}</span>

  <span class="k">\begin</span><span class="nb">{</span>block<span class="nb">}{</span>Key Insight<span class="nb">}</span>
    LSTMs offer significant computational savings with minor accuracy trade-offs.
  <span class="k">\end</span><span class="nb">{</span>block<span class="nb">}</span>
<span class="k">\end</span><span class="nb">{</span>frame<span class="nb">}</span>

<span class="c">% Section 4</span>
<span class="k">\section</span><span class="nb">{</span>Conclusion<span class="nb">}</span>

<span class="k">\begin</span><span class="nb">{</span>frame<span class="nb">}{</span>Summary<span class="nb">}</span>
  <span class="k">\textbf</span><span class="nb">{</span>Contributions<span class="nb">}</span>:
  <span class="k">\begin</span><span class="nb">{</span>itemize<span class="nb">}</span>
    <span class="k">\item</span> Systematic comparison on 3 benchmark datasets
    <span class="k">\item</span> Analysis of accuracy vs. efficiency trade-offs
    <span class="k">\item</span> Open-source code and trained models
  <span class="k">\end</span><span class="nb">{</span>itemize<span class="nb">}</span>

  <span class="k">\vspace</span><span class="nb">{</span>0.8cm<span class="nb">}</span>

  <span class="k">\pause</span>

  <span class="k">\textbf</span><span class="nb">{</span>Recommendations<span class="nb">}</span>:
  <span class="k">\begin</span><span class="nb">{</span>itemize<span class="nb">}</span>
    <span class="k">\item</span> Use <span class="k">\alert</span><span class="nb">{</span>Transformer<span class="nb">}</span> for maximum accuracy on long sequences
    <span class="k">\item</span> Use <span class="k">\alert</span><span class="nb">{</span>LSTM<span class="nb">}</span> for resource-constrained environments
    <span class="k">\item</span> Consider hybrid models for best of both worlds
  <span class="k">\end</span><span class="nb">{</span>itemize<span class="nb">}</span>
<span class="k">\end</span><span class="nb">{</span>frame<span class="nb">}</span>

<span class="k">\begin</span><span class="nb">{</span>frame<span class="nb">}{</span>Future Work<span class="nb">}</span>
  <span class="k">\begin</span><span class="nb">{</span>enumerate<span class="nb">}</span>
    <span class="k">\item</span> Hybrid LSTM-Transformer architectures
    <span class="k">\item</span> Multivariate forecasting with graph neural networks
    <span class="k">\item</span> Real-time inference optimization
    <span class="k">\item</span> Application to finance and healthcare domains
  <span class="k">\end</span><span class="nb">{</span>enumerate<span class="nb">}</span>

  <span class="k">\vspace</span><span class="nb">{</span>1cm<span class="nb">}</span>

  <span class="k">\centering</span>
  <span class="k">\Large</span> <span class="k">\textbf</span><span class="nb">{</span>Thank you!<span class="nb">}</span>

  <span class="k">\vspace</span><span class="nb">{</span>0.5cm<span class="nb">}</span>

  <span class="k">\normalsize</span>
  Questions? <span class="k">\\</span>
  <span class="k">\texttt</span><span class="nb">{</span>alice.johnson@example.edu<span class="nb">}</span>
<span class="k">\end</span><span class="nb">{</span>frame<span class="nb">}</span>

<span class="c">% Backup slides</span>
<span class="k">\appendix</span>

<span class="k">\begin</span><span class="nb">{</span>frame<span class="nb">}{</span>Backup: Hyperparameters<span class="nb">}</span>
  <span class="k">\begin</span><span class="nb">{</span>table<span class="nb">}</span>
    <span class="k">\centering</span>
    <span class="k">\small</span>
    <span class="k">\begin</span><span class="nb">{</span>tabular<span class="nb">}{</span>@<span class="nb">{}</span>lcc@<span class="nb">{}}</span>
      <span class="k">\toprule</span>
      Parameter <span class="nb">&amp;</span> LSTM <span class="nb">&amp;</span> Transformer <span class="k">\\</span>
      <span class="k">\midrule</span>
      Hidden size <span class="nb">&amp;</span> 128 <span class="nb">&amp;</span> 256 <span class="k">\\</span>
      Layers <span class="nb">&amp;</span> 2 <span class="nb">&amp;</span> 4 <span class="k">\\</span>
      Dropout <span class="nb">&amp;</span> 0.2 <span class="nb">&amp;</span> 0.1 <span class="k">\\</span>
      Learning rate <span class="nb">&amp;</span> <span class="s">$</span><span class="m">10</span><span class="nb">^{</span><span class="o">-</span><span class="m">4</span><span class="nb">}</span><span class="s">$</span> <span class="nb">&amp;</span> <span class="s">$</span><span class="m">10</span><span class="nb">^{</span><span class="o">-</span><span class="m">4</span><span class="nb">}</span><span class="s">$</span> <span class="k">\\</span>
      Batch size <span class="nb">&amp;</span> 32 <span class="nb">&amp;</span> 32 <span class="k">\\</span>
      <span class="k">\bottomrule</span>
    <span class="k">\end</span><span class="nb">{</span>tabular<span class="nb">}</span>
  <span class="k">\end</span><span class="nb">{</span>table<span class="nb">}</span>
<span class="k">\end</span><span class="nb">{</span>frame<span class="nb">}</span>

<span class="k">\end</span><span class="nb">{</span>document<span class="nb">}</span>
</code></pre></div>

<h3 id="_9">ì»´íŒŒì¼<a class="header-link" href="#_9" title="Permanent link">&para;</a></h3>
<div class="highlight"><pre><span></span><code>pdflatex<span class="w"> </span>presentation.tex
pdflatex<span class="w"> </span>presentation.tex
</code></pre></div>

<h3 id="_10">í•¸ë“œì•„ì›ƒ ìƒì„±<a class="header-link" href="#_10" title="Permanent link">&para;</a></h3>
<p>ì´ ì˜µì…˜ ì¶”ê°€:</p>
<div class="highlight"><pre><span></span><code><span class="k">\documentclass</span><span class="na">[aspectratio=169,handout]</span><span class="nb">{</span>beamer<span class="nb">}</span>
</code></pre></div>

<p>í‰ì†ŒëŒ€ë¡œ ì»´íŒŒì¼. ì˜¤ë²„ë ˆì´ê°€ ì¶•ì†Œë©ë‹ˆë‹¤.</p>
<h3 id="_11">ë°œí‘œì ë…¸íŠ¸<a class="header-link" href="#_11" title="Permanent link">&para;</a></h3>
<p>ì£¼ì„ì„ ì§€ì›í•˜ëŠ” PDF ë·°ì–´ì—ì„œ ë…¸íŠ¸ ë³´ê¸°, ë˜ëŠ” ë‹¤ìŒ ì‚¬ìš©:</p>
<div class="highlight"><pre><span></span><code>pdfpc<span class="w"> </span>presentation.pdf
</code></pre></div>

<p>(<code>pdfpc</code> ë„êµ¬ í•„ìš”)</p>
<h3 id="_12">ì¼ë°˜ì ì¸ í•¨ì •<a class="header-link" href="#_12" title="Permanent link">&para;</a></h3>
<p><strong>ë¬¸ì œ</strong>: ì˜¤ë²„ë ˆì´ê°€ ì‘ë™í•˜ì§€ ì•ŠìŒ
- <strong>í•´ê²°ì±…</strong>: <code>\pause</code>, <code>\only&lt;2-&gt;</code>, <code>\item&lt;3-&gt;</code> êµ¬ë¬¸ì„ ì˜¬ë°”ë¥´ê²Œ ì‚¬ìš©</p>
<p><strong>ë¬¸ì œ</strong>: ìŠ¬ë¼ì´ë“œë‹¹ í…ìŠ¤íŠ¸ê°€ ë„ˆë¬´ ë§ìŒ
- <strong>í•´ê²°ì±…</strong>: "6Ã—6 ê·œì¹™" ë”°ë¥´ê¸°: ìµœëŒ€ 6ê°œ ê¸€ë¨¸ë¦¬ ê¸°í˜¸, ê° 6ë‹¨ì–´</p>
<p><strong>ë¬¸ì œ</strong>: TikZ ë‹¤ì´ì–´ê·¸ë¨ì´ ë„ˆë¬´ ë³µì¡í•¨
- <strong>í•´ê²°ì±…</strong>: ë‹¨ìˆœí™”í•˜ê±°ë‚˜ ì™¸ë¶€ ë„êµ¬ì—ì„œ ìƒì„±, PDFë¡œ ê°€ì ¸ì˜¤ê¸°</p>
<hr />
<h2 id="3-tikz">í”„ë¡œì íŠ¸ 3: TikZ ê³¼í•™ í¬ìŠ¤í„°<a class="header-link" href="#3-tikz" title="Permanent link">&para;</a></h2>
<h3 id="_13">ê°œìš”<a class="header-link" href="#_13" title="Permanent link">&para;</a></h3>
<p>ë‹¤ìŒì„ í¬í•¨í•˜ëŠ” í•™íšŒìš© A0 í¬ìŠ¤í„° (841 Ã— 1189 mm):
- ë‹¤ì¤‘ ì—´ ë ˆì´ì•„ì›ƒ (3ì—´)
- ë¡œê³ ê°€ ìˆëŠ” ì œëª© ë°°ë„ˆ
- ì„œë¡ , ë°©ë²•, ê²°ê³¼, ê²°ë¡  ë¸”ë¡
- ë°ì´í„° ì‹œê°í™”ë¥¼ ìœ„í•œ PGFPlots
- TikZ ìˆœì„œë„
- ì°¸ì¡°ìš© QR ì½”ë“œ
- ì‚¬ìš©ì ì •ì˜ ìƒ‰ìƒ ì²´ê³„</p>
<h3 id="_14">ì™„ì „í•œ ì†ŒìŠ¤ ì½”ë“œ<a class="header-link" href="#_14" title="Permanent link">&para;</a></h3>
<p><strong>íŒŒì¼: <code>poster.tex</code></strong></p>
<div class="highlight"><pre><span></span><code><span class="k">\documentclass</span><span class="na">[a0paper,portrait]</span><span class="nb">{</span>tikzposter<span class="nb">}</span>

<span class="c">% Packages</span>
<span class="k">\usepackage</span><span class="na">[utf8]</span><span class="nb">{</span>inputenc<span class="nb">}</span>
<span class="k">\usepackage</span><span class="nb">{</span>amsmath,amssymb<span class="nb">}</span>
<span class="k">\usepackage</span><span class="nb">{</span>graphicx<span class="nb">}</span>
<span class="k">\usepackage</span><span class="nb">{</span>booktabs<span class="nb">}</span>
<span class="k">\usepackage</span><span class="nb">{</span>pgfplots<span class="nb">}</span>
<span class="k">\pgfplotsset</span><span class="nb">{</span>compat=1.18<span class="nb">}</span>
<span class="k">\usepackage</span><span class="nb">{</span>qrcode<span class="nb">}</span>

<span class="c">% Theme</span>
<span class="k">\usetheme</span><span class="nb">{</span>Default<span class="nb">}</span>
<span class="k">\usecolorstyle</span><span class="nb">{</span>Denmark<span class="nb">}</span>

<span class="c">% Custom colors</span>
<span class="k">\definecolor</span><span class="nb">{</span>primaryblue<span class="nb">}{</span>RGB<span class="nb">}{</span>0,82,155<span class="nb">}</span>
<span class="k">\definecolor</span><span class="nb">{</span>lightblue<span class="nb">}{</span>RGB<span class="nb">}{</span>204,229,255<span class="nb">}</span>
<span class="k">\definecolor</span><span class="nb">{</span>darkgray<span class="nb">}{</span>RGB<span class="nb">}{</span>51,51,51<span class="nb">}</span>

<span class="k">\colorlet</span><span class="nb">{</span>backgroundcolor<span class="nb">}{</span>lightblue!30<span class="nb">}</span>
<span class="k">\colorlet</span><span class="nb">{</span>blocktitlefgcolor<span class="nb">}{</span>white<span class="nb">}</span>
<span class="k">\colorlet</span><span class="nb">{</span>blocktitlebgcolor<span class="nb">}{</span>primaryblue<span class="nb">}</span>
<span class="k">\colorlet</span><span class="nb">{</span>blockbodyfgcolor<span class="nb">}{</span>darkgray<span class="nb">}</span>
<span class="k">\colorlet</span><span class="nb">{</span>blockbodybgcolor<span class="nb">}{</span>white<span class="nb">}</span>

<span class="c">% Title</span>
<span class="k">\title</span><span class="nb">{</span><span class="k">\parbox</span><span class="nb">{</span>0.8<span class="k">\linewidth</span><span class="nb">}{</span><span class="k">\centering</span> Deep Learning for Time Series Forecasting: LSTM vs. Transformer<span class="nb">}}</span>
<span class="k">\author</span><span class="nb">{</span>Alice Johnson<span class="s">$</span><span class="nb">^</span><span class="m">1</span><span class="s">$</span>, Bob Smith<span class="s">$</span><span class="nb">^</span><span class="m">2</span><span class="s">$</span><span class="nb">}</span>
<span class="k">\institute</span><span class="nb">{</span><span class="s">$</span><span class="nb">^</span><span class="m">1</span><span class="s">$</span>University of Example, <span class="s">$</span><span class="nb">^</span><span class="m">2</span><span class="s">$</span>Research Lab XYZ<span class="nb">}</span>

<span class="c">% Logos (placeholders - replace with actual logos)</span>
<span class="k">\titlegraphic</span><span class="nb">{</span>
  <span class="k">\includegraphics</span><span class="na">[width=0.1\textwidth]</span><span class="nb">{</span>logo1.pdf<span class="nb">}</span>
  <span class="k">\hspace</span><span class="nb">{</span>2cm<span class="nb">}</span>
  <span class="k">\includegraphics</span><span class="na">[width=0.1\textwidth]</span><span class="nb">{</span>logo2.pdf<span class="nb">}</span>
<span class="nb">}</span>

<span class="k">\begin</span><span class="nb">{</span>document<span class="nb">}</span>

<span class="k">\maketitle</span>

<span class="k">\begin</span><span class="nb">{</span>columns<span class="nb">}</span>
  <span class="c">% Column 1</span>
  <span class="k">\column</span><span class="nb">{</span>0.33<span class="nb">}</span>

  <span class="k">\block</span><span class="nb">{</span>Introduction<span class="nb">}{</span>
    <span class="k">\textbf</span><span class="nb">{</span>Motivation:<span class="nb">}</span> Time series forecasting is critical in finance, energy, and healthcare.

    <span class="k">\vspace</span><span class="nb">{</span>0.5cm<span class="nb">}</span>

    <span class="k">\textbf</span><span class="nb">{</span>Problem:<span class="nb">}</span> Traditional methods (ARIMA, exponential smoothing) struggle with complex, high-dimensional data.

    <span class="k">\vspace</span><span class="nb">{</span>0.5cm<span class="nb">}</span>

    <span class="k">\textbf</span><span class="nb">{</span>Solution:<span class="nb">}</span> Deep learning models (LSTM, Transformer) capture nonlinear dependencies.

    <span class="k">\vspace</span><span class="nb">{</span>0.5cm<span class="nb">}</span>

    <span class="k">\textbf</span><span class="nb">{</span>Research Questions:<span class="nb">}</span>
    <span class="k">\begin</span><span class="nb">{</span>itemize<span class="nb">}</span>
      <span class="k">\item</span> How do LSTM and Transformer compare in accuracy?
      <span class="k">\item</span> What are computational trade-offs?
      <span class="k">\item</span> Which model should practitioners choose?
    <span class="k">\end</span><span class="nb">{</span>itemize<span class="nb">}</span>
  <span class="nb">}</span>

  <span class="k">\block</span><span class="nb">{</span>Model Architectures<span class="nb">}{</span>
    <span class="k">\textbf</span><span class="nb">{</span>LSTM:<span class="nb">}</span> Recurrent architecture with gating mechanisms
    <span class="sb">\[</span>
<span class="nb">      </span><span class="nv">\mathbf</span><span class="nb">{c}_t </span><span class="o">=</span><span class="nb"> </span><span class="nv">\mathbf</span><span class="nb">{f}_t </span><span class="nv">\odot</span><span class="nb"> </span><span class="nv">\mathbf</span><span class="nb">{c}_{t</span><span class="o">-</span><span class="m">1</span><span class="nb">} </span><span class="o">+</span><span class="nb"> </span><span class="nv">\mathbf</span><span class="nb">{i}_t </span><span class="nv">\odot</span><span class="nb"> </span><span class="nv">\tilde</span><span class="nb">{</span><span class="nv">\mathbf</span><span class="nb">{c}}_t</span>
<span class="nb">    </span><span class="s">\]</span>

    <span class="k">\vspace</span><span class="nb">{</span>1cm<span class="nb">}</span>

    <span class="k">\textbf</span><span class="nb">{</span>Transformer:<span class="nb">}</span> Self-attention for parallel sequence processing
    <span class="sb">\[</span>
<span class="nb">      </span><span class="nv">\text</span><span class="nb">{Attention}</span><span class="o">(</span><span class="nv">\mathbf</span><span class="nb">{Q}, </span><span class="nv">\mathbf</span><span class="nb">{K}, </span><span class="nv">\mathbf</span><span class="nb">{V}</span><span class="o">)</span><span class="nb"> </span><span class="o">=</span><span class="nb"> </span><span class="nv">\text</span><span class="nb">{softmax}</span><span class="nv">\left</span><span class="o">(</span><span class="nv">\frac</span><span class="nb">{</span><span class="nv">\mathbf</span><span class="nb">{Q}</span><span class="nv">\mathbf</span><span class="nb">{K}^T}{</span><span class="nv">\sqrt</span><span class="nb">{d_k}}</span><span class="nv">\right</span><span class="o">)</span><span class="nb"> </span><span class="nv">\mathbf</span><span class="nb">{V}</span>
<span class="nb">    </span><span class="s">\]</span>

    <span class="k">\vspace</span><span class="nb">{</span>1cm<span class="nb">}</span>

    <span class="k">\begin</span><span class="nb">{</span>tikzfigure<span class="nb">}</span>[Comparison of architectures]
      <span class="k">\begin</span><span class="nb">{</span>tikzpicture<span class="nb">}</span>[scale=1.5,
        node/.style=<span class="nb">{</span>rectangle,draw,minimum width=2.5cm,minimum height=1cm,fill=white<span class="nb">}</span>]

        <span class="c">% LSTM</span>
        <span class="k">\node</span><span class="na">[node]</span> (x1) at (0,0) <span class="nb">{</span>Input<span class="nb">}</span>;
        <span class="k">\node</span><span class="na">[node]</span> (lstm1) at (0,2) <span class="nb">{</span>LSTM Layer 1<span class="nb">}</span>;
        <span class="k">\node</span><span class="na">[node]</span> (lstm2) at (0,4) <span class="nb">{</span>LSTM Layer 2<span class="nb">}</span>;
        <span class="k">\node</span><span class="na">[node]</span> (fc1) at (0,6) <span class="nb">{</span>Dense<span class="nb">}</span>;
        <span class="k">\node</span><span class="na">[node]</span> (y1) at (0,8) <span class="nb">{</span>Output<span class="nb">}</span>;

        <span class="k">\draw</span><span class="na">[-&gt;,thick]</span> (x1) -- (lstm1);
        <span class="k">\draw</span><span class="na">[-&gt;,thick]</span> (lstm1) -- (lstm2);
        <span class="k">\draw</span><span class="na">[-&gt;,thick]</span> (lstm2) -- (fc1);
        <span class="k">\draw</span><span class="na">[-&gt;,thick]</span> (fc1) -- (y1);

        <span class="k">\node</span> at (0,-1) <span class="nb">{</span><span class="k">\textbf</span><span class="nb">{</span>LSTM<span class="nb">}}</span>;

        <span class="c">% Transformer</span>
        <span class="k">\node</span><span class="na">[node]</span> (x2) at (6,0) <span class="nb">{</span>Input<span class="nb">}</span>;
        <span class="k">\node</span><span class="na">[node]</span> (emb) at (6,2) <span class="nb">{</span>Embedding<span class="nb">}</span>;
        <span class="k">\node</span><span class="na">[node]</span> (att1) at (6,4) <span class="nb">{</span>Attention <span class="s">$</span><span class="nv">\times</span><span class="s">$</span> 4<span class="nb">}</span>;
        <span class="k">\node</span><span class="na">[node]</span> (fc2) at (6,6) <span class="nb">{</span>Dense<span class="nb">}</span>;
        <span class="k">\node</span><span class="na">[node]</span> (y2) at (6,8) <span class="nb">{</span>Output<span class="nb">}</span>;

        <span class="k">\draw</span><span class="na">[-&gt;,thick]</span> (x2) -- (emb);
        <span class="k">\draw</span><span class="na">[-&gt;,thick]</span> (emb) -- (att1);
        <span class="k">\draw</span><span class="na">[-&gt;,thick]</span> (att1) -- (fc2);
        <span class="k">\draw</span><span class="na">[-&gt;,thick]</span> (fc2) -- (y2);

        <span class="k">\node</span> at (6,-1) <span class="nb">{</span><span class="k">\textbf</span><span class="nb">{</span>Transformer<span class="nb">}}</span>;
      <span class="k">\end</span><span class="nb">{</span>tikzpicture<span class="nb">}</span>
    <span class="k">\end</span><span class="nb">{</span>tikzfigure<span class="nb">}</span>
  <span class="nb">}</span>

  <span class="c">% Column 2</span>
  <span class="k">\column</span><span class="nb">{</span>0.33<span class="nb">}</span>

  <span class="k">\block</span><span class="nb">{</span>Datasets<span class="nb">}{</span>
    <span class="k">\begin</span><span class="nb">{</span>tabular<span class="nb">}{</span>@<span class="nb">{}</span>lccc@<span class="nb">{}}</span>
      <span class="k">\toprule</span>
      Dataset <span class="nb">&amp;</span> Samples <span class="nb">&amp;</span> Features <span class="nb">&amp;</span> Frequency <span class="k">\\</span>
      <span class="k">\midrule</span>
      ETTh1 <span class="nb">&amp;</span> 17,420 <span class="nb">&amp;</span> 7 <span class="nb">&amp;</span> Hourly <span class="k">\\</span>
      Weather <span class="nb">&amp;</span> 52,696 <span class="nb">&amp;</span> 21 <span class="nb">&amp;</span> 10 min <span class="k">\\</span>
      Electricity <span class="nb">&amp;</span> 26,304 <span class="nb">&amp;</span> 321 <span class="nb">&amp;</span> Hourly <span class="k">\\</span>
      <span class="k">\bottomrule</span>
    <span class="k">\end</span><span class="nb">{</span>tabular<span class="nb">}</span>

    <span class="k">\vspace</span><span class="nb">{</span>0.5cm<span class="nb">}</span>

    <span class="k">\textbf</span><span class="nb">{</span>Split:<span class="nb">}</span> 70<span class="k">\%</span> train, 15<span class="k">\%</span> validation, 15<span class="k">\%</span> test

    <span class="k">\textbf</span><span class="nb">{</span>Metrics:<span class="nb">}</span> Mean Absolute Error (MAE), Root Mean Squared Error (RMSE)
  <span class="nb">}</span>

  <span class="k">\block</span><span class="nb">{</span>Results: Accuracy<span class="nb">}{</span>
    <span class="k">\begin</span><span class="nb">{</span>tikzfigure<span class="nb">}</span>[MAE comparison across datasets]
      <span class="k">\begin</span><span class="nb">{</span>tikzpicture<span class="nb">}</span>
        <span class="k">\begin</span><span class="nb">{</span>axis<span class="nb">}</span>[
          ybar,
          width=0.9<span class="k">\linewidth</span>,
          height=10cm,
          ylabel=<span class="nb">{</span>Mean Absolute Error (MAE)<span class="nb">}</span>,
          xlabel=<span class="nb">{</span>Dataset<span class="nb">}</span>,
          symbolic x coords=<span class="nb">{</span>ETTh1, Weather, Electricity<span class="nb">}</span>,
          xtick=data,
          legend pos=north west,
          ymajorgrids=true,
          bar width=0.8cm,
          enlarge x limits=0.2,
          ymin=0
        ]
        <span class="k">\addplot</span> coordinates <span class="nb">{</span>(ETTh1,0.42) (Weather,0.31) (Electricity,0.18)<span class="nb">}</span>;
        <span class="k">\addplot</span> coordinates <span class="nb">{</span>(ETTh1,0.37) (Weather,0.28) (Electricity,0.19)<span class="nb">}</span>;
        <span class="k">\legend</span><span class="nb">{</span>LSTM, Transformer<span class="nb">}</span>
        <span class="k">\end</span><span class="nb">{</span>axis<span class="nb">}</span>
      <span class="k">\end</span><span class="nb">{</span>tikzpicture<span class="nb">}</span>
    <span class="k">\end</span><span class="nb">{</span>tikzfigure<span class="nb">}</span>

    <span class="k">\textbf</span><span class="nb">{</span>Key Finding:<span class="nb">}</span> Transformer achieves 12<span class="k">\%</span> lower MAE on ETTh1 (long sequences)
  <span class="nb">}</span>

  <span class="c">% Column 3</span>
  <span class="k">\column</span><span class="nb">{</span>0.33<span class="nb">}</span>

  <span class="k">\block</span><span class="nb">{</span>Results: Efficiency<span class="nb">}{</span>
    <span class="k">\begin</span><span class="nb">{</span>tikzfigure<span class="nb">}</span>[Training time and memory usage]
      <span class="k">\begin</span><span class="nb">{</span>tikzpicture<span class="nb">}</span>
        <span class="k">\begin</span><span class="nb">{</span>axis<span class="nb">}</span>[
          ybar,
          width=0.9<span class="k">\linewidth</span>,
          height=8cm,
          ylabel=<span class="nb">{</span>Training Time (s/epoch)<span class="nb">}</span>,
          symbolic x coords=<span class="nb">{</span>LSTM, Transformer<span class="nb">}</span>,
          xtick=data,
          bar width=1.5cm,
          ymin=0,
          ymajorgrids=true,
          nodes near coords,
          enlarge x limits=0.5
        ]
        <span class="k">\addplot</span> coordinates <span class="nb">{</span>(LSTM,45.2) (Transformer,128.7)<span class="nb">}</span>;
        <span class="k">\end</span><span class="nb">{</span>axis<span class="nb">}</span>
      <span class="k">\end</span><span class="nb">{</span>tikzpicture<span class="nb">}</span>
    <span class="k">\end</span><span class="nb">{</span>tikzfigure<span class="nb">}</span>

    <span class="k">\vspace</span><span class="nb">{</span>1cm<span class="nb">}</span>

    <span class="k">\textbf</span><span class="nb">{</span>Computational Trade-offs:<span class="nb">}</span>
    <span class="k">\begin</span><span class="nb">{</span>itemize<span class="nb">}</span>
      <span class="k">\item</span> LSTM: 2.9<span class="s">$</span><span class="nv">\times</span><span class="s">$</span> faster training
      <span class="k">\item</span> LSTM: 1.9<span class="s">$</span><span class="nv">\times</span><span class="s">$</span> less GPU memory
      <span class="k">\item</span> Transformer: Better accuracy on long sequences
    <span class="k">\end</span><span class="nb">{</span>itemize<span class="nb">}</span>
  <span class="nb">}</span>

  <span class="k">\block</span><span class="nb">{</span>Conclusion<span class="nb">}{</span>
    <span class="k">\textbf</span><span class="nb">{</span>Contributions:<span class="nb">}</span>
    <span class="k">\begin</span><span class="nb">{</span>itemize<span class="nb">}</span>
      <span class="k">\item</span> Comprehensive comparison on 3 datasets
      <span class="k">\item</span> Accuracy vs. efficiency trade-off analysis
      <span class="k">\item</span> Open-source implementation
    <span class="k">\end</span><span class="nb">{</span>itemize<span class="nb">}</span>

    <span class="k">\vspace</span><span class="nb">{</span>0.5cm<span class="nb">}</span>

    <span class="k">\textbf</span><span class="nb">{</span>Recommendations:<span class="nb">}</span>
    <span class="k">\begin</span><span class="nb">{</span>itemize<span class="nb">}</span>
      <span class="k">\item</span> <span class="k">\textbf</span><span class="nb">{</span>Transformer:<span class="nb">}</span> Maximum accuracy, long sequences
      <span class="k">\item</span> <span class="k">\textbf</span><span class="nb">{</span>LSTM:<span class="nb">}</span> Resource-constrained environments
    <span class="k">\end</span><span class="nb">{</span>itemize<span class="nb">}</span>

    <span class="k">\vspace</span><span class="nb">{</span>1cm<span class="nb">}</span>

    <span class="k">\textbf</span><span class="nb">{</span>Future Work:<span class="nb">}</span> Hybrid architectures, multivariate forecasting, real-time inference

    <span class="k">\vspace</span><span class="nb">{</span>1cm<span class="nb">}</span>

    <span class="k">\begin</span><span class="nb">{</span>center<span class="nb">}</span>
      <span class="k">\textbf</span><span class="nb">{</span>Code <span class="k">\&amp;</span> Data:<span class="nb">}</span><span class="k">\\</span>
      <span class="k">\qrcode</span><span class="na">[height=3cm]</span><span class="nb">{</span>https://github.com/example/time-series-forecast<span class="nb">}</span>
    <span class="k">\end</span><span class="nb">{</span>center<span class="nb">}</span>

    <span class="k">\vspace</span><span class="nb">{</span>0.5cm<span class="nb">}</span>

    <span class="k">\begin</span><span class="nb">{</span>center<span class="nb">}</span>
      <span class="k">\textbf</span><span class="nb">{</span>Contact:<span class="nb">}</span> <span class="k">\texttt</span><span class="nb">{</span>alice.johnson@example.edu<span class="nb">}</span>
    <span class="k">\end</span><span class="nb">{</span>center<span class="nb">}</span>
  <span class="nb">}</span>

<span class="k">\end</span><span class="nb">{</span>columns<span class="nb">}</span>

<span class="k">\end</span><span class="nb">{</span>document<span class="nb">}</span>
</code></pre></div>

<h3 id="_15">ì»´íŒŒì¼<a class="header-link" href="#_15" title="Permanent link">&para;</a></h3>
<div class="highlight"><pre><span></span><code>pdflatex<span class="w"> </span>poster.tex
</code></pre></div>

<p><strong>ì°¸ê³ </strong>: TikZ ë³µì¡ë„ë¡œ ì¸í•´ ì»´íŒŒì¼ ì‹œê°„ì´ ë” ê±¸ë¦´ ìˆ˜ ìˆìŠµë‹ˆë‹¤.</p>
<h3 id="_16">ì¸ì‡„<a class="header-link" href="#_16" title="Permanent link">&para;</a></h3>
<p>ì‹¤ì œ í•™íšŒ í¬ìŠ¤í„°ì˜ ê²½ìš°:
1. PDFë¡œ ë‚´ë³´ë‚´ê¸°
2. ì „ë¬¸ í¬ìŠ¤í„° ì¸ì‡„ ì„œë¹„ìŠ¤ë¡œ ì „ì†¡
3. ì§€ì •: A0 í¬ê¸°, ì„¸ë¡œ, ê³ í’ˆì§ˆ (600 dpi)</p>
<h3 id="_17">ì¼ë°˜ì ì¸ í•¨ì •<a class="header-link" href="#_17" title="Permanent link">&para;</a></h3>
<p><strong>ë¬¸ì œ</strong>: ì¸ì‡„ ì‹œ í…ìŠ¤íŠ¸ê°€ ë„ˆë¬´ ì‘ìŒ
- <strong>í•´ê²°ì±…</strong>: <code>\tikzposter</code> ì˜µì…˜ì—ì„œ ë” í° ê¸€ê¼´ í¬ê¸° ì‚¬ìš©</p>
<p><strong>ë¬¸ì œ</strong>: QR ì½”ë“œê°€ ìŠ¤ìº”ë˜ì§€ ì•ŠìŒ
- <strong>í•´ê²°ì±…</strong>: <code>height</code> ë§¤ê°œë³€ìˆ˜ ì¦ê°€, ì¸ì‡„ ì „ í…ŒìŠ¤íŠ¸</p>
<p><strong>ë¬¸ì œ</strong>: í™”ë©´ê³¼ ì¸ì‡„ë¬¼ì˜ ìƒ‰ìƒì´ ë‹¤ë¦„
- <strong>í•´ê²°ì±…</strong>: CMYK ìƒ‰ìƒ ê³µê°„ ì‚¬ìš©, ì¸ì‡„ ì‹œë®¬ë ˆì´ì…˜ìœ¼ë¡œ ë¯¸ë¦¬ë³´ê¸°</p>
<hr />
<h2 id="_18">ë ˆìŠ¨ í†µí•©: í†µí•© ë§µ<a class="header-link" href="#_18" title="Permanent link">&para;</a></h2>
<p>ì„¸ ê°€ì§€ í”„ë¡œì íŠ¸ ëª¨ë‘ ì´ì „ ë ˆìŠ¨ì˜ ê°œë…ì„ ì‚¬ìš©í•©ë‹ˆë‹¤:</p>
<table>
<thead>
<tr>
<th>ë ˆìŠ¨</th>
<th>í”„ë¡œì íŠ¸ 1 (ë…¼ë¬¸)</th>
<th>í”„ë¡œì íŠ¸ 2 (Beamer)</th>
<th>í”„ë¡œì íŠ¸ 3 (í¬ìŠ¤í„°)</th>
</tr>
</thead>
<tbody>
<tr>
<td>L01-02</td>
<td>ë¬¸ì„œ êµ¬ì¡°</td>
<td>í”„ë ˆì„ êµ¬ì¡°</td>
<td>ë¸”ë¡ êµ¬ì¡°</td>
</tr>
<tr>
<td>L03</td>
<td>í…ìŠ¤íŠ¸ ì„œì‹</td>
<td>í…Œë§ˆ ìƒ‰ìƒ</td>
<td>ì‚¬ìš©ì ì •ì˜ ìƒ‰ìƒ</td>
</tr>
<tr>
<td>L05</td>
<td>í‘œ (booktabs)</td>
<td>í‘œ</td>
<td>í‘œ</td>
</tr>
<tr>
<td>L06</td>
<td>ê·¸ë¦¼, í•˜ìœ„ ê·¸ë¦¼</td>
<td>ì´ë¯¸ì§€</td>
<td>TikZ ê·¸ë¦¼</td>
</tr>
<tr>
<td>L07-08</td>
<td>ìˆ˜ì‹, align</td>
<td>ìŠ¬ë¼ì´ë“œì˜ ìˆ˜í•™</td>
<td>ë¸”ë¡ì˜ ìˆ˜í•™</td>
</tr>
<tr>
<td>L09</td>
<td>ìƒí˜¸ ì°¸ì¡°</td>
<td>í”„ë ˆì„ ì°¸ì¡°</td>
<td>â€”</td>
</tr>
<tr>
<td>L10</td>
<td>BibLaTeX</td>
<td>ì¸ìš©</td>
<td>â€”</td>
</tr>
<tr>
<td>L11</td>
<td>â€”</td>
<td>Beamer í…Œë§ˆ, ì˜¤ë²„ë ˆì´</td>
<td>â€”</td>
</tr>
<tr>
<td>L12</td>
<td>â€”</td>
<td>TikZ ë‹¤ì´ì–´ê·¸ë¨</td>
<td>PGFPlots, TikZ</td>
</tr>
<tr>
<td>L13</td>
<td>ì‚¬ìš©ì ì •ì˜ ëª…ë ¹</td>
<td>â€”</td>
<td>â€”</td>
</tr>
<tr>
<td>L14</td>
<td>IEEE í´ë˜ìŠ¤</td>
<td>Beamer í´ë˜ìŠ¤</td>
<td>tikzposter í´ë˜ìŠ¤</td>
</tr>
<tr>
<td>L15</td>
<td>latexmk</td>
<td>latexmk</td>
<td>â€”</td>
</tr>
</tbody>
</table>
<hr />
<h2 id="_19">ë‹¤ìŒ ë‹¨ê³„<a class="header-link" href="#_19" title="Permanent link">&para;</a></h2>
<h3 id="_20">ê³ ê¸‰ ì£¼ì œ íƒìƒ‰<a class="header-link" href="#_20" title="Permanent link">&para;</a></h3>
<ol>
<li><strong>LuaLaTeX í”„ë¡œê·¸ë˜ë°</strong>: ë³µì¡í•œ ë¬¸ì„œ ìƒì„± ìë™í™”</li>
<li><strong>ì™¸ë¶€í™”(Externalization)</strong>: TikZ ì»´íŒŒì¼ ì†ë„ í–¥ìƒ</li>
<li><strong>ConTeXt</strong>: ê³ ê¸‰ íƒ€ì´í¬ê·¸ë˜í”¼ë¥¼ ìœ„í•œ LaTeX ëŒ€ì•ˆ</li>
<li><strong>arXiv ì œì¶œ</strong>: ì‚¬ì „ ì¸ì‡„ ì„œë²„ìš© ë…¼ë¬¸ ì¤€ë¹„</li>
<li><strong>ì €ë„ë³„ í…œí”Œë¦¿</strong>: IEEE, ACM, Springer, Elsevier</li>
</ol>
<h3 id="_21">ì»¤ë®¤ë‹ˆí‹° ì°¸ì—¬<a class="header-link" href="#_21" title="Permanent link">&para;</a></h3>
<ul>
<li><strong>TeX StackExchange</strong>: ë¬¸ì œ í•´ê²°ì„ ìœ„í•œ Q&amp;A</li>
<li><strong>LaTeX Project</strong>: ê³µì‹ ë‰´ìŠ¤ ë° ë¦´ë¦¬ìŠ¤</li>
<li><strong>CTAN</strong>: 6000ê°œ ì´ìƒì˜ íŒ¨í‚¤ì§€ íƒìƒ‰</li>
<li><strong>Overleaf íŠœí† ë¦¬ì–¼</strong>: ë¹„ë””ì˜¤ ê°€ì´ë“œ ë° ì›¨ë¹„ë‚˜</li>
<li><strong>ë¡œì»¬ TeX ì‚¬ìš©ì ê·¸ë£¹</strong>: TUG, UK-TUG ë“±</li>
</ul>
<h3 id="_22">ì—°ìŠµ í”„ë¡œì íŠ¸<a class="header-link" href="#_22" title="Permanent link">&para;</a></h3>
<ul>
<li>LaTeXë¡œ ì´ë ¥ì„œ ì‘ì„±</li>
<li>ë‹¤ê°€ì˜¤ëŠ” ë°œí‘œë¥¼ ìœ„í•œ í”„ë ˆì  í…Œì´ì…˜ ìƒì„±</li>
<li>ë…¸íŠ¸ë‚˜ ë¬¸ì„œ ì¡°íŒ</li>
<li>ì˜¤í”ˆ ì†ŒìŠ¤ LaTeX íŒ¨í‚¤ì§€ì— ê¸°ì—¬</li>
</ul>
<hr />
<h2 id="_23">ìš”ì•½<a class="header-link" href="#_23" title="Permanent link">&para;</a></h2>
<p>ì´ ë ˆìŠ¨ì€ ì„¸ ê°€ì§€ ì™„ì „í•œ ì‹¤ì œ LaTeX í”„ë¡œì íŠ¸ë¥¼ ì œì‹œí–ˆìŠµë‹ˆë‹¤:</p>
<ol>
<li><strong>í•™ìˆ  ë…¼ë¬¸(Academic Paper)</strong>: ê·¸ë¦¼, í‘œ, ìˆ˜í•™, ì°¸ê³ ë¬¸í—Œì´ ìˆëŠ” IEEE ìŠ¤íƒ€ì¼ í•™íšŒ ë…¼ë¬¸</li>
<li><strong>Beamer í”„ë ˆì  í…Œì´ì…˜(Beamer Presentation)</strong>: ì˜¤ë²„ë ˆì´, TikZ, ì‚¬ìš©ì ì •ì˜ í…Œë§ˆê°€ ìˆëŠ” 15ìŠ¬ë¼ì´ë“œ ë°œí‘œ</li>
<li><strong>ê³¼í•™ í¬ìŠ¤í„°(Scientific Poster)</strong>: ë‹¤ì¤‘ ì—´ ë ˆì´ì•„ì›ƒ, í”Œë¡¯, QR ì½”ë“œê°€ ìˆëŠ” A0 í¬ìŠ¤í„°</li>
</ol>
<p><strong>ì‹œì—°ëœ í•µì‹¬ ê¸°ìˆ </strong>:
- ë¬¸ì„œ í´ë˜ìŠ¤ ì„ íƒ ë° êµ¬ì„±
- íŒ¨í‚¤ì§€ í†µí•© (ê·¸ë˜í”½, ìˆ˜í•™, ì°¸ê³ ë¬¸í—Œ, TikZ)
- ì‚¬ìš©ì ì •ì˜ ëª…ë ¹ ë° í™˜ê²½
- ìƒí˜¸ ì°¸ì¡° ë° ì¸ìš©
- ì‹œê°ì  ë””ìì¸ (ìƒ‰ìƒ, ë ˆì´ì•„ì›ƒ, í…Œë§ˆ)
- ì»´íŒŒì¼ ì›Œí¬í”Œë¡œìš°</p>
<p><strong>ì¶•í•˜í•©ë‹ˆë‹¤!</strong> LaTeX ê³¼ì •ì˜ 16ê°œ ë ˆìŠ¨ì„ ëª¨ë‘ ì™„ë£Œí–ˆìŠµë‹ˆë‹¤. ì´ì œ í•™ìˆ  ë° ì „ë¬¸ ë§¥ë½ì—ì„œ ì „ë¬¸ ë¬¸ì„œ, í”„ë ˆì  í…Œì´ì…˜, í¬ìŠ¤í„°ë¥¼ ìƒì„±í•˜ëŠ” ê¸°ìˆ ì„ ê°–ì¶”ì—ˆìŠµë‹ˆë‹¤.</p>
<hr />
<p><strong>íƒìƒ‰</strong></p>
<ul>
<li>ì´ì „: <a href="15_Automation_and_Build.md">15_Automation_and_Build.md</a></li>
<li>ê³¼ì • ì¢…ë£Œ</li>
</ul>
    </div>

    
<div class="lesson-toolbar lesson-toolbar--bottom">
    <div class="toolbar-nav toolbar-nav--prev">
        
        <a href="/study/ko/LaTeX/15_Automation_and_Build.html" class="nav-prev">
            <span class="nav-label">Previous</span>
            <span class="nav-title">ë¹Œë“œ ì‹œìŠ¤í…œ ë° ìë™í™”(Build Systems & Automation)</span>
        </a>
        
    </div>
    <div class="toolbar-actions">
        <button class="btn btn-copy-link" data-action="copy-link" title="Copy link">
            <span class="icon">ğŸ”—</span>
            <span class="text">Copy link</span>
        </button>
        <a href="/study/ko/LaTeX/" class="btn btn-topic-link" title="Back to topic list">
            <span class="icon">ğŸ“‹</span>
            <span class="text">Topic list</span>
        </a>
    </div>
    <div class="toolbar-nav toolbar-nav--next">
        
    </div>
</div>


    <div class="toolbar-keyboard-hint">
        <kbd>&larr;</kbd> <kbd>&rarr;</kbd> to navigate between lessons
    </div>
</article>

<!-- Scroll to top floating button -->
<button class="scroll-to-top" id="scroll-to-top" title="Scroll to top" aria-label="Scroll to top">â†‘</button>

            </div>
        </main>
    </div>

    <script src="/study/static/js/app.js"></script>
    <script>
        function switchLanguage(newLang) {
            const path = window.location.pathname;
            const base = '/study';
            const currentLang = 'ko';
            const newPath = path.replace(base + '/' + currentLang + '/', base + '/' + newLang + '/');
            window.location.href = newPath + window.location.search;
        }
    </script>
    
<script>
    document.addEventListener('DOMContentLoaded', function() {
        // Copy link - independent feedback per button
        document.querySelectorAll('[data-action="copy-link"]').forEach(function(btn) {
            btn.addEventListener('click', function() {
                navigator.clipboard.writeText(window.location.href);
                var textEl = this.querySelector('.text');
                var originalText = textEl.textContent;
                textEl.textContent = 'Copied!';
                setTimeout(function() { textEl.textContent = originalText; }, 2000);
            });
        });

        // Add copy buttons to code blocks
        document.querySelectorAll('.lesson-content pre').forEach(function(pre) {
            var copyBtn = document.createElement('button');
            copyBtn.className = 'code-copy-btn';
            copyBtn.textContent = 'Copy';
            copyBtn.addEventListener('click', function() {
                var code = pre.querySelector('code');
                var text = code ? code.textContent : pre.textContent;
                navigator.clipboard.writeText(text);
                copyBtn.textContent = 'Copied!';
                setTimeout(function() { copyBtn.textContent = 'Copy'; }, 2000);
            });
            pre.style.position = 'relative';
            pre.appendChild(copyBtn);
        });

        // Scroll to top button
        var scrollBtn = document.getElementById('scroll-to-top');
        window.addEventListener('scroll', function() {
            scrollBtn.classList.toggle('visible', window.scrollY > 300);
        });
        scrollBtn.addEventListener('click', function() {
            window.scrollTo({ top: 0, behavior: 'smooth' });
        });

        // Keyboard shortcuts: left/right arrows for lesson navigation
        document.addEventListener('keydown', function(e) {
            var tag = document.activeElement.tagName;
            if (tag === 'INPUT' || tag === 'TEXTAREA' || tag === 'SELECT') return;

            if (e.key === 'ArrowLeft') {
                var prevLink = document.querySelector('.nav-prev');
                if (prevLink) prevLink.click();
            } else if (e.key === 'ArrowRight') {
                var nextLink = document.querySelector('.nav-next');
                if (nextLink) nextLink.click();
            }
        });
    });
</script>

</body>
</html>
{% endraw %}